{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f933f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset (Bangla ( Bengali ) sentiment analysis classification benchmark dataset corpus) : https://data.mendeley.com/datasets/p6zc7krs37/4\n",
    "\"\"\"\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import *\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc3c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 10\n",
    "w_decay = 0.05\n",
    "dropout_rate = 0.2\n",
    "epochs_to_run = 500\n",
    "sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8966bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 positive sentences\n",
      "3307 negative sentences\n",
      "3307 positive sentences\n",
      "3307 negative sentences\n"
     ]
    }
   ],
   "source": [
    "# Loading Bangla ( Bengali ) sentiment analysis classification benchmark dataset\n",
    "positive_sentences = []\n",
    "f = open('../datasets/all_positive_8500.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    positive_sentences.append(line.strip())\n",
    "\n",
    "negative_sentences = []\n",
    "f = open('../datasets/all_negative_3307.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    negative_sentences.append(line.strip())\n",
    "    \n",
    "print(len(positive_sentences), 'positive sentences')\n",
    "print(len(negative_sentences), 'negative sentences')\n",
    "\n",
    "import random\n",
    "random.shuffle(positive_sentences)\n",
    "\n",
    "for i in range(len(positive_sentences)-len(negative_sentences)):\n",
    "    positive_sentences.pop(0)\n",
    "\n",
    "print(len(positive_sentences), 'positive sentences')\n",
    "print(len(negative_sentences), 'negative sentences')\n",
    "\n",
    "\n",
    "y_pos = [1 for i in range(len(positive_sentences))]\n",
    "y_neg = [0 for i in range(len(negative_sentences))]\n",
    "\n",
    "X = positive_sentences + negative_sentences\n",
    "y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c24841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141148\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4113e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(X)\n",
    "y = np.array(y)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "X_train_val = X_train\n",
    "y_train_val = y_train\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_val, test_size=0.2, random_state=1, stratify=y_train_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747457da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856b4b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 100)          14114800  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 200, 10), (None,  4440      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 14,119,251\n",
      "Trainable params: 4,451\n",
      "Non-trainable params: 14,114,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 3.8011 - accuracy: 0.4688\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.49386, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 3.8011 - accuracy: 0.4688 - val_loss: 3.5427 - val_accuracy: 0.4939\n",
      "Epoch 2/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 3.3360 - accuracy: 0.4884\n",
      "Epoch 00002: val_accuracy improved from 0.49386 to 0.50991, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 3.3301 - accuracy: 0.4896 - val_loss: 3.1049 - val_accuracy: 0.5099\n",
      "Epoch 3/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 2.9232 - accuracy: 0.5135\n",
      "Epoch 00003: val_accuracy improved from 0.50991 to 0.54202, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 2.9229 - accuracy: 0.5130 - val_loss: 2.7283 - val_accuracy: 0.5420\n",
      "Epoch 4/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 2.5788 - accuracy: 0.5168\n",
      "Epoch 00004: val_accuracy improved from 0.54202 to 0.56752, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 2.5757 - accuracy: 0.5191 - val_loss: 2.4031 - val_accuracy: 0.5675\n",
      "Epoch 5/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 2.2748 - accuracy: 0.5584\n",
      "Epoch 00005: val_accuracy improved from 0.56752 to 0.59396, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 2.2720 - accuracy: 0.5572 - val_loss: 2.1236 - val_accuracy: 0.5940\n",
      "Epoch 6/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.0149 - accuracy: 0.5747\n",
      "Epoch 00006: val_accuracy improved from 0.59396 to 0.60434, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 2.0149 - accuracy: 0.5747 - val_loss: 1.8845 - val_accuracy: 0.6043\n",
      "Epoch 7/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.7938 - accuracy: 0.5874\n",
      "Epoch 00007: val_accuracy improved from 0.60434 to 0.62606, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.7935 - accuracy: 0.5874 - val_loss: 1.6797 - val_accuracy: 0.6261\n",
      "Epoch 8/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 1.6044 - accuracy: 0.6138\n",
      "Epoch 00008: val_accuracy improved from 0.62606 to 0.64400, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.6017 - accuracy: 0.6127 - val_loss: 1.5054 - val_accuracy: 0.6440\n",
      "Epoch 9/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.4436 - accuracy: 0.6212\n",
      "Epoch 00009: val_accuracy improved from 0.64400 to 0.66950, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.4421 - accuracy: 0.6212 - val_loss: 1.3550 - val_accuracy: 0.6695\n",
      "Epoch 10/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.3010 - accuracy: 0.6536\n",
      "Epoch 00010: val_accuracy improved from 0.66950 to 0.68933, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3010 - accuracy: 0.6536 - val_loss: 1.2262 - val_accuracy: 0.6893\n",
      "Epoch 11/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.1833 - accuracy: 0.6692\n",
      "Epoch 00011: val_accuracy improved from 0.68933 to 0.71199, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1826 - accuracy: 0.6690 - val_loss: 1.1153 - val_accuracy: 0.7120\n",
      "Epoch 12/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0804 - accuracy: 0.6918\n",
      "Epoch 00012: val_accuracy improved from 0.71199 to 0.72238, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0794 - accuracy: 0.6926 - val_loss: 1.0190 - val_accuracy: 0.7224\n",
      "Epoch 13/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9944 - accuracy: 0.7061\n",
      "Epoch 00013: val_accuracy improved from 0.72238 to 0.73843, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.9936 - accuracy: 0.7070 - val_loss: 0.9371 - val_accuracy: 0.7384\n",
      "Epoch 14/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.9211 - accuracy: 0.7261\n",
      "Epoch 00014: val_accuracy improved from 0.73843 to 0.75165, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9202 - accuracy: 0.7259 - val_loss: 0.8724 - val_accuracy: 0.7517\n",
      "Epoch 15/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8601 - accuracy: 0.7311\n",
      "Epoch 00015: val_accuracy improved from 0.75165 to 0.76393, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.8600 - accuracy: 0.7309 - val_loss: 0.8085 - val_accuracy: 0.7639\n",
      "Epoch 16/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8063 - accuracy: 0.7365\n",
      "Epoch 00016: val_accuracy improved from 0.76393 to 0.77526, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.8063 - accuracy: 0.7365 - val_loss: 0.7600 - val_accuracy: 0.7753\n",
      "Epoch 17/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7671 - accuracy: 0.7498\n",
      "Epoch 00017: val_accuracy improved from 0.77526 to 0.78659, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7672 - accuracy: 0.7493 - val_loss: 0.7193 - val_accuracy: 0.7866\n",
      "Epoch 18/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7337 - accuracy: 0.7564\n",
      "Epoch 00018: val_accuracy improved from 0.78659 to 0.79603, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7337 - accuracy: 0.7564 - val_loss: 0.6855 - val_accuracy: 0.7960\n",
      "Epoch 19/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7016 - accuracy: 0.7669\n",
      "Epoch 00019: val_accuracy improved from 0.79603 to 0.79981, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7017 - accuracy: 0.7672 - val_loss: 0.6582 - val_accuracy: 0.7998\n",
      "Epoch 20/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.7679\n",
      "Epoch 00020: val_accuracy improved from 0.79981 to 0.80453, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6765 - accuracy: 0.7687 - val_loss: 0.6338 - val_accuracy: 0.8045\n",
      "Epoch 21/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6569 - accuracy: 0.7637\n",
      "Epoch 00021: val_accuracy did not improve from 0.80453\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.6569 - accuracy: 0.7637 - val_loss: 0.6139 - val_accuracy: 0.8036\n",
      "Epoch 22/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6384 - accuracy: 0.7639\n",
      "Epoch 00022: val_accuracy improved from 0.80453 to 0.81114, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6376 - accuracy: 0.7656 - val_loss: 0.5953 - val_accuracy: 0.8111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6212 - accuracy: 0.7753\n",
      "Epoch 00023: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.6212 - accuracy: 0.7753 - val_loss: 0.5805 - val_accuracy: 0.8111\n",
      "Epoch 24/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6106 - accuracy: 0.7772\n",
      "Epoch 00024: val_accuracy improved from 0.81114 to 0.81964, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6106 - accuracy: 0.7772 - val_loss: 0.5664 - val_accuracy: 0.8196\n",
      "Epoch 25/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7865\n",
      "Epoch 00025: val_accuracy improved from 0.81964 to 0.82342, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5926 - accuracy: 0.7864 - val_loss: 0.5548 - val_accuracy: 0.8234\n",
      "Epoch 26/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5931 - accuracy: 0.7847\n",
      "Epoch 00026: val_accuracy did not improve from 0.82342\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5931 - accuracy: 0.7847 - val_loss: 0.5509 - val_accuracy: 0.8149\n",
      "Epoch 27/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5777 - accuracy: 0.7862\n",
      "Epoch 00027: val_accuracy did not improve from 0.82342\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5777 - accuracy: 0.7862 - val_loss: 0.5348 - val_accuracy: 0.8234\n",
      "Epoch 28/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5726 - accuracy: 0.7937\n",
      "Epoch 00028: val_accuracy improved from 0.82342 to 0.82436, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5732 - accuracy: 0.7923 - val_loss: 0.5285 - val_accuracy: 0.8244\n",
      "Epoch 29/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.7864\n",
      "Epoch 00029: val_accuracy improved from 0.82436 to 0.82720, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5663 - accuracy: 0.7864 - val_loss: 0.5225 - val_accuracy: 0.8272\n",
      "Epoch 30/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5584 - accuracy: 0.7936\n",
      "Epoch 00030: val_accuracy improved from 0.82720 to 0.83192, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5581 - accuracy: 0.7940 - val_loss: 0.5154 - val_accuracy: 0.8319\n",
      "Epoch 31/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.7909\n",
      "Epoch 00031: val_accuracy improved from 0.83192 to 0.83286, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5554 - accuracy: 0.7909 - val_loss: 0.5094 - val_accuracy: 0.8329\n",
      "Epoch 32/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5453 - accuracy: 0.7968\n",
      "Epoch 00032: val_accuracy improved from 0.83286 to 0.83758, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5453 - accuracy: 0.7968 - val_loss: 0.5040 - val_accuracy: 0.8376\n",
      "Epoch 33/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5413 - accuracy: 0.7954\n",
      "Epoch 00033: val_accuracy improved from 0.83758 to 0.84042, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5413 - accuracy: 0.7954 - val_loss: 0.4975 - val_accuracy: 0.8404\n",
      "Epoch 34/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5447 - accuracy: 0.7936\n",
      "Epoch 00034: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5445 - accuracy: 0.7937 - val_loss: 0.5007 - val_accuracy: 0.8319\n",
      "Epoch 35/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5373 - accuracy: 0.7973\n",
      "Epoch 00035: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5373 - accuracy: 0.7973 - val_loss: 0.4992 - val_accuracy: 0.8310\n",
      "Epoch 36/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5298 - accuracy: 0.8097\n",
      "Epoch 00036: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5301 - accuracy: 0.8095 - val_loss: 0.4874 - val_accuracy: 0.8404\n",
      "Epoch 37/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5247 - accuracy: 0.8042\n",
      "Epoch 00037: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5246 - accuracy: 0.8043 - val_loss: 0.4926 - val_accuracy: 0.8329\n",
      "Epoch 38/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.8098\n",
      "Epoch 00038: val_accuracy improved from 0.84042 to 0.84325, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5217 - accuracy: 0.8098 - val_loss: 0.4782 - val_accuracy: 0.8432\n",
      "Epoch 39/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 0.8112\n",
      "Epoch 00039: val_accuracy improved from 0.84325 to 0.84419, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5159 - accuracy: 0.8112 - val_loss: 0.4759 - val_accuracy: 0.8442\n",
      "Epoch 40/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5145 - accuracy: 0.8118\n",
      "Epoch 00040: val_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5138 - accuracy: 0.8124 - val_loss: 0.4753 - val_accuracy: 0.8395\n",
      "Epoch 41/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.8103\n",
      "Epoch 00041: val_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5176 - accuracy: 0.8103 - val_loss: 0.4707 - val_accuracy: 0.8442\n",
      "Epoch 42/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.8138\n",
      "Epoch 00042: val_accuracy improved from 0.84419 to 0.84986, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5047 - accuracy: 0.8138 - val_loss: 0.4675 - val_accuracy: 0.8499\n",
      "Epoch 43/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5104 - accuracy: 0.8110\n",
      "Epoch 00043: val_accuracy improved from 0.84986 to 0.85080, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5104 - accuracy: 0.8110 - val_loss: 0.4646 - val_accuracy: 0.8508\n",
      "Epoch 44/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5065 - accuracy: 0.8154\n",
      "Epoch 00044: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5053 - accuracy: 0.8164 - val_loss: 0.4645 - val_accuracy: 0.8442\n",
      "Epoch 45/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4995 - accuracy: 0.8178\n",
      "Epoch 00045: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4995 - accuracy: 0.8178 - val_loss: 0.4613 - val_accuracy: 0.8423\n",
      "Epoch 46/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.8143\n",
      "Epoch 00046: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5038 - accuracy: 0.8143 - val_loss: 0.4588 - val_accuracy: 0.8480\n",
      "Epoch 47/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.8233\n",
      "Epoch 00047: val_accuracy improved from 0.85080 to 0.85175, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4976 - accuracy: 0.8233 - val_loss: 0.4569 - val_accuracy: 0.8517\n",
      "Epoch 48/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4940 - accuracy: 0.8229\n",
      "Epoch 00048: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4938 - accuracy: 0.8230 - val_loss: 0.4694 - val_accuracy: 0.8442\n",
      "Epoch 49/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4919 - accuracy: 0.8225\n",
      "Epoch 00049: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4919 - accuracy: 0.8225 - val_loss: 0.4558 - val_accuracy: 0.8423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.8273\n",
      "Epoch 00050: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4870 - accuracy: 0.8273 - val_loss: 0.4515 - val_accuracy: 0.8432\n",
      "Epoch 51/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4874 - accuracy: 0.8284\n",
      "Epoch 00051: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4870 - accuracy: 0.8287 - val_loss: 0.4521 - val_accuracy: 0.8423\n",
      "Epoch 52/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4881 - accuracy: 0.8204\n",
      "Epoch 00052: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4881 - accuracy: 0.8204 - val_loss: 0.4684 - val_accuracy: 0.8376\n",
      "Epoch 53/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4811 - accuracy: 0.8329\n",
      "Epoch 00053: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4822 - accuracy: 0.8329 - val_loss: 0.4473 - val_accuracy: 0.8470\n",
      "Epoch 54/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4775 - accuracy: 0.8352\n",
      "Epoch 00054: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4772 - accuracy: 0.8353 - val_loss: 0.4619 - val_accuracy: 0.8423\n",
      "Epoch 55/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4787 - accuracy: 0.8253\n",
      "Epoch 00055: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4788 - accuracy: 0.8249 - val_loss: 0.4460 - val_accuracy: 0.8489\n",
      "Epoch 56/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.8305\n",
      "Epoch 00056: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4777 - accuracy: 0.8306 - val_loss: 0.4469 - val_accuracy: 0.8499\n",
      "Epoch 57/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.8304\n",
      "Epoch 00057: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4752 - accuracy: 0.8308 - val_loss: 0.4397 - val_accuracy: 0.8499\n",
      "Epoch 58/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.8355\n",
      "Epoch 00058: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4704 - accuracy: 0.8355 - val_loss: 0.4401 - val_accuracy: 0.8470\n",
      "Epoch 59/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4763 - accuracy: 0.8323\n",
      "Epoch 00059: val_accuracy improved from 0.85175 to 0.85458, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4758 - accuracy: 0.8325 - val_loss: 0.4371 - val_accuracy: 0.8546\n",
      "Epoch 60/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4721 - accuracy: 0.8349\n",
      "Epoch 00060: val_accuracy did not improve from 0.85458\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4735 - accuracy: 0.8334 - val_loss: 0.4355 - val_accuracy: 0.8536\n",
      "Epoch 61/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4671 - accuracy: 0.8407\n",
      "Epoch 00061: val_accuracy improved from 0.85458 to 0.85552, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4671 - accuracy: 0.8407 - val_loss: 0.4341 - val_accuracy: 0.8555\n",
      "Epoch 62/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.8317\n",
      "Epoch 00062: val_accuracy did not improve from 0.85552\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4687 - accuracy: 0.8315 - val_loss: 0.4320 - val_accuracy: 0.8508\n",
      "Epoch 63/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.8430\n",
      "Epoch 00063: val_accuracy did not improve from 0.85552\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4579 - accuracy: 0.8431 - val_loss: 0.4297 - val_accuracy: 0.8451\n",
      "Epoch 64/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4602 - accuracy: 0.8411\n",
      "Epoch 00064: val_accuracy did not improve from 0.85552\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4598 - accuracy: 0.8414 - val_loss: 0.4289 - val_accuracy: 0.8470\n",
      "Epoch 65/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4631 - accuracy: 0.8388\n",
      "Epoch 00065: val_accuracy improved from 0.85552 to 0.85647, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4633 - accuracy: 0.8388 - val_loss: 0.4286 - val_accuracy: 0.8565\n",
      "Epoch 66/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4605 - accuracy: 0.8411\n",
      "Epoch 00066: val_accuracy did not improve from 0.85647\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4602 - accuracy: 0.8414 - val_loss: 0.4325 - val_accuracy: 0.8489\n",
      "Epoch 67/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4604 - accuracy: 0.8374\n",
      "Epoch 00067: val_accuracy improved from 0.85647 to 0.85836, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4605 - accuracy: 0.8372 - val_loss: 0.4280 - val_accuracy: 0.8584\n",
      "Epoch 68/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4623 - accuracy: 0.8430\n",
      "Epoch 00068: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4626 - accuracy: 0.8429 - val_loss: 0.4255 - val_accuracy: 0.8527\n",
      "Epoch 69/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4552 - accuracy: 0.8464\n",
      "Epoch 00069: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4552 - accuracy: 0.8464 - val_loss: 0.4243 - val_accuracy: 0.8584\n",
      "Epoch 70/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4561 - accuracy: 0.8440\n",
      "Epoch 00070: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4560 - accuracy: 0.8440 - val_loss: 0.4248 - val_accuracy: 0.8555\n",
      "Epoch 71/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4490 - accuracy: 0.8497\n",
      "Epoch 00071: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4500 - accuracy: 0.8490 - val_loss: 0.4227 - val_accuracy: 0.8546\n",
      "Epoch 72/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.8452\n",
      "Epoch 00072: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4516 - accuracy: 0.8452 - val_loss: 0.4238 - val_accuracy: 0.8527\n",
      "Epoch 73/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4438 - accuracy: 0.8507\n",
      "Epoch 00073: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4438 - accuracy: 0.8507 - val_loss: 0.4197 - val_accuracy: 0.8574\n",
      "Epoch 74/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.8535\n",
      "Epoch 00074: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4424 - accuracy: 0.8530 - val_loss: 0.4267 - val_accuracy: 0.8536\n",
      "Epoch 75/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.8485\n",
      "Epoch 00075: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4447 - accuracy: 0.8485 - val_loss: 0.4191 - val_accuracy: 0.8527\n",
      "Epoch 76/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4436 - accuracy: 0.8475\n",
      "Epoch 00076: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4434 - accuracy: 0.8478 - val_loss: 0.4229 - val_accuracy: 0.8517\n",
      "Epoch 77/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.8525\n",
      "Epoch 00077: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4417 - accuracy: 0.8526 - val_loss: 0.4167 - val_accuracy: 0.8574\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.8466\n",
      "Epoch 00078: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4412 - accuracy: 0.8466 - val_loss: 0.4148 - val_accuracy: 0.8565\n",
      "Epoch 79/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4432 - accuracy: 0.8519\n",
      "Epoch 00079: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4426 - accuracy: 0.8521 - val_loss: 0.4154 - val_accuracy: 0.8584\n",
      "Epoch 80/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.8543\n",
      "Epoch 00080: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4337 - accuracy: 0.8544 - val_loss: 0.4194 - val_accuracy: 0.8555\n",
      "Epoch 81/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4344 - accuracy: 0.8575\n",
      "Epoch 00081: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4345 - accuracy: 0.8575 - val_loss: 0.4144 - val_accuracy: 0.8574\n",
      "Epoch 82/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4373 - accuracy: 0.8488\n",
      "Epoch 00082: val_accuracy improved from 0.85836 to 0.86686, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4370 - accuracy: 0.8485 - val_loss: 0.4183 - val_accuracy: 0.8669\n",
      "Epoch 83/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.8552\n",
      "Epoch 00083: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4394 - accuracy: 0.8552 - val_loss: 0.4128 - val_accuracy: 0.8631\n",
      "Epoch 84/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.8528\n",
      "Epoch 00084: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4397 - accuracy: 0.8528 - val_loss: 0.4108 - val_accuracy: 0.8602\n",
      "Epoch 85/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.8455\n",
      "Epoch 00085: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4407 - accuracy: 0.8455 - val_loss: 0.4184 - val_accuracy: 0.8546\n",
      "Epoch 86/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8642\n",
      "Epoch 00086: val_accuracy improved from 0.86686 to 0.86780, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4284 - accuracy: 0.8632 - val_loss: 0.4093 - val_accuracy: 0.8678\n",
      "Epoch 87/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4366 - accuracy: 0.8530\n",
      "Epoch 00087: val_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4363 - accuracy: 0.8530 - val_loss: 0.4091 - val_accuracy: 0.8640\n",
      "Epoch 88/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4238 - accuracy: 0.8613\n",
      "Epoch 00088: val_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4238 - accuracy: 0.8613 - val_loss: 0.4094 - val_accuracy: 0.8593\n",
      "Epoch 89/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.8556\n",
      "Epoch 00089: val_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4301 - accuracy: 0.8556 - val_loss: 0.4079 - val_accuracy: 0.8659\n",
      "Epoch 90/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8575\n",
      "Epoch 00090: val_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4282 - accuracy: 0.8575 - val_loss: 0.4079 - val_accuracy: 0.8602\n",
      "Epoch 91/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4288 - accuracy: 0.8570\n",
      "Epoch 00091: val_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4288 - accuracy: 0.8570 - val_loss: 0.4077 - val_accuracy: 0.8631\n",
      "Epoch 92/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4239 - accuracy: 0.8603\n",
      "Epoch 00092: val_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4244 - accuracy: 0.8599 - val_loss: 0.4099 - val_accuracy: 0.8584\n",
      "Epoch 93/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4244 - accuracy: 0.8587\n",
      "Epoch 00093: val_accuracy improved from 0.86780 to 0.86969, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4244 - accuracy: 0.8587 - val_loss: 0.4051 - val_accuracy: 0.8697\n",
      "Epoch 94/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4226 - accuracy: 0.8601\n",
      "Epoch 00094: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4226 - accuracy: 0.8601 - val_loss: 0.4053 - val_accuracy: 0.8602\n",
      "Epoch 95/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8561\n",
      "Epoch 00095: val_accuracy improved from 0.86969 to 0.87630, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4266 - accuracy: 0.8561 - val_loss: 0.4032 - val_accuracy: 0.8763\n",
      "Epoch 96/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4233 - accuracy: 0.8648\n",
      "Epoch 00096: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4244 - accuracy: 0.8641 - val_loss: 0.4035 - val_accuracy: 0.8640\n",
      "Epoch 97/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4236 - accuracy: 0.8603\n",
      "Epoch 00097: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4234 - accuracy: 0.8606 - val_loss: 0.4058 - val_accuracy: 0.8602\n",
      "Epoch 98/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4200 - accuracy: 0.8639\n",
      "Epoch 00098: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4200 - accuracy: 0.8639 - val_loss: 0.4013 - val_accuracy: 0.8678\n",
      "Epoch 99/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8660\n",
      "Epoch 00099: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4170 - accuracy: 0.8660 - val_loss: 0.4022 - val_accuracy: 0.8687\n",
      "Epoch 100/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8583\n",
      "Epoch 00100: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4228 - accuracy: 0.8589 - val_loss: 0.4098 - val_accuracy: 0.8555\n",
      "Epoch 101/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4190 - accuracy: 0.8608\n",
      "Epoch 00101: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4191 - accuracy: 0.8606 - val_loss: 0.4048 - val_accuracy: 0.8659\n",
      "Epoch 102/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4197 - accuracy: 0.8575\n",
      "Epoch 00102: val_accuracy improved from 0.87630 to 0.88102, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4197 - accuracy: 0.8575 - val_loss: 0.3995 - val_accuracy: 0.8810\n",
      "Epoch 103/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4127 - accuracy: 0.8665\n",
      "Epoch 00103: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4127 - accuracy: 0.8667 - val_loss: 0.3998 - val_accuracy: 0.8650\n",
      "Epoch 104/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.8608\n",
      "Epoch 00104: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4157 - accuracy: 0.8608 - val_loss: 0.4006 - val_accuracy: 0.8659\n",
      "Epoch 105/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4091 - accuracy: 0.8643\n",
      "Epoch 00105: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4099 - accuracy: 0.8644 - val_loss: 0.4025 - val_accuracy: 0.8602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4143 - accuracy: 0.8629\n",
      "Epoch 00106: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4143 - accuracy: 0.8629 - val_loss: 0.3973 - val_accuracy: 0.8772\n",
      "Epoch 107/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4128 - accuracy: 0.8643\n",
      "Epoch 00107: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4125 - accuracy: 0.8646 - val_loss: 0.4132 - val_accuracy: 0.8489\n",
      "Epoch 108/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8632\n",
      "Epoch 00108: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4133 - accuracy: 0.8634 - val_loss: 0.4073 - val_accuracy: 0.8546\n",
      "Epoch 109/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8698\n",
      "Epoch 00109: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4088 - accuracy: 0.8686 - val_loss: 0.3947 - val_accuracy: 0.8791\n",
      "Epoch 110/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8672\n",
      "Epoch 00110: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4113 - accuracy: 0.8672 - val_loss: 0.3943 - val_accuracy: 0.8687\n",
      "Epoch 111/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4055 - accuracy: 0.8662\n",
      "Epoch 00111: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4056 - accuracy: 0.8658 - val_loss: 0.3939 - val_accuracy: 0.8697\n",
      "Epoch 112/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4146 - accuracy: 0.8613\n",
      "Epoch 00112: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4146 - accuracy: 0.8613 - val_loss: 0.3996 - val_accuracy: 0.8593\n",
      "Epoch 113/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4069 - accuracy: 0.8684\n",
      "Epoch 00113: val_accuracy improved from 0.88102 to 0.88291, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4072 - accuracy: 0.8681 - val_loss: 0.3921 - val_accuracy: 0.8829\n",
      "Epoch 114/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4033 - accuracy: 0.8743\n",
      "Epoch 00114: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4034 - accuracy: 0.8738 - val_loss: 0.4043 - val_accuracy: 0.8555\n",
      "Epoch 115/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.8689\n",
      "Epoch 00115: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4084 - accuracy: 0.8689 - val_loss: 0.3927 - val_accuracy: 0.8735\n",
      "Epoch 116/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4037 - accuracy: 0.8651\n",
      "Epoch 00116: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4034 - accuracy: 0.8653 - val_loss: 0.3893 - val_accuracy: 0.8782\n",
      "Epoch 117/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4102 - accuracy: 0.8636\n",
      "Epoch 00117: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4100 - accuracy: 0.8639 - val_loss: 0.4033 - val_accuracy: 0.8612\n",
      "Epoch 118/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4024 - accuracy: 0.8707\n",
      "Epoch 00118: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4024 - accuracy: 0.8707 - val_loss: 0.3894 - val_accuracy: 0.8744\n",
      "Epoch 119/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4019 - accuracy: 0.8671\n",
      "Epoch 00119: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4022 - accuracy: 0.8660 - val_loss: 0.3972 - val_accuracy: 0.8631\n",
      "Epoch 120/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4022 - accuracy: 0.8705\n",
      "Epoch 00120: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4022 - accuracy: 0.8705 - val_loss: 0.4054 - val_accuracy: 0.8678\n",
      "Epoch 121/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8681\n",
      "Epoch 00121: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4032 - accuracy: 0.8679 - val_loss: 0.3899 - val_accuracy: 0.8772\n",
      "Epoch 122/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.8644\n",
      "Epoch 00122: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4061 - accuracy: 0.8644 - val_loss: 0.3961 - val_accuracy: 0.8631\n",
      "Epoch 123/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.8672\n",
      "Epoch 00123: val_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4015 - accuracy: 0.8672 - val_loss: 0.3932 - val_accuracy: 0.8772\n",
      "Epoch 124/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4005 - accuracy: 0.8722\n",
      "Epoch 00124: val_accuracy improved from 0.88291 to 0.88480, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4003 - accuracy: 0.8724 - val_loss: 0.3863 - val_accuracy: 0.8848\n",
      "Epoch 125/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.8745\n",
      "Epoch 00125: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3952 - accuracy: 0.8745 - val_loss: 0.4098 - val_accuracy: 0.8593\n",
      "Epoch 126/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8705\n",
      "Epoch 00126: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3987 - accuracy: 0.8703 - val_loss: 0.3865 - val_accuracy: 0.8801\n",
      "Epoch 127/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4014 - accuracy: 0.8634\n",
      "Epoch 00127: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4013 - accuracy: 0.8632 - val_loss: 0.3903 - val_accuracy: 0.8697\n",
      "Epoch 128/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3884 - accuracy: 0.8774\n",
      "Epoch 00128: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3879 - accuracy: 0.8776 - val_loss: 0.3916 - val_accuracy: 0.8772\n",
      "Epoch 129/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4039 - accuracy: 0.8670\n",
      "Epoch 00129: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4039 - accuracy: 0.8670 - val_loss: 0.3875 - val_accuracy: 0.8735\n",
      "Epoch 130/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3960 - accuracy: 0.8688\n",
      "Epoch 00130: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3959 - accuracy: 0.8691 - val_loss: 0.3897 - val_accuracy: 0.8678\n",
      "Epoch 131/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3947 - accuracy: 0.8722\n",
      "Epoch 00131: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3950 - accuracy: 0.8719 - val_loss: 0.3921 - val_accuracy: 0.8782\n",
      "Epoch 132/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.8738\n",
      "Epoch 00132: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3951 - accuracy: 0.8738 - val_loss: 0.3848 - val_accuracy: 0.8810\n",
      "Epoch 133/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.8757\n",
      "Epoch 00133: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3941 - accuracy: 0.8757 - val_loss: 0.3822 - val_accuracy: 0.8810\n",
      "Epoch 134/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.3865 - accuracy: 0.8807\n",
      "Epoch 00134: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3862 - accuracy: 0.8809 - val_loss: 0.4009 - val_accuracy: 0.8640\n",
      "Epoch 135/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3914 - accuracy: 0.8707\n",
      "Epoch 00135: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3917 - accuracy: 0.8705 - val_loss: 0.3821 - val_accuracy: 0.8820\n",
      "Epoch 136/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3860 - accuracy: 0.8816\n",
      "Epoch 00136: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3862 - accuracy: 0.8816 - val_loss: 0.3815 - val_accuracy: 0.8820\n",
      "Epoch 137/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3904 - accuracy: 0.8731\n",
      "Epoch 00137: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3909 - accuracy: 0.8729 - val_loss: 0.3993 - val_accuracy: 0.8640\n",
      "Epoch 138/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3919 - accuracy: 0.8745\n",
      "Epoch 00138: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3919 - accuracy: 0.8745 - val_loss: 0.3822 - val_accuracy: 0.8744\n",
      "Epoch 139/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3922 - accuracy: 0.8745\n",
      "Epoch 00139: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3932 - accuracy: 0.8736 - val_loss: 0.3827 - val_accuracy: 0.8744\n",
      "Epoch 140/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3870 - accuracy: 0.8752\n",
      "Epoch 00140: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3870 - accuracy: 0.8752 - val_loss: 0.3820 - val_accuracy: 0.8763\n",
      "Epoch 141/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3886 - accuracy: 0.8776\n",
      "Epoch 00141: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3881 - accuracy: 0.8778 - val_loss: 0.3832 - val_accuracy: 0.8716\n",
      "Epoch 142/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3873 - accuracy: 0.8779\n",
      "Epoch 00142: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3872 - accuracy: 0.8776 - val_loss: 0.3852 - val_accuracy: 0.8782\n",
      "Epoch 143/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.8783\n",
      "Epoch 00143: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3851 - accuracy: 0.8783 - val_loss: 0.3795 - val_accuracy: 0.8801\n",
      "Epoch 144/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3935 - accuracy: 0.8690\n",
      "Epoch 00144: val_accuracy improved from 0.88480 to 0.88669, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3928 - accuracy: 0.8693 - val_loss: 0.3771 - val_accuracy: 0.8867\n",
      "Epoch 145/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.8741\n",
      "Epoch 00145: val_accuracy improved from 0.88669 to 0.89046, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3920 - accuracy: 0.8741 - val_loss: 0.3769 - val_accuracy: 0.8905\n",
      "Epoch 146/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.8767\n",
      "Epoch 00146: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3834 - accuracy: 0.8767 - val_loss: 0.3956 - val_accuracy: 0.8659\n",
      "Epoch 147/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8769\n",
      "Epoch 00147: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3824 - accuracy: 0.8771 - val_loss: 0.3774 - val_accuracy: 0.8829\n",
      "Epoch 148/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3827 - accuracy: 0.8752\n",
      "Epoch 00148: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3827 - accuracy: 0.8752 - val_loss: 0.4078 - val_accuracy: 0.8536\n",
      "Epoch 149/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3826 - accuracy: 0.8748\n",
      "Epoch 00149: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3826 - accuracy: 0.8748 - val_loss: 0.3783 - val_accuracy: 0.8791\n",
      "Epoch 150/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8790\n",
      "Epoch 00150: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3796 - accuracy: 0.8790 - val_loss: 0.3777 - val_accuracy: 0.8876\n",
      "Epoch 151/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8802\n",
      "Epoch 00151: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3828 - accuracy: 0.8802 - val_loss: 0.3772 - val_accuracy: 0.8839\n",
      "Epoch 152/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.8833\n",
      "Epoch 00152: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3817 - accuracy: 0.8833 - val_loss: 0.3851 - val_accuracy: 0.8735\n",
      "Epoch 153/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3743 - accuracy: 0.8845\n",
      "Epoch 00153: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3743 - accuracy: 0.8845 - val_loss: 0.3769 - val_accuracy: 0.8876\n",
      "Epoch 154/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8807\n",
      "Epoch 00154: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3775 - accuracy: 0.8807 - val_loss: 0.3789 - val_accuracy: 0.8744\n",
      "Epoch 155/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3813 - accuracy: 0.8771\n",
      "Epoch 00155: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3813 - accuracy: 0.8771 - val_loss: 0.3731 - val_accuracy: 0.8876\n",
      "Epoch 156/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8810\n",
      "Epoch 00156: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3829 - accuracy: 0.8804 - val_loss: 0.3759 - val_accuracy: 0.8839\n",
      "Epoch 157/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.8825\n",
      "Epoch 00157: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3803 - accuracy: 0.8816 - val_loss: 0.3829 - val_accuracy: 0.8744\n",
      "Epoch 158/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3799 - accuracy: 0.8800\n",
      "Epoch 00158: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3802 - accuracy: 0.8797 - val_loss: 0.3758 - val_accuracy: 0.8876\n",
      "Epoch 159/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3808 - accuracy: 0.8783\n",
      "Epoch 00159: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3807 - accuracy: 0.8783 - val_loss: 0.3725 - val_accuracy: 0.8876\n",
      "Epoch 160/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3733 - accuracy: 0.8800\n",
      "Epoch 00160: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3740 - accuracy: 0.8800 - val_loss: 0.4214 - val_accuracy: 0.8404\n",
      "Epoch 161/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3754 - accuracy: 0.8814\n",
      "Epoch 00161: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3754 - accuracy: 0.8814 - val_loss: 0.4066 - val_accuracy: 0.8555\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.8743\n",
      "Epoch 00162: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3823 - accuracy: 0.8743 - val_loss: 0.3822 - val_accuracy: 0.8763\n",
      "Epoch 163/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3758 - accuracy: 0.8771\n",
      "Epoch 00163: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3758 - accuracy: 0.8771 - val_loss: 0.3764 - val_accuracy: 0.8829\n",
      "Epoch 164/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3749 - accuracy: 0.8800\n",
      "Epoch 00164: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3748 - accuracy: 0.8804 - val_loss: 0.3735 - val_accuracy: 0.8782\n",
      "Epoch 165/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3719 - accuracy: 0.8837\n",
      "Epoch 00165: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3719 - accuracy: 0.8837 - val_loss: 0.3761 - val_accuracy: 0.8848\n",
      "Epoch 166/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3724 - accuracy: 0.8850\n",
      "Epoch 00166: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3737 - accuracy: 0.8845 - val_loss: 0.3772 - val_accuracy: 0.8810\n",
      "Epoch 167/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8793\n",
      "Epoch 00167: val_accuracy improved from 0.89046 to 0.89141, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3763 - accuracy: 0.8795 - val_loss: 0.3701 - val_accuracy: 0.8914\n",
      "Epoch 168/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3781 - accuracy: 0.8795\n",
      "Epoch 00168: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3781 - accuracy: 0.8795 - val_loss: 0.3752 - val_accuracy: 0.8857\n",
      "Epoch 169/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.8802\n",
      "Epoch 00169: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3692 - accuracy: 0.8802 - val_loss: 0.3805 - val_accuracy: 0.8791\n",
      "Epoch 170/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.8810\n",
      "Epoch 00170: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3731 - accuracy: 0.8811 - val_loss: 0.3687 - val_accuracy: 0.8848\n",
      "Epoch 171/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8821\n",
      "Epoch 00171: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3764 - accuracy: 0.8823 - val_loss: 0.3703 - val_accuracy: 0.8867\n",
      "Epoch 172/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3742 - accuracy: 0.8771\n",
      "Epoch 00172: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3741 - accuracy: 0.8774 - val_loss: 0.3722 - val_accuracy: 0.8867\n",
      "Epoch 173/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8879\n",
      "Epoch 00173: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3702 - accuracy: 0.8871 - val_loss: 0.3708 - val_accuracy: 0.8829\n",
      "Epoch 174/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3711 - accuracy: 0.8818\n",
      "Epoch 00174: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3732 - accuracy: 0.8819 - val_loss: 0.3736 - val_accuracy: 0.8839\n",
      "Epoch 175/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3651 - accuracy: 0.8875\n",
      "Epoch 00175: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3665 - accuracy: 0.8868 - val_loss: 0.3684 - val_accuracy: 0.8895\n",
      "Epoch 176/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3735 - accuracy: 0.8802\n",
      "Epoch 00176: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3739 - accuracy: 0.8800 - val_loss: 0.3743 - val_accuracy: 0.8839\n",
      "Epoch 177/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.8840\n",
      "Epoch 00177: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3691 - accuracy: 0.8840 - val_loss: 0.3682 - val_accuracy: 0.8876\n",
      "Epoch 178/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3680 - accuracy: 0.8836\n",
      "Epoch 00178: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3682 - accuracy: 0.8835 - val_loss: 0.3745 - val_accuracy: 0.8744\n",
      "Epoch 179/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3675 - accuracy: 0.8868\n",
      "Epoch 00179: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3675 - accuracy: 0.8868 - val_loss: 0.3925 - val_accuracy: 0.8593\n",
      "Epoch 180/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8859\n",
      "Epoch 00180: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3646 - accuracy: 0.8859 - val_loss: 0.3682 - val_accuracy: 0.8895\n",
      "Epoch 181/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8873\n",
      "Epoch 00181: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3651 - accuracy: 0.8863 - val_loss: 0.3820 - val_accuracy: 0.8678\n",
      "Epoch 182/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8845\n",
      "Epoch 00182: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3646 - accuracy: 0.8845 - val_loss: 0.3683 - val_accuracy: 0.8905\n",
      "Epoch 183/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.8889\n",
      "Epoch 00183: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3603 - accuracy: 0.8889 - val_loss: 0.3692 - val_accuracy: 0.8895\n",
      "Epoch 184/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3614 - accuracy: 0.8915\n",
      "Epoch 00184: val_accuracy improved from 0.89141 to 0.89235, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3614 - accuracy: 0.8915 - val_loss: 0.3699 - val_accuracy: 0.8924\n",
      "Epoch 185/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3716 - accuracy: 0.8857\n",
      "Epoch 00185: val_accuracy did not improve from 0.89235\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3716 - accuracy: 0.8856 - val_loss: 0.3699 - val_accuracy: 0.8886\n",
      "Epoch 186/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3702 - accuracy: 0.8835\n",
      "Epoch 00186: val_accuracy did not improve from 0.89235\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3702 - accuracy: 0.8835 - val_loss: 0.3701 - val_accuracy: 0.8857\n",
      "Epoch 187/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8823\n",
      "Epoch 00187: val_accuracy did not improve from 0.89235\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3685 - accuracy: 0.8823 - val_loss: 0.3657 - val_accuracy: 0.8924\n",
      "Epoch 188/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3723 - accuracy: 0.8840\n",
      "Epoch 00188: val_accuracy did not improve from 0.89235\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3723 - accuracy: 0.8840 - val_loss: 0.3663 - val_accuracy: 0.8924\n",
      "Epoch 189/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3663 - accuracy: 0.8847\n",
      "Epoch 00189: val_accuracy improved from 0.89235 to 0.89424, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3663 - accuracy: 0.8847 - val_loss: 0.3675 - val_accuracy: 0.8942\n",
      "Epoch 190/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8870\n",
      "Epoch 00190: val_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3632 - accuracy: 0.8871 - val_loss: 0.3651 - val_accuracy: 0.8895\n",
      "Epoch 191/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3556 - accuracy: 0.8960\n",
      "Epoch 00191: val_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3556 - accuracy: 0.8960 - val_loss: 0.3678 - val_accuracy: 0.8924\n",
      "Epoch 192/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.8916\n",
      "Epoch 00192: val_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3624 - accuracy: 0.8918 - val_loss: 0.3650 - val_accuracy: 0.8886\n",
      "Epoch 193/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.8890\n",
      "Epoch 00193: val_accuracy improved from 0.89424 to 0.89613, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3594 - accuracy: 0.8889 - val_loss: 0.3676 - val_accuracy: 0.8961\n",
      "Epoch 194/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.8899\n",
      "Epoch 00194: val_accuracy did not improve from 0.89613\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3620 - accuracy: 0.8899 - val_loss: 0.3710 - val_accuracy: 0.8791\n",
      "Epoch 195/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3714 - accuracy: 0.8798\n",
      "Epoch 00195: val_accuracy did not improve from 0.89613\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3719 - accuracy: 0.8795 - val_loss: 0.3649 - val_accuracy: 0.8914\n",
      "Epoch 196/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3590 - accuracy: 0.8913\n",
      "Epoch 00196: val_accuracy did not improve from 0.89613\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3588 - accuracy: 0.8915 - val_loss: 0.3696 - val_accuracy: 0.8820\n",
      "Epoch 197/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8845\n",
      "Epoch 00197: val_accuracy did not improve from 0.89613\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3667 - accuracy: 0.8845 - val_loss: 0.3630 - val_accuracy: 0.8961\n",
      "Epoch 198/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3586 - accuracy: 0.8911\n",
      "Epoch 00198: val_accuracy did not improve from 0.89613\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3586 - accuracy: 0.8911 - val_loss: 0.3687 - val_accuracy: 0.8810\n",
      "Epoch 199/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8885\n",
      "Epoch 00199: val_accuracy improved from 0.89613 to 0.89802, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3618 - accuracy: 0.8887 - val_loss: 0.3630 - val_accuracy: 0.8980\n",
      "Epoch 200/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.8854\n",
      "Epoch 00200: val_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3595 - accuracy: 0.8856 - val_loss: 0.3719 - val_accuracy: 0.8782\n",
      "Epoch 201/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3604 - accuracy: 0.8897\n",
      "Epoch 00201: val_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3604 - accuracy: 0.8897 - val_loss: 0.3679 - val_accuracy: 0.8867\n",
      "Epoch 202/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.8918\n",
      "Epoch 00202: val_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3567 - accuracy: 0.8918 - val_loss: 0.3666 - val_accuracy: 0.8886\n",
      "Epoch 203/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.8901\n",
      "Epoch 00203: val_accuracy improved from 0.89802 to 0.89896, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3544 - accuracy: 0.8901 - val_loss: 0.3616 - val_accuracy: 0.8990\n",
      "Epoch 204/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8904\n",
      "Epoch 00204: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3548 - accuracy: 0.8904 - val_loss: 0.3654 - val_accuracy: 0.8914\n",
      "Epoch 205/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.8916\n",
      "Epoch 00205: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3525 - accuracy: 0.8915 - val_loss: 0.3801 - val_accuracy: 0.8754\n",
      "Epoch 206/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8828\n",
      "Epoch 00206: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3641 - accuracy: 0.8828 - val_loss: 0.3613 - val_accuracy: 0.8924\n",
      "Epoch 207/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3553 - accuracy: 0.8918\n",
      "Epoch 00207: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3553 - accuracy: 0.8918 - val_loss: 0.3617 - val_accuracy: 0.8933\n",
      "Epoch 208/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3606 - accuracy: 0.8866\n",
      "Epoch 00208: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3606 - accuracy: 0.8866 - val_loss: 0.3623 - val_accuracy: 0.8924\n",
      "Epoch 209/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3555 - accuracy: 0.8925\n",
      "Epoch 00209: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3554 - accuracy: 0.8925 - val_loss: 0.3620 - val_accuracy: 0.8933\n",
      "Epoch 210/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8906\n",
      "Epoch 00210: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3573 - accuracy: 0.8908 - val_loss: 0.3628 - val_accuracy: 0.8990\n",
      "Epoch 211/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3571 - accuracy: 0.8867\n",
      "Epoch 00211: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3580 - accuracy: 0.8854 - val_loss: 0.3806 - val_accuracy: 0.8716\n",
      "Epoch 212/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.8857\n",
      "Epoch 00212: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3595 - accuracy: 0.8856 - val_loss: 0.3640 - val_accuracy: 0.8895\n",
      "Epoch 213/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3552 - accuracy: 0.8881\n",
      "Epoch 00213: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3538 - accuracy: 0.8887 - val_loss: 0.3636 - val_accuracy: 0.8905\n",
      "Epoch 214/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8890\n",
      "Epoch 00214: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3584 - accuracy: 0.8892 - val_loss: 0.3630 - val_accuracy: 0.8942\n",
      "Epoch 215/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8920\n",
      "Epoch 00215: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3506 - accuracy: 0.8920 - val_loss: 0.3670 - val_accuracy: 0.8895\n",
      "Epoch 216/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3557 - accuracy: 0.8875\n",
      "Epoch 00216: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3557 - accuracy: 0.8875 - val_loss: 0.3619 - val_accuracy: 0.8886\n",
      "Epoch 217/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8900\n",
      "Epoch 00217: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3529 - accuracy: 0.8897 - val_loss: 0.3795 - val_accuracy: 0.8725\n",
      "Epoch 218/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.3508 - accuracy: 0.8953\n",
      "Epoch 00218: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3513 - accuracy: 0.8951 - val_loss: 0.3601 - val_accuracy: 0.8980\n",
      "Epoch 219/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3507 - accuracy: 0.8922\n",
      "Epoch 00219: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3507 - accuracy: 0.8922 - val_loss: 0.3627 - val_accuracy: 0.8971\n",
      "Epoch 220/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3569 - accuracy: 0.8904\n",
      "Epoch 00220: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3569 - accuracy: 0.8904 - val_loss: 0.3643 - val_accuracy: 0.8924\n",
      "Epoch 221/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3479 - accuracy: 0.8946\n",
      "Epoch 00221: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3480 - accuracy: 0.8946 - val_loss: 0.3647 - val_accuracy: 0.8886\n",
      "Epoch 222/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 0.8845\n",
      "Epoch 00222: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3547 - accuracy: 0.8845 - val_loss: 0.3764 - val_accuracy: 0.8791\n",
      "Epoch 223/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.8962\n",
      "Epoch 00223: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3531 - accuracy: 0.8963 - val_loss: 0.3628 - val_accuracy: 0.8801\n",
      "Epoch 224/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3470 - accuracy: 0.8951\n",
      "Epoch 00224: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3470 - accuracy: 0.8953 - val_loss: 0.3591 - val_accuracy: 0.8914\n",
      "Epoch 225/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8904\n",
      "Epoch 00225: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3508 - accuracy: 0.8904 - val_loss: 0.3761 - val_accuracy: 0.8763\n",
      "Epoch 226/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3488 - accuracy: 0.8946\n",
      "Epoch 00226: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3486 - accuracy: 0.8946 - val_loss: 0.4084 - val_accuracy: 0.8555\n",
      "Epoch 227/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.8908\n",
      "Epoch 00227: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3502 - accuracy: 0.8908 - val_loss: 0.3580 - val_accuracy: 0.8961\n",
      "Epoch 228/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.8934\n",
      "Epoch 00228: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3468 - accuracy: 0.8934 - val_loss: 0.3759 - val_accuracy: 0.8791\n",
      "Epoch 229/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8913\n",
      "Epoch 00229: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3472 - accuracy: 0.8915 - val_loss: 0.3578 - val_accuracy: 0.8933\n",
      "Epoch 230/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3460 - accuracy: 0.8951\n",
      "Epoch 00230: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3459 - accuracy: 0.8951 - val_loss: 0.3610 - val_accuracy: 0.8886\n",
      "Epoch 231/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.8934\n",
      "Epoch 00231: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3504 - accuracy: 0.8934 - val_loss: 0.3614 - val_accuracy: 0.8942\n",
      "Epoch 232/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.8984\n",
      "Epoch 00232: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3412 - accuracy: 0.8984 - val_loss: 0.3604 - val_accuracy: 0.8933\n",
      "Epoch 233/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3454 - accuracy: 0.8935\n",
      "Epoch 00233: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3457 - accuracy: 0.8932 - val_loss: 0.4011 - val_accuracy: 0.8584\n",
      "Epoch 234/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3451 - accuracy: 0.8948\n",
      "Epoch 00234: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3452 - accuracy: 0.8946 - val_loss: 0.3628 - val_accuracy: 0.8905\n",
      "Epoch 235/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3470 - accuracy: 0.8923\n",
      "Epoch 00235: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3468 - accuracy: 0.8925 - val_loss: 0.3682 - val_accuracy: 0.8791\n",
      "Epoch 236/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8970\n",
      "Epoch 00236: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3443 - accuracy: 0.8970 - val_loss: 0.3701 - val_accuracy: 0.8754\n",
      "Epoch 237/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3440 - accuracy: 0.8965\n",
      "Epoch 00237: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3437 - accuracy: 0.8967 - val_loss: 0.3618 - val_accuracy: 0.8886\n",
      "Epoch 238/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.8987\n",
      "Epoch 00238: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3439 - accuracy: 0.8986 - val_loss: 0.3624 - val_accuracy: 0.8801\n",
      "Epoch 239/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3454 - accuracy: 0.8987\n",
      "Epoch 00239: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3452 - accuracy: 0.8989 - val_loss: 0.3539 - val_accuracy: 0.8924\n",
      "Epoch 240/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.8982\n",
      "Epoch 00240: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3448 - accuracy: 0.8982 - val_loss: 0.3537 - val_accuracy: 0.8961\n",
      "Epoch 241/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8958\n",
      "Epoch 00241: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3443 - accuracy: 0.8958 - val_loss: 0.3622 - val_accuracy: 0.8848\n",
      "Epoch 242/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3416 - accuracy: 0.8965\n",
      "Epoch 00242: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3413 - accuracy: 0.8967 - val_loss: 0.3642 - val_accuracy: 0.8801\n",
      "Epoch 243/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3432 - accuracy: 0.8975\n",
      "Epoch 00243: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3434 - accuracy: 0.8974 - val_loss: 0.3560 - val_accuracy: 0.8924\n",
      "Epoch 244/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.8941\n",
      "Epoch 00244: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3400 - accuracy: 0.8948 - val_loss: 0.3612 - val_accuracy: 0.8905\n",
      "Epoch 245/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.8894\n",
      "Epoch 00245: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3525 - accuracy: 0.8894 - val_loss: 0.3536 - val_accuracy: 0.8961\n",
      "Epoch 246/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.9015\n",
      "Epoch 00246: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3402 - accuracy: 0.9015 - val_loss: 0.3531 - val_accuracy: 0.8980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8974\n",
      "Epoch 00247: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3401 - accuracy: 0.8974 - val_loss: 0.3554 - val_accuracy: 0.8924\n",
      "Epoch 248/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.8972\n",
      "Epoch 00248: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3389 - accuracy: 0.8972 - val_loss: 0.3538 - val_accuracy: 0.8990\n",
      "Epoch 249/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8984\n",
      "Epoch 00249: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3401 - accuracy: 0.8984 - val_loss: 0.3585 - val_accuracy: 0.8867\n",
      "Epoch 250/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.8932\n",
      "Epoch 00250: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3490 - accuracy: 0.8932 - val_loss: 0.3525 - val_accuracy: 0.8990\n",
      "Epoch 251/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.8956\n",
      "Epoch 00251: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3379 - accuracy: 0.8956 - val_loss: 0.3535 - val_accuracy: 0.8876\n",
      "Epoch 252/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8949\n",
      "Epoch 00252: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3416 - accuracy: 0.8951 - val_loss: 0.3697 - val_accuracy: 0.8820\n",
      "Epoch 253/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8939\n",
      "Epoch 00253: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3388 - accuracy: 0.8941 - val_loss: 0.3695 - val_accuracy: 0.8791\n",
      "Epoch 254/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8977\n",
      "Epoch 00254: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3361 - accuracy: 0.8979 - val_loss: 0.3569 - val_accuracy: 0.8886\n",
      "Epoch 255/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8963\n",
      "Epoch 00255: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3444 - accuracy: 0.8963 - val_loss: 0.3570 - val_accuracy: 0.8961\n",
      "Epoch 256/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.8982\n",
      "Epoch 00256: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3438 - accuracy: 0.8982 - val_loss: 0.3506 - val_accuracy: 0.8952\n",
      "Epoch 257/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3422 - accuracy: 0.8956\n",
      "Epoch 00257: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3422 - accuracy: 0.8956 - val_loss: 0.3513 - val_accuracy: 0.8942\n",
      "Epoch 258/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3401 - accuracy: 0.8977\n",
      "Epoch 00258: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3399 - accuracy: 0.8977 - val_loss: 0.3639 - val_accuracy: 0.8905\n",
      "Epoch 259/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.8999\n",
      "Epoch 00259: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3386 - accuracy: 0.8996 - val_loss: 0.3521 - val_accuracy: 0.8942\n",
      "Epoch 260/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3349 - accuracy: 0.9017\n",
      "Epoch 00260: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3349 - accuracy: 0.9017 - val_loss: 0.3792 - val_accuracy: 0.8697\n",
      "Epoch 261/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8958\n",
      "Epoch 00261: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3401 - accuracy: 0.8958 - val_loss: 0.3578 - val_accuracy: 0.8914\n",
      "Epoch 262/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3450 - accuracy: 0.8942\n",
      "Epoch 00262: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3453 - accuracy: 0.8939 - val_loss: 0.3582 - val_accuracy: 0.8867\n",
      "Epoch 263/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3363 - accuracy: 0.8934\n",
      "Epoch 00263: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3363 - accuracy: 0.8934 - val_loss: 0.3527 - val_accuracy: 0.8942\n",
      "Epoch 264/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.8970\n",
      "Epoch 00264: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3427 - accuracy: 0.8970 - val_loss: 0.3674 - val_accuracy: 0.8848\n",
      "Epoch 265/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3467 - accuracy: 0.8932\n",
      "Epoch 00265: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3467 - accuracy: 0.8932 - val_loss: 0.3744 - val_accuracy: 0.8735\n",
      "Epoch 266/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8996\n",
      "Epoch 00266: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3405 - accuracy: 0.8998 - val_loss: 0.3515 - val_accuracy: 0.8924\n",
      "Epoch 267/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8989\n",
      "Epoch 00267: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3383 - accuracy: 0.8979 - val_loss: 0.3539 - val_accuracy: 0.8914\n",
      "Epoch 268/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.8958\n",
      "Epoch 00268: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3392 - accuracy: 0.8958 - val_loss: 0.3555 - val_accuracy: 0.8886\n",
      "Epoch 269/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3352 - accuracy: 0.9022\n",
      "Epoch 00269: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3358 - accuracy: 0.9019 - val_loss: 0.3506 - val_accuracy: 0.8961\n",
      "Epoch 270/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3337 - accuracy: 0.9012\n",
      "Epoch 00270: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3337 - accuracy: 0.9015 - val_loss: 0.3527 - val_accuracy: 0.8886\n",
      "Epoch 271/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8948\n",
      "Epoch 00271: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3358 - accuracy: 0.8951 - val_loss: 0.3518 - val_accuracy: 0.8933\n",
      "Epoch 272/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.9005\n",
      "Epoch 00272: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3312 - accuracy: 0.9005 - val_loss: 0.3564 - val_accuracy: 0.8867\n",
      "Epoch 273/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.9017\n",
      "Epoch 00273: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3358 - accuracy: 0.9017 - val_loss: 0.3533 - val_accuracy: 0.8895\n",
      "Epoch 274/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.8986\n",
      "Epoch 00274: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3342 - accuracy: 0.8986 - val_loss: 0.3520 - val_accuracy: 0.8886\n",
      "Epoch 275/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.9060\n",
      "Epoch 00275: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3298 - accuracy: 0.9060 - val_loss: 0.3577 - val_accuracy: 0.8886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8958\n",
      "Epoch 00276: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3422 - accuracy: 0.8958 - val_loss: 0.3553 - val_accuracy: 0.8876\n",
      "Epoch 277/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.9019\n",
      "Epoch 00277: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3317 - accuracy: 0.9019 - val_loss: 0.3468 - val_accuracy: 0.8961\n",
      "Epoch 278/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.8941\n",
      "Epoch 00278: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3368 - accuracy: 0.8941 - val_loss: 0.3751 - val_accuracy: 0.8716\n",
      "Epoch 279/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.9008\n",
      "Epoch 00279: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3301 - accuracy: 0.9010 - val_loss: 0.3538 - val_accuracy: 0.8876\n",
      "Epoch 280/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.9045\n",
      "Epoch 00280: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3289 - accuracy: 0.9045 - val_loss: 0.3546 - val_accuracy: 0.8867\n",
      "Epoch 281/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.9001\n",
      "Epoch 00281: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3276 - accuracy: 0.9003 - val_loss: 0.3505 - val_accuracy: 0.8942\n",
      "Epoch 282/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.9034\n",
      "Epoch 00282: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3290 - accuracy: 0.9036 - val_loss: 0.3639 - val_accuracy: 0.8791\n",
      "Epoch 283/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.8994\n",
      "Epoch 00283: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3316 - accuracy: 0.8996 - val_loss: 0.3638 - val_accuracy: 0.8829\n",
      "Epoch 284/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.8972\n",
      "Epoch 00284: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3346 - accuracy: 0.8972 - val_loss: 0.3499 - val_accuracy: 0.8971\n",
      "Epoch 285/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3252 - accuracy: 0.9048\n",
      "Epoch 00285: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3252 - accuracy: 0.9048 - val_loss: 0.3609 - val_accuracy: 0.8867\n",
      "Epoch 286/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3356 - accuracy: 0.8995\n",
      "Epoch 00286: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3365 - accuracy: 0.8989 - val_loss: 0.3615 - val_accuracy: 0.8839\n",
      "Epoch 287/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.9008\n",
      "Epoch 00287: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3286 - accuracy: 0.9003 - val_loss: 0.3500 - val_accuracy: 0.8990\n",
      "Epoch 288/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3232 - accuracy: 0.9079\n",
      "Epoch 00288: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3232 - accuracy: 0.9076 - val_loss: 0.3559 - val_accuracy: 0.8876\n",
      "Epoch 289/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.8948\n",
      "Epoch 00289: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3348 - accuracy: 0.8948 - val_loss: 0.3530 - val_accuracy: 0.8924\n",
      "Epoch 290/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.9048\n",
      "Epoch 00290: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3302 - accuracy: 0.9048 - val_loss: 0.3552 - val_accuracy: 0.8848\n",
      "Epoch 291/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.8949\n",
      "Epoch 00291: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3372 - accuracy: 0.8948 - val_loss: 0.3498 - val_accuracy: 0.8942\n",
      "Epoch 292/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.9006\n",
      "Epoch 00292: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3288 - accuracy: 0.9008 - val_loss: 0.3471 - val_accuracy: 0.8952\n",
      "Epoch 293/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.9064\n",
      "Epoch 00293: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3242 - accuracy: 0.9064 - val_loss: 0.3501 - val_accuracy: 0.8933\n",
      "Epoch 294/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.9062\n",
      "Epoch 00294: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3235 - accuracy: 0.9067 - val_loss: 0.3614 - val_accuracy: 0.8829\n",
      "Epoch 295/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.9027\n",
      "Epoch 00295: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3282 - accuracy: 0.9026 - val_loss: 0.3461 - val_accuracy: 0.8980\n",
      "Epoch 296/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.9074\n",
      "Epoch 00296: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3201 - accuracy: 0.9074 - val_loss: 0.3507 - val_accuracy: 0.8924\n",
      "Epoch 297/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3256 - accuracy: 0.9039\n",
      "Epoch 00297: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3257 - accuracy: 0.9038 - val_loss: 0.3508 - val_accuracy: 0.8895\n",
      "Epoch 298/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.9027\n",
      "Epoch 00298: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3247 - accuracy: 0.9026 - val_loss: 0.3706 - val_accuracy: 0.8754\n",
      "Epoch 299/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.9005\n",
      "Epoch 00299: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3271 - accuracy: 0.9005 - val_loss: 0.3454 - val_accuracy: 0.8942\n",
      "Epoch 300/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.9001\n",
      "Epoch 00300: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3289 - accuracy: 0.9000 - val_loss: 0.3601 - val_accuracy: 0.8867\n",
      "Epoch 301/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3322 - accuracy: 0.9043\n",
      "Epoch 00301: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3319 - accuracy: 0.9034 - val_loss: 0.3456 - val_accuracy: 0.8980\n",
      "Epoch 302/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.9006\n",
      "Epoch 00302: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3265 - accuracy: 0.9008 - val_loss: 0.3507 - val_accuracy: 0.8914\n",
      "Epoch 303/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.8998\n",
      "Epoch 00303: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3308 - accuracy: 0.8998 - val_loss: 0.3541 - val_accuracy: 0.8867\n",
      "Epoch 304/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.9020\n",
      "Epoch 00304: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3258 - accuracy: 0.9019 - val_loss: 0.3468 - val_accuracy: 0.8942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.8953\n",
      "Epoch 00305: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3302 - accuracy: 0.8953 - val_loss: 0.3480 - val_accuracy: 0.8933\n",
      "Epoch 306/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.9048\n",
      "Epoch 00306: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3224 - accuracy: 0.9050 - val_loss: 0.3655 - val_accuracy: 0.8820\n",
      "Epoch 307/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.9034\n",
      "Epoch 00307: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3224 - accuracy: 0.9029 - val_loss: 0.3666 - val_accuracy: 0.8772\n",
      "Epoch 308/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3262 - accuracy: 0.9019\n",
      "Epoch 00308: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3262 - accuracy: 0.9019 - val_loss: 0.3458 - val_accuracy: 0.8952\n",
      "Epoch 309/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3248 - accuracy: 0.9058\n",
      "Epoch 00309: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3244 - accuracy: 0.9060 - val_loss: 0.3486 - val_accuracy: 0.8952\n",
      "Epoch 310/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3254 - accuracy: 0.9022\n",
      "Epoch 00310: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3254 - accuracy: 0.9022 - val_loss: 0.3680 - val_accuracy: 0.8820\n",
      "Epoch 311/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.9045\n",
      "Epoch 00311: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3256 - accuracy: 0.9045 - val_loss: 0.3495 - val_accuracy: 0.8990\n",
      "Epoch 312/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.9095\n",
      "Epoch 00312: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3145 - accuracy: 0.9095 - val_loss: 0.3467 - val_accuracy: 0.8990\n",
      "Epoch 313/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.9086\n",
      "Epoch 00313: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3239 - accuracy: 0.9086 - val_loss: 0.3500 - val_accuracy: 0.8933\n",
      "Epoch 314/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9043\n",
      "Epoch 00314: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3281 - accuracy: 0.9043 - val_loss: 0.3583 - val_accuracy: 0.8905\n",
      "Epoch 315/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.8986\n",
      "Epoch 00315: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3316 - accuracy: 0.8989 - val_loss: 0.3438 - val_accuracy: 0.8961\n",
      "Epoch 316/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.9058\n",
      "Epoch 00316: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3244 - accuracy: 0.9057 - val_loss: 0.3504 - val_accuracy: 0.8971\n",
      "Epoch 317/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.9015\n",
      "Epoch 00317: val_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3229 - accuracy: 0.9017 - val_loss: 0.3456 - val_accuracy: 0.8990\n",
      "Epoch 318/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.9074\n",
      "Epoch 00318: val_accuracy improved from 0.89896 to 0.89991, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3167 - accuracy: 0.9074 - val_loss: 0.3484 - val_accuracy: 0.8999\n",
      "Epoch 319/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.9075\n",
      "Epoch 00319: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3246 - accuracy: 0.9064 - val_loss: 0.3503 - val_accuracy: 0.8905\n",
      "Epoch 320/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3229 - accuracy: 0.9015\n",
      "Epoch 00320: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3232 - accuracy: 0.9012 - val_loss: 0.3674 - val_accuracy: 0.8744\n",
      "Epoch 321/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.8986\n",
      "Epoch 00321: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3324 - accuracy: 0.8986 - val_loss: 0.3810 - val_accuracy: 0.8772\n",
      "Epoch 322/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.9058\n",
      "Epoch 00322: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3227 - accuracy: 0.9060 - val_loss: 0.3485 - val_accuracy: 0.8924\n",
      "Epoch 323/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.9069\n",
      "Epoch 00323: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3165 - accuracy: 0.9069 - val_loss: 0.3465 - val_accuracy: 0.8980\n",
      "Epoch 324/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.9029\n",
      "Epoch 00324: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3220 - accuracy: 0.9031 - val_loss: 0.3596 - val_accuracy: 0.8829\n",
      "Epoch 325/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.9029\n",
      "Epoch 00325: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3224 - accuracy: 0.9029 - val_loss: 0.3527 - val_accuracy: 0.8867\n",
      "Epoch 326/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.9079\n",
      "Epoch 00326: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3207 - accuracy: 0.9081 - val_loss: 0.3458 - val_accuracy: 0.8933\n",
      "Epoch 327/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.9010\n",
      "Epoch 00327: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3206 - accuracy: 0.9010 - val_loss: 0.3430 - val_accuracy: 0.8952\n",
      "Epoch 328/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.9043\n",
      "Epoch 00328: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3165 - accuracy: 0.9043 - val_loss: 0.3503 - val_accuracy: 0.8952\n",
      "Epoch 329/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.9025\n",
      "Epoch 00329: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3207 - accuracy: 0.9026 - val_loss: 0.3438 - val_accuracy: 0.8980\n",
      "Epoch 330/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.9072\n",
      "Epoch 00330: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3201 - accuracy: 0.9071 - val_loss: 0.3479 - val_accuracy: 0.8867\n",
      "Epoch 331/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.9086\n",
      "Epoch 00331: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3149 - accuracy: 0.9088 - val_loss: 0.3477 - val_accuracy: 0.8952\n",
      "Epoch 332/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3235 - accuracy: 0.9055\n",
      "Epoch 00332: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3235 - accuracy: 0.9055 - val_loss: 0.3451 - val_accuracy: 0.8952\n",
      "Epoch 333/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3203 - accuracy: 0.9081\n",
      "Epoch 00333: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3203 - accuracy: 0.9081 - val_loss: 0.3414 - val_accuracy: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.9081\n",
      "Epoch 00334: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3139 - accuracy: 0.9081 - val_loss: 0.3460 - val_accuracy: 0.8990\n",
      "Epoch 335/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.9048\n",
      "Epoch 00335: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3184 - accuracy: 0.9048 - val_loss: 0.3447 - val_accuracy: 0.8924\n",
      "Epoch 336/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3158 - accuracy: 0.9045\n",
      "Epoch 00336: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3158 - accuracy: 0.9045 - val_loss: 0.3462 - val_accuracy: 0.8905\n",
      "Epoch 337/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.9050\n",
      "Epoch 00337: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3181 - accuracy: 0.9050 - val_loss: 0.3458 - val_accuracy: 0.8895\n",
      "Epoch 338/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.9057\n",
      "Epoch 00338: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3129 - accuracy: 0.9057 - val_loss: 0.3522 - val_accuracy: 0.8942\n",
      "Epoch 339/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.9098\n",
      "Epoch 00339: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3123 - accuracy: 0.9093 - val_loss: 0.3398 - val_accuracy: 0.8990\n",
      "Epoch 340/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.9048\n",
      "Epoch 00340: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3216 - accuracy: 0.9048 - val_loss: 0.3464 - val_accuracy: 0.8961\n",
      "Epoch 341/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.9065\n",
      "Epoch 00341: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3178 - accuracy: 0.9067 - val_loss: 0.3711 - val_accuracy: 0.8791\n",
      "Epoch 342/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.9045\n",
      "Epoch 00342: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3145 - accuracy: 0.9045 - val_loss: 0.3507 - val_accuracy: 0.8924\n",
      "Epoch 343/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.9044\n",
      "Epoch 00343: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3197 - accuracy: 0.9043 - val_loss: 0.3392 - val_accuracy: 0.8961\n",
      "Epoch 344/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3141 - accuracy: 0.9053\n",
      "Epoch 00344: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3144 - accuracy: 0.9052 - val_loss: 0.3729 - val_accuracy: 0.8725\n",
      "Epoch 345/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.9078\n",
      "Epoch 00345: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3181 - accuracy: 0.9078 - val_loss: 0.3455 - val_accuracy: 0.8942\n",
      "Epoch 346/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.9022\n",
      "Epoch 00346: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3177 - accuracy: 0.9022 - val_loss: 0.3622 - val_accuracy: 0.8848\n",
      "Epoch 347/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3119 - accuracy: 0.9119\n",
      "Epoch 00347: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3119 - accuracy: 0.9119 - val_loss: 0.3483 - val_accuracy: 0.8942\n",
      "Epoch 348/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.9076\n",
      "Epoch 00348: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3143 - accuracy: 0.9076 - val_loss: 0.3415 - val_accuracy: 0.8952\n",
      "Epoch 349/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.9088\n",
      "Epoch 00349: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3130 - accuracy: 0.9088 - val_loss: 0.3449 - val_accuracy: 0.8905\n",
      "Epoch 350/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.9050\n",
      "Epoch 00350: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3125 - accuracy: 0.9050 - val_loss: 0.3637 - val_accuracy: 0.8801\n",
      "Epoch 351/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.9079\n",
      "Epoch 00351: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3161 - accuracy: 0.9078 - val_loss: 0.3436 - val_accuracy: 0.8952\n",
      "Epoch 352/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.9060\n",
      "Epoch 00352: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3206 - accuracy: 0.9060 - val_loss: 0.3524 - val_accuracy: 0.8848\n",
      "Epoch 353/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.9103\n",
      "Epoch 00353: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3130 - accuracy: 0.9100 - val_loss: 0.3544 - val_accuracy: 0.8867\n",
      "Epoch 354/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.9093\n",
      "Epoch 00354: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3114 - accuracy: 0.9093 - val_loss: 0.3415 - val_accuracy: 0.8961\n",
      "Epoch 355/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.9089\n",
      "Epoch 00355: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3132 - accuracy: 0.9090 - val_loss: 0.3434 - val_accuracy: 0.8999\n",
      "Epoch 356/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3093 - accuracy: 0.9086\n",
      "Epoch 00356: val_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3093 - accuracy: 0.9088 - val_loss: 0.3493 - val_accuracy: 0.8914\n",
      "Epoch 357/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.9081\n",
      "Epoch 00357: val_accuracy improved from 0.89991 to 0.90179, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3170 - accuracy: 0.9081 - val_loss: 0.3395 - val_accuracy: 0.9018\n",
      "Epoch 358/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.9133\n",
      "Epoch 00358: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3080 - accuracy: 0.9133 - val_loss: 0.3418 - val_accuracy: 0.8999\n",
      "Epoch 359/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.9078\n",
      "Epoch 00359: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3156 - accuracy: 0.9078 - val_loss: 0.3404 - val_accuracy: 0.8990\n",
      "Epoch 360/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3163 - accuracy: 0.9086\n",
      "Epoch 00360: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3162 - accuracy: 0.9088 - val_loss: 0.3393 - val_accuracy: 0.9018\n",
      "Epoch 361/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3063 - accuracy: 0.9114\n",
      "Epoch 00361: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3063 - accuracy: 0.9114 - val_loss: 0.3568 - val_accuracy: 0.8848\n",
      "Epoch 362/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3123 - accuracy: 0.9072\n",
      "Epoch 00362: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3135 - accuracy: 0.9062 - val_loss: 0.3518 - val_accuracy: 0.8886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3118 - accuracy: 0.9126\n",
      "Epoch 00363: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3111 - accuracy: 0.9130 - val_loss: 0.3434 - val_accuracy: 0.8990\n",
      "Epoch 364/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.9058\n",
      "Epoch 00364: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3131 - accuracy: 0.9062 - val_loss: 0.3419 - val_accuracy: 0.8952\n",
      "Epoch 365/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.9089\n",
      "Epoch 00365: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3124 - accuracy: 0.9097 - val_loss: 0.3433 - val_accuracy: 0.8999\n",
      "Epoch 366/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3074 - accuracy: 0.9126\n",
      "Epoch 00366: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3076 - accuracy: 0.9128 - val_loss: 0.3474 - val_accuracy: 0.8942\n",
      "Epoch 367/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.9127\n",
      "Epoch 00367: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3049 - accuracy: 0.9128 - val_loss: 0.3430 - val_accuracy: 0.8990\n",
      "Epoch 368/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.9101\n",
      "Epoch 00368: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3088 - accuracy: 0.9095 - val_loss: 0.3406 - val_accuracy: 0.8961\n",
      "Epoch 369/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.9116\n",
      "Epoch 00369: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3085 - accuracy: 0.9116 - val_loss: 0.3437 - val_accuracy: 0.8895\n",
      "Epoch 370/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.9084\n",
      "Epoch 00370: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3088 - accuracy: 0.9076 - val_loss: 0.3415 - val_accuracy: 0.8980\n",
      "Epoch 371/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.9120\n",
      "Epoch 00371: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3049 - accuracy: 0.9119 - val_loss: 0.3405 - val_accuracy: 0.8961\n",
      "Epoch 372/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.9062\n",
      "Epoch 00372: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3123 - accuracy: 0.9067 - val_loss: 0.3706 - val_accuracy: 0.8801\n",
      "Epoch 373/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.9110\n",
      "Epoch 00373: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3113 - accuracy: 0.9109 - val_loss: 0.3486 - val_accuracy: 0.8942\n",
      "Epoch 374/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3093 - accuracy: 0.9060\n",
      "Epoch 00374: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3091 - accuracy: 0.9062 - val_loss: 0.3416 - val_accuracy: 0.8980\n",
      "Epoch 375/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.9096\n",
      "Epoch 00375: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3047 - accuracy: 0.9095 - val_loss: 0.3516 - val_accuracy: 0.8857\n",
      "Epoch 376/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.9077\n",
      "Epoch 00376: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3143 - accuracy: 0.9076 - val_loss: 0.3423 - val_accuracy: 0.8980\n",
      "Epoch 377/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.9060\n",
      "Epoch 00377: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3167 - accuracy: 0.9062 - val_loss: 0.3445 - val_accuracy: 0.8942\n",
      "Epoch 378/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.9079\n",
      "Epoch 00378: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3134 - accuracy: 0.9071 - val_loss: 0.3545 - val_accuracy: 0.8924\n",
      "Epoch 379/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.9088\n",
      "Epoch 00379: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3110 - accuracy: 0.9088 - val_loss: 0.3526 - val_accuracy: 0.8876\n",
      "Epoch 380/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3069 - accuracy: 0.9093\n",
      "Epoch 00380: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3069 - accuracy: 0.9093 - val_loss: 0.3377 - val_accuracy: 0.8980\n",
      "Epoch 381/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.9067\n",
      "Epoch 00381: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3081 - accuracy: 0.9064 - val_loss: 0.3464 - val_accuracy: 0.8895\n",
      "Epoch 382/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.9125\n",
      "Epoch 00382: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3029 - accuracy: 0.9126 - val_loss: 0.3382 - val_accuracy: 0.9018\n",
      "Epoch 383/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.9132\n",
      "Epoch 00383: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3110 - accuracy: 0.9133 - val_loss: 0.3402 - val_accuracy: 0.8942\n",
      "Epoch 384/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.9127\n",
      "Epoch 00384: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3083 - accuracy: 0.9126 - val_loss: 0.3403 - val_accuracy: 0.8971\n",
      "Epoch 385/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3062 - accuracy: 0.9135\n",
      "Epoch 00385: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3073 - accuracy: 0.9126 - val_loss: 0.3625 - val_accuracy: 0.8839\n",
      "Epoch 386/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3083 - accuracy: 0.9133\n",
      "Epoch 00386: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3082 - accuracy: 0.9123 - val_loss: 0.3593 - val_accuracy: 0.8839\n",
      "Epoch 387/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.9041\n",
      "Epoch 00387: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3157 - accuracy: 0.9041 - val_loss: 0.3419 - val_accuracy: 0.8999\n",
      "Epoch 388/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3093 - accuracy: 0.9115\n",
      "Epoch 00388: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3106 - accuracy: 0.9114 - val_loss: 0.3377 - val_accuracy: 0.8999\n",
      "Epoch 389/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3080 - accuracy: 0.9116\n",
      "Epoch 00389: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3104 - accuracy: 0.9100 - val_loss: 0.3398 - val_accuracy: 0.8980\n",
      "Epoch 390/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.9077\n",
      "Epoch 00390: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3114 - accuracy: 0.9076 - val_loss: 0.3393 - val_accuracy: 0.8971\n",
      "Epoch 391/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.9036\n",
      "Epoch 00391: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3089 - accuracy: 0.9045 - val_loss: 0.3421 - val_accuracy: 0.8980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.9101\n",
      "Epoch 00392: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3109 - accuracy: 0.9102 - val_loss: 0.3398 - val_accuracy: 0.8914\n",
      "Epoch 393/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3037 - accuracy: 0.9128\n",
      "Epoch 00393: val_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3055 - accuracy: 0.9126 - val_loss: 0.3468 - val_accuracy: 0.8905\n",
      "Epoch 394/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.9058\n",
      "Epoch 00394: val_accuracy improved from 0.90179 to 0.90274, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3112 - accuracy: 0.9052 - val_loss: 0.3420 - val_accuracy: 0.9027\n",
      "Epoch 395/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.9050\n",
      "Epoch 00395: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3102 - accuracy: 0.9050 - val_loss: 0.3774 - val_accuracy: 0.8791\n",
      "Epoch 396/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.9081\n",
      "Epoch 00396: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3072 - accuracy: 0.9081 - val_loss: 0.3550 - val_accuracy: 0.8829\n",
      "Epoch 397/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.9103\n",
      "Epoch 00397: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3064 - accuracy: 0.9104 - val_loss: 0.3546 - val_accuracy: 0.8876\n",
      "Epoch 398/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.9137\n",
      "Epoch 00398: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3026 - accuracy: 0.9130 - val_loss: 0.3619 - val_accuracy: 0.8867\n",
      "Epoch 399/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3051 - accuracy: 0.9124\n",
      "Epoch 00399: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3060 - accuracy: 0.9112 - val_loss: 0.3393 - val_accuracy: 0.8999\n",
      "Epoch 400/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.9120\n",
      "Epoch 00400: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3064 - accuracy: 0.9123 - val_loss: 0.3421 - val_accuracy: 0.8971\n",
      "Epoch 401/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.9143\n",
      "Epoch 00401: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3030 - accuracy: 0.9140 - val_loss: 0.3401 - val_accuracy: 0.8961\n",
      "Epoch 402/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.9132\n",
      "Epoch 00402: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3062 - accuracy: 0.9130 - val_loss: 0.3544 - val_accuracy: 0.8942\n",
      "Epoch 403/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.9103\n",
      "Epoch 00403: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3039 - accuracy: 0.9104 - val_loss: 0.3508 - val_accuracy: 0.8924\n",
      "Epoch 404/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.9130\n",
      "Epoch 00404: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3098 - accuracy: 0.9130 - val_loss: 0.3496 - val_accuracy: 0.8829\n",
      "Epoch 405/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.9077\n",
      "Epoch 00405: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3033 - accuracy: 0.9076 - val_loss: 0.3430 - val_accuracy: 0.8961\n",
      "Epoch 406/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3056 - accuracy: 0.9114\n",
      "Epoch 00406: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3055 - accuracy: 0.9112 - val_loss: 0.3433 - val_accuracy: 0.8980\n",
      "Epoch 407/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.9111\n",
      "Epoch 00407: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3047 - accuracy: 0.9116 - val_loss: 0.3469 - val_accuracy: 0.8886\n",
      "Epoch 408/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.9095\n",
      "Epoch 00408: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3095 - accuracy: 0.9095 - val_loss: 0.3410 - val_accuracy: 0.8961\n",
      "Epoch 409/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2985 - accuracy: 0.9163\n",
      "Epoch 00409: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2997 - accuracy: 0.9156 - val_loss: 0.3497 - val_accuracy: 0.8876\n",
      "Epoch 410/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.9102\n",
      "Epoch 00410: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3061 - accuracy: 0.9102 - val_loss: 0.3493 - val_accuracy: 0.9008\n",
      "Epoch 411/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.9070\n",
      "Epoch 00411: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3127 - accuracy: 0.9071 - val_loss: 0.3363 - val_accuracy: 0.8961\n",
      "Epoch 412/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.9125\n",
      "Epoch 00412: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3012 - accuracy: 0.9128 - val_loss: 0.3387 - val_accuracy: 0.8914\n",
      "Epoch 413/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.9072\n",
      "Epoch 00413: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3068 - accuracy: 0.9071 - val_loss: 0.3578 - val_accuracy: 0.8857\n",
      "Epoch 414/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.9091\n",
      "Epoch 00414: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3037 - accuracy: 0.9093 - val_loss: 0.3790 - val_accuracy: 0.8839\n",
      "Epoch 415/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3115 - accuracy: 0.9084\n",
      "Epoch 00415: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3117 - accuracy: 0.9088 - val_loss: 0.3461 - val_accuracy: 0.8895\n",
      "Epoch 416/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.9120\n",
      "Epoch 00416: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2989 - accuracy: 0.9121 - val_loss: 0.3403 - val_accuracy: 0.8942\n",
      "Epoch 417/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.9115\n",
      "Epoch 00417: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3022 - accuracy: 0.9116 - val_loss: 0.3607 - val_accuracy: 0.8886\n",
      "Epoch 418/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.9084\n",
      "Epoch 00418: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3052 - accuracy: 0.9083 - val_loss: 0.3467 - val_accuracy: 0.8905\n",
      "Epoch 419/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.9108\n",
      "Epoch 00419: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3021 - accuracy: 0.9114 - val_loss: 0.3426 - val_accuracy: 0.8990\n",
      "Epoch 420/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9192\n",
      "Epoch 00420: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2977 - accuracy: 0.9180 - val_loss: 0.3431 - val_accuracy: 0.8971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2953 - accuracy: 0.9131\n",
      "Epoch 00421: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2974 - accuracy: 0.9121 - val_loss: 0.3481 - val_accuracy: 0.8952\n",
      "Epoch 422/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9130\n",
      "Epoch 00422: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2996 - accuracy: 0.9130 - val_loss: 0.3589 - val_accuracy: 0.8876\n",
      "Epoch 423/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.9147\n",
      "Epoch 00423: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2989 - accuracy: 0.9154 - val_loss: 0.3577 - val_accuracy: 0.8886\n",
      "Epoch 424/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9215\n",
      "Epoch 00424: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2963 - accuracy: 0.9206 - val_loss: 0.3446 - val_accuracy: 0.8971\n",
      "Epoch 425/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.9133\n",
      "Epoch 00425: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2990 - accuracy: 0.9128 - val_loss: 0.3583 - val_accuracy: 0.8839\n",
      "Epoch 426/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.9115\n",
      "Epoch 00426: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3039 - accuracy: 0.9121 - val_loss: 0.3426 - val_accuracy: 0.8952\n",
      "Epoch 427/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.9096\n",
      "Epoch 00427: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3038 - accuracy: 0.9100 - val_loss: 0.3597 - val_accuracy: 0.8820\n",
      "Epoch 428/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.9101\n",
      "Epoch 00428: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3055 - accuracy: 0.9100 - val_loss: 0.3435 - val_accuracy: 0.8942\n",
      "Epoch 429/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.9160\n",
      "Epoch 00429: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2956 - accuracy: 0.9159 - val_loss: 0.3349 - val_accuracy: 0.8971\n",
      "Epoch 430/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2954 - accuracy: 0.9189\n",
      "Epoch 00430: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2963 - accuracy: 0.9180 - val_loss: 0.3411 - val_accuracy: 0.8980\n",
      "Epoch 431/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2988 - accuracy: 0.9143\n",
      "Epoch 00431: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2976 - accuracy: 0.9152 - val_loss: 0.3373 - val_accuracy: 0.8971\n",
      "Epoch 432/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.9150\n",
      "Epoch 00432: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3004 - accuracy: 0.9140 - val_loss: 0.3393 - val_accuracy: 0.8990\n",
      "Epoch 433/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.9168\n",
      "Epoch 00433: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2979 - accuracy: 0.9149 - val_loss: 0.3358 - val_accuracy: 0.9027\n",
      "Epoch 434/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2947 - accuracy: 0.9182\n",
      "Epoch 00434: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2963 - accuracy: 0.9171 - val_loss: 0.3409 - val_accuracy: 0.8961\n",
      "Epoch 435/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.9072\n",
      "Epoch 00435: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3104 - accuracy: 0.9076 - val_loss: 0.3341 - val_accuracy: 0.9018\n",
      "Epoch 436/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2977 - accuracy: 0.9143\n",
      "Epoch 00436: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2989 - accuracy: 0.9138 - val_loss: 0.3540 - val_accuracy: 0.8867\n",
      "Epoch 437/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2963 - accuracy: 0.9160\n",
      "Epoch 00437: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2955 - accuracy: 0.9159 - val_loss: 0.3394 - val_accuracy: 0.8980\n",
      "Epoch 438/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.9123\n",
      "Epoch 00438: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3004 - accuracy: 0.9135 - val_loss: 0.3420 - val_accuracy: 0.8990\n",
      "Epoch 439/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.9150\n",
      "Epoch 00439: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2957 - accuracy: 0.9152 - val_loss: 0.3502 - val_accuracy: 0.8933\n",
      "Epoch 440/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.9175\n",
      "Epoch 00440: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2984 - accuracy: 0.9178 - val_loss: 0.3436 - val_accuracy: 0.8999\n",
      "Epoch 441/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.9170\n",
      "Epoch 00441: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2970 - accuracy: 0.9173 - val_loss: 0.3473 - val_accuracy: 0.8924\n",
      "Epoch 442/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3013 - accuracy: 0.9092\n",
      "Epoch 00442: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.3010 - accuracy: 0.9100 - val_loss: 0.3660 - val_accuracy: 0.8914\n",
      "Epoch 443/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2942 - accuracy: 0.9141\n",
      "Epoch 00443: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2959 - accuracy: 0.9126 - val_loss: 0.3383 - val_accuracy: 0.8999\n",
      "Epoch 444/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.9086\n",
      "Epoch 00444: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3019 - accuracy: 0.9083 - val_loss: 0.3379 - val_accuracy: 0.8999\n",
      "Epoch 445/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.9132\n",
      "Epoch 00445: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3035 - accuracy: 0.9133 - val_loss: 0.3390 - val_accuracy: 0.8990\n",
      "Epoch 446/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2948 - accuracy: 0.9175\n",
      "Epoch 00446: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2948 - accuracy: 0.9171 - val_loss: 0.3548 - val_accuracy: 0.8876\n",
      "Epoch 447/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.9148\n",
      "Epoch 00447: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3035 - accuracy: 0.9149 - val_loss: 0.3531 - val_accuracy: 0.8886\n",
      "Epoch 448/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2949 - accuracy: 0.9138\n",
      "Epoch 00448: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2948 - accuracy: 0.9133 - val_loss: 0.3442 - val_accuracy: 0.8914\n",
      "Epoch 449/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.9155\n",
      "Epoch 00449: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3027 - accuracy: 0.9147 - val_loss: 0.3434 - val_accuracy: 0.8924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9169\n",
      "Epoch 00450: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2937 - accuracy: 0.9161 - val_loss: 0.3433 - val_accuracy: 0.8924\n",
      "Epoch 451/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.9121\n",
      "Epoch 00451: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2946 - accuracy: 0.9128 - val_loss: 0.3424 - val_accuracy: 0.8961\n",
      "Epoch 452/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.9161\n",
      "Epoch 00452: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2971 - accuracy: 0.9161 - val_loss: 0.3518 - val_accuracy: 0.8952\n",
      "Epoch 453/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.9132\n",
      "Epoch 00453: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2985 - accuracy: 0.9138 - val_loss: 0.3569 - val_accuracy: 0.8848\n",
      "Epoch 454/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2874 - accuracy: 0.9221\n",
      "Epoch 00454: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2885 - accuracy: 0.9211 - val_loss: 0.3463 - val_accuracy: 0.8952\n",
      "Epoch 455/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.9128\n",
      "Epoch 00455: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2967 - accuracy: 0.9112 - val_loss: 0.3505 - val_accuracy: 0.8942\n",
      "Epoch 456/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.9179\n",
      "Epoch 00456: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2970 - accuracy: 0.9178 - val_loss: 0.3348 - val_accuracy: 0.8942\n",
      "Epoch 457/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9194\n",
      "Epoch 00457: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2903 - accuracy: 0.9194 - val_loss: 0.3501 - val_accuracy: 0.8933\n",
      "Epoch 458/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.9145\n",
      "Epoch 00458: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2995 - accuracy: 0.9145 - val_loss: 0.3419 - val_accuracy: 0.8952\n",
      "Epoch 459/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2942 - accuracy: 0.9128\n",
      "Epoch 00459: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2939 - accuracy: 0.9130 - val_loss: 0.3471 - val_accuracy: 0.8961\n",
      "Epoch 460/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.9181\n",
      "Epoch 00460: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2954 - accuracy: 0.9182 - val_loss: 0.3403 - val_accuracy: 0.8990\n",
      "Epoch 461/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.9172\n",
      "Epoch 00461: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2940 - accuracy: 0.9180 - val_loss: 0.3396 - val_accuracy: 0.9008\n",
      "Epoch 462/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9145\n",
      "Epoch 00462: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2925 - accuracy: 0.9147 - val_loss: 0.3448 - val_accuracy: 0.8961\n",
      "Epoch 463/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9174\n",
      "Epoch 00463: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2951 - accuracy: 0.9175 - val_loss: 0.3605 - val_accuracy: 0.8876\n",
      "Epoch 464/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9189\n",
      "Epoch 00464: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2885 - accuracy: 0.9187 - val_loss: 0.3436 - val_accuracy: 0.8961\n",
      "Epoch 465/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2894 - accuracy: 0.9180\n",
      "Epoch 00465: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2897 - accuracy: 0.9182 - val_loss: 0.3438 - val_accuracy: 0.8933\n",
      "Epoch 466/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9213\n",
      "Epoch 00466: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2860 - accuracy: 0.9216 - val_loss: 0.3481 - val_accuracy: 0.8876\n",
      "Epoch 467/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9182\n",
      "Epoch 00467: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2884 - accuracy: 0.9182 - val_loss: 0.3473 - val_accuracy: 0.8857\n",
      "Epoch 468/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2876 - accuracy: 0.9209\n",
      "Epoch 00468: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2879 - accuracy: 0.9204 - val_loss: 0.3425 - val_accuracy: 0.8914\n",
      "Epoch 469/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2920 - accuracy: 0.9177\n",
      "Epoch 00469: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2919 - accuracy: 0.9182 - val_loss: 0.3692 - val_accuracy: 0.8791\n",
      "Epoch 470/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9144\n",
      "Epoch 00470: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2891 - accuracy: 0.9138 - val_loss: 0.3613 - val_accuracy: 0.8867\n",
      "Epoch 471/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9162\n",
      "Epoch 00471: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2915 - accuracy: 0.9168 - val_loss: 0.3395 - val_accuracy: 0.8961\n",
      "Epoch 472/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.9167\n",
      "Epoch 00472: val_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2933 - accuracy: 0.9166 - val_loss: 0.3378 - val_accuracy: 0.8924\n",
      "Epoch 473/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.9138\n",
      "Epoch 00473: val_accuracy improved from 0.90274 to 0.90368, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2964 - accuracy: 0.9130 - val_loss: 0.3494 - val_accuracy: 0.9037\n",
      "Epoch 474/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.9127\n",
      "Epoch 00474: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.3001 - accuracy: 0.9130 - val_loss: 0.3417 - val_accuracy: 0.8952\n",
      "Epoch 475/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.9142\n",
      "Epoch 00475: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2988 - accuracy: 0.9142 - val_loss: 0.3302 - val_accuracy: 0.8971\n",
      "Epoch 476/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.9156\n",
      "Epoch 00476: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2992 - accuracy: 0.9149 - val_loss: 0.3321 - val_accuracy: 0.8971\n",
      "Epoch 477/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9140\n",
      "Epoch 00477: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2979 - accuracy: 0.9126 - val_loss: 0.3376 - val_accuracy: 0.8961\n",
      "Epoch 478/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9188\n",
      "Epoch 00478: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2862 - accuracy: 0.9187 - val_loss: 0.3445 - val_accuracy: 0.8980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.9156\n",
      "Epoch 00479: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2887 - accuracy: 0.9156 - val_loss: 0.3383 - val_accuracy: 0.8971\n",
      "Epoch 480/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.9142\n",
      "Epoch 00480: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2917 - accuracy: 0.9149 - val_loss: 0.3350 - val_accuracy: 0.9008\n",
      "Epoch 481/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9169\n",
      "Epoch 00481: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2887 - accuracy: 0.9171 - val_loss: 0.3435 - val_accuracy: 0.8952\n",
      "Epoch 482/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.9136\n",
      "Epoch 00482: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2963 - accuracy: 0.9138 - val_loss: 0.3317 - val_accuracy: 0.8952\n",
      "Epoch 483/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9184\n",
      "Epoch 00483: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2878 - accuracy: 0.9190 - val_loss: 0.3422 - val_accuracy: 0.8905\n",
      "Epoch 484/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9188\n",
      "Epoch 00484: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2875 - accuracy: 0.9201 - val_loss: 0.3410 - val_accuracy: 0.8961\n",
      "Epoch 485/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9118\n",
      "Epoch 00485: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2892 - accuracy: 0.9133 - val_loss: 0.3616 - val_accuracy: 0.8895\n",
      "Epoch 486/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9113\n",
      "Epoch 00486: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2912 - accuracy: 0.9119 - val_loss: 0.3377 - val_accuracy: 0.8924\n",
      "Epoch 487/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.9175\n",
      "Epoch 00487: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2913 - accuracy: 0.9175 - val_loss: 0.3389 - val_accuracy: 0.8905\n",
      "Epoch 488/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2811 - accuracy: 0.9241\n",
      "Epoch 00488: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2811 - accuracy: 0.9241 - val_loss: 0.3586 - val_accuracy: 0.8914\n",
      "Epoch 489/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2830 - accuracy: 0.9219\n",
      "Epoch 00489: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2833 - accuracy: 0.9216 - val_loss: 0.3399 - val_accuracy: 0.8952\n",
      "Epoch 490/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.9164\n",
      "Epoch 00490: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2948 - accuracy: 0.9161 - val_loss: 0.3563 - val_accuracy: 0.8820\n",
      "Epoch 491/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.9208\n",
      "Epoch 00491: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2893 - accuracy: 0.9208 - val_loss: 0.3510 - val_accuracy: 0.8924\n",
      "Epoch 492/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.9172\n",
      "Epoch 00492: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2825 - accuracy: 0.9173 - val_loss: 0.3394 - val_accuracy: 0.9018\n",
      "Epoch 493/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9165\n",
      "Epoch 00493: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2901 - accuracy: 0.9161 - val_loss: 0.3766 - val_accuracy: 0.8876\n",
      "Epoch 494/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.9107\n",
      "Epoch 00494: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2933 - accuracy: 0.9104 - val_loss: 0.3399 - val_accuracy: 0.8990\n",
      "Epoch 495/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2899 - accuracy: 0.9175\n",
      "Epoch 00495: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2883 - accuracy: 0.9187 - val_loss: 0.3504 - val_accuracy: 0.8942\n",
      "Epoch 496/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9158\n",
      "Epoch 00496: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.2864 - accuracy: 0.9164 - val_loss: 0.3524 - val_accuracy: 0.8961\n",
      "Epoch 497/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9180\n",
      "Epoch 00497: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2850 - accuracy: 0.9187 - val_loss: 0.3427 - val_accuracy: 0.8971\n",
      "Epoch 498/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9202\n",
      "Epoch 00498: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2887 - accuracy: 0.9199 - val_loss: 0.3352 - val_accuracy: 0.8999\n",
      "Epoch 499/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9238\n",
      "Epoch 00499: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2867 - accuracy: 0.9230 - val_loss: 0.3410 - val_accuracy: 0.8942\n",
      "Epoch 500/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9160\n",
      "Epoch 00500: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.2904 - accuracy: 0.9159 - val_loss: 0.3594 - val_accuracy: 0.8961\n"
     ]
    }
   ],
   "source": [
    "def nlp_lstm(w2v):\n",
    "    inputs = Input(shape=(X_train[0].shape[-1],))\n",
    "\n",
    "    embedding_layer = gensim_to_keras_embedding(w2v)\n",
    "    \n",
    "    embedding = embedding_layer(inputs)\n",
    "\n",
    "    lstm1 = LSTM(lstm_units,return_sequences=True, return_state=True, kernel_regularizer=l2(w_decay),recurrent_regularizer=l2(w_decay), dropout=dropout_rate)(embedding)\n",
    "    output = Dense(units=1, activation='sigmoid')(lstm1[1])\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = nlp_lstm(w2v_model)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./weight_cp/weight_lstm1.hdf5', save_freq=\"epoch\",  verbose=1, monitor='val_accuracy', save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(0.0001)\n",
    "model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=epochs_to_run, validation_data=(X_val, y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd2d3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97d4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF LSTM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89       661\n",
      "           1       0.92      0.84      0.88       662\n",
      "\n",
      "    accuracy                           0.89      1323\n",
      "   macro avg       0.89      0.89      0.88      1323\n",
      "weighted avg       0.89      0.89      0.88      1323\n",
      "\n",
      "0.8851095993953136\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./weight_cp/weight_lstm1.hdf5')\n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "y_pred = []\n",
    "for p in predictions:\n",
    "    y_pred.append(p[0])\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"CLASSIFICATION REPORT OF LSTM\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41679010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 80)\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     14114800    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200, 10), (N 4440        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 200, 1)       11          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 200)          0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 200)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "before_split (Activation)       (None, 200)          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 20), (None,  0           before_split[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 80)           0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][1]          \n",
      "                                                                 tf_op_layer_split[0][8]          \n",
      "                                                                 tf_op_layer_split[0][9]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 8, 10, 1)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 8, 10, 2)     130         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 8, 10, 2)     8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 160)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "op_main (Dense)                 (None, 1)            11          lstm_1[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "op_conv (Dense)                 (None, 1)            161         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "avg (Average)                   (None, 1)            0           op_main[0][0]                    \n",
      "                                                                 op_conv[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,119,561\n",
      "Trainable params: 4,757\n",
      "Non-trainable params: 14,114,804\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 5.4637 - op_main_loss: 0.7881 - op_conv_loss: 0.6999 - avg_loss: 0.7167 - op_main_accuracy: 0.4991 - op_conv_accuracy: 0.4986 - avg_accuracy: 0.5031\n",
      "Epoch 00001: val_avg_accuracy improved from -inf to 0.51747, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 3s 21ms/step - loss: 5.4636 - op_main_loss: 0.7883 - op_conv_loss: 0.6999 - avg_loss: 0.7168 - op_main_accuracy: 0.4986 - op_conv_accuracy: 0.4991 - avg_accuracy: 0.5026 - val_loss: 5.2152 - val_op_main_loss: 0.7623 - val_op_conv_loss: 0.6932 - val_avg_loss: 0.7099 - val_op_main_accuracy: 0.5175 - val_op_conv_accuracy: 0.4995 - val_avg_accuracy: 0.5175\n",
      "Epoch 2/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 5.0064 - op_main_loss: 0.7399 - op_conv_loss: 0.6980 - avg_loss: 0.7061 - op_main_accuracy: 0.5062 - op_conv_accuracy: 0.4950 - avg_accuracy: 0.5054\n",
      "Epoch 00002: val_avg_accuracy did not improve from 0.51747\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 5.0060 - op_main_loss: 0.7399 - op_conv_loss: 0.6980 - avg_loss: 0.7060 - op_main_accuracy: 0.5059 - op_conv_accuracy: 0.4950 - avg_accuracy: 0.5054 - val_loss: 4.7727 - val_op_main_loss: 0.7082 - val_op_conv_loss: 0.6933 - val_avg_loss: 0.6951 - val_op_main_accuracy: 0.5184 - val_op_conv_accuracy: 0.4958 - val_avg_accuracy: 0.5156\n",
      "Epoch 3/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 4.6089 - op_main_loss: 0.7041 - op_conv_loss: 0.6984 - avg_loss: 0.6952 - op_main_accuracy: 0.5174 - op_conv_accuracy: 0.4917 - avg_accuracy: 0.5150\n",
      "Epoch 00003: val_avg_accuracy improved from 0.51747 to 0.54108, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 4.6068 - op_main_loss: 0.7039 - op_conv_loss: 0.6983 - avg_loss: 0.6951 - op_main_accuracy: 0.5175 - op_conv_accuracy: 0.4920 - avg_accuracy: 0.5163 - val_loss: 4.4112 - val_op_main_loss: 0.6859 - val_op_conv_loss: 0.6936 - val_avg_loss: 0.6864 - val_op_main_accuracy: 0.5430 - val_op_conv_accuracy: 0.5024 - val_avg_accuracy: 0.5411\n",
      "Epoch 4/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 4.2714 - op_main_loss: 0.6899 - op_conv_loss: 0.6949 - avg_loss: 0.6874 - op_main_accuracy: 0.5421 - op_conv_accuracy: 0.4976 - avg_accuracy: 0.5373\n",
      "Epoch 00004: val_avg_accuracy improved from 0.54108 to 0.56374, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 4.2714 - op_main_loss: 0.6899 - op_conv_loss: 0.6949 - avg_loss: 0.6874 - op_main_accuracy: 0.5421 - op_conv_accuracy: 0.4976 - avg_accuracy: 0.5373 - val_loss: 4.1087 - val_op_main_loss: 0.6770 - val_op_conv_loss: 0.6940 - val_avg_loss: 0.6822 - val_op_main_accuracy: 0.5703 - val_op_conv_accuracy: 0.4958 - val_avg_accuracy: 0.5637\n",
      "Epoch 5/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 3.9831 - op_main_loss: 0.6790 - op_conv_loss: 0.6944 - avg_loss: 0.6821 - op_main_accuracy: 0.5662 - op_conv_accuracy: 0.4924 - avg_accuracy: 0.5685\n",
      "Epoch 00005: val_avg_accuracy improved from 0.56374 to 0.60340, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 3.9831 - op_main_loss: 0.6790 - op_conv_loss: 0.6944 - avg_loss: 0.6821 - op_main_accuracy: 0.5662 - op_conv_accuracy: 0.4924 - avg_accuracy: 0.5685 - val_loss: 3.8419 - val_op_main_loss: 0.6681 - val_op_conv_loss: 0.6945 - val_avg_loss: 0.6777 - val_op_main_accuracy: 0.6062 - val_op_conv_accuracy: 0.4910 - val_avg_accuracy: 0.6034\n",
      "Epoch 6/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 3.7280 - op_main_loss: 0.6697 - op_conv_loss: 0.6925 - avg_loss: 0.6765 - op_main_accuracy: 0.5903 - op_conv_accuracy: 0.4981 - avg_accuracy: 0.5929\n",
      "Epoch 00006: val_avg_accuracy improved from 0.60340 to 0.62701, saving model to ./weight_cp\\weight_lstm2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 16ms/step - loss: 3.7280 - op_main_loss: 0.6697 - op_conv_loss: 0.6925 - avg_loss: 0.6765 - op_main_accuracy: 0.5903 - op_conv_accuracy: 0.4981 - avg_accuracy: 0.5929 - val_loss: 3.6055 - val_op_main_loss: 0.6592 - val_op_conv_loss: 0.6944 - val_avg_loss: 0.6729 - val_op_main_accuracy: 0.6185 - val_op_conv_accuracy: 0.4910 - val_avg_accuracy: 0.6270\n",
      "Epoch 7/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 3.4997 - op_main_loss: 0.6565 - op_conv_loss: 0.6918 - avg_loss: 0.6695 - op_main_accuracy: 0.6157 - op_conv_accuracy: 0.5167 - avg_accuracy: 0.6217\n",
      "Epoch 00007: val_avg_accuracy improved from 0.62701 to 0.63834, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 3.4991 - op_main_loss: 0.6566 - op_conv_loss: 0.6919 - avg_loss: 0.6696 - op_main_accuracy: 0.6158 - op_conv_accuracy: 0.5163 - avg_accuracy: 0.6215 - val_loss: 3.3964 - val_op_main_loss: 0.6501 - val_op_conv_loss: 0.6939 - val_avg_loss: 0.6677 - val_op_main_accuracy: 0.6327 - val_op_conv_accuracy: 0.5165 - val_avg_accuracy: 0.6383\n",
      "Epoch 8/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 3.3034 - op_main_loss: 0.6485 - op_conv_loss: 0.6911 - avg_loss: 0.6650 - op_main_accuracy: 0.6233 - op_conv_accuracy: 0.5295 - avg_accuracy: 0.6293\n",
      "Epoch 00008: val_avg_accuracy improved from 0.63834 to 0.65250, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 3.3034 - op_main_loss: 0.6485 - op_conv_loss: 0.6911 - avg_loss: 0.6650 - op_main_accuracy: 0.6233 - op_conv_accuracy: 0.5295 - avg_accuracy: 0.6293 - val_loss: 3.2099 - val_op_main_loss: 0.6404 - val_op_conv_loss: 0.6928 - val_avg_loss: 0.6620 - val_op_main_accuracy: 0.6364 - val_op_conv_accuracy: 0.5307 - val_avg_accuracy: 0.6525\n",
      "Epoch 9/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 3.1317 - op_main_loss: 0.6416 - op_conv_loss: 0.6898 - avg_loss: 0.6605 - op_main_accuracy: 0.6368 - op_conv_accuracy: 0.5354 - avg_accuracy: 0.6363\n",
      "Epoch 00009: val_avg_accuracy improved from 0.65250 to 0.66950, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 3.1317 - op_main_loss: 0.6416 - op_conv_loss: 0.6898 - avg_loss: 0.6605 - op_main_accuracy: 0.6368 - op_conv_accuracy: 0.5354 - avg_accuracy: 0.6363 - val_loss: 3.0410 - val_op_main_loss: 0.6305 - val_op_conv_loss: 0.6896 - val_avg_loss: 0.6545 - val_op_main_accuracy: 0.6572 - val_op_conv_accuracy: 0.5600 - val_avg_accuracy: 0.6695\n",
      "Epoch 10/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.9663 - op_main_loss: 0.6309 - op_conv_loss: 0.6829 - avg_loss: 0.6509 - op_main_accuracy: 0.6519 - op_conv_accuracy: 0.5818 - avg_accuracy: 0.6612\n",
      "Epoch 00010: val_avg_accuracy improved from 0.66950 to 0.68555, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.9663 - op_main_loss: 0.6309 - op_conv_loss: 0.6829 - avg_loss: 0.6509 - op_main_accuracy: 0.6519 - op_conv_accuracy: 0.5818 - avg_accuracy: 0.6612 - val_loss: 2.8778 - val_op_main_loss: 0.6189 - val_op_conv_loss: 0.6783 - val_avg_loss: 0.6418 - val_op_main_accuracy: 0.6676 - val_op_conv_accuracy: 0.6053 - val_avg_accuracy: 0.6856\n",
      "Epoch 11/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.8065 - op_main_loss: 0.6186 - op_conv_loss: 0.6678 - avg_loss: 0.6355 - op_main_accuracy: 0.6715 - op_conv_accuracy: 0.6083 - avg_accuracy: 0.6877\n",
      "Epoch 00011: val_avg_accuracy improved from 0.68555 to 0.70349, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.8053 - op_main_loss: 0.6184 - op_conv_loss: 0.6675 - avg_loss: 0.6353 - op_main_accuracy: 0.6720 - op_conv_accuracy: 0.6094 - avg_accuracy: 0.6886 - val_loss: 2.7154 - val_op_main_loss: 0.6115 - val_op_conv_loss: 0.6509 - val_avg_loss: 0.6219 - val_op_main_accuracy: 0.6912 - val_op_conv_accuracy: 0.6619 - val_avg_accuracy: 0.7035\n",
      "Epoch 12/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 2.6613 - op_main_loss: 0.6120 - op_conv_loss: 0.6440 - avg_loss: 0.6186 - op_main_accuracy: 0.6764 - op_conv_accuracy: 0.6490 - avg_accuracy: 0.7026\n",
      "Epoch 00012: val_avg_accuracy improved from 0.70349 to 0.75165, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.6592 - op_main_loss: 0.6116 - op_conv_loss: 0.6435 - avg_loss: 0.6181 - op_main_accuracy: 0.6777 - op_conv_accuracy: 0.6496 - avg_accuracy: 0.7034 - val_loss: 2.5471 - val_op_main_loss: 0.5995 - val_op_conv_loss: 0.6106 - val_avg_loss: 0.5944 - val_op_main_accuracy: 0.7073 - val_op_conv_accuracy: 0.7139 - val_avg_accuracy: 0.7517\n",
      "Epoch 13/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.5025 - op_main_loss: 0.5981 - op_conv_loss: 0.6078 - avg_loss: 0.5908 - op_main_accuracy: 0.6992 - op_conv_accuracy: 0.6883 - avg_accuracy: 0.7330\n",
      "Epoch 00013: val_avg_accuracy improved from 0.75165 to 0.75826, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.5025 - op_main_loss: 0.5981 - op_conv_loss: 0.6078 - avg_loss: 0.5908 - op_main_accuracy: 0.6992 - op_conv_accuracy: 0.6883 - avg_accuracy: 0.7330 - val_loss: 2.3970 - val_op_main_loss: 0.5893 - val_op_conv_loss: 0.5736 - val_avg_loss: 0.5633 - val_op_main_accuracy: 0.7063 - val_op_conv_accuracy: 0.7299 - val_avg_accuracy: 0.7583\n",
      "Epoch 14/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 2.3578 - op_main_loss: 0.5896 - op_conv_loss: 0.5661 - avg_loss: 0.5610 - op_main_accuracy: 0.7034 - op_conv_accuracy: 0.7202 - avg_accuracy: 0.7493\n",
      "Epoch 00014: val_avg_accuracy improved from 0.75826 to 0.79131, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.3572 - op_main_loss: 0.5895 - op_conv_loss: 0.5659 - avg_loss: 0.5608 - op_main_accuracy: 0.7034 - op_conv_accuracy: 0.7202 - avg_accuracy: 0.7495 - val_loss: 2.2364 - val_op_main_loss: 0.5765 - val_op_conv_loss: 0.5199 - val_avg_loss: 0.5281 - val_op_main_accuracy: 0.7195 - val_op_conv_accuracy: 0.7639 - val_avg_accuracy: 0.7913\n",
      "Epoch 15/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.2465 - op_main_loss: 0.5785 - op_conv_loss: 0.5390 - avg_loss: 0.5411 - op_main_accuracy: 0.7183 - op_conv_accuracy: 0.7288 - avg_accuracy: 0.7562\n",
      "Epoch 00015: val_avg_accuracy improved from 0.79131 to 0.79792, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.2426 - op_main_loss: 0.5778 - op_conv_loss: 0.5372 - avg_loss: 0.5399 - op_main_accuracy: 0.7198 - op_conv_accuracy: 0.7306 - avg_accuracy: 0.7578 - val_loss: 2.1247 - val_op_main_loss: 0.5670 - val_op_conv_loss: 0.4895 - val_avg_loss: 0.5034 - val_op_main_accuracy: 0.7299 - val_op_conv_accuracy: 0.7753 - val_avg_accuracy: 0.7979\n",
      "Epoch 16/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 2.1214 - op_main_loss: 0.5663 - op_conv_loss: 0.4987 - avg_loss: 0.5109 - op_main_accuracy: 0.7332 - op_conv_accuracy: 0.7589 - avg_accuracy: 0.7849\n",
      "Epoch 00016: val_avg_accuracy improved from 0.79792 to 0.81114, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.1183 - op_main_loss: 0.5656 - op_conv_loss: 0.4975 - avg_loss: 0.5100 - op_main_accuracy: 0.7346 - op_conv_accuracy: 0.7595 - avg_accuracy: 0.7852 - val_loss: 2.0223 - val_op_main_loss: 0.5576 - val_op_conv_loss: 0.4596 - val_avg_loss: 0.4790 - val_op_main_accuracy: 0.7403 - val_op_conv_accuracy: 0.7904 - val_avg_accuracy: 0.8111\n",
      "Epoch 17/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.0449 - op_main_loss: 0.5581 - op_conv_loss: 0.4823 - avg_loss: 0.4954 - op_main_accuracy: 0.7352 - op_conv_accuracy: 0.7748 - avg_accuracy: 0.7968\n",
      "Epoch 00017: val_avg_accuracy improved from 0.81114 to 0.82625, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 2.0442 - op_main_loss: 0.5582 - op_conv_loss: 0.4818 - avg_loss: 0.4953 - op_main_accuracy: 0.7353 - op_conv_accuracy: 0.7758 - avg_accuracy: 0.7973 - val_loss: 1.9303 - val_op_main_loss: 0.5468 - val_op_conv_loss: 0.4300 - val_avg_loss: 0.4608 - val_op_main_accuracy: 0.7460 - val_op_conv_accuracy: 0.8017 - val_avg_accuracy: 0.8263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.9596 - op_main_loss: 0.5481 - op_conv_loss: 0.4563 - avg_loss: 0.4768 - op_main_accuracy: 0.7440 - op_conv_accuracy: 0.7908 - avg_accuracy: 0.8101\n",
      "Epoch 00018: val_avg_accuracy improved from 0.82625 to 0.82814, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.9589 - op_main_loss: 0.5481 - op_conv_loss: 0.4558 - avg_loss: 0.4766 - op_main_accuracy: 0.7434 - op_conv_accuracy: 0.7909 - avg_accuracy: 0.8098 - val_loss: 1.8608 - val_op_main_loss: 0.5383 - val_op_conv_loss: 0.4124 - val_avg_loss: 0.4457 - val_op_main_accuracy: 0.7498 - val_op_conv_accuracy: 0.8083 - val_avg_accuracy: 0.8281\n",
      "Epoch 19/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.9057 - op_main_loss: 0.5438 - op_conv_loss: 0.4430 - avg_loss: 0.4667 - op_main_accuracy: 0.7466 - op_conv_accuracy: 0.7906 - avg_accuracy: 0.8075\n",
      "Epoch 00019: val_avg_accuracy did not improve from 0.82814\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.9057 - op_main_loss: 0.5433 - op_conv_loss: 0.4437 - avg_loss: 0.4668 - op_main_accuracy: 0.7474 - op_conv_accuracy: 0.7906 - avg_accuracy: 0.8077 - val_loss: 1.8120 - val_op_main_loss: 0.5318 - val_op_conv_loss: 0.4022 - val_avg_loss: 0.4379 - val_op_main_accuracy: 0.7488 - val_op_conv_accuracy: 0.8196 - val_avg_accuracy: 0.8225\n",
      "Epoch 20/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.8507 - op_main_loss: 0.5368 - op_conv_loss: 0.4279 - avg_loss: 0.4561 - op_main_accuracy: 0.7526 - op_conv_accuracy: 0.8044 - avg_accuracy: 0.8201\n",
      "Epoch 00020: val_avg_accuracy improved from 0.82814 to 0.83381, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.8507 - op_main_loss: 0.5370 - op_conv_loss: 0.4277 - avg_loss: 0.4562 - op_main_accuracy: 0.7528 - op_conv_accuracy: 0.8046 - avg_accuracy: 0.8202 - val_loss: 1.7574 - val_op_main_loss: 0.5221 - val_op_conv_loss: 0.3883 - val_avg_loss: 0.4277 - val_op_main_accuracy: 0.7696 - val_op_conv_accuracy: 0.8253 - val_avg_accuracy: 0.8338\n",
      "Epoch 21/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.8114 - op_main_loss: 0.5291 - op_conv_loss: 0.4235 - avg_loss: 0.4487 - op_main_accuracy: 0.7643 - op_conv_accuracy: 0.8089 - avg_accuracy: 0.8218\n",
      "Epoch 00021: val_avg_accuracy improved from 0.83381 to 0.84230, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.8095 - op_main_loss: 0.5286 - op_conv_loss: 0.4227 - avg_loss: 0.4482 - op_main_accuracy: 0.7654 - op_conv_accuracy: 0.8098 - avg_accuracy: 0.8228 - val_loss: 1.7140 - val_op_main_loss: 0.5155 - val_op_conv_loss: 0.3782 - val_avg_loss: 0.4197 - val_op_main_accuracy: 0.7686 - val_op_conv_accuracy: 0.8310 - val_avg_accuracy: 0.8423\n",
      "Epoch 22/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.7514 - op_main_loss: 0.5214 - op_conv_loss: 0.4031 - avg_loss: 0.4342 - op_main_accuracy: 0.7700 - op_conv_accuracy: 0.8280 - avg_accuracy: 0.8325\n",
      "Epoch 00022: val_avg_accuracy did not improve from 0.84230\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.7499 - op_main_loss: 0.5216 - op_conv_loss: 0.4020 - avg_loss: 0.4337 - op_main_accuracy: 0.7698 - op_conv_accuracy: 0.8289 - avg_accuracy: 0.8334 - val_loss: 1.6805 - val_op_main_loss: 0.5108 - val_op_conv_loss: 0.3723 - val_avg_loss: 0.4128 - val_op_main_accuracy: 0.7658 - val_op_conv_accuracy: 0.8300 - val_avg_accuracy: 0.8395\n",
      "Epoch 23/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.7259 - op_main_loss: 0.5182 - op_conv_loss: 0.3998 - avg_loss: 0.4307 - op_main_accuracy: 0.7696 - op_conv_accuracy: 0.8224 - avg_accuracy: 0.8303\n",
      "Epoch 00023: val_avg_accuracy improved from 0.84230 to 0.84419, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.7268 - op_main_loss: 0.5185 - op_conv_loss: 0.4002 - avg_loss: 0.4309 - op_main_accuracy: 0.7694 - op_conv_accuracy: 0.8225 - avg_accuracy: 0.8301 - val_loss: 1.6392 - val_op_main_loss: 0.5043 - val_op_conv_loss: 0.3604 - val_avg_loss: 0.4047 - val_op_main_accuracy: 0.7743 - val_op_conv_accuracy: 0.8366 - val_avg_accuracy: 0.8442\n",
      "Epoch 24/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.6770 - op_main_loss: 0.5120 - op_conv_loss: 0.3814 - avg_loss: 0.4201 - op_main_accuracy: 0.7800 - op_conv_accuracy: 0.8289 - avg_accuracy: 0.8341\n",
      "Epoch 00024: val_avg_accuracy improved from 0.84419 to 0.84891, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.6770 - op_main_loss: 0.5120 - op_conv_loss: 0.3814 - avg_loss: 0.4201 - op_main_accuracy: 0.7800 - op_conv_accuracy: 0.8289 - avg_accuracy: 0.8341 - val_loss: 1.6065 - val_op_main_loss: 0.4969 - val_op_conv_loss: 0.3551 - val_avg_loss: 0.3971 - val_op_main_accuracy: 0.7923 - val_op_conv_accuracy: 0.8442 - val_avg_accuracy: 0.8489\n",
      "Epoch 25/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.6600 - op_main_loss: 0.5071 - op_conv_loss: 0.3843 - avg_loss: 0.4167 - op_main_accuracy: 0.7752 - op_conv_accuracy: 0.8313 - avg_accuracy: 0.8397\n",
      "Epoch 00025: val_avg_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.6589 - op_main_loss: 0.5070 - op_conv_loss: 0.3839 - avg_loss: 0.4163 - op_main_accuracy: 0.7762 - op_conv_accuracy: 0.8318 - avg_accuracy: 0.8400 - val_loss: 1.5763 - val_op_main_loss: 0.4915 - val_op_conv_loss: 0.3466 - val_avg_loss: 0.3920 - val_op_main_accuracy: 0.7941 - val_op_conv_accuracy: 0.8451 - val_avg_accuracy: 0.8470\n",
      "Epoch 26/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.6325 - op_main_loss: 0.4999 - op_conv_loss: 0.3800 - avg_loss: 0.4114 - op_main_accuracy: 0.7803 - op_conv_accuracy: 0.8318 - avg_accuracy: 0.8399\n",
      "Epoch 00026: val_avg_accuracy improved from 0.84891 to 0.85175, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.6315 - op_main_loss: 0.5001 - op_conv_loss: 0.3791 - avg_loss: 0.4111 - op_main_accuracy: 0.7802 - op_conv_accuracy: 0.8325 - avg_accuracy: 0.8405 - val_loss: 1.5520 - val_op_main_loss: 0.4871 - val_op_conv_loss: 0.3414 - val_avg_loss: 0.3875 - val_op_main_accuracy: 0.7923 - val_op_conv_accuracy: 0.8470 - val_avg_accuracy: 0.8517\n",
      "Epoch 27/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.6117 - op_main_loss: 0.4998 - op_conv_loss: 0.3725 - avg_loss: 0.4082 - op_main_accuracy: 0.7855 - op_conv_accuracy: 0.8350 - avg_accuracy: 0.8435\n",
      "Epoch 00027: val_avg_accuracy improved from 0.85175 to 0.85741, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 1.6112 - op_main_loss: 0.4997 - op_conv_loss: 0.3722 - avg_loss: 0.4080 - op_main_accuracy: 0.7859 - op_conv_accuracy: 0.8351 - avg_accuracy: 0.8438 - val_loss: 1.5297 - val_op_main_loss: 0.4839 - val_op_conv_loss: 0.3367 - val_avg_loss: 0.3823 - val_op_main_accuracy: 0.7951 - val_op_conv_accuracy: 0.8489 - val_avg_accuracy: 0.8574\n",
      "Epoch 28/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.5820 - op_main_loss: 0.4946 - op_conv_loss: 0.3636 - avg_loss: 0.4012 - op_main_accuracy: 0.7860 - op_conv_accuracy: 0.8421 - avg_accuracy: 0.8433\n",
      "Epoch 00028: val_avg_accuracy did not improve from 0.85741\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.5813 - op_main_loss: 0.4943 - op_conv_loss: 0.3634 - avg_loss: 0.4010 - op_main_accuracy: 0.7864 - op_conv_accuracy: 0.8424 - avg_accuracy: 0.8436 - val_loss: 1.5086 - val_op_main_loss: 0.4782 - val_op_conv_loss: 0.3339 - val_avg_loss: 0.3780 - val_op_main_accuracy: 0.7989 - val_op_conv_accuracy: 0.8480 - val_avg_accuracy: 0.8517\n",
      "Epoch 29/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.5616 - op_main_loss: 0.4894 - op_conv_loss: 0.3609 - avg_loss: 0.3963 - op_main_accuracy: 0.7895 - op_conv_accuracy: 0.8435 - avg_accuracy: 0.8459\n",
      "Epoch 00029: val_avg_accuracy did not improve from 0.85741\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.5634 - op_main_loss: 0.4891 - op_conv_loss: 0.3627 - avg_loss: 0.3966 - op_main_accuracy: 0.7899 - op_conv_accuracy: 0.8433 - avg_accuracy: 0.8455 - val_loss: 1.4937 - val_op_main_loss: 0.4739 - val_op_conv_loss: 0.3337 - val_avg_loss: 0.3745 - val_op_main_accuracy: 0.8017 - val_op_conv_accuracy: 0.8508 - val_avg_accuracy: 0.8499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.5501 - op_main_loss: 0.4868 - op_conv_loss: 0.3589 - avg_loss: 0.3964 - op_main_accuracy: 0.7863 - op_conv_accuracy: 0.8496 - avg_accuracy: 0.8464\n",
      "Epoch 00030: val_avg_accuracy did not improve from 0.85741\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.5496 - op_main_loss: 0.4866 - op_conv_loss: 0.3589 - avg_loss: 0.3963 - op_main_accuracy: 0.7876 - op_conv_accuracy: 0.8497 - avg_accuracy: 0.8471 - val_loss: 1.4671 - val_op_main_loss: 0.4672 - val_op_conv_loss: 0.3242 - val_avg_loss: 0.3710 - val_op_main_accuracy: 0.8074 - val_op_conv_accuracy: 0.8546 - val_avg_accuracy: 0.8574\n",
      "Epoch 31/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.5363 - op_main_loss: 0.4874 - op_conv_loss: 0.3532 - avg_loss: 0.3939 - op_main_accuracy: 0.7895 - op_conv_accuracy: 0.8462 - avg_accuracy: 0.8508\n",
      "Epoch 00031: val_avg_accuracy improved from 0.85741 to 0.86213, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.5341 - op_main_loss: 0.4861 - op_conv_loss: 0.3532 - avg_loss: 0.3932 - op_main_accuracy: 0.7902 - op_conv_accuracy: 0.8459 - avg_accuracy: 0.8509 - val_loss: 1.4465 - val_op_main_loss: 0.4626 - val_op_conv_loss: 0.3192 - val_avg_loss: 0.3662 - val_op_main_accuracy: 0.8064 - val_op_conv_accuracy: 0.8546 - val_avg_accuracy: 0.8621\n",
      "Epoch 32/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.5082 - op_main_loss: 0.4773 - op_conv_loss: 0.3484 - avg_loss: 0.3868 - op_main_accuracy: 0.7996 - op_conv_accuracy: 0.8459 - avg_accuracy: 0.8495\n",
      "Epoch 00032: val_avg_accuracy did not improve from 0.86213\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.5082 - op_main_loss: 0.4773 - op_conv_loss: 0.3484 - avg_loss: 0.3868 - op_main_accuracy: 0.7996 - op_conv_accuracy: 0.8459 - avg_accuracy: 0.8495 - val_loss: 1.4496 - val_op_main_loss: 0.4612 - val_op_conv_loss: 0.3277 - val_avg_loss: 0.3677 - val_op_main_accuracy: 0.8055 - val_op_conv_accuracy: 0.8574 - val_avg_accuracy: 0.8602\n",
      "Epoch 33/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.4804 - op_main_loss: 0.4718 - op_conv_loss: 0.3383 - avg_loss: 0.3800 - op_main_accuracy: 0.7977 - op_conv_accuracy: 0.8518 - avg_accuracy: 0.8547\n",
      "Epoch 00033: val_avg_accuracy improved from 0.86213 to 0.86591, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4804 - op_main_loss: 0.4718 - op_conv_loss: 0.3383 - avg_loss: 0.3800 - op_main_accuracy: 0.7977 - op_conv_accuracy: 0.8518 - avg_accuracy: 0.8547 - val_loss: 1.4211 - val_op_main_loss: 0.4543 - val_op_conv_loss: 0.3181 - val_avg_loss: 0.3612 - val_op_main_accuracy: 0.8093 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8659\n",
      "Epoch 34/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.4639 - op_main_loss: 0.4670 - op_conv_loss: 0.3365 - avg_loss: 0.3748 - op_main_accuracy: 0.7982 - op_conv_accuracy: 0.8507 - avg_accuracy: 0.8557\n",
      "Epoch 00034: val_avg_accuracy improved from 0.86591 to 0.86686, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4659 - op_main_loss: 0.4678 - op_conv_loss: 0.3371 - avg_loss: 0.3755 - op_main_accuracy: 0.7980 - op_conv_accuracy: 0.8507 - avg_accuracy: 0.8556 - val_loss: 1.3980 - val_op_main_loss: 0.4499 - val_op_conv_loss: 0.3117 - val_avg_loss: 0.3536 - val_op_main_accuracy: 0.8121 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8669\n",
      "Epoch 35/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.4352 - op_main_loss: 0.4595 - op_conv_loss: 0.3267 - avg_loss: 0.3681 - op_main_accuracy: 0.8048 - op_conv_accuracy: 0.8649 - avg_accuracy: 0.8596\n",
      "Epoch 00035: val_avg_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.4330 - op_main_loss: 0.4592 - op_conv_loss: 0.3256 - avg_loss: 0.3673 - op_main_accuracy: 0.8048 - op_conv_accuracy: 0.8651 - avg_accuracy: 0.8608 - val_loss: 1.4003 - val_op_main_loss: 0.4482 - val_op_conv_loss: 0.3175 - val_avg_loss: 0.3556 - val_op_main_accuracy: 0.8111 - val_op_conv_accuracy: 0.8584 - val_avg_accuracy: 0.8650\n",
      "Epoch 36/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.4368 - op_main_loss: 0.4615 - op_conv_loss: 0.3284 - avg_loss: 0.3697 - op_main_accuracy: 0.8018 - op_conv_accuracy: 0.8585 - avg_accuracy: 0.8616\n",
      "Epoch 00036: val_avg_accuracy improved from 0.86686 to 0.86969, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4382 - op_main_loss: 0.4616 - op_conv_loss: 0.3294 - avg_loss: 0.3701 - op_main_accuracy: 0.8006 - op_conv_accuracy: 0.8578 - avg_accuracy: 0.8608 - val_loss: 1.3707 - val_op_main_loss: 0.4418 - val_op_conv_loss: 0.3056 - val_avg_loss: 0.3480 - val_op_main_accuracy: 0.8159 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8697\n",
      "Epoch 37/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.4196 - op_main_loss: 0.4531 - op_conv_loss: 0.3279 - avg_loss: 0.3651 - op_main_accuracy: 0.8072 - op_conv_accuracy: 0.8560 - avg_accuracy: 0.8627\n",
      "Epoch 00037: val_avg_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.4175 - op_main_loss: 0.4530 - op_conv_loss: 0.3266 - avg_loss: 0.3644 - op_main_accuracy: 0.8067 - op_conv_accuracy: 0.8575 - avg_accuracy: 0.8637 - val_loss: 1.3545 - val_op_main_loss: 0.4376 - val_op_conv_loss: 0.3004 - val_avg_loss: 0.3448 - val_op_main_accuracy: 0.8206 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8650\n",
      "Epoch 38/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.4081 - op_main_loss: 0.4539 - op_conv_loss: 0.3211 - avg_loss: 0.3628 - op_main_accuracy: 0.8074 - op_conv_accuracy: 0.8639 - avg_accuracy: 0.8626\n",
      "Epoch 00038: val_avg_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.4065 - op_main_loss: 0.4529 - op_conv_loss: 0.3212 - avg_loss: 0.3622 - op_main_accuracy: 0.8077 - op_conv_accuracy: 0.8637 - avg_accuracy: 0.8629 - val_loss: 1.3678 - val_op_main_loss: 0.4359 - val_op_conv_loss: 0.3149 - val_avg_loss: 0.3482 - val_op_main_accuracy: 0.8168 - val_op_conv_accuracy: 0.8612 - val_avg_accuracy: 0.8697\n",
      "Epoch 39/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.3833 - op_main_loss: 0.4478 - op_conv_loss: 0.3125 - avg_loss: 0.3555 - op_main_accuracy: 0.8075 - op_conv_accuracy: 0.8685 - avg_accuracy: 0.8683\n",
      "Epoch 00039: val_avg_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3821 - op_main_loss: 0.4476 - op_conv_loss: 0.3118 - avg_loss: 0.3552 - op_main_accuracy: 0.8074 - op_conv_accuracy: 0.8686 - avg_accuracy: 0.8679 - val_loss: 1.3385 - val_op_main_loss: 0.4302 - val_op_conv_loss: 0.3031 - val_avg_loss: 0.3391 - val_op_main_accuracy: 0.8225 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8687\n",
      "Epoch 40/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.3799 - op_main_loss: 0.4431 - op_conv_loss: 0.3172 - avg_loss: 0.3548 - op_main_accuracy: 0.8118 - op_conv_accuracy: 0.8680 - avg_accuracy: 0.8706\n",
      "Epoch 00040: val_avg_accuracy improved from 0.86969 to 0.87724, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3827 - op_main_loss: 0.4443 - op_conv_loss: 0.3179 - avg_loss: 0.3558 - op_main_accuracy: 0.8112 - op_conv_accuracy: 0.8681 - avg_accuracy: 0.8707 - val_loss: 1.3214 - val_op_main_loss: 0.4261 - val_op_conv_loss: 0.2947 - val_avg_loss: 0.3374 - val_op_main_accuracy: 0.8272 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8772\n",
      "Epoch 41/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.3591 - op_main_loss: 0.4422 - op_conv_loss: 0.3051 - avg_loss: 0.3499 - op_main_accuracy: 0.8169 - op_conv_accuracy: 0.8711 - avg_accuracy: 0.8692\n",
      "Epoch 00041: val_avg_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3604 - op_main_loss: 0.4424 - op_conv_loss: 0.3059 - avg_loss: 0.3502 - op_main_accuracy: 0.8166 - op_conv_accuracy: 0.8712 - avg_accuracy: 0.8691 - val_loss: 1.3154 - val_op_main_loss: 0.4219 - val_op_conv_loss: 0.2993 - val_avg_loss: 0.3334 - val_op_main_accuracy: 0.8291 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.3537 - op_main_loss: 0.4379 - op_conv_loss: 0.3085 - avg_loss: 0.3480 - op_main_accuracy: 0.8181 - op_conv_accuracy: 0.8694 - avg_accuracy: 0.8687\n",
      "Epoch 00042: val_avg_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3515 - op_main_loss: 0.4373 - op_conv_loss: 0.3075 - avg_loss: 0.3474 - op_main_accuracy: 0.8176 - op_conv_accuracy: 0.8693 - avg_accuracy: 0.8689 - val_loss: 1.3216 - val_op_main_loss: 0.4197 - val_op_conv_loss: 0.3069 - val_avg_loss: 0.3370 - val_op_main_accuracy: 0.8253 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8725\n",
      "Epoch 43/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.3479 - op_main_loss: 0.4372 - op_conv_loss: 0.3064 - avg_loss: 0.3473 - op_main_accuracy: 0.8168 - op_conv_accuracy: 0.8716 - avg_accuracy: 0.8690\n",
      "Epoch 00043: val_avg_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3516 - op_main_loss: 0.4378 - op_conv_loss: 0.3083 - avg_loss: 0.3486 - op_main_accuracy: 0.8166 - op_conv_accuracy: 0.8703 - avg_accuracy: 0.8677 - val_loss: 1.3112 - val_op_main_loss: 0.4185 - val_op_conv_loss: 0.3022 - val_avg_loss: 0.3345 - val_op_main_accuracy: 0.8310 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8744\n",
      "Epoch 44/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.3460 - op_main_loss: 0.4336 - op_conv_loss: 0.3096 - avg_loss: 0.3478 - op_main_accuracy: 0.8120 - op_conv_accuracy: 0.8671 - avg_accuracy: 0.8674\n",
      "Epoch 00044: val_avg_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3462 - op_main_loss: 0.4336 - op_conv_loss: 0.3098 - avg_loss: 0.3479 - op_main_accuracy: 0.8121 - op_conv_accuracy: 0.8665 - avg_accuracy: 0.8670 - val_loss: 1.3076 - val_op_main_loss: 0.4166 - val_op_conv_loss: 0.3031 - val_avg_loss: 0.3341 - val_op_main_accuracy: 0.8291 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8716\n",
      "Epoch 45/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.3028 - op_main_loss: 0.4272 - op_conv_loss: 0.2875 - avg_loss: 0.3351 - op_main_accuracy: 0.8189 - op_conv_accuracy: 0.8822 - avg_accuracy: 0.8819\n",
      "Epoch 00045: val_avg_accuracy improved from 0.87724 to 0.87913, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3034 - op_main_loss: 0.4273 - op_conv_loss: 0.2879 - avg_loss: 0.3352 - op_main_accuracy: 0.8188 - op_conv_accuracy: 0.8821 - avg_accuracy: 0.8814 - val_loss: 1.2880 - val_op_main_loss: 0.4094 - val_op_conv_loss: 0.2989 - val_avg_loss: 0.3274 - val_op_main_accuracy: 0.8329 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8791\n",
      "Epoch 46/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.3111 - op_main_loss: 0.4255 - op_conv_loss: 0.2959 - avg_loss: 0.3381 - op_main_accuracy: 0.8217 - op_conv_accuracy: 0.8704 - avg_accuracy: 0.8709\n",
      "Epoch 00046: val_avg_accuracy improved from 0.87913 to 0.88102, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3081 - op_main_loss: 0.4250 - op_conv_loss: 0.2944 - avg_loss: 0.3371 - op_main_accuracy: 0.8228 - op_conv_accuracy: 0.8712 - avg_accuracy: 0.8717 - val_loss: 1.2773 - val_op_main_loss: 0.4081 - val_op_conv_loss: 0.2931 - val_avg_loss: 0.3252 - val_op_main_accuracy: 0.8357 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8810\n",
      "Epoch 47/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.3228 - op_main_loss: 0.4234 - op_conv_loss: 0.3083 - avg_loss: 0.3414 - op_main_accuracy: 0.8227 - op_conv_accuracy: 0.8677 - avg_accuracy: 0.8731\n",
      "Epoch 00047: val_avg_accuracy improved from 0.88102 to 0.88385, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3200 - op_main_loss: 0.4225 - op_conv_loss: 0.3072 - avg_loss: 0.3406 - op_main_accuracy: 0.8230 - op_conv_accuracy: 0.8681 - avg_accuracy: 0.8733 - val_loss: 1.2595 - val_op_main_loss: 0.4032 - val_op_conv_loss: 0.2871 - val_avg_loss: 0.3206 - val_op_main_accuracy: 0.8366 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8839\n",
      "Epoch 48/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.2910 - op_main_loss: 0.4184 - op_conv_loss: 0.2928 - avg_loss: 0.3319 - op_main_accuracy: 0.8213 - op_conv_accuracy: 0.8781 - avg_accuracy: 0.8802\n",
      "Epoch 00048: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2900 - op_main_loss: 0.4181 - op_conv_loss: 0.2925 - avg_loss: 0.3316 - op_main_accuracy: 0.8223 - op_conv_accuracy: 0.8781 - avg_accuracy: 0.8804 - val_loss: 1.2484 - val_op_main_loss: 0.4002 - val_op_conv_loss: 0.2840 - val_avg_loss: 0.3171 - val_op_main_accuracy: 0.8395 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8839\n",
      "Epoch 49/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2687 - op_main_loss: 0.4139 - op_conv_loss: 0.2818 - avg_loss: 0.3263 - op_main_accuracy: 0.8293 - op_conv_accuracy: 0.8800 - avg_accuracy: 0.8817\n",
      "Epoch 00049: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2738 - op_main_loss: 0.4148 - op_conv_loss: 0.2845 - avg_loss: 0.3279 - op_main_accuracy: 0.8277 - op_conv_accuracy: 0.8790 - avg_accuracy: 0.8807 - val_loss: 1.2701 - val_op_main_loss: 0.3977 - val_op_conv_loss: 0.3026 - val_avg_loss: 0.3240 - val_op_main_accuracy: 0.8414 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8716\n",
      "Epoch 50/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.2940 - op_main_loss: 0.4186 - op_conv_loss: 0.2960 - avg_loss: 0.3342 - op_main_accuracy: 0.8297 - op_conv_accuracy: 0.8794 - avg_accuracy: 0.8779\n",
      "Epoch 00050: val_avg_accuracy improved from 0.88385 to 0.88952, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2887 - op_main_loss: 0.4170 - op_conv_loss: 0.2941 - avg_loss: 0.3325 - op_main_accuracy: 0.8308 - op_conv_accuracy: 0.8802 - avg_accuracy: 0.8793 - val_loss: 1.2336 - val_op_main_loss: 0.3943 - val_op_conv_loss: 0.2810 - val_avg_loss: 0.3139 - val_op_main_accuracy: 0.8432 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8895\n",
      "Epoch 51/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.2666 - op_main_loss: 0.4120 - op_conv_loss: 0.2854 - avg_loss: 0.3255 - op_main_accuracy: 0.8357 - op_conv_accuracy: 0.8814 - avg_accuracy: 0.8816\n",
      "Epoch 00051: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2660 - op_main_loss: 0.4119 - op_conv_loss: 0.2851 - avg_loss: 0.3253 - op_main_accuracy: 0.8358 - op_conv_accuracy: 0.8816 - avg_accuracy: 0.8819 - val_loss: 1.2332 - val_op_main_loss: 0.3927 - val_op_conv_loss: 0.2845 - val_avg_loss: 0.3129 - val_op_main_accuracy: 0.8423 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8867\n",
      "Epoch 52/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.2542 - op_main_loss: 0.4083 - op_conv_loss: 0.2815 - avg_loss: 0.3219 - op_main_accuracy: 0.8266 - op_conv_accuracy: 0.8835 - avg_accuracy: 0.8806\n",
      "Epoch 00052: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2558 - op_main_loss: 0.4092 - op_conv_loss: 0.2819 - avg_loss: 0.3222 - op_main_accuracy: 0.8263 - op_conv_accuracy: 0.8835 - avg_accuracy: 0.8804 - val_loss: 1.2301 - val_op_main_loss: 0.3892 - val_op_conv_loss: 0.2872 - val_avg_loss: 0.3118 - val_op_main_accuracy: 0.8442 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8848\n",
      "Epoch 53/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.2579 - op_main_loss: 0.4110 - op_conv_loss: 0.2819 - avg_loss: 0.3239 - op_main_accuracy: 0.8228 - op_conv_accuracy: 0.8826 - avg_accuracy: 0.8776\n",
      "Epoch 00053: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2583 - op_main_loss: 0.4106 - op_conv_loss: 0.2826 - avg_loss: 0.3240 - op_main_accuracy: 0.8240 - op_conv_accuracy: 0.8823 - avg_accuracy: 0.8778 - val_loss: 1.2252 - val_op_main_loss: 0.3864 - val_op_conv_loss: 0.2879 - val_avg_loss: 0.3105 - val_op_main_accuracy: 0.8461 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2490 - op_main_loss: 0.4047 - op_conv_loss: 0.2835 - avg_loss: 0.3211 - op_main_accuracy: 0.8306 - op_conv_accuracy: 0.8852 - avg_accuracy: 0.8854\n",
      "Epoch 00054: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2490 - op_main_loss: 0.4047 - op_conv_loss: 0.2835 - avg_loss: 0.3211 - op_main_accuracy: 0.8306 - op_conv_accuracy: 0.8852 - avg_accuracy: 0.8854 - val_loss: 1.2173 - val_op_main_loss: 0.3874 - val_op_conv_loss: 0.2813 - val_avg_loss: 0.3094 - val_op_main_accuracy: 0.8423 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8848\n",
      "Epoch 55/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2285 - op_main_loss: 0.3998 - op_conv_loss: 0.2749 - avg_loss: 0.3152 - op_main_accuracy: 0.8332 - op_conv_accuracy: 0.8877 - avg_accuracy: 0.8873\n",
      "Epoch 00055: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2275 - op_main_loss: 0.3994 - op_conv_loss: 0.2747 - avg_loss: 0.3147 - op_main_accuracy: 0.8332 - op_conv_accuracy: 0.8882 - avg_accuracy: 0.8875 - val_loss: 1.2299 - val_op_main_loss: 0.3839 - val_op_conv_loss: 0.2952 - val_avg_loss: 0.3125 - val_op_main_accuracy: 0.8423 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8839\n",
      "Epoch 56/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2336 - op_main_loss: 0.3998 - op_conv_loss: 0.2795 - avg_loss: 0.3168 - op_main_accuracy: 0.8310 - op_conv_accuracy: 0.8863 - avg_accuracy: 0.8880\n",
      "Epoch 00056: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2329 - op_main_loss: 0.4000 - op_conv_loss: 0.2787 - avg_loss: 0.3166 - op_main_accuracy: 0.8313 - op_conv_accuracy: 0.8863 - avg_accuracy: 0.8882 - val_loss: 1.2153 - val_op_main_loss: 0.3793 - val_op_conv_loss: 0.2910 - val_avg_loss: 0.3079 - val_op_main_accuracy: 0.8461 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8829\n",
      "Epoch 57/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2249 - op_main_loss: 0.3957 - op_conv_loss: 0.2777 - avg_loss: 0.3150 - op_main_accuracy: 0.8411 - op_conv_accuracy: 0.8817 - avg_accuracy: 0.8832\n",
      "Epoch 00057: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2249 - op_main_loss: 0.3951 - op_conv_loss: 0.2783 - avg_loss: 0.3150 - op_main_accuracy: 0.8419 - op_conv_accuracy: 0.8811 - avg_accuracy: 0.8826 - val_loss: 1.2182 - val_op_main_loss: 0.3768 - val_op_conv_loss: 0.2968 - val_avg_loss: 0.3089 - val_op_main_accuracy: 0.8499 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8829\n",
      "Epoch 58/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.2405 - op_main_loss: 0.4008 - op_conv_loss: 0.2849 - avg_loss: 0.3197 - op_main_accuracy: 0.8302 - op_conv_accuracy: 0.8857 - avg_accuracy: 0.8735\n",
      "Epoch 00058: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2386 - op_main_loss: 0.4001 - op_conv_loss: 0.2843 - avg_loss: 0.3192 - op_main_accuracy: 0.8313 - op_conv_accuracy: 0.8861 - avg_accuracy: 0.8743 - val_loss: 1.1958 - val_op_main_loss: 0.3747 - val_op_conv_loss: 0.2827 - val_avg_loss: 0.3040 - val_op_main_accuracy: 0.8470 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8876\n",
      "Epoch 59/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.2202 - op_main_loss: 0.3940 - op_conv_loss: 0.2779 - avg_loss: 0.3144 - op_main_accuracy: 0.8368 - op_conv_accuracy: 0.8810 - avg_accuracy: 0.8793\n",
      "Epoch 00059: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2201 - op_main_loss: 0.3943 - op_conv_loss: 0.2775 - avg_loss: 0.3144 - op_main_accuracy: 0.8362 - op_conv_accuracy: 0.8811 - avg_accuracy: 0.8790 - val_loss: 1.1932 - val_op_main_loss: 0.3760 - val_op_conv_loss: 0.2811 - val_avg_loss: 0.3027 - val_op_main_accuracy: 0.8451 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8867\n",
      "Epoch 60/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2142 - op_main_loss: 0.3930 - op_conv_loss: 0.2757 - avg_loss: 0.3124 - op_main_accuracy: 0.8380 - op_conv_accuracy: 0.8849 - avg_accuracy: 0.8832\n",
      "Epoch 00060: val_avg_accuracy improved from 0.88952 to 0.89330, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2133 - op_main_loss: 0.3932 - op_conv_loss: 0.2749 - avg_loss: 0.3121 - op_main_accuracy: 0.8377 - op_conv_accuracy: 0.8856 - avg_accuracy: 0.8835 - val_loss: 1.1809 - val_op_main_loss: 0.3712 - val_op_conv_loss: 0.2779 - val_avg_loss: 0.2992 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8933\n",
      "Epoch 61/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2021 - op_main_loss: 0.3884 - op_conv_loss: 0.2719 - avg_loss: 0.3095 - op_main_accuracy: 0.8361 - op_conv_accuracy: 0.8901 - avg_accuracy: 0.8825\n",
      "Epoch 00061: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2055 - op_main_loss: 0.3896 - op_conv_loss: 0.2731 - avg_loss: 0.3106 - op_main_accuracy: 0.8353 - op_conv_accuracy: 0.8889 - avg_accuracy: 0.8814 - val_loss: 1.1790 - val_op_main_loss: 0.3695 - val_op_conv_loss: 0.2768 - val_avg_loss: 0.3008 - val_op_main_accuracy: 0.8536 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8895\n",
      "Epoch 62/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1930 - op_main_loss: 0.3893 - op_conv_loss: 0.2652 - avg_loss: 0.3070 - op_main_accuracy: 0.8380 - op_conv_accuracy: 0.8893 - avg_accuracy: 0.8879\n",
      "Epoch 00062: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1911 - op_main_loss: 0.3890 - op_conv_loss: 0.2642 - avg_loss: 0.3064 - op_main_accuracy: 0.8384 - op_conv_accuracy: 0.8899 - avg_accuracy: 0.8882 - val_loss: 1.1941 - val_op_main_loss: 0.3660 - val_op_conv_loss: 0.2940 - val_avg_loss: 0.3025 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8857\n",
      "Epoch 63/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.1743 - op_main_loss: 0.3811 - op_conv_loss: 0.2612 - avg_loss: 0.3008 - op_main_accuracy: 0.8495 - op_conv_accuracy: 0.8889 - avg_accuracy: 0.8916\n",
      "Epoch 00063: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1769 - op_main_loss: 0.3814 - op_conv_loss: 0.2627 - avg_loss: 0.3016 - op_main_accuracy: 0.8497 - op_conv_accuracy: 0.8885 - avg_accuracy: 0.8911 - val_loss: 1.1740 - val_op_main_loss: 0.3638 - val_op_conv_loss: 0.2819 - val_avg_loss: 0.2974 - val_op_main_accuracy: 0.8565 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8857\n",
      "Epoch 64/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1862 - op_main_loss: 0.3817 - op_conv_loss: 0.2692 - avg_loss: 0.3049 - op_main_accuracy: 0.8399 - op_conv_accuracy: 0.8866 - avg_accuracy: 0.8876\n",
      "Epoch 00064: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1829 - op_main_loss: 0.3807 - op_conv_loss: 0.2679 - avg_loss: 0.3039 - op_main_accuracy: 0.8407 - op_conv_accuracy: 0.8878 - avg_accuracy: 0.8885 - val_loss: 1.1807 - val_op_main_loss: 0.3630 - val_op_conv_loss: 0.2878 - val_avg_loss: 0.3000 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8857\n",
      "Epoch 65/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1626 - op_main_loss: 0.3796 - op_conv_loss: 0.2563 - avg_loss: 0.2970 - op_main_accuracy: 0.8390 - op_conv_accuracy: 0.8927 - avg_accuracy: 0.8934\n",
      "Epoch 00065: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1640 - op_main_loss: 0.3795 - op_conv_loss: 0.2573 - avg_loss: 0.2975 - op_main_accuracy: 0.8396 - op_conv_accuracy: 0.8918 - avg_accuracy: 0.8927 - val_loss: 1.1794 - val_op_main_loss: 0.3584 - val_op_conv_loss: 0.2944 - val_avg_loss: 0.2972 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8876\n",
      "Epoch 66/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 1.1623 - op_main_loss: 0.3754 - op_conv_loss: 0.2607 - avg_loss: 0.2973 - op_main_accuracy: 0.8442 - op_conv_accuracy: 0.8920 - avg_accuracy: 0.8898\n",
      "Epoch 00066: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1652 - op_main_loss: 0.3765 - op_conv_loss: 0.2617 - avg_loss: 0.2981 - op_main_accuracy: 0.8433 - op_conv_accuracy: 0.8915 - avg_accuracy: 0.8892 - val_loss: 1.1526 - val_op_main_loss: 0.3578 - val_op_conv_loss: 0.2747 - val_avg_loss: 0.2916 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8905\n",
      "Epoch 67/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1610 - op_main_loss: 0.3748 - op_conv_loss: 0.2614 - avg_loss: 0.2965 - op_main_accuracy: 0.8459 - op_conv_accuracy: 0.8893 - avg_accuracy: 0.8874\n",
      "Epoch 00067: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1625 - op_main_loss: 0.3748 - op_conv_loss: 0.2625 - avg_loss: 0.2969 - op_main_accuracy: 0.8457 - op_conv_accuracy: 0.8889 - avg_accuracy: 0.8868 - val_loss: 1.1594 - val_op_main_loss: 0.3557 - val_op_conv_loss: 0.2833 - val_avg_loss: 0.2926 - val_op_main_accuracy: 0.8565 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8933\n",
      "Epoch 68/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.1679 - op_main_loss: 0.3766 - op_conv_loss: 0.2641 - avg_loss: 0.2998 - op_main_accuracy: 0.8404 - op_conv_accuracy: 0.8925 - avg_accuracy: 0.8854\n",
      "Epoch 00068: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1677 - op_main_loss: 0.3765 - op_conv_loss: 0.2640 - avg_loss: 0.2997 - op_main_accuracy: 0.8405 - op_conv_accuracy: 0.8925 - avg_accuracy: 0.8854 - val_loss: 1.1416 - val_op_main_loss: 0.3540 - val_op_conv_loss: 0.2718 - val_avg_loss: 0.2887 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8895\n",
      "Epoch 69/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1416 - op_main_loss: 0.3681 - op_conv_loss: 0.2553 - avg_loss: 0.2913 - op_main_accuracy: 0.8566 - op_conv_accuracy: 0.8958 - avg_accuracy: 0.8958\n",
      "Epoch 00069: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1401 - op_main_loss: 0.3668 - op_conv_loss: 0.2556 - avg_loss: 0.2908 - op_main_accuracy: 0.8573 - op_conv_accuracy: 0.8956 - avg_accuracy: 0.8956 - val_loss: 1.1475 - val_op_main_loss: 0.3519 - val_op_conv_loss: 0.2792 - val_avg_loss: 0.2896 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8924\n",
      "Epoch 70/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1545 - op_main_loss: 0.3661 - op_conv_loss: 0.2674 - avg_loss: 0.2948 - op_main_accuracy: 0.8486 - op_conv_accuracy: 0.8859 - avg_accuracy: 0.8878\n",
      "Epoch 00070: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1576 - op_main_loss: 0.3678 - op_conv_loss: 0.2679 - avg_loss: 0.2957 - op_main_accuracy: 0.8471 - op_conv_accuracy: 0.8852 - avg_accuracy: 0.8875 - val_loss: 1.1315 - val_op_main_loss: 0.3498 - val_op_conv_loss: 0.2689 - val_avg_loss: 0.2873 - val_op_main_accuracy: 0.8593 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8895\n",
      "Epoch 71/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1289 - op_main_loss: 0.3660 - op_conv_loss: 0.2495 - avg_loss: 0.2880 - op_main_accuracy: 0.8462 - op_conv_accuracy: 0.8963 - avg_accuracy: 0.8956\n",
      "Epoch 00071: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1265 - op_main_loss: 0.3652 - op_conv_loss: 0.2487 - avg_loss: 0.2872 - op_main_accuracy: 0.8462 - op_conv_accuracy: 0.8963 - avg_accuracy: 0.8958 - val_loss: 1.1463 - val_op_main_loss: 0.3470 - val_op_conv_loss: 0.2859 - val_avg_loss: 0.2883 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8848\n",
      "Epoch 72/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1626 - op_main_loss: 0.3723 - op_conv_loss: 0.2671 - avg_loss: 0.2985 - op_main_accuracy: 0.8438 - op_conv_accuracy: 0.8937 - avg_accuracy: 0.8881\n",
      "Epoch 00072: val_avg_accuracy improved from 0.89330 to 0.89424, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1583 - op_main_loss: 0.3719 - op_conv_loss: 0.2646 - avg_loss: 0.2970 - op_main_accuracy: 0.8443 - op_conv_accuracy: 0.8953 - avg_accuracy: 0.8894 - val_loss: 1.1325 - val_op_main_loss: 0.3467 - val_op_conv_loss: 0.2756 - val_avg_loss: 0.2856 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8942\n",
      "Epoch 73/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1358 - op_main_loss: 0.3635 - op_conv_loss: 0.2583 - avg_loss: 0.2899 - op_main_accuracy: 0.8534 - op_conv_accuracy: 0.8970 - avg_accuracy: 0.8973\n",
      "Epoch 00073: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1382 - op_main_loss: 0.3650 - op_conv_loss: 0.2585 - avg_loss: 0.2906 - op_main_accuracy: 0.8526 - op_conv_accuracy: 0.8972 - avg_accuracy: 0.8970 - val_loss: 1.1236 - val_op_main_loss: 0.3456 - val_op_conv_loss: 0.2704 - val_avg_loss: 0.2839 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8933\n",
      "Epoch 74/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1305 - op_main_loss: 0.3628 - op_conv_loss: 0.2552 - avg_loss: 0.2890 - op_main_accuracy: 0.8491 - op_conv_accuracy: 0.8946 - avg_accuracy: 0.8907\n",
      "Epoch 00074: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1346 - op_main_loss: 0.3636 - op_conv_loss: 0.2572 - avg_loss: 0.2903 - op_main_accuracy: 0.8478 - op_conv_accuracy: 0.8939 - avg_accuracy: 0.8901 - val_loss: 1.1295 - val_op_main_loss: 0.3424 - val_op_conv_loss: 0.2788 - val_avg_loss: 0.2849 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8876\n",
      "Epoch 75/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1312 - op_main_loss: 0.3647 - op_conv_loss: 0.2535 - avg_loss: 0.2899 - op_main_accuracy: 0.8500 - op_conv_accuracy: 0.8929 - avg_accuracy: 0.8919\n",
      "Epoch 00075: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1317 - op_main_loss: 0.3649 - op_conv_loss: 0.2537 - avg_loss: 0.2901 - op_main_accuracy: 0.8495 - op_conv_accuracy: 0.8925 - avg_accuracy: 0.8918 - val_loss: 1.1195 - val_op_main_loss: 0.3407 - val_op_conv_loss: 0.2738 - val_avg_loss: 0.2824 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8895\n",
      "Epoch 76/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.1009 - op_main_loss: 0.3536 - op_conv_loss: 0.2454 - avg_loss: 0.2795 - op_main_accuracy: 0.8565 - op_conv_accuracy: 0.8966 - avg_accuracy: 0.9000\n",
      "Epoch 00076: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1005 - op_main_loss: 0.3529 - op_conv_loss: 0.2457 - avg_loss: 0.2794 - op_main_accuracy: 0.8563 - op_conv_accuracy: 0.8963 - avg_accuracy: 0.9000 - val_loss: 1.1270 - val_op_main_loss: 0.3399 - val_op_conv_loss: 0.2794 - val_avg_loss: 0.2854 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8876\n",
      "Epoch 77/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.1094 - op_main_loss: 0.3589 - op_conv_loss: 0.2458 - avg_loss: 0.2827 - op_main_accuracy: 0.8475 - op_conv_accuracy: 0.9006 - avg_accuracy: 0.9013\n",
      "Epoch 00077: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1104 - op_main_loss: 0.3592 - op_conv_loss: 0.2462 - avg_loss: 0.2830 - op_main_accuracy: 0.8474 - op_conv_accuracy: 0.9000 - avg_accuracy: 0.9010 - val_loss: 1.1156 - val_op_main_loss: 0.3377 - val_op_conv_loss: 0.2734 - val_avg_loss: 0.2822 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8905\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 1.1129 - op_main_loss: 0.3553 - op_conv_loss: 0.2515 - avg_loss: 0.2841 - op_main_accuracy: 0.8493 - op_conv_accuracy: 0.8951 - avg_accuracy: 0.8963\n",
      "Epoch 00078: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1162 - op_main_loss: 0.3565 - op_conv_loss: 0.2525 - avg_loss: 0.2851 - op_main_accuracy: 0.8483 - op_conv_accuracy: 0.8946 - avg_accuracy: 0.8956 - val_loss: 1.1179 - val_op_main_loss: 0.3380 - val_op_conv_loss: 0.2741 - val_avg_loss: 0.2842 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8876\n",
      "Epoch 79/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.1136 - op_main_loss: 0.3581 - op_conv_loss: 0.2488 - avg_loss: 0.2855 - op_main_accuracy: 0.8502 - op_conv_accuracy: 0.8957 - avg_accuracy: 0.8935\n",
      "Epoch 00079: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1126 - op_main_loss: 0.3577 - op_conv_loss: 0.2484 - avg_loss: 0.2852 - op_main_accuracy: 0.8509 - op_conv_accuracy: 0.8953 - avg_accuracy: 0.8934 - val_loss: 1.1105 - val_op_main_loss: 0.3357 - val_op_conv_loss: 0.2738 - val_avg_loss: 0.2798 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8914\n",
      "Epoch 80/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0877 - op_main_loss: 0.3474 - op_conv_loss: 0.2437 - avg_loss: 0.2755 - op_main_accuracy: 0.8600 - op_conv_accuracy: 0.9005 - avg_accuracy: 0.9005\n",
      "Epoch 00080: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0889 - op_main_loss: 0.3477 - op_conv_loss: 0.2441 - avg_loss: 0.2759 - op_main_accuracy: 0.8596 - op_conv_accuracy: 0.9003 - avg_accuracy: 0.9005 - val_loss: 1.1282 - val_op_main_loss: 0.3346 - val_op_conv_loss: 0.2866 - val_avg_loss: 0.2860 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8905\n",
      "Epoch 81/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.1003 - op_main_loss: 0.3513 - op_conv_loss: 0.2484 - avg_loss: 0.2796 - op_main_accuracy: 0.8543 - op_conv_accuracy: 0.9022 - avg_accuracy: 0.8945\n",
      "Epoch 00081: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0965 - op_main_loss: 0.3500 - op_conv_loss: 0.2471 - avg_loss: 0.2785 - op_main_accuracy: 0.8554 - op_conv_accuracy: 0.9031 - avg_accuracy: 0.8951 - val_loss: 1.0994 - val_op_main_loss: 0.3323 - val_op_conv_loss: 0.2680 - val_avg_loss: 0.2785 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8886\n",
      "Epoch 82/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.1097 - op_main_loss: 0.3559 - op_conv_loss: 0.2498 - avg_loss: 0.2835 - op_main_accuracy: 0.8520 - op_conv_accuracy: 0.8973 - avg_accuracy: 0.8916\n",
      "Epoch 00082: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1099 - op_main_loss: 0.3560 - op_conv_loss: 0.2499 - avg_loss: 0.2836 - op_main_accuracy: 0.8518 - op_conv_accuracy: 0.8972 - avg_accuracy: 0.8915 - val_loss: 1.1014 - val_op_main_loss: 0.3306 - val_op_conv_loss: 0.2735 - val_avg_loss: 0.2771 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8914\n",
      "Epoch 83/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0663 - op_main_loss: 0.3417 - op_conv_loss: 0.2347 - avg_loss: 0.2696 - op_main_accuracy: 0.8609 - op_conv_accuracy: 0.9026 - avg_accuracy: 0.9019\n",
      "Epoch 00083: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0655 - op_main_loss: 0.3405 - op_conv_loss: 0.2352 - avg_loss: 0.2694 - op_main_accuracy: 0.8620 - op_conv_accuracy: 0.9017 - avg_accuracy: 0.9022 - val_loss: 1.1029 - val_op_main_loss: 0.3291 - val_op_conv_loss: 0.2766 - val_avg_loss: 0.2770 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8933\n",
      "Epoch 84/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0907 - op_main_loss: 0.3503 - op_conv_loss: 0.2426 - avg_loss: 0.2778 - op_main_accuracy: 0.8546 - op_conv_accuracy: 0.8952 - avg_accuracy: 0.8945\n",
      "Epoch 00084: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0915 - op_main_loss: 0.3507 - op_conv_loss: 0.2427 - avg_loss: 0.2780 - op_main_accuracy: 0.8542 - op_conv_accuracy: 0.8956 - avg_accuracy: 0.8951 - val_loss: 1.1130 - val_op_main_loss: 0.3279 - val_op_conv_loss: 0.2872 - val_avg_loss: 0.2781 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8895\n",
      "Epoch 85/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.1001 - op_main_loss: 0.3498 - op_conv_loss: 0.2501 - avg_loss: 0.2807 - op_main_accuracy: 0.8558 - op_conv_accuracy: 0.8986 - avg_accuracy: 0.8971\n",
      "Epoch 00085: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0984 - op_main_loss: 0.3492 - op_conv_loss: 0.2495 - avg_loss: 0.2802 - op_main_accuracy: 0.8566 - op_conv_accuracy: 0.8991 - avg_accuracy: 0.8977 - val_loss: 1.1959 - val_op_main_loss: 0.3366 - val_op_conv_loss: 0.3351 - val_avg_loss: 0.3049 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8735\n",
      "Epoch 86/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0955 - op_main_loss: 0.3486 - op_conv_loss: 0.2478 - avg_loss: 0.2800 - op_main_accuracy: 0.8595 - op_conv_accuracy: 0.8980 - avg_accuracy: 0.8958\n",
      "Epoch 00086: val_avg_accuracy improved from 0.89424 to 0.89802, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1022 - op_main_loss: 0.3501 - op_conv_loss: 0.2507 - avg_loss: 0.2822 - op_main_accuracy: 0.8578 - op_conv_accuracy: 0.8965 - avg_accuracy: 0.8937 - val_loss: 1.1012 - val_op_main_loss: 0.3260 - val_op_conv_loss: 0.2789 - val_avg_loss: 0.2776 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8980\n",
      "Epoch 87/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0874 - op_main_loss: 0.3444 - op_conv_loss: 0.2478 - avg_loss: 0.2768 - op_main_accuracy: 0.8565 - op_conv_accuracy: 0.9031 - avg_accuracy: 0.9007\n",
      "Epoch 00087: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0892 - op_main_loss: 0.3447 - op_conv_loss: 0.2487 - avg_loss: 0.2774 - op_main_accuracy: 0.8568 - op_conv_accuracy: 0.9029 - avg_accuracy: 0.9005 - val_loss: 1.1012 - val_op_main_loss: 0.3252 - val_op_conv_loss: 0.2805 - val_avg_loss: 0.2775 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8848\n",
      "Epoch 88/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0729 - op_main_loss: 0.3406 - op_conv_loss: 0.2419 - avg_loss: 0.2725 - op_main_accuracy: 0.8591 - op_conv_accuracy: 0.8958 - avg_accuracy: 0.8961\n",
      "Epoch 00088: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0731 - op_main_loss: 0.3408 - op_conv_loss: 0.2419 - avg_loss: 0.2726 - op_main_accuracy: 0.8592 - op_conv_accuracy: 0.8958 - avg_accuracy: 0.8960 - val_loss: 1.1128 - val_op_main_loss: 0.3244 - val_op_conv_loss: 0.2913 - val_avg_loss: 0.2793 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8952\n",
      "Epoch 89/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0567 - op_main_loss: 0.3396 - op_conv_loss: 0.2321 - avg_loss: 0.2672 - op_main_accuracy: 0.8647 - op_conv_accuracy: 0.8993 - avg_accuracy: 0.9010\n",
      "Epoch 00089: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0556 - op_main_loss: 0.3391 - op_conv_loss: 0.2319 - avg_loss: 0.2669 - op_main_accuracy: 0.8648 - op_conv_accuracy: 0.8993 - avg_accuracy: 0.9010 - val_loss: 1.1347 - val_op_main_loss: 0.3235 - val_op_conv_loss: 0.3090 - val_avg_loss: 0.2844 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8876\n",
      "Epoch 90/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 1.0910 - op_main_loss: 0.3444 - op_conv_loss: 0.2507 - avg_loss: 0.2783 - op_main_accuracy: 0.8558 - op_conv_accuracy: 0.8965 - avg_accuracy: 0.8942\n",
      "Epoch 00090: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0903 - op_main_loss: 0.3443 - op_conv_loss: 0.2503 - avg_loss: 0.2781 - op_main_accuracy: 0.8561 - op_conv_accuracy: 0.8967 - avg_accuracy: 0.8944 - val_loss: 1.0891 - val_op_main_loss: 0.3218 - val_op_conv_loss: 0.2750 - val_avg_loss: 0.2751 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8980\n",
      "Epoch 91/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0581 - op_main_loss: 0.3348 - op_conv_loss: 0.2371 - avg_loss: 0.2688 - op_main_accuracy: 0.8632 - op_conv_accuracy: 0.9029 - avg_accuracy: 0.9003\n",
      "Epoch 00091: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0581 - op_main_loss: 0.3348 - op_conv_loss: 0.2371 - avg_loss: 0.2688 - op_main_accuracy: 0.8632 - op_conv_accuracy: 0.9029 - avg_accuracy: 0.9003 - val_loss: 1.0924 - val_op_main_loss: 0.3212 - val_op_conv_loss: 0.2778 - val_avg_loss: 0.2761 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8905\n",
      "Epoch 92/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0570 - op_main_loss: 0.3383 - op_conv_loss: 0.2333 - avg_loss: 0.2682 - op_main_accuracy: 0.8604 - op_conv_accuracy: 0.9010 - avg_accuracy: 0.8981\n",
      "Epoch 00092: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0601 - op_main_loss: 0.3392 - op_conv_loss: 0.2345 - avg_loss: 0.2692 - op_main_accuracy: 0.8599 - op_conv_accuracy: 0.9005 - avg_accuracy: 0.8977 - val_loss: 1.0953 - val_op_main_loss: 0.3203 - val_op_conv_loss: 0.2809 - val_avg_loss: 0.2769 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8914\n",
      "Epoch 93/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0604 - op_main_loss: 0.3323 - op_conv_loss: 0.2417 - avg_loss: 0.2694 - op_main_accuracy: 0.8675 - op_conv_accuracy: 0.8993 - avg_accuracy: 0.9005\n",
      "Epoch 00093: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0610 - op_main_loss: 0.3332 - op_conv_loss: 0.2412 - avg_loss: 0.2696 - op_main_accuracy: 0.8667 - op_conv_accuracy: 0.8996 - avg_accuracy: 0.9005 - val_loss: 1.0831 - val_op_main_loss: 0.3194 - val_op_conv_loss: 0.2724 - val_avg_loss: 0.2747 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8961\n",
      "Epoch 94/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0605 - op_main_loss: 0.3348 - op_conv_loss: 0.2398 - avg_loss: 0.2694 - op_main_accuracy: 0.8658 - op_conv_accuracy: 0.9038 - avg_accuracy: 0.8995\n",
      "Epoch 00094: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0617 - op_main_loss: 0.3353 - op_conv_loss: 0.2401 - avg_loss: 0.2697 - op_main_accuracy: 0.8651 - op_conv_accuracy: 0.9043 - avg_accuracy: 0.8993 - val_loss: 1.1094 - val_op_main_loss: 0.3179 - val_op_conv_loss: 0.2946 - val_avg_loss: 0.2805 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8924\n",
      "Epoch 95/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0470 - op_main_loss: 0.3298 - op_conv_loss: 0.2365 - avg_loss: 0.2646 - op_main_accuracy: 0.8651 - op_conv_accuracy: 0.9046 - avg_accuracy: 0.9016\n",
      "Epoch 00095: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0487 - op_main_loss: 0.3297 - op_conv_loss: 0.2379 - avg_loss: 0.2651 - op_main_accuracy: 0.8651 - op_conv_accuracy: 0.9041 - avg_accuracy: 0.9000 - val_loss: 1.0864 - val_op_main_loss: 0.3169 - val_op_conv_loss: 0.2801 - val_avg_loss: 0.2733 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8961\n",
      "Epoch 96/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0460 - op_main_loss: 0.3320 - op_conv_loss: 0.2335 - avg_loss: 0.2645 - op_main_accuracy: 0.8646 - op_conv_accuracy: 0.9050 - avg_accuracy: 0.9045\n",
      "Epoch 00096: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0460 - op_main_loss: 0.3320 - op_conv_loss: 0.2335 - avg_loss: 0.2645 - op_main_accuracy: 0.8646 - op_conv_accuracy: 0.9050 - avg_accuracy: 0.9045 - val_loss: 1.0813 - val_op_main_loss: 0.3156 - val_op_conv_loss: 0.2770 - val_avg_loss: 0.2729 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8942\n",
      "Epoch 97/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0307 - op_main_loss: 0.3252 - op_conv_loss: 0.2296 - avg_loss: 0.2600 - op_main_accuracy: 0.8689 - op_conv_accuracy: 0.9052 - avg_accuracy: 0.9041\n",
      "Epoch 00097: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0307 - op_main_loss: 0.3252 - op_conv_loss: 0.2296 - avg_loss: 0.2600 - op_main_accuracy: 0.8689 - op_conv_accuracy: 0.9052 - avg_accuracy: 0.9041 - val_loss: 1.0677 - val_op_main_loss: 0.3129 - val_op_conv_loss: 0.2693 - val_avg_loss: 0.2697 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8952\n",
      "Epoch 98/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0370 - op_main_loss: 0.3266 - op_conv_loss: 0.2326 - avg_loss: 0.2622 - op_main_accuracy: 0.8634 - op_conv_accuracy: 0.9084 - avg_accuracy: 0.8990\n",
      "Epoch 00098: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0358 - op_main_loss: 0.3264 - op_conv_loss: 0.2319 - avg_loss: 0.2619 - op_main_accuracy: 0.8632 - op_conv_accuracy: 0.9086 - avg_accuracy: 0.8991 - val_loss: 1.0723 - val_op_main_loss: 0.3137 - val_op_conv_loss: 0.2724 - val_avg_loss: 0.2706 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8971\n",
      "Epoch 99/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0358 - op_main_loss: 0.3263 - op_conv_loss: 0.2333 - avg_loss: 0.2607 - op_main_accuracy: 0.8620 - op_conv_accuracy: 0.9055 - avg_accuracy: 0.9024\n",
      "Epoch 00099: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0324 - op_main_loss: 0.3253 - op_conv_loss: 0.2319 - avg_loss: 0.2597 - op_main_accuracy: 0.8629 - op_conv_accuracy: 0.9062 - avg_accuracy: 0.9031 - val_loss: 1.0960 - val_op_main_loss: 0.3131 - val_op_conv_loss: 0.2917 - val_avg_loss: 0.2758 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8914\n",
      "Epoch 100/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0267 - op_main_loss: 0.3264 - op_conv_loss: 0.2261 - avg_loss: 0.2590 - op_main_accuracy: 0.8690 - op_conv_accuracy: 0.9067 - avg_accuracy: 0.9000\n",
      "Epoch 00100: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0235 - op_main_loss: 0.3259 - op_conv_loss: 0.2245 - avg_loss: 0.2579 - op_main_accuracy: 0.8691 - op_conv_accuracy: 0.9076 - avg_accuracy: 0.9010 - val_loss: 1.1609 - val_op_main_loss: 0.3146 - val_op_conv_loss: 0.3374 - val_avg_loss: 0.2937 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8772\n",
      "Epoch 101/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0249 - op_main_loss: 0.3213 - op_conv_loss: 0.2299 - avg_loss: 0.2585 - op_main_accuracy: 0.8700 - op_conv_accuracy: 0.9072 - avg_accuracy: 0.9067\n",
      "Epoch 00101: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0259 - op_main_loss: 0.3223 - op_conv_loss: 0.2296 - avg_loss: 0.2589 - op_main_accuracy: 0.8689 - op_conv_accuracy: 0.9074 - avg_accuracy: 0.9067 - val_loss: 1.0735 - val_op_main_loss: 0.3109 - val_op_conv_loss: 0.2753 - val_avg_loss: 0.2721 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8952\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 1.0373 - op_main_loss: 0.3246 - op_conv_loss: 0.2360 - avg_loss: 0.2617 - op_main_accuracy: 0.8637 - op_conv_accuracy: 0.9022 - avg_accuracy: 0.8998\n",
      "Epoch 00102: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0373 - op_main_loss: 0.3246 - op_conv_loss: 0.2360 - avg_loss: 0.2617 - op_main_accuracy: 0.8637 - op_conv_accuracy: 0.9022 - avg_accuracy: 0.8998 - val_loss: 1.0848 - val_op_main_loss: 0.3095 - val_op_conv_loss: 0.2885 - val_avg_loss: 0.2721 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8933\n",
      "Epoch 103/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0286 - op_main_loss: 0.3226 - op_conv_loss: 0.2316 - avg_loss: 0.2597 - op_main_accuracy: 0.8709 - op_conv_accuracy: 0.9046 - avg_accuracy: 0.8978\n",
      "Epoch 00103: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0286 - op_main_loss: 0.3227 - op_conv_loss: 0.2313 - avg_loss: 0.2598 - op_main_accuracy: 0.8717 - op_conv_accuracy: 0.9045 - avg_accuracy: 0.8984 - val_loss: 1.0779 - val_op_main_loss: 0.3125 - val_op_conv_loss: 0.2794 - val_avg_loss: 0.2712 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8971\n",
      "Epoch 104/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0193 - op_main_loss: 0.3219 - op_conv_loss: 0.2262 - avg_loss: 0.2565 - op_main_accuracy: 0.8672 - op_conv_accuracy: 0.9070 - avg_accuracy: 0.9048\n",
      "Epoch 00104: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0211 - op_main_loss: 0.3219 - op_conv_loss: 0.2275 - avg_loss: 0.2570 - op_main_accuracy: 0.8679 - op_conv_accuracy: 0.9069 - avg_accuracy: 0.9048 - val_loss: 1.1170 - val_op_main_loss: 0.3149 - val_op_conv_loss: 0.3033 - val_avg_loss: 0.2843 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8848\n",
      "Epoch 105/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9985 - op_main_loss: 0.3121 - op_conv_loss: 0.2214 - avg_loss: 0.2505 - op_main_accuracy: 0.8769 - op_conv_accuracy: 0.9096 - avg_accuracy: 0.9070\n",
      "Epoch 00105: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9947 - op_main_loss: 0.3110 - op_conv_loss: 0.2198 - avg_loss: 0.2493 - op_main_accuracy: 0.8776 - op_conv_accuracy: 0.9104 - avg_accuracy: 0.9078 - val_loss: 1.1481 - val_op_main_loss: 0.3064 - val_op_conv_loss: 0.3400 - val_avg_loss: 0.2871 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8820\n",
      "Epoch 106/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0060 - op_main_loss: 0.3186 - op_conv_loss: 0.2204 - avg_loss: 0.2523 - op_main_accuracy: 0.8676 - op_conv_accuracy: 0.9082 - avg_accuracy: 0.9070\n",
      "Epoch 00106: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0043 - op_main_loss: 0.3184 - op_conv_loss: 0.2195 - avg_loss: 0.2518 - op_main_accuracy: 0.8677 - op_conv_accuracy: 0.9088 - avg_accuracy: 0.9076 - val_loss: 1.0808 - val_op_main_loss: 0.3057 - val_op_conv_loss: 0.2898 - val_avg_loss: 0.2706 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8971\n",
      "Epoch 107/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0019 - op_main_loss: 0.3121 - op_conv_loss: 0.2251 - avg_loss: 0.2501 - op_main_accuracy: 0.8729 - op_conv_accuracy: 0.9053 - avg_accuracy: 0.9053\n",
      "Epoch 00107: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0012 - op_main_loss: 0.3119 - op_conv_loss: 0.2248 - avg_loss: 0.2499 - op_main_accuracy: 0.8731 - op_conv_accuracy: 0.9055 - avg_accuracy: 0.9055 - val_loss: 1.0758 - val_op_main_loss: 0.3069 - val_op_conv_loss: 0.2828 - val_avg_loss: 0.2719 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8942\n",
      "Epoch 108/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9939 - op_main_loss: 0.3096 - op_conv_loss: 0.2210 - avg_loss: 0.2493 - op_main_accuracy: 0.8772 - op_conv_accuracy: 0.9101 - avg_accuracy: 0.9048\n",
      "Epoch 00108: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9937 - op_main_loss: 0.3102 - op_conv_loss: 0.2203 - avg_loss: 0.2493 - op_main_accuracy: 0.8767 - op_conv_accuracy: 0.9100 - avg_accuracy: 0.9050 - val_loss: 1.0995 - val_op_main_loss: 0.3067 - val_op_conv_loss: 0.3033 - val_avg_loss: 0.2754 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8933\n",
      "Epoch 109/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0060 - op_main_loss: 0.3179 - op_conv_loss: 0.2212 - avg_loss: 0.2531 - op_main_accuracy: 0.8709 - op_conv_accuracy: 0.9106 - avg_accuracy: 0.9065\n",
      "Epoch 00109: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0054 - op_main_loss: 0.3183 - op_conv_loss: 0.2203 - avg_loss: 0.2529 - op_main_accuracy: 0.8705 - op_conv_accuracy: 0.9112 - avg_accuracy: 0.9067 - val_loss: 1.0690 - val_op_main_loss: 0.3044 - val_op_conv_loss: 0.2823 - val_avg_loss: 0.2685 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8980\n",
      "Epoch 110/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9951 - op_main_loss: 0.3085 - op_conv_loss: 0.2237 - avg_loss: 0.2491 - op_main_accuracy: 0.8774 - op_conv_accuracy: 0.9058 - avg_accuracy: 0.9065\n",
      "Epoch 00110: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0008 - op_main_loss: 0.3099 - op_conv_loss: 0.2261 - avg_loss: 0.2511 - op_main_accuracy: 0.8767 - op_conv_accuracy: 0.9041 - avg_accuracy: 0.9050 - val_loss: 1.0778 - val_op_main_loss: 0.3041 - val_op_conv_loss: 0.2875 - val_avg_loss: 0.2726 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8895\n",
      "Epoch 111/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0023 - op_main_loss: 0.3121 - op_conv_loss: 0.2249 - avg_loss: 0.2520 - op_main_accuracy: 0.8740 - op_conv_accuracy: 0.9058 - avg_accuracy: 0.9048\n",
      "Epoch 00111: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0033 - op_main_loss: 0.3119 - op_conv_loss: 0.2259 - avg_loss: 0.2521 - op_main_accuracy: 0.8738 - op_conv_accuracy: 0.9057 - avg_accuracy: 0.9048 - val_loss: 1.0726 - val_op_main_loss: 0.3038 - val_op_conv_loss: 0.2851 - val_avg_loss: 0.2704 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8942\n",
      "Epoch 112/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0024 - op_main_loss: 0.3111 - op_conv_loss: 0.2262 - avg_loss: 0.2521 - op_main_accuracy: 0.8738 - op_conv_accuracy: 0.9071 - avg_accuracy: 0.9071\n",
      "Epoch 00112: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0024 - op_main_loss: 0.3111 - op_conv_loss: 0.2262 - avg_loss: 0.2521 - op_main_accuracy: 0.8738 - op_conv_accuracy: 0.9071 - avg_accuracy: 0.9071 - val_loss: 1.0659 - val_op_main_loss: 0.3029 - val_op_conv_loss: 0.2809 - val_avg_loss: 0.2693 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8971\n",
      "Epoch 113/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9834 - op_main_loss: 0.3080 - op_conv_loss: 0.2167 - avg_loss: 0.2458 - op_main_accuracy: 0.8791 - op_conv_accuracy: 0.9134 - avg_accuracy: 0.9091\n",
      "Epoch 00113: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9832 - op_main_loss: 0.3075 - op_conv_loss: 0.2171 - avg_loss: 0.2457 - op_main_accuracy: 0.8790 - op_conv_accuracy: 0.9133 - avg_accuracy: 0.9093 - val_loss: 1.0633 - val_op_main_loss: 0.3026 - val_op_conv_loss: 0.2793 - val_avg_loss: 0.2686 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8924\n",
      "Epoch 114/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.9806 - op_main_loss: 0.3083 - op_conv_loss: 0.2153 - avg_loss: 0.2443 - op_main_accuracy: 0.8714 - op_conv_accuracy: 0.9147 - avg_accuracy: 0.9082\n",
      "Epoch 00114: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9805 - op_main_loss: 0.3081 - op_conv_loss: 0.2155 - avg_loss: 0.2442 - op_main_accuracy: 0.8715 - op_conv_accuracy: 0.9145 - avg_accuracy: 0.9081 - val_loss: 1.0849 - val_op_main_loss: 0.3025 - val_op_conv_loss: 0.2965 - val_avg_loss: 0.2732 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8980\n",
      "Epoch 115/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9647 - op_main_loss: 0.3048 - op_conv_loss: 0.2074 - avg_loss: 0.2400 - op_main_accuracy: 0.8726 - op_conv_accuracy: 0.9164 - avg_accuracy: 0.9079\n",
      "Epoch 00115: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9679 - op_main_loss: 0.3055 - op_conv_loss: 0.2088 - avg_loss: 0.2411 - op_main_accuracy: 0.8726 - op_conv_accuracy: 0.9154 - avg_accuracy: 0.9074 - val_loss: 1.1249 - val_op_main_loss: 0.3056 - val_op_conv_loss: 0.3226 - val_avg_loss: 0.2842 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8867\n",
      "Epoch 116/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9915 - op_main_loss: 0.3076 - op_conv_loss: 0.2228 - avg_loss: 0.2487 - op_main_accuracy: 0.8779 - op_conv_accuracy: 0.9129 - avg_accuracy: 0.9101\n",
      "Epoch 00116: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9901 - op_main_loss: 0.3071 - op_conv_loss: 0.2223 - avg_loss: 0.2483 - op_main_accuracy: 0.8781 - op_conv_accuracy: 0.9130 - avg_accuracy: 0.9102 - val_loss: 1.1237 - val_op_main_loss: 0.3031 - val_op_conv_loss: 0.3249 - val_avg_loss: 0.2834 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8867\n",
      "Epoch 117/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9864 - op_main_loss: 0.3054 - op_conv_loss: 0.2211 - avg_loss: 0.2479 - op_main_accuracy: 0.8784 - op_conv_accuracy: 0.9094 - avg_accuracy: 0.9079\n",
      "Epoch 00117: val_avg_accuracy did not improve from 0.89802\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9880 - op_main_loss: 0.3057 - op_conv_loss: 0.2218 - avg_loss: 0.2484 - op_main_accuracy: 0.8783 - op_conv_accuracy: 0.9095 - avg_accuracy: 0.9083 - val_loss: 1.0587 - val_op_main_loss: 0.2983 - val_op_conv_loss: 0.2821 - val_avg_loss: 0.2664 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8971\n",
      "Epoch 118/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9818 - op_main_loss: 0.3010 - op_conv_loss: 0.2228 - avg_loss: 0.2463 - op_main_accuracy: 0.8776 - op_conv_accuracy: 0.9097 - avg_accuracy: 0.9041\n",
      "Epoch 00118: val_avg_accuracy improved from 0.89802 to 0.89991, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9818 - op_main_loss: 0.3010 - op_conv_loss: 0.2228 - avg_loss: 0.2463 - op_main_accuracy: 0.8776 - op_conv_accuracy: 0.9097 - avg_accuracy: 0.9041 - val_loss: 1.0694 - val_op_main_loss: 0.2986 - val_op_conv_loss: 0.2915 - val_avg_loss: 0.2680 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8999\n",
      "Epoch 119/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9701 - op_main_loss: 0.3019 - op_conv_loss: 0.2139 - avg_loss: 0.2430 - op_main_accuracy: 0.8743 - op_conv_accuracy: 0.9046 - avg_accuracy: 0.9075\n",
      "Epoch 00119: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9769 - op_main_loss: 0.3036 - op_conv_loss: 0.2169 - avg_loss: 0.2451 - op_main_accuracy: 0.8731 - op_conv_accuracy: 0.9031 - avg_accuracy: 0.9062 - val_loss: 1.0837 - val_op_main_loss: 0.2984 - val_op_conv_loss: 0.3018 - val_avg_loss: 0.2721 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8905\n",
      "Epoch 120/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9737 - op_main_loss: 0.3023 - op_conv_loss: 0.2168 - avg_loss: 0.2431 - op_main_accuracy: 0.8740 - op_conv_accuracy: 0.9115 - avg_accuracy: 0.9067\n",
      "Epoch 00120: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9781 - op_main_loss: 0.3038 - op_conv_loss: 0.2184 - avg_loss: 0.2445 - op_main_accuracy: 0.8731 - op_conv_accuracy: 0.9107 - avg_accuracy: 0.9052 - val_loss: 1.0630 - val_op_main_loss: 0.2964 - val_op_conv_loss: 0.2880 - val_avg_loss: 0.2673 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8999\n",
      "Epoch 121/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9755 - op_main_loss: 0.3002 - op_conv_loss: 0.2199 - avg_loss: 0.2441 - op_main_accuracy: 0.8848 - op_conv_accuracy: 0.9125 - avg_accuracy: 0.9113\n",
      "Epoch 00121: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9735 - op_main_loss: 0.2994 - op_conv_loss: 0.2193 - avg_loss: 0.2434 - op_main_accuracy: 0.8849 - op_conv_accuracy: 0.9126 - avg_accuracy: 0.9114 - val_loss: 1.0546 - val_op_main_loss: 0.2977 - val_op_conv_loss: 0.2798 - val_avg_loss: 0.2658 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8990\n",
      "Epoch 122/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9698 - op_main_loss: 0.2997 - op_conv_loss: 0.2168 - avg_loss: 0.2423 - op_main_accuracy: 0.8811 - op_conv_accuracy: 0.9150 - avg_accuracy: 0.9125\n",
      "Epoch 00122: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9682 - op_main_loss: 0.2988 - op_conv_loss: 0.2166 - avg_loss: 0.2418 - op_main_accuracy: 0.8819 - op_conv_accuracy: 0.9154 - avg_accuracy: 0.9133 - val_loss: 1.0548 - val_op_main_loss: 0.2972 - val_op_conv_loss: 0.2799 - val_avg_loss: 0.2669 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8990\n",
      "Epoch 123/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9706 - op_main_loss: 0.2980 - op_conv_loss: 0.2183 - avg_loss: 0.2437 - op_main_accuracy: 0.8777 - op_conv_accuracy: 0.9099 - avg_accuracy: 0.9087\n",
      "Epoch 00123: val_avg_accuracy improved from 0.89991 to 0.90085, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9675 - op_main_loss: 0.2970 - op_conv_loss: 0.2172 - avg_loss: 0.2426 - op_main_accuracy: 0.8788 - op_conv_accuracy: 0.9107 - avg_accuracy: 0.9097 - val_loss: 1.0550 - val_op_main_loss: 0.2972 - val_op_conv_loss: 0.2798 - val_avg_loss: 0.2673 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 124/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9721 - op_main_loss: 0.3007 - op_conv_loss: 0.2172 - avg_loss: 0.2436 - op_main_accuracy: 0.8796 - op_conv_accuracy: 0.9104 - avg_accuracy: 0.9104\n",
      "Epoch 00124: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9719 - op_main_loss: 0.3008 - op_conv_loss: 0.2170 - avg_loss: 0.2435 - op_main_accuracy: 0.8804 - op_conv_accuracy: 0.9102 - avg_accuracy: 0.9107 - val_loss: 1.1358 - val_op_main_loss: 0.3009 - val_op_conv_loss: 0.3362 - val_avg_loss: 0.2882 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8820\n",
      "Epoch 125/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9674 - op_main_loss: 0.2991 - op_conv_loss: 0.2163 - avg_loss: 0.2415 - op_main_accuracy: 0.8740 - op_conv_accuracy: 0.9120 - avg_accuracy: 0.9082\n",
      "Epoch 00125: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9695 - op_main_loss: 0.3003 - op_conv_loss: 0.2167 - avg_loss: 0.2421 - op_main_accuracy: 0.8736 - op_conv_accuracy: 0.9112 - avg_accuracy: 0.9076 - val_loss: 1.0492 - val_op_main_loss: 0.2934 - val_op_conv_loss: 0.2813 - val_avg_loss: 0.2639 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9628 - op_main_loss: 0.2956 - op_conv_loss: 0.2162 - avg_loss: 0.2407 - op_main_accuracy: 0.8841 - op_conv_accuracy: 0.9125 - avg_accuracy: 0.9086\n",
      "Epoch 00126: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9623 - op_main_loss: 0.2954 - op_conv_loss: 0.2161 - avg_loss: 0.2405 - op_main_accuracy: 0.8845 - op_conv_accuracy: 0.9126 - avg_accuracy: 0.9088 - val_loss: 1.0462 - val_op_main_loss: 0.2942 - val_op_conv_loss: 0.2775 - val_avg_loss: 0.2642 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9008\n",
      "Epoch 127/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9604 - op_main_loss: 0.2984 - op_conv_loss: 0.2116 - avg_loss: 0.2401 - op_main_accuracy: 0.8823 - op_conv_accuracy: 0.9104 - avg_accuracy: 0.9072\n",
      "Epoch 00127: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9686 - op_main_loss: 0.3007 - op_conv_loss: 0.2149 - avg_loss: 0.2427 - op_main_accuracy: 0.8821 - op_conv_accuracy: 0.9093 - avg_accuracy: 0.9062 - val_loss: 1.0414 - val_op_main_loss: 0.2923 - val_op_conv_loss: 0.2758 - val_avg_loss: 0.2631 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9008\n",
      "Epoch 128/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9529 - op_main_loss: 0.2923 - op_conv_loss: 0.2128 - avg_loss: 0.2377 - op_main_accuracy: 0.8892 - op_conv_accuracy: 0.9138 - avg_accuracy: 0.9150\n",
      "Epoch 00128: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9530 - op_main_loss: 0.2922 - op_conv_loss: 0.2130 - avg_loss: 0.2377 - op_main_accuracy: 0.8892 - op_conv_accuracy: 0.9138 - avg_accuracy: 0.9149 - val_loss: 1.0520 - val_op_main_loss: 0.2904 - val_op_conv_loss: 0.2861 - val_avg_loss: 0.2656 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8961\n",
      "Epoch 129/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9588 - op_main_loss: 0.2937 - op_conv_loss: 0.2150 - avg_loss: 0.2401 - op_main_accuracy: 0.8833 - op_conv_accuracy: 0.9160 - avg_accuracy: 0.9117\n",
      "Epoch 00129: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9589 - op_main_loss: 0.2940 - op_conv_loss: 0.2149 - avg_loss: 0.2402 - op_main_accuracy: 0.8833 - op_conv_accuracy: 0.9161 - avg_accuracy: 0.9116 - val_loss: 1.0454 - val_op_main_loss: 0.2912 - val_op_conv_loss: 0.2796 - val_avg_loss: 0.2648 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 130/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9409 - op_main_loss: 0.2950 - op_conv_loss: 0.2023 - avg_loss: 0.2337 - op_main_accuracy: 0.8845 - op_conv_accuracy: 0.9175 - avg_accuracy: 0.9122\n",
      "Epoch 00130: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9392 - op_main_loss: 0.2943 - op_conv_loss: 0.2018 - avg_loss: 0.2332 - op_main_accuracy: 0.8849 - op_conv_accuracy: 0.9178 - avg_accuracy: 0.9126 - val_loss: 1.0619 - val_op_main_loss: 0.2920 - val_op_conv_loss: 0.2943 - val_avg_loss: 0.2657 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8971\n",
      "Epoch 131/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9752 - op_main_loss: 0.2991 - op_conv_loss: 0.2218 - avg_loss: 0.2445 - op_main_accuracy: 0.8788 - op_conv_accuracy: 0.9094 - avg_accuracy: 0.9053\n",
      "Epoch 00131: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9737 - op_main_loss: 0.2988 - op_conv_loss: 0.2210 - avg_loss: 0.2440 - op_main_accuracy: 0.8790 - op_conv_accuracy: 0.9097 - avg_accuracy: 0.9057 - val_loss: 1.0380 - val_op_main_loss: 0.2903 - val_op_conv_loss: 0.2751 - val_avg_loss: 0.2630 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8990\n",
      "Epoch 132/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9470 - op_main_loss: 0.2914 - op_conv_loss: 0.2101 - avg_loss: 0.2358 - op_main_accuracy: 0.8842 - op_conv_accuracy: 0.9107 - avg_accuracy: 0.9114\n",
      "Epoch 00132: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9470 - op_main_loss: 0.2914 - op_conv_loss: 0.2101 - avg_loss: 0.2358 - op_main_accuracy: 0.8842 - op_conv_accuracy: 0.9107 - avg_accuracy: 0.9114 - val_loss: 1.0352 - val_op_main_loss: 0.2897 - val_op_conv_loss: 0.2738 - val_avg_loss: 0.2622 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 133/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9420 - op_main_loss: 0.2902 - op_conv_loss: 0.2079 - avg_loss: 0.2343 - op_main_accuracy: 0.8880 - op_conv_accuracy: 0.9159 - avg_accuracy: 0.9171\n",
      "Epoch 00133: val_avg_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9420 - op_main_loss: 0.2902 - op_conv_loss: 0.2079 - avg_loss: 0.2343 - op_main_accuracy: 0.8880 - op_conv_accuracy: 0.9159 - avg_accuracy: 0.9171 - val_loss: 1.0677 - val_op_main_loss: 0.2911 - val_op_conv_loss: 0.2992 - val_avg_loss: 0.2681 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8924\n",
      "Epoch 134/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9340 - op_main_loss: 0.2864 - op_conv_loss: 0.2068 - avg_loss: 0.2315 - op_main_accuracy: 0.8881 - op_conv_accuracy: 0.9125 - avg_accuracy: 0.9109\n",
      "Epoch 00134: val_avg_accuracy improved from 0.90085 to 0.90368, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9340 - op_main_loss: 0.2868 - op_conv_loss: 0.2064 - avg_loss: 0.2315 - op_main_accuracy: 0.8880 - op_conv_accuracy: 0.9123 - avg_accuracy: 0.9107 - val_loss: 1.0272 - val_op_main_loss: 0.2877 - val_op_conv_loss: 0.2705 - val_avg_loss: 0.2598 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9037\n",
      "Epoch 135/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9391 - op_main_loss: 0.2861 - op_conv_loss: 0.2110 - avg_loss: 0.2329 - op_main_accuracy: 0.8854 - op_conv_accuracy: 0.9172 - avg_accuracy: 0.9140\n",
      "Epoch 00135: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.9378 - op_main_loss: 0.2858 - op_conv_loss: 0.2105 - avg_loss: 0.2324 - op_main_accuracy: 0.8861 - op_conv_accuracy: 0.9178 - avg_accuracy: 0.9142 - val_loss: 1.0314 - val_op_main_loss: 0.2890 - val_op_conv_loss: 0.2716 - val_avg_loss: 0.2618 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9027\n",
      "Epoch 136/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9368 - op_main_loss: 0.2916 - op_conv_loss: 0.2031 - avg_loss: 0.2330 - op_main_accuracy: 0.8858 - op_conv_accuracy: 0.9183 - avg_accuracy: 0.9154\n",
      "Epoch 00136: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9342 - op_main_loss: 0.2909 - op_conv_loss: 0.2021 - avg_loss: 0.2322 - op_main_accuracy: 0.8863 - op_conv_accuracy: 0.9190 - avg_accuracy: 0.9161 - val_loss: 1.0450 - val_op_main_loss: 0.2904 - val_op_conv_loss: 0.2807 - val_avg_loss: 0.2646 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 137/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9463 - op_main_loss: 0.2893 - op_conv_loss: 0.2120 - avg_loss: 0.2358 - op_main_accuracy: 0.8807 - op_conv_accuracy: 0.9112 - avg_accuracy: 0.9096\n",
      "Epoch 00137: val_avg_accuracy improved from 0.90368 to 0.90652, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9466 - op_main_loss: 0.2892 - op_conv_loss: 0.2123 - avg_loss: 0.2358 - op_main_accuracy: 0.8807 - op_conv_accuracy: 0.9109 - avg_accuracy: 0.9095 - val_loss: 1.0466 - val_op_main_loss: 0.2938 - val_op_conv_loss: 0.2807 - val_avg_loss: 0.2632 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9385 - op_main_loss: 0.2837 - op_conv_loss: 0.2123 - avg_loss: 0.2335 - op_main_accuracy: 0.8857 - op_conv_accuracy: 0.9145 - avg_accuracy: 0.9128\n",
      "Epoch 00138: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9408 - op_main_loss: 0.2848 - op_conv_loss: 0.2128 - avg_loss: 0.2342 - op_main_accuracy: 0.8854 - op_conv_accuracy: 0.9140 - avg_accuracy: 0.9119 - val_loss: 1.0353 - val_op_main_loss: 0.2874 - val_op_conv_loss: 0.2765 - val_avg_loss: 0.2624 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9018\n",
      "Epoch 139/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9227 - op_main_loss: 0.2824 - op_conv_loss: 0.2033 - avg_loss: 0.2282 - op_main_accuracy: 0.8913 - op_conv_accuracy: 0.9216 - avg_accuracy: 0.9197\n",
      "Epoch 00139: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9232 - op_main_loss: 0.2825 - op_conv_loss: 0.2035 - avg_loss: 0.2283 - op_main_accuracy: 0.8913 - op_conv_accuracy: 0.9216 - avg_accuracy: 0.9194 - val_loss: 1.0367 - val_op_main_loss: 0.2875 - val_op_conv_loss: 0.2787 - val_avg_loss: 0.2617 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 140/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9262 - op_main_loss: 0.2864 - op_conv_loss: 0.2013 - avg_loss: 0.2296 - op_main_accuracy: 0.8800 - op_conv_accuracy: 0.9153 - avg_accuracy: 0.9096\n",
      "Epoch 00140: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9245 - op_main_loss: 0.2860 - op_conv_loss: 0.2005 - avg_loss: 0.2291 - op_main_accuracy: 0.8807 - op_conv_accuracy: 0.9159 - avg_accuracy: 0.9102 - val_loss: 1.0535 - val_op_main_loss: 0.2885 - val_op_conv_loss: 0.2904 - val_avg_loss: 0.2655 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8980\n",
      "Epoch 141/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9125 - op_main_loss: 0.2760 - op_conv_loss: 0.2028 - avg_loss: 0.2245 - op_main_accuracy: 0.8912 - op_conv_accuracy: 0.9191 - avg_accuracy: 0.9164\n",
      "Epoch 00141: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9187 - op_main_loss: 0.2781 - op_conv_loss: 0.2050 - avg_loss: 0.2264 - op_main_accuracy: 0.8901 - op_conv_accuracy: 0.9180 - avg_accuracy: 0.9156 - val_loss: 1.0673 - val_op_main_loss: 0.2898 - val_op_conv_loss: 0.2998 - val_avg_loss: 0.2684 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8952\n",
      "Epoch 142/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9226 - op_main_loss: 0.2828 - op_conv_loss: 0.2026 - avg_loss: 0.2283 - op_main_accuracy: 0.8903 - op_conv_accuracy: 0.9172 - avg_accuracy: 0.9146\n",
      "Epoch 00142: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9229 - op_main_loss: 0.2828 - op_conv_loss: 0.2028 - avg_loss: 0.2284 - op_main_accuracy: 0.8906 - op_conv_accuracy: 0.9173 - avg_accuracy: 0.9147 - val_loss: 1.0662 - val_op_main_loss: 0.2921 - val_op_conv_loss: 0.2983 - val_avg_loss: 0.2669 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 143/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9148 - op_main_loss: 0.2788 - op_conv_loss: 0.2005 - avg_loss: 0.2266 - op_main_accuracy: 0.8863 - op_conv_accuracy: 0.9175 - avg_accuracy: 0.9142\n",
      "Epoch 00143: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9125 - op_main_loss: 0.2782 - op_conv_loss: 0.1995 - avg_loss: 0.2259 - op_main_accuracy: 0.8875 - op_conv_accuracy: 0.9182 - avg_accuracy: 0.9152 - val_loss: 1.0718 - val_op_main_loss: 0.2874 - val_op_conv_loss: 0.3068 - val_avg_loss: 0.2686 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8924\n",
      "Epoch 144/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9145 - op_main_loss: 0.2792 - op_conv_loss: 0.2007 - avg_loss: 0.2255 - op_main_accuracy: 0.8865 - op_conv_accuracy: 0.9182 - avg_accuracy: 0.9165\n",
      "Epoch 00144: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9160 - op_main_loss: 0.2796 - op_conv_loss: 0.2013 - avg_loss: 0.2260 - op_main_accuracy: 0.8863 - op_conv_accuracy: 0.9182 - avg_accuracy: 0.9166 - val_loss: 1.0320 - val_op_main_loss: 0.2847 - val_op_conv_loss: 0.2777 - val_avg_loss: 0.2605 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 145/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9117 - op_main_loss: 0.2795 - op_conv_loss: 0.1986 - avg_loss: 0.2244 - op_main_accuracy: 0.8886 - op_conv_accuracy: 0.9198 - avg_accuracy: 0.9156\n",
      "Epoch 00145: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9098 - op_main_loss: 0.2786 - op_conv_loss: 0.1982 - avg_loss: 0.2238 - op_main_accuracy: 0.8894 - op_conv_accuracy: 0.9199 - avg_accuracy: 0.9159 - val_loss: 1.0420 - val_op_main_loss: 0.2932 - val_op_conv_loss: 0.2773 - val_avg_loss: 0.2625 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 146/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9077 - op_main_loss: 0.2748 - op_conv_loss: 0.2005 - avg_loss: 0.2236 - op_main_accuracy: 0.8866 - op_conv_accuracy: 0.9167 - avg_accuracy: 0.9152\n",
      "Epoch 00146: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9093 - op_main_loss: 0.2744 - op_conv_loss: 0.2019 - avg_loss: 0.2241 - op_main_accuracy: 0.8866 - op_conv_accuracy: 0.9154 - avg_accuracy: 0.9142 - val_loss: 1.0415 - val_op_main_loss: 0.2897 - val_op_conv_loss: 0.2766 - val_avg_loss: 0.2665 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8933\n",
      "Epoch 147/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9221 - op_main_loss: 0.2806 - op_conv_loss: 0.2043 - avg_loss: 0.2286 - op_main_accuracy: 0.8862 - op_conv_accuracy: 0.9163 - avg_accuracy: 0.9136\n",
      "Epoch 00147: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9232 - op_main_loss: 0.2809 - op_conv_loss: 0.2049 - avg_loss: 0.2288 - op_main_accuracy: 0.8859 - op_conv_accuracy: 0.9164 - avg_accuracy: 0.9133 - val_loss: 1.1766 - val_op_main_loss: 0.2933 - val_op_conv_loss: 0.3802 - val_avg_loss: 0.2945 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8744\n",
      "Epoch 148/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9076 - op_main_loss: 0.2754 - op_conv_loss: 0.2000 - avg_loss: 0.2237 - op_main_accuracy: 0.8903 - op_conv_accuracy: 0.9210 - avg_accuracy: 0.9158\n",
      "Epoch 00148: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9045 - op_main_loss: 0.2743 - op_conv_loss: 0.1989 - avg_loss: 0.2228 - op_main_accuracy: 0.8911 - op_conv_accuracy: 0.9216 - avg_accuracy: 0.9164 - val_loss: 1.0408 - val_op_main_loss: 0.2847 - val_op_conv_loss: 0.2849 - val_avg_loss: 0.2628 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8971\n",
      "Epoch 149/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9075 - op_main_loss: 0.2734 - op_conv_loss: 0.2016 - avg_loss: 0.2240 - op_main_accuracy: 0.8957 - op_conv_accuracy: 0.9156 - avg_accuracy: 0.9149\n",
      "Epoch 00149: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9056 - op_main_loss: 0.2729 - op_conv_loss: 0.2008 - avg_loss: 0.2235 - op_main_accuracy: 0.8960 - op_conv_accuracy: 0.9161 - avg_accuracy: 0.9152 - val_loss: 1.0322 - val_op_main_loss: 0.2845 - val_op_conv_loss: 0.2779 - val_avg_loss: 0.2613 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9056\n",
      "Epoch 150/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.9015 - op_main_loss: 0.2727 - op_conv_loss: 0.1984 - avg_loss: 0.2220 - op_main_accuracy: 0.8964 - op_conv_accuracy: 0.9195 - avg_accuracy: 0.9163\n",
      "Epoch 00150: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8985 - op_main_loss: 0.2715 - op_conv_loss: 0.1975 - avg_loss: 0.2211 - op_main_accuracy: 0.8977 - op_conv_accuracy: 0.9201 - avg_accuracy: 0.9168 - val_loss: 1.0628 - val_op_main_loss: 0.2877 - val_op_conv_loss: 0.2999 - val_avg_loss: 0.2670 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8990\n",
      "Epoch 151/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9125 - op_main_loss: 0.2735 - op_conv_loss: 0.2059 - avg_loss: 0.2252 - op_main_accuracy: 0.8961 - op_conv_accuracy: 0.9145 - avg_accuracy: 0.9140\n",
      "Epoch 00151: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9159 - op_main_loss: 0.2744 - op_conv_loss: 0.2072 - avg_loss: 0.2263 - op_main_accuracy: 0.8941 - op_conv_accuracy: 0.9135 - avg_accuracy: 0.9128 - val_loss: 1.0321 - val_op_main_loss: 0.2851 - val_op_conv_loss: 0.2767 - val_avg_loss: 0.2622 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 152/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8973 - op_main_loss: 0.2724 - op_conv_loss: 0.1956 - avg_loss: 0.2212 - op_main_accuracy: 0.8906 - op_conv_accuracy: 0.9204 - avg_accuracy: 0.9185\n",
      "Epoch 00152: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8950 - op_main_loss: 0.2714 - op_conv_loss: 0.1950 - avg_loss: 0.2205 - op_main_accuracy: 0.8915 - op_conv_accuracy: 0.9204 - avg_accuracy: 0.9185 - val_loss: 1.0438 - val_op_main_loss: 0.2838 - val_op_conv_loss: 0.2876 - val_avg_loss: 0.2644 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.9027\n",
      "Epoch 153/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9001 - op_main_loss: 0.2753 - op_conv_loss: 0.1954 - avg_loss: 0.2217 - op_main_accuracy: 0.8940 - op_conv_accuracy: 0.9221 - avg_accuracy: 0.9202\n",
      "Epoch 00153: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8994 - op_main_loss: 0.2749 - op_conv_loss: 0.1952 - avg_loss: 0.2215 - op_main_accuracy: 0.8937 - op_conv_accuracy: 0.9227 - avg_accuracy: 0.9201 - val_loss: 1.0874 - val_op_main_loss: 0.2906 - val_op_conv_loss: 0.3142 - val_avg_loss: 0.2751 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8876\n",
      "Epoch 154/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8984 - op_main_loss: 0.2726 - op_conv_loss: 0.1965 - avg_loss: 0.2219 - op_main_accuracy: 0.8885 - op_conv_accuracy: 0.9164 - avg_accuracy: 0.9135\n",
      "Epoch 00154: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8984 - op_main_loss: 0.2726 - op_conv_loss: 0.1965 - avg_loss: 0.2219 - op_main_accuracy: 0.8885 - op_conv_accuracy: 0.9164 - avg_accuracy: 0.9135 - val_loss: 1.0579 - val_op_main_loss: 0.2859 - val_op_conv_loss: 0.2981 - val_avg_loss: 0.2667 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8952\n",
      "Epoch 155/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8955 - op_main_loss: 0.2704 - op_conv_loss: 0.1985 - avg_loss: 0.2195 - op_main_accuracy: 0.8965 - op_conv_accuracy: 0.9237 - avg_accuracy: 0.9208\n",
      "Epoch 00155: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8955 - op_main_loss: 0.2704 - op_conv_loss: 0.1985 - avg_loss: 0.2195 - op_main_accuracy: 0.8965 - op_conv_accuracy: 0.9237 - avg_accuracy: 0.9208 - val_loss: 1.0491 - val_op_main_loss: 0.2875 - val_op_conv_loss: 0.2884 - val_avg_loss: 0.2661 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8971\n",
      "Epoch 156/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8968 - op_main_loss: 0.2675 - op_conv_loss: 0.2005 - avg_loss: 0.2217 - op_main_accuracy: 0.9002 - op_conv_accuracy: 0.9147 - avg_accuracy: 0.9184\n",
      "Epoch 00156: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8970 - op_main_loss: 0.2677 - op_conv_loss: 0.2004 - avg_loss: 0.2218 - op_main_accuracy: 0.9003 - op_conv_accuracy: 0.9145 - avg_accuracy: 0.9180 - val_loss: 1.0545 - val_op_main_loss: 0.2828 - val_op_conv_loss: 0.2983 - val_avg_loss: 0.2664 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8914\n",
      "Epoch 157/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8970 - op_main_loss: 0.2694 - op_conv_loss: 0.1999 - avg_loss: 0.2208 - op_main_accuracy: 0.8976 - op_conv_accuracy: 0.9214 - avg_accuracy: 0.9209\n",
      "Epoch 00157: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9024 - op_main_loss: 0.2718 - op_conv_loss: 0.2013 - avg_loss: 0.2225 - op_main_accuracy: 0.8970 - op_conv_accuracy: 0.9204 - avg_accuracy: 0.9197 - val_loss: 1.0633 - val_op_main_loss: 0.2853 - val_op_conv_loss: 0.3011 - val_avg_loss: 0.2701 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8980\n",
      "Epoch 158/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8926 - op_main_loss: 0.2708 - op_conv_loss: 0.1950 - avg_loss: 0.2200 - op_main_accuracy: 0.8924 - op_conv_accuracy: 0.9198 - avg_accuracy: 0.9158\n",
      "Epoch 00158: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8911 - op_main_loss: 0.2703 - op_conv_loss: 0.1945 - avg_loss: 0.2195 - op_main_accuracy: 0.8927 - op_conv_accuracy: 0.9201 - avg_accuracy: 0.9164 - val_loss: 1.0487 - val_op_main_loss: 0.2882 - val_op_conv_loss: 0.2874 - val_avg_loss: 0.2665 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8999\n",
      "Epoch 159/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8863 - op_main_loss: 0.2665 - op_conv_loss: 0.1950 - avg_loss: 0.2179 - op_main_accuracy: 0.8995 - op_conv_accuracy: 0.9205 - avg_accuracy: 0.9208\n",
      "Epoch 00159: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8848 - op_main_loss: 0.2658 - op_conv_loss: 0.1947 - avg_loss: 0.2175 - op_main_accuracy: 0.8998 - op_conv_accuracy: 0.9208 - avg_accuracy: 0.9211 - val_loss: 1.0325 - val_op_main_loss: 0.2827 - val_op_conv_loss: 0.2815 - val_avg_loss: 0.2618 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8999\n",
      "Epoch 160/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8918 - op_main_loss: 0.2680 - op_conv_loss: 0.1984 - avg_loss: 0.2191 - op_main_accuracy: 0.8957 - op_conv_accuracy: 0.9187 - avg_accuracy: 0.9178\n",
      "Epoch 00160: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8906 - op_main_loss: 0.2673 - op_conv_loss: 0.1983 - avg_loss: 0.2187 - op_main_accuracy: 0.8960 - op_conv_accuracy: 0.9190 - avg_accuracy: 0.9180 - val_loss: 1.0343 - val_op_main_loss: 0.2809 - val_op_conv_loss: 0.2857 - val_avg_loss: 0.2616 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9008\n",
      "Epoch 161/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8946 - op_main_loss: 0.2687 - op_conv_loss: 0.1986 - avg_loss: 0.2215 - op_main_accuracy: 0.8911 - op_conv_accuracy: 0.9214 - avg_accuracy: 0.9181\n",
      "Epoch 00161: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8942 - op_main_loss: 0.2686 - op_conv_loss: 0.1984 - avg_loss: 0.2213 - op_main_accuracy: 0.8911 - op_conv_accuracy: 0.9216 - avg_accuracy: 0.9180 - val_loss: 1.0319 - val_op_main_loss: 0.2834 - val_op_conv_loss: 0.2799 - val_avg_loss: 0.2630 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.8764 - op_main_loss: 0.2655 - op_conv_loss: 0.1899 - avg_loss: 0.2154 - op_main_accuracy: 0.8987 - op_conv_accuracy: 0.9237 - avg_accuracy: 0.9215\n",
      "Epoch 00162: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8761 - op_main_loss: 0.2653 - op_conv_loss: 0.1898 - avg_loss: 0.2153 - op_main_accuracy: 0.8991 - op_conv_accuracy: 0.9232 - avg_accuracy: 0.9213 - val_loss: 1.0380 - val_op_main_loss: 0.2808 - val_op_conv_loss: 0.2909 - val_avg_loss: 0.2606 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9027\n",
      "Epoch 163/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8975 - op_main_loss: 0.2678 - op_conv_loss: 0.2018 - avg_loss: 0.2223 - op_main_accuracy: 0.8929 - op_conv_accuracy: 0.9174 - avg_accuracy: 0.9164\n",
      "Epoch 00163: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9011 - op_main_loss: 0.2687 - op_conv_loss: 0.2034 - avg_loss: 0.2234 - op_main_accuracy: 0.8920 - op_conv_accuracy: 0.9171 - avg_accuracy: 0.9161 - val_loss: 1.0244 - val_op_main_loss: 0.2794 - val_op_conv_loss: 0.2798 - val_avg_loss: 0.2597 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8971\n",
      "Epoch 164/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8771 - op_main_loss: 0.2650 - op_conv_loss: 0.1911 - avg_loss: 0.2155 - op_main_accuracy: 0.8965 - op_conv_accuracy: 0.9209 - avg_accuracy: 0.9219\n",
      "Epoch 00164: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8765 - op_main_loss: 0.2649 - op_conv_loss: 0.1909 - avg_loss: 0.2153 - op_main_accuracy: 0.8967 - op_conv_accuracy: 0.9211 - avg_accuracy: 0.9220 - val_loss: 1.0398 - val_op_main_loss: 0.2859 - val_op_conv_loss: 0.2859 - val_avg_loss: 0.2625 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 165/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8823 - op_main_loss: 0.2654 - op_conv_loss: 0.1946 - avg_loss: 0.2170 - op_main_accuracy: 0.8991 - op_conv_accuracy: 0.9203 - avg_accuracy: 0.9194\n",
      "Epoch 00165: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8807 - op_main_loss: 0.2650 - op_conv_loss: 0.1940 - avg_loss: 0.2166 - op_main_accuracy: 0.8998 - op_conv_accuracy: 0.9206 - avg_accuracy: 0.9197 - val_loss: 1.0649 - val_op_main_loss: 0.2825 - val_op_conv_loss: 0.3090 - val_avg_loss: 0.2684 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8952\n",
      "Epoch 166/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8693 - op_main_loss: 0.2623 - op_conv_loss: 0.1889 - avg_loss: 0.2131 - op_main_accuracy: 0.8998 - op_conv_accuracy: 0.9238 - avg_accuracy: 0.9190\n",
      "Epoch 00166: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8704 - op_main_loss: 0.2627 - op_conv_loss: 0.1891 - avg_loss: 0.2135 - op_main_accuracy: 0.8996 - op_conv_accuracy: 0.9232 - avg_accuracy: 0.9187 - val_loss: 1.0457 - val_op_main_loss: 0.2815 - val_op_conv_loss: 0.2962 - val_avg_loss: 0.2625 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9018\n",
      "Epoch 167/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8738 - op_main_loss: 0.2627 - op_conv_loss: 0.1913 - avg_loss: 0.2144 - op_main_accuracy: 0.8946 - op_conv_accuracy: 0.9220 - avg_accuracy: 0.9201\n",
      "Epoch 00167: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8755 - op_main_loss: 0.2636 - op_conv_loss: 0.1915 - avg_loss: 0.2150 - op_main_accuracy: 0.8944 - op_conv_accuracy: 0.9220 - avg_accuracy: 0.9201 - val_loss: 1.0797 - val_op_main_loss: 0.2810 - val_op_conv_loss: 0.3226 - val_avg_loss: 0.2708 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8857\n",
      "Epoch 168/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8743 - op_main_loss: 0.2629 - op_conv_loss: 0.1910 - avg_loss: 0.2151 - op_main_accuracy: 0.8998 - op_conv_accuracy: 0.9230 - avg_accuracy: 0.9201\n",
      "Epoch 00168: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8743 - op_main_loss: 0.2629 - op_conv_loss: 0.1910 - avg_loss: 0.2151 - op_main_accuracy: 0.8998 - op_conv_accuracy: 0.9230 - avg_accuracy: 0.9201 - val_loss: 1.0474 - val_op_main_loss: 0.2824 - val_op_conv_loss: 0.2951 - val_avg_loss: 0.2646 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8952\n",
      "Epoch 169/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8780 - op_main_loss: 0.2595 - op_conv_loss: 0.1970 - avg_loss: 0.2162 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9198 - avg_accuracy: 0.9206\n",
      "Epoch 00169: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8777 - op_main_loss: 0.2597 - op_conv_loss: 0.1966 - avg_loss: 0.2161 - op_main_accuracy: 0.9034 - op_conv_accuracy: 0.9197 - avg_accuracy: 0.9204 - val_loss: 1.1681 - val_op_main_loss: 0.2947 - val_op_conv_loss: 0.3719 - val_avg_loss: 0.2966 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8772\n",
      "Epoch 170/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8618 - op_main_loss: 0.2564 - op_conv_loss: 0.1889 - avg_loss: 0.2116 - op_main_accuracy: 0.9016 - op_conv_accuracy: 0.9230 - avg_accuracy: 0.9232\n",
      "Epoch 00170: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8646 - op_main_loss: 0.2581 - op_conv_loss: 0.1890 - avg_loss: 0.2126 - op_main_accuracy: 0.8996 - op_conv_accuracy: 0.9223 - avg_accuracy: 0.9225 - val_loss: 1.0337 - val_op_main_loss: 0.2828 - val_op_conv_loss: 0.2825 - val_avg_loss: 0.2637 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9056\n",
      "Epoch 171/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8673 - op_main_loss: 0.2608 - op_conv_loss: 0.1893 - avg_loss: 0.2126 - op_main_accuracy: 0.8991 - op_conv_accuracy: 0.9201 - avg_accuracy: 0.9210\n",
      "Epoch 00171: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8710 - op_main_loss: 0.2615 - op_conv_loss: 0.1912 - avg_loss: 0.2137 - op_main_accuracy: 0.8982 - op_conv_accuracy: 0.9194 - avg_accuracy: 0.9199 - val_loss: 1.0415 - val_op_main_loss: 0.2799 - val_op_conv_loss: 0.2945 - val_avg_loss: 0.2627 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8999\n",
      "Epoch 172/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8902 - op_main_loss: 0.2680 - op_conv_loss: 0.1990 - avg_loss: 0.2191 - op_main_accuracy: 0.8927 - op_conv_accuracy: 0.9208 - avg_accuracy: 0.9186\n",
      "Epoch 00172: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8876 - op_main_loss: 0.2669 - op_conv_loss: 0.1983 - avg_loss: 0.2184 - op_main_accuracy: 0.8932 - op_conv_accuracy: 0.9206 - avg_accuracy: 0.9190 - val_loss: 1.0481 - val_op_main_loss: 0.2815 - val_op_conv_loss: 0.2966 - val_avg_loss: 0.2662 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9027\n",
      "Epoch 173/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8394 - op_main_loss: 0.2515 - op_conv_loss: 0.1801 - avg_loss: 0.2038 - op_main_accuracy: 0.9062 - op_conv_accuracy: 0.9279 - avg_accuracy: 0.9276\n",
      "Epoch 00173: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8374 - op_main_loss: 0.2507 - op_conv_loss: 0.1795 - avg_loss: 0.2032 - op_main_accuracy: 0.9064 - op_conv_accuracy: 0.9275 - avg_accuracy: 0.9277 - val_loss: 1.0293 - val_op_main_loss: 0.2800 - val_op_conv_loss: 0.2850 - val_avg_loss: 0.2604 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8971\n",
      "Epoch 174/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.8774 - op_main_loss: 0.2622 - op_conv_loss: 0.1954 - avg_loss: 0.2158 - op_main_accuracy: 0.8989 - op_conv_accuracy: 0.9215 - avg_accuracy: 0.9222\n",
      "Epoch 00174: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8792 - op_main_loss: 0.2629 - op_conv_loss: 0.1960 - avg_loss: 0.2164 - op_main_accuracy: 0.8984 - op_conv_accuracy: 0.9213 - avg_accuracy: 0.9218 - val_loss: 1.0292 - val_op_main_loss: 0.2786 - val_op_conv_loss: 0.2865 - val_avg_loss: 0.2603 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8999\n",
      "Epoch 175/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8651 - op_main_loss: 0.2591 - op_conv_loss: 0.1895 - avg_loss: 0.2129 - op_main_accuracy: 0.8935 - op_conv_accuracy: 0.9216 - avg_accuracy: 0.9190\n",
      "Epoch 00175: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8645 - op_main_loss: 0.2590 - op_conv_loss: 0.1891 - avg_loss: 0.2127 - op_main_accuracy: 0.8939 - op_conv_accuracy: 0.9223 - avg_accuracy: 0.9192 - val_loss: 1.0448 - val_op_main_loss: 0.2842 - val_op_conv_loss: 0.2894 - val_avg_loss: 0.2675 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8971\n",
      "Epoch 176/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8590 - op_main_loss: 0.2541 - op_conv_loss: 0.1904 - avg_loss: 0.2107 - op_main_accuracy: 0.9024 - op_conv_accuracy: 0.9191 - avg_accuracy: 0.9196\n",
      "Epoch 00176: val_avg_accuracy did not improve from 0.90652\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8638 - op_main_loss: 0.2558 - op_conv_loss: 0.1920 - avg_loss: 0.2122 - op_main_accuracy: 0.9019 - op_conv_accuracy: 0.9182 - avg_accuracy: 0.9187 - val_loss: 1.0312 - val_op_main_loss: 0.2787 - val_op_conv_loss: 0.2883 - val_avg_loss: 0.2603 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8961\n",
      "Epoch 177/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8655 - op_main_loss: 0.2585 - op_conv_loss: 0.1907 - avg_loss: 0.2126 - op_main_accuracy: 0.9000 - op_conv_accuracy: 0.9215 - avg_accuracy: 0.9218\n",
      "Epoch 00177: val_avg_accuracy improved from 0.90652 to 0.91124, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8644 - op_main_loss: 0.2583 - op_conv_loss: 0.1902 - avg_loss: 0.2122 - op_main_accuracy: 0.9003 - op_conv_accuracy: 0.9220 - avg_accuracy: 0.9225 - val_loss: 1.0143 - val_op_main_loss: 0.2746 - val_op_conv_loss: 0.2802 - val_avg_loss: 0.2561 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9112\n",
      "Epoch 178/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8731 - op_main_loss: 0.2604 - op_conv_loss: 0.1944 - avg_loss: 0.2148 - op_main_accuracy: 0.8946 - op_conv_accuracy: 0.9215 - avg_accuracy: 0.9176\n",
      "Epoch 00178: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8745 - op_main_loss: 0.2612 - op_conv_loss: 0.1945 - avg_loss: 0.2154 - op_main_accuracy: 0.8944 - op_conv_accuracy: 0.9213 - avg_accuracy: 0.9175 - val_loss: 1.0203 - val_op_main_loss: 0.2765 - val_op_conv_loss: 0.2811 - val_avg_loss: 0.2595 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9037\n",
      "Epoch 179/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8653 - op_main_loss: 0.2611 - op_conv_loss: 0.1884 - avg_loss: 0.2126 - op_main_accuracy: 0.8983 - op_conv_accuracy: 0.9191 - avg_accuracy: 0.9181\n",
      "Epoch 00179: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8693 - op_main_loss: 0.2625 - op_conv_loss: 0.1898 - avg_loss: 0.2138 - op_main_accuracy: 0.8979 - op_conv_accuracy: 0.9187 - avg_accuracy: 0.9182 - val_loss: 1.0265 - val_op_main_loss: 0.2763 - val_op_conv_loss: 0.2889 - val_avg_loss: 0.2582 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9065\n",
      "Epoch 180/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8519 - op_main_loss: 0.2543 - op_conv_loss: 0.1864 - avg_loss: 0.2081 - op_main_accuracy: 0.9031 - op_conv_accuracy: 0.9260 - avg_accuracy: 0.9228\n",
      "Epoch 00180: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8543 - op_main_loss: 0.2551 - op_conv_loss: 0.1873 - avg_loss: 0.2089 - op_main_accuracy: 0.9031 - op_conv_accuracy: 0.9256 - avg_accuracy: 0.9225 - val_loss: 1.0387 - val_op_main_loss: 0.2779 - val_op_conv_loss: 0.2971 - val_avg_loss: 0.2606 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9065\n",
      "Epoch 181/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8580 - op_main_loss: 0.2562 - op_conv_loss: 0.1890 - avg_loss: 0.2098 - op_main_accuracy: 0.8984 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9220\n",
      "Epoch 00181: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8577 - op_main_loss: 0.2561 - op_conv_loss: 0.1889 - avg_loss: 0.2097 - op_main_accuracy: 0.8984 - op_conv_accuracy: 0.9246 - avg_accuracy: 0.9220 - val_loss: 1.0509 - val_op_main_loss: 0.2751 - val_op_conv_loss: 0.3096 - val_avg_loss: 0.2633 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8914\n",
      "Epoch 182/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8598 - op_main_loss: 0.2545 - op_conv_loss: 0.1914 - avg_loss: 0.2112 - op_main_accuracy: 0.9020 - op_conv_accuracy: 0.9214 - avg_accuracy: 0.9207\n",
      "Epoch 00182: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8646 - op_main_loss: 0.2555 - op_conv_loss: 0.1940 - avg_loss: 0.2124 - op_main_accuracy: 0.9015 - op_conv_accuracy: 0.9211 - avg_accuracy: 0.9201 - val_loss: 1.0056 - val_op_main_loss: 0.2779 - val_op_conv_loss: 0.2691 - val_avg_loss: 0.2562 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9084\n",
      "Epoch 183/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8505 - op_main_loss: 0.2526 - op_conv_loss: 0.1870 - avg_loss: 0.2084 - op_main_accuracy: 0.9012 - op_conv_accuracy: 0.9249 - avg_accuracy: 0.9256\n",
      "Epoch 00183: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8485 - op_main_loss: 0.2516 - op_conv_loss: 0.1867 - avg_loss: 0.2077 - op_main_accuracy: 0.9022 - op_conv_accuracy: 0.9251 - avg_accuracy: 0.9260 - val_loss: 1.0049 - val_op_main_loss: 0.2756 - val_op_conv_loss: 0.2711 - val_avg_loss: 0.2557 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9056\n",
      "Epoch 184/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8462 - op_main_loss: 0.2560 - op_conv_loss: 0.1806 - avg_loss: 0.2071 - op_main_accuracy: 0.8959 - op_conv_accuracy: 0.9226 - avg_accuracy: 0.9187\n",
      "Epoch 00184: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8454 - op_main_loss: 0.2557 - op_conv_loss: 0.1805 - avg_loss: 0.2069 - op_main_accuracy: 0.8960 - op_conv_accuracy: 0.9230 - avg_accuracy: 0.9190 - val_loss: 1.0218 - val_op_main_loss: 0.2746 - val_op_conv_loss: 0.2875 - val_avg_loss: 0.2574 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9018\n",
      "Epoch 185/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8461 - op_main_loss: 0.2551 - op_conv_loss: 0.1821 - avg_loss: 0.2067 - op_main_accuracy: 0.9031 - op_conv_accuracy: 0.9283 - avg_accuracy: 0.9268\n",
      "Epoch 00185: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8498 - op_main_loss: 0.2559 - op_conv_loss: 0.1838 - avg_loss: 0.2079 - op_main_accuracy: 0.9024 - op_conv_accuracy: 0.9272 - avg_accuracy: 0.9267 - val_loss: 1.0203 - val_op_main_loss: 0.2749 - val_op_conv_loss: 0.2854 - val_avg_loss: 0.2580 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8990\n",
      "Epoch 186/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.8502 - op_main_loss: 0.2559 - op_conv_loss: 0.1845 - avg_loss: 0.2078 - op_main_accuracy: 0.9012 - op_conv_accuracy: 0.9278 - avg_accuracy: 0.9234\n",
      "Epoch 00186: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8496 - op_main_loss: 0.2561 - op_conv_loss: 0.1839 - avg_loss: 0.2076 - op_main_accuracy: 0.9003 - op_conv_accuracy: 0.9279 - avg_accuracy: 0.9232 - val_loss: 1.0210 - val_op_main_loss: 0.2799 - val_op_conv_loss: 0.2769 - val_avg_loss: 0.2623 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8980\n",
      "Epoch 187/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8636 - op_main_loss: 0.2592 - op_conv_loss: 0.1897 - avg_loss: 0.2125 - op_main_accuracy: 0.8989 - op_conv_accuracy: 0.9220 - avg_accuracy: 0.9196\n",
      "Epoch 00187: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8617 - op_main_loss: 0.2587 - op_conv_loss: 0.1889 - avg_loss: 0.2119 - op_main_accuracy: 0.8993 - op_conv_accuracy: 0.9225 - avg_accuracy: 0.9201 - val_loss: 1.0205 - val_op_main_loss: 0.2750 - val_op_conv_loss: 0.2863 - val_avg_loss: 0.2570 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9018\n",
      "Epoch 188/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8271 - op_main_loss: 0.2485 - op_conv_loss: 0.1753 - avg_loss: 0.2011 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9255 - avg_accuracy: 0.9243\n",
      "Epoch 00188: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8237 - op_main_loss: 0.2473 - op_conv_loss: 0.1742 - avg_loss: 0.2001 - op_main_accuracy: 0.9057 - op_conv_accuracy: 0.9263 - avg_accuracy: 0.9253 - val_loss: 1.0129 - val_op_main_loss: 0.2756 - val_op_conv_loss: 0.2803 - val_avg_loss: 0.2549 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9027\n",
      "Epoch 189/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8412 - op_main_loss: 0.2511 - op_conv_loss: 0.1822 - avg_loss: 0.2058 - op_main_accuracy: 0.9041 - op_conv_accuracy: 0.9256 - avg_accuracy: 0.9215\n",
      "Epoch 00189: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8418 - op_main_loss: 0.2512 - op_conv_loss: 0.1826 - avg_loss: 0.2058 - op_main_accuracy: 0.9038 - op_conv_accuracy: 0.9256 - avg_accuracy: 0.9216 - val_loss: 1.0457 - val_op_main_loss: 0.2770 - val_op_conv_loss: 0.3066 - val_avg_loss: 0.2600 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8980\n",
      "Epoch 190/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8393 - op_main_loss: 0.2520 - op_conv_loss: 0.1809 - avg_loss: 0.2046 - op_main_accuracy: 0.8962 - op_conv_accuracy: 0.9233 - avg_accuracy: 0.9207\n",
      "Epoch 00190: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8361 - op_main_loss: 0.2508 - op_conv_loss: 0.1799 - avg_loss: 0.2036 - op_main_accuracy: 0.8972 - op_conv_accuracy: 0.9239 - avg_accuracy: 0.9213 - val_loss: 1.0292 - val_op_main_loss: 0.2742 - val_op_conv_loss: 0.2950 - val_avg_loss: 0.2580 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8999\n",
      "Epoch 191/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8448 - op_main_loss: 0.2488 - op_conv_loss: 0.1886 - avg_loss: 0.2056 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9263 - avg_accuracy: 0.9258\n",
      "Epoch 00191: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8434 - op_main_loss: 0.2482 - op_conv_loss: 0.1884 - avg_loss: 0.2051 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9263 - avg_accuracy: 0.9258 - val_loss: 1.0536 - val_op_main_loss: 0.2770 - val_op_conv_loss: 0.3111 - val_avg_loss: 0.2639 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8961\n",
      "Epoch 192/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8374 - op_main_loss: 0.2507 - op_conv_loss: 0.1811 - avg_loss: 0.2039 - op_main_accuracy: 0.9026 - op_conv_accuracy: 0.9297 - avg_accuracy: 0.9242\n",
      "Epoch 00192: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8404 - op_main_loss: 0.2512 - op_conv_loss: 0.1827 - avg_loss: 0.2048 - op_main_accuracy: 0.9031 - op_conv_accuracy: 0.9298 - avg_accuracy: 0.9241 - val_loss: 1.0455 - val_op_main_loss: 0.2753 - val_op_conv_loss: 0.3051 - val_avg_loss: 0.2635 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8905\n",
      "Epoch 193/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8167 - op_main_loss: 0.2430 - op_conv_loss: 0.1744 - avg_loss: 0.1975 - op_main_accuracy: 0.9079 - op_conv_accuracy: 0.9274 - avg_accuracy: 0.9291\n",
      "Epoch 00193: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8213 - op_main_loss: 0.2436 - op_conv_loss: 0.1770 - avg_loss: 0.1988 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9265 - avg_accuracy: 0.9282 - val_loss: 1.0073 - val_op_main_loss: 0.2744 - val_op_conv_loss: 0.2757 - val_avg_loss: 0.2553 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9084\n",
      "Epoch 194/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8424 - op_main_loss: 0.2517 - op_conv_loss: 0.1834 - avg_loss: 0.2061 - op_main_accuracy: 0.9022 - op_conv_accuracy: 0.9272 - avg_accuracy: 0.9256\n",
      "Epoch 00194: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8406 - op_main_loss: 0.2509 - op_conv_loss: 0.1829 - avg_loss: 0.2055 - op_main_accuracy: 0.9024 - op_conv_accuracy: 0.9275 - avg_accuracy: 0.9258 - val_loss: 1.0021 - val_op_main_loss: 0.2703 - val_op_conv_loss: 0.2783 - val_avg_loss: 0.2520 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9008\n",
      "Epoch 195/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8462 - op_main_loss: 0.2550 - op_conv_loss: 0.1826 - avg_loss: 0.2074 - op_main_accuracy: 0.9007 - op_conv_accuracy: 0.9250 - avg_accuracy: 0.9212\n",
      "Epoch 00195: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8461 - op_main_loss: 0.2547 - op_conv_loss: 0.1828 - avg_loss: 0.2073 - op_main_accuracy: 0.9008 - op_conv_accuracy: 0.9249 - avg_accuracy: 0.9211 - val_loss: 0.9966 - val_op_main_loss: 0.2699 - val_op_conv_loss: 0.2741 - val_avg_loss: 0.2513 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9065\n",
      "Epoch 196/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8527 - op_main_loss: 0.2549 - op_conv_loss: 0.1873 - avg_loss: 0.2094 - op_main_accuracy: 0.9033 - op_conv_accuracy: 0.9237 - avg_accuracy: 0.9215\n",
      "Epoch 00196: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8473 - op_main_loss: 0.2534 - op_conv_loss: 0.1851 - avg_loss: 0.2077 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9249 - avg_accuracy: 0.9227 - val_loss: 1.0156 - val_op_main_loss: 0.2715 - val_op_conv_loss: 0.2882 - val_avg_loss: 0.2547 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9065\n",
      "Epoch 197/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8406 - op_main_loss: 0.2502 - op_conv_loss: 0.1841 - avg_loss: 0.2052 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9247 - avg_accuracy: 0.9210\n",
      "Epoch 00197: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8452 - op_main_loss: 0.2517 - op_conv_loss: 0.1857 - avg_loss: 0.2066 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9241 - avg_accuracy: 0.9201 - val_loss: 1.0075 - val_op_main_loss: 0.2724 - val_op_conv_loss: 0.2789 - val_avg_loss: 0.2553 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 198/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.8285 - op_main_loss: 0.2488 - op_conv_loss: 0.1764 - avg_loss: 0.2024 - op_main_accuracy: 0.9046 - op_conv_accuracy: 0.9271 - avg_accuracy: 0.9254\n",
      "Epoch 00198: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8284 - op_main_loss: 0.2481 - op_conv_loss: 0.1770 - avg_loss: 0.2024 - op_main_accuracy: 0.9050 - op_conv_accuracy: 0.9263 - avg_accuracy: 0.9251 - val_loss: 1.0165 - val_op_main_loss: 0.2705 - val_op_conv_loss: 0.2895 - val_avg_loss: 0.2557 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9065\n",
      "Epoch 199/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8140 - op_main_loss: 0.2418 - op_conv_loss: 0.1744 - avg_loss: 0.1971 - op_main_accuracy: 0.9075 - op_conv_accuracy: 0.9283 - avg_accuracy: 0.9276\n",
      "Epoch 00199: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8154 - op_main_loss: 0.2424 - op_conv_loss: 0.1750 - avg_loss: 0.1974 - op_main_accuracy: 0.9069 - op_conv_accuracy: 0.9282 - avg_accuracy: 0.9275 - val_loss: 1.0109 - val_op_main_loss: 0.2716 - val_op_conv_loss: 0.2844 - val_avg_loss: 0.2540 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9075\n",
      "Epoch 200/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8340 - op_main_loss: 0.2493 - op_conv_loss: 0.1800 - avg_loss: 0.2039 - op_main_accuracy: 0.9029 - op_conv_accuracy: 0.9260 - avg_accuracy: 0.9210\n",
      "Epoch 00200: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8311 - op_main_loss: 0.2484 - op_conv_loss: 0.1789 - avg_loss: 0.2030 - op_main_accuracy: 0.9034 - op_conv_accuracy: 0.9267 - avg_accuracy: 0.9216 - val_loss: 1.0298 - val_op_main_loss: 0.2720 - val_op_conv_loss: 0.2968 - val_avg_loss: 0.2603 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8990\n",
      "Epoch 201/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8061 - op_main_loss: 0.2408 - op_conv_loss: 0.1698 - avg_loss: 0.1946 - op_main_accuracy: 0.9082 - op_conv_accuracy: 0.9334 - avg_accuracy: 0.9308\n",
      "Epoch 00201: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8048 - op_main_loss: 0.2409 - op_conv_loss: 0.1688 - avg_loss: 0.1943 - op_main_accuracy: 0.9086 - op_conv_accuracy: 0.9341 - avg_accuracy: 0.9315 - val_loss: 1.0153 - val_op_main_loss: 0.2708 - val_op_conv_loss: 0.2902 - val_avg_loss: 0.2534 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 202/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8232 - op_main_loss: 0.2458 - op_conv_loss: 0.1771 - avg_loss: 0.1995 - op_main_accuracy: 0.9084 - op_conv_accuracy: 0.9318 - avg_accuracy: 0.9297\n",
      "Epoch 00202: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8221 - op_main_loss: 0.2454 - op_conv_loss: 0.1768 - avg_loss: 0.1992 - op_main_accuracy: 0.9086 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9298 - val_loss: 1.0339 - val_op_main_loss: 0.2734 - val_op_conv_loss: 0.2975 - val_avg_loss: 0.2623 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8990\n",
      "Epoch 203/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8330 - op_main_loss: 0.2488 - op_conv_loss: 0.1810 - avg_loss: 0.2027 - op_main_accuracy: 0.9065 - op_conv_accuracy: 0.9257 - avg_accuracy: 0.9231\n",
      "Epoch 00203: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8308 - op_main_loss: 0.2480 - op_conv_loss: 0.1802 - avg_loss: 0.2021 - op_main_accuracy: 0.9071 - op_conv_accuracy: 0.9263 - avg_accuracy: 0.9234 - val_loss: 1.0141 - val_op_main_loss: 0.2768 - val_op_conv_loss: 0.2792 - val_avg_loss: 0.2575 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9018\n",
      "Epoch 204/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8023 - op_main_loss: 0.2378 - op_conv_loss: 0.1710 - avg_loss: 0.1929 - op_main_accuracy: 0.9144 - op_conv_accuracy: 0.9315 - avg_accuracy: 0.9323\n",
      "Epoch 00204: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8029 - op_main_loss: 0.2379 - op_conv_loss: 0.1715 - avg_loss: 0.1931 - op_main_accuracy: 0.9145 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9322 - val_loss: 1.0517 - val_op_main_loss: 0.2774 - val_op_conv_loss: 0.3098 - val_avg_loss: 0.2639 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 205/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8287 - op_main_loss: 0.2482 - op_conv_loss: 0.1779 - avg_loss: 0.2022 - op_main_accuracy: 0.9084 - op_conv_accuracy: 0.9264 - avg_accuracy: 0.9276\n",
      "Epoch 00205: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8298 - op_main_loss: 0.2479 - op_conv_loss: 0.1789 - avg_loss: 0.2026 - op_main_accuracy: 0.9086 - op_conv_accuracy: 0.9258 - avg_accuracy: 0.9270 - val_loss: 1.0152 - val_op_main_loss: 0.2722 - val_op_conv_loss: 0.2856 - val_avg_loss: 0.2573 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9027\n",
      "Epoch 206/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8395 - op_main_loss: 0.2488 - op_conv_loss: 0.1852 - avg_loss: 0.2053 - op_main_accuracy: 0.9053 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9239\n",
      "Epoch 00206: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8401 - op_main_loss: 0.2491 - op_conv_loss: 0.1853 - avg_loss: 0.2055 - op_main_accuracy: 0.9052 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9237 - val_loss: 1.0166 - val_op_main_loss: 0.2710 - val_op_conv_loss: 0.2889 - val_avg_loss: 0.2567 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9046\n",
      "Epoch 207/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8030 - op_main_loss: 0.2383 - op_conv_loss: 0.1707 - avg_loss: 0.1940 - op_main_accuracy: 0.9110 - op_conv_accuracy: 0.9320 - avg_accuracy: 0.9325\n",
      "Epoch 00207: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8035 - op_main_loss: 0.2385 - op_conv_loss: 0.1707 - avg_loss: 0.1942 - op_main_accuracy: 0.9109 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9322 - val_loss: 1.0464 - val_op_main_loss: 0.2727 - val_op_conv_loss: 0.3105 - val_avg_loss: 0.2629 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8971\n",
      "Epoch 208/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8155 - op_main_loss: 0.2434 - op_conv_loss: 0.1741 - avg_loss: 0.1978 - op_main_accuracy: 0.9091 - op_conv_accuracy: 0.9298 - avg_accuracy: 0.9298\n",
      "Epoch 00208: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8156 - op_main_loss: 0.2434 - op_conv_loss: 0.1742 - avg_loss: 0.1977 - op_main_accuracy: 0.9093 - op_conv_accuracy: 0.9298 - avg_accuracy: 0.9298 - val_loss: 1.0089 - val_op_main_loss: 0.2692 - val_op_conv_loss: 0.2851 - val_avg_loss: 0.2544 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9046\n",
      "Epoch 209/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8272 - op_main_loss: 0.2440 - op_conv_loss: 0.1819 - avg_loss: 0.2011 - op_main_accuracy: 0.9097 - op_conv_accuracy: 0.9220 - avg_accuracy: 0.9218\n",
      "Epoch 00209: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8272 - op_main_loss: 0.2440 - op_conv_loss: 0.1819 - avg_loss: 0.2011 - op_main_accuracy: 0.9097 - op_conv_accuracy: 0.9220 - avg_accuracy: 0.9218 - val_loss: 1.0136 - val_op_main_loss: 0.2708 - val_op_conv_loss: 0.2870 - val_avg_loss: 0.2557 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9018\n",
      "Epoch 210/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.8226 - op_main_loss: 0.2457 - op_conv_loss: 0.1766 - avg_loss: 0.2002 - op_main_accuracy: 0.9067 - op_conv_accuracy: 0.9279 - avg_accuracy: 0.9276\n",
      "Epoch 00210: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8277 - op_main_loss: 0.2473 - op_conv_loss: 0.1785 - avg_loss: 0.2018 - op_main_accuracy: 0.9060 - op_conv_accuracy: 0.9270 - avg_accuracy: 0.9267 - val_loss: 1.0068 - val_op_main_loss: 0.2701 - val_op_conv_loss: 0.2814 - val_avg_loss: 0.2554 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9056\n",
      "Epoch 211/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8092 - op_main_loss: 0.2385 - op_conv_loss: 0.1751 - avg_loss: 0.1959 - op_main_accuracy: 0.9086 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9256\n",
      "Epoch 00211: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8086 - op_main_loss: 0.2383 - op_conv_loss: 0.1748 - avg_loss: 0.1957 - op_main_accuracy: 0.9083 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9256 - val_loss: 1.0085 - val_op_main_loss: 0.2685 - val_op_conv_loss: 0.2841 - val_avg_loss: 0.2560 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9018\n",
      "Epoch 212/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7952 - op_main_loss: 0.2366 - op_conv_loss: 0.1674 - avg_loss: 0.1914 - op_main_accuracy: 0.9103 - op_conv_accuracy: 0.9308 - avg_accuracy: 0.9292\n",
      "Epoch 00212: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7983 - op_main_loss: 0.2372 - op_conv_loss: 0.1690 - avg_loss: 0.1923 - op_main_accuracy: 0.9097 - op_conv_accuracy: 0.9305 - avg_accuracy: 0.9289 - val_loss: 1.0293 - val_op_main_loss: 0.2697 - val_op_conv_loss: 0.3014 - val_avg_loss: 0.2585 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 213/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8183 - op_main_loss: 0.2441 - op_conv_loss: 0.1759 - avg_loss: 0.1986 - op_main_accuracy: 0.9058 - op_conv_accuracy: 0.9295 - avg_accuracy: 0.9278\n",
      "Epoch 00213: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8146 - op_main_loss: 0.2429 - op_conv_loss: 0.1745 - avg_loss: 0.1974 - op_main_accuracy: 0.9067 - op_conv_accuracy: 0.9296 - avg_accuracy: 0.9282 - val_loss: 1.0123 - val_op_main_loss: 0.2711 - val_op_conv_loss: 0.2852 - val_avg_loss: 0.2563 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 214/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8324 - op_main_loss: 0.2452 - op_conv_loss: 0.1835 - avg_loss: 0.2041 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9267 - avg_accuracy: 0.9239\n",
      "Epoch 00214: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8324 - op_main_loss: 0.2452 - op_conv_loss: 0.1835 - avg_loss: 0.2041 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9267 - avg_accuracy: 0.9239 - val_loss: 1.0098 - val_op_main_loss: 0.2701 - val_op_conv_loss: 0.2831 - val_avg_loss: 0.2570 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8961\n",
      "Epoch 215/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8349 - op_main_loss: 0.2464 - op_conv_loss: 0.1855 - avg_loss: 0.2037 - op_main_accuracy: 0.9046 - op_conv_accuracy: 0.9297 - avg_accuracy: 0.9220\n",
      "Epoch 00215: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8317 - op_main_loss: 0.2454 - op_conv_loss: 0.1843 - avg_loss: 0.2027 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9225 - val_loss: 1.0122 - val_op_main_loss: 0.2665 - val_op_conv_loss: 0.2901 - val_avg_loss: 0.2564 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8952\n",
      "Epoch 216/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8377 - op_main_loss: 0.2475 - op_conv_loss: 0.1859 - avg_loss: 0.2052 - op_main_accuracy: 0.9053 - op_conv_accuracy: 0.9258 - avg_accuracy: 0.9263\n",
      "Epoch 00216: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8357 - op_main_loss: 0.2467 - op_conv_loss: 0.1853 - avg_loss: 0.2045 - op_main_accuracy: 0.9057 - op_conv_accuracy: 0.9260 - avg_accuracy: 0.9267 - val_loss: 1.0479 - val_op_main_loss: 0.2720 - val_op_conv_loss: 0.3103 - val_avg_loss: 0.2667 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8980\n",
      "Epoch 217/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8004 - op_main_loss: 0.2348 - op_conv_loss: 0.1731 - avg_loss: 0.1936 - op_main_accuracy: 0.9135 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9289\n",
      "Epoch 00217: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8004 - op_main_loss: 0.2348 - op_conv_loss: 0.1731 - avg_loss: 0.1936 - op_main_accuracy: 0.9135 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9289 - val_loss: 1.0269 - val_op_main_loss: 0.2688 - val_op_conv_loss: 0.2997 - val_avg_loss: 0.2594 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8961\n",
      "Epoch 218/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8122 - op_main_loss: 0.2419 - op_conv_loss: 0.1741 - avg_loss: 0.1972 - op_main_accuracy: 0.9078 - op_conv_accuracy: 0.9258 - avg_accuracy: 0.9267\n",
      "Epoch 00218: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8122 - op_main_loss: 0.2419 - op_conv_loss: 0.1741 - avg_loss: 0.1972 - op_main_accuracy: 0.9078 - op_conv_accuracy: 0.9258 - avg_accuracy: 0.9267 - val_loss: 1.0085 - val_op_main_loss: 0.2687 - val_op_conv_loss: 0.2852 - val_avg_loss: 0.2556 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9046\n",
      "Epoch 219/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8109 - op_main_loss: 0.2409 - op_conv_loss: 0.1746 - avg_loss: 0.1965 - op_main_accuracy: 0.9094 - op_conv_accuracy: 0.9295 - avg_accuracy: 0.9268\n",
      "Epoch 00219: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8088 - op_main_loss: 0.2404 - op_conv_loss: 0.1736 - avg_loss: 0.1958 - op_main_accuracy: 0.9093 - op_conv_accuracy: 0.9296 - avg_accuracy: 0.9270 - val_loss: 0.9970 - val_op_main_loss: 0.2731 - val_op_conv_loss: 0.2704 - val_avg_loss: 0.2547 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9075\n",
      "Epoch 220/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7974 - op_main_loss: 0.2360 - op_conv_loss: 0.1701 - avg_loss: 0.1924 - op_main_accuracy: 0.9112 - op_conv_accuracy: 0.9356 - avg_accuracy: 0.9344\n",
      "Epoch 00220: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7983 - op_main_loss: 0.2362 - op_conv_loss: 0.1705 - avg_loss: 0.1928 - op_main_accuracy: 0.9109 - op_conv_accuracy: 0.9355 - avg_accuracy: 0.9343 - val_loss: 0.9951 - val_op_main_loss: 0.2727 - val_op_conv_loss: 0.2710 - val_avg_loss: 0.2526 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9037\n",
      "Epoch 221/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8418 - op_main_loss: 0.2479 - op_conv_loss: 0.1887 - avg_loss: 0.2065 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9230\n",
      "Epoch 00221: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8418 - op_main_loss: 0.2479 - op_conv_loss: 0.1887 - avg_loss: 0.2065 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9230 - val_loss: 0.9982 - val_op_main_loss: 0.2673 - val_op_conv_loss: 0.2793 - val_avg_loss: 0.2529 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9065\n",
      "Epoch 222/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.8165 - op_main_loss: 0.2425 - op_conv_loss: 0.1765 - avg_loss: 0.1991 - op_main_accuracy: 0.9062 - op_conv_accuracy: 0.9300 - avg_accuracy: 0.9276\n",
      "Epoch 00222: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8122 - op_main_loss: 0.2409 - op_conv_loss: 0.1751 - avg_loss: 0.1977 - op_main_accuracy: 0.9071 - op_conv_accuracy: 0.9303 - avg_accuracy: 0.9284 - val_loss: 1.0066 - val_op_main_loss: 0.2694 - val_op_conv_loss: 0.2842 - val_avg_loss: 0.2541 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9065\n",
      "Epoch 223/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7974 - op_main_loss: 0.2350 - op_conv_loss: 0.1711 - avg_loss: 0.1925 - op_main_accuracy: 0.9121 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9307\n",
      "Epoch 00223: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7965 - op_main_loss: 0.2348 - op_conv_loss: 0.1707 - avg_loss: 0.1921 - op_main_accuracy: 0.9112 - op_conv_accuracy: 0.9322 - avg_accuracy: 0.9308 - val_loss: 1.0134 - val_op_main_loss: 0.2693 - val_op_conv_loss: 0.2876 - val_avg_loss: 0.2576 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9018\n",
      "Epoch 224/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8001 - op_main_loss: 0.2353 - op_conv_loss: 0.1726 - avg_loss: 0.1934 - op_main_accuracy: 0.9113 - op_conv_accuracy: 0.9315 - avg_accuracy: 0.9296\n",
      "Epoch 00224: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8006 - op_main_loss: 0.2354 - op_conv_loss: 0.1728 - avg_loss: 0.1936 - op_main_accuracy: 0.9112 - op_conv_accuracy: 0.9315 - avg_accuracy: 0.9296 - val_loss: 1.0160 - val_op_main_loss: 0.2762 - val_op_conv_loss: 0.2827 - val_avg_loss: 0.2585 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.8999\n",
      "Epoch 225/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8100 - op_main_loss: 0.2375 - op_conv_loss: 0.1772 - avg_loss: 0.1968 - op_main_accuracy: 0.9158 - op_conv_accuracy: 0.9315 - avg_accuracy: 0.9287\n",
      "Epoch 00225: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8091 - op_main_loss: 0.2374 - op_conv_loss: 0.1766 - avg_loss: 0.1965 - op_main_accuracy: 0.9159 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9291 - val_loss: 1.0214 - val_op_main_loss: 0.2774 - val_op_conv_loss: 0.2833 - val_avg_loss: 0.2625 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8942\n",
      "Epoch 226/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7784 - op_main_loss: 0.2304 - op_conv_loss: 0.1634 - avg_loss: 0.1863 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9358 - avg_accuracy: 0.9342\n",
      "Epoch 00226: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7789 - op_main_loss: 0.2306 - op_conv_loss: 0.1636 - avg_loss: 0.1865 - op_main_accuracy: 0.9161 - op_conv_accuracy: 0.9355 - avg_accuracy: 0.9341 - val_loss: 1.0115 - val_op_main_loss: 0.2677 - val_op_conv_loss: 0.2925 - val_avg_loss: 0.2529 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 227/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7968 - op_main_loss: 0.2363 - op_conv_loss: 0.1692 - avg_loss: 0.1931 - op_main_accuracy: 0.9125 - op_conv_accuracy: 0.9291 - avg_accuracy: 0.9300\n",
      "Epoch 00227: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8002 - op_main_loss: 0.2383 - op_conv_loss: 0.1696 - avg_loss: 0.1941 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9291 - avg_accuracy: 0.9293 - val_loss: 1.0144 - val_op_main_loss: 0.2720 - val_op_conv_loss: 0.2861 - val_avg_loss: 0.2583 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8999\n",
      "Epoch 228/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8072 - op_main_loss: 0.2400 - op_conv_loss: 0.1737 - avg_loss: 0.1957 - op_main_accuracy: 0.9084 - op_conv_accuracy: 0.9302 - avg_accuracy: 0.9278\n",
      "Epoch 00228: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8077 - op_main_loss: 0.2401 - op_conv_loss: 0.1738 - avg_loss: 0.1959 - op_main_accuracy: 0.9083 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9277 - val_loss: 0.9966 - val_op_main_loss: 0.2660 - val_op_conv_loss: 0.2799 - val_avg_loss: 0.2528 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9037\n",
      "Epoch 229/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8004 - op_main_loss: 0.2359 - op_conv_loss: 0.1735 - avg_loss: 0.1929 - op_main_accuracy: 0.9084 - op_conv_accuracy: 0.9286 - avg_accuracy: 0.9286\n",
      "Epoch 00229: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8026 - op_main_loss: 0.2367 - op_conv_loss: 0.1742 - avg_loss: 0.1936 - op_main_accuracy: 0.9081 - op_conv_accuracy: 0.9286 - avg_accuracy: 0.9282 - val_loss: 1.0214 - val_op_main_loss: 0.2681 - val_op_conv_loss: 0.2996 - val_avg_loss: 0.2554 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8999\n",
      "Epoch 230/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7916 - op_main_loss: 0.2324 - op_conv_loss: 0.1710 - avg_loss: 0.1900 - op_main_accuracy: 0.9138 - op_conv_accuracy: 0.9348 - avg_accuracy: 0.9289\n",
      "Epoch 00230: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7916 - op_main_loss: 0.2324 - op_conv_loss: 0.1710 - avg_loss: 0.1900 - op_main_accuracy: 0.9138 - op_conv_accuracy: 0.9348 - avg_accuracy: 0.9289 - val_loss: 1.0405 - val_op_main_loss: 0.2717 - val_op_conv_loss: 0.3064 - val_avg_loss: 0.2644 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8961\n",
      "Epoch 231/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8079 - op_main_loss: 0.2347 - op_conv_loss: 0.1793 - avg_loss: 0.1961 - op_main_accuracy: 0.9138 - op_conv_accuracy: 0.9285 - avg_accuracy: 0.9257\n",
      "Epoch 00231: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8075 - op_main_loss: 0.2346 - op_conv_loss: 0.1791 - avg_loss: 0.1959 - op_main_accuracy: 0.9138 - op_conv_accuracy: 0.9286 - avg_accuracy: 0.9256 - val_loss: 1.0788 - val_op_main_loss: 0.2811 - val_op_conv_loss: 0.3281 - val_avg_loss: 0.2718 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8980\n",
      "Epoch 232/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7936 - op_main_loss: 0.2350 - op_conv_loss: 0.1692 - avg_loss: 0.1916 - op_main_accuracy: 0.9136 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9284\n",
      "Epoch 00232: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7925 - op_main_loss: 0.2347 - op_conv_loss: 0.1687 - avg_loss: 0.1913 - op_main_accuracy: 0.9140 - op_conv_accuracy: 0.9303 - avg_accuracy: 0.9284 - val_loss: 0.9985 - val_op_main_loss: 0.2668 - val_op_conv_loss: 0.2827 - val_avg_loss: 0.2512 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9046\n",
      "Epoch 233/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7665 - op_main_loss: 0.2250 - op_conv_loss: 0.1608 - avg_loss: 0.1828 - op_main_accuracy: 0.9192 - op_conv_accuracy: 0.9367 - avg_accuracy: 0.9336\n",
      "Epoch 00233: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7665 - op_main_loss: 0.2250 - op_conv_loss: 0.1608 - avg_loss: 0.1828 - op_main_accuracy: 0.9192 - op_conv_accuracy: 0.9367 - avg_accuracy: 0.9336 - val_loss: 1.0090 - val_op_main_loss: 0.2667 - val_op_conv_loss: 0.2911 - val_avg_loss: 0.2535 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9056\n",
      "Epoch 234/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.7994 - op_main_loss: 0.2346 - op_conv_loss: 0.1738 - avg_loss: 0.1933 - op_main_accuracy: 0.9157 - op_conv_accuracy: 0.9285 - avg_accuracy: 0.9290\n",
      "Epoch 00234: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7985 - op_main_loss: 0.2342 - op_conv_loss: 0.1735 - avg_loss: 0.1930 - op_main_accuracy: 0.9159 - op_conv_accuracy: 0.9286 - avg_accuracy: 0.9291 - val_loss: 0.9970 - val_op_main_loss: 0.2647 - val_op_conv_loss: 0.2832 - val_avg_loss: 0.2515 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9056\n",
      "Epoch 235/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7830 - op_main_loss: 0.2296 - op_conv_loss: 0.1675 - avg_loss: 0.1881 - op_main_accuracy: 0.9120 - op_conv_accuracy: 0.9346 - avg_accuracy: 0.9310\n",
      "Epoch 00235: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7856 - op_main_loss: 0.2304 - op_conv_loss: 0.1685 - avg_loss: 0.1889 - op_main_accuracy: 0.9112 - op_conv_accuracy: 0.9338 - avg_accuracy: 0.9301 - val_loss: 1.0278 - val_op_main_loss: 0.2745 - val_op_conv_loss: 0.2923 - val_avg_loss: 0.2633 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8961\n",
      "Epoch 236/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8117 - op_main_loss: 0.2385 - op_conv_loss: 0.1787 - avg_loss: 0.1967 - op_main_accuracy: 0.9060 - op_conv_accuracy: 0.9270 - avg_accuracy: 0.9260\n",
      "Epoch 00236: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8182 - op_main_loss: 0.2398 - op_conv_loss: 0.1818 - avg_loss: 0.1987 - op_main_accuracy: 0.9045 - op_conv_accuracy: 0.9256 - avg_accuracy: 0.9246 - val_loss: 1.0194 - val_op_main_loss: 0.2692 - val_op_conv_loss: 0.2967 - val_avg_loss: 0.2558 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9056\n",
      "Epoch 237/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8050 - op_main_loss: 0.2370 - op_conv_loss: 0.1754 - avg_loss: 0.1950 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9292 - avg_accuracy: 0.9227\n",
      "Epoch 00237: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8037 - op_main_loss: 0.2364 - op_conv_loss: 0.1751 - avg_loss: 0.1946 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9291 - avg_accuracy: 0.9227 - val_loss: 1.0244 - val_op_main_loss: 0.2705 - val_op_conv_loss: 0.2980 - val_avg_loss: 0.2585 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9056\n",
      "Epoch 238/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8027 - op_main_loss: 0.2368 - op_conv_loss: 0.1734 - avg_loss: 0.1951 - op_main_accuracy: 0.9120 - op_conv_accuracy: 0.9325 - avg_accuracy: 0.9308\n",
      "Epoch 00238: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8008 - op_main_loss: 0.2363 - op_conv_loss: 0.1726 - avg_loss: 0.1945 - op_main_accuracy: 0.9123 - op_conv_accuracy: 0.9329 - avg_accuracy: 0.9312 - val_loss: 1.0291 - val_op_main_loss: 0.2700 - val_op_conv_loss: 0.3030 - val_avg_loss: 0.2586 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9008\n",
      "Epoch 239/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8049 - op_main_loss: 0.2384 - op_conv_loss: 0.1736 - avg_loss: 0.1954 - op_main_accuracy: 0.9079 - op_conv_accuracy: 0.9297 - avg_accuracy: 0.9240\n",
      "Epoch 00239: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8047 - op_main_loss: 0.2384 - op_conv_loss: 0.1735 - avg_loss: 0.1953 - op_main_accuracy: 0.9081 - op_conv_accuracy: 0.9298 - avg_accuracy: 0.9241 - val_loss: 1.1564 - val_op_main_loss: 0.2866 - val_op_conv_loss: 0.3806 - val_avg_loss: 0.2918 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8867\n",
      "Epoch 240/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7889 - op_main_loss: 0.2340 - op_conv_loss: 0.1674 - avg_loss: 0.1901 - op_main_accuracy: 0.9095 - op_conv_accuracy: 0.9293 - avg_accuracy: 0.9275\n",
      "Epoch 00240: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7889 - op_main_loss: 0.2340 - op_conv_loss: 0.1674 - avg_loss: 0.1901 - op_main_accuracy: 0.9095 - op_conv_accuracy: 0.9293 - avg_accuracy: 0.9275 - val_loss: 1.0027 - val_op_main_loss: 0.2659 - val_op_conv_loss: 0.2854 - val_avg_loss: 0.2540 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9037\n",
      "Epoch 241/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7689 - op_main_loss: 0.2271 - op_conv_loss: 0.1605 - avg_loss: 0.1839 - op_main_accuracy: 0.9132 - op_conv_accuracy: 0.9349 - avg_accuracy: 0.9322\n",
      "Epoch 00241: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7643 - op_main_loss: 0.2254 - op_conv_loss: 0.1591 - avg_loss: 0.1824 - op_main_accuracy: 0.9145 - op_conv_accuracy: 0.9355 - avg_accuracy: 0.9329 - val_loss: 1.0313 - val_op_main_loss: 0.2765 - val_op_conv_loss: 0.2959 - val_avg_loss: 0.2614 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9084\n",
      "Epoch 242/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7958 - op_main_loss: 0.2382 - op_conv_loss: 0.1676 - avg_loss: 0.1925 - op_main_accuracy: 0.9089 - op_conv_accuracy: 0.9339 - avg_accuracy: 0.9317\n",
      "Epoch 00242: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7964 - op_main_loss: 0.2384 - op_conv_loss: 0.1679 - avg_loss: 0.1927 - op_main_accuracy: 0.9090 - op_conv_accuracy: 0.9338 - avg_accuracy: 0.9315 - val_loss: 1.1480 - val_op_main_loss: 0.2898 - val_op_conv_loss: 0.3645 - val_avg_loss: 0.2966 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8829\n",
      "Epoch 243/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7770 - op_main_loss: 0.2306 - op_conv_loss: 0.1627 - avg_loss: 0.1865 - op_main_accuracy: 0.9111 - op_conv_accuracy: 0.9324 - avg_accuracy: 0.9293\n",
      "Epoch 00243: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7777 - op_main_loss: 0.2305 - op_conv_loss: 0.1633 - avg_loss: 0.1868 - op_main_accuracy: 0.9112 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9291 - val_loss: 1.0183 - val_op_main_loss: 0.2658 - val_op_conv_loss: 0.2985 - val_avg_loss: 0.2569 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 244/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7762 - op_main_loss: 0.2281 - op_conv_loss: 0.1644 - avg_loss: 0.1865 - op_main_accuracy: 0.9139 - op_conv_accuracy: 0.9354 - avg_accuracy: 0.9296\n",
      "Epoch 00244: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7763 - op_main_loss: 0.2279 - op_conv_loss: 0.1647 - avg_loss: 0.1864 - op_main_accuracy: 0.9138 - op_conv_accuracy: 0.9355 - avg_accuracy: 0.9296 - val_loss: 1.0186 - val_op_main_loss: 0.2708 - val_op_conv_loss: 0.2933 - val_avg_loss: 0.2576 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9037\n",
      "Epoch 245/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7765 - op_main_loss: 0.2290 - op_conv_loss: 0.1641 - avg_loss: 0.1865 - op_main_accuracy: 0.9136 - op_conv_accuracy: 0.9330 - avg_accuracy: 0.9306\n",
      "Epoch 00245: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7746 - op_main_loss: 0.2285 - op_conv_loss: 0.1634 - avg_loss: 0.1859 - op_main_accuracy: 0.9140 - op_conv_accuracy: 0.9334 - avg_accuracy: 0.9310 - val_loss: 1.0158 - val_op_main_loss: 0.2719 - val_op_conv_loss: 0.2897 - val_avg_loss: 0.2575 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9027\n",
      "Epoch 246/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.7942 - op_main_loss: 0.2336 - op_conv_loss: 0.1717 - avg_loss: 0.1924 - op_main_accuracy: 0.9086 - op_conv_accuracy: 0.9297 - avg_accuracy: 0.9278\n",
      "Epoch 00246: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7938 - op_main_loss: 0.2335 - op_conv_loss: 0.1716 - avg_loss: 0.1923 - op_main_accuracy: 0.9088 - op_conv_accuracy: 0.9296 - avg_accuracy: 0.9277 - val_loss: 1.0114 - val_op_main_loss: 0.2711 - val_op_conv_loss: 0.2867 - val_avg_loss: 0.2573 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9037\n",
      "Epoch 247/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7719 - op_main_loss: 0.2290 - op_conv_loss: 0.1611 - avg_loss: 0.1854 - op_main_accuracy: 0.9130 - op_conv_accuracy: 0.9346 - avg_accuracy: 0.9308\n",
      "Epoch 00247: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7756 - op_main_loss: 0.2296 - op_conv_loss: 0.1631 - avg_loss: 0.1864 - op_main_accuracy: 0.9121 - op_conv_accuracy: 0.9336 - avg_accuracy: 0.9301 - val_loss: 1.0262 - val_op_main_loss: 0.2761 - val_op_conv_loss: 0.2910 - val_avg_loss: 0.2625 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 248/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7665 - op_main_loss: 0.2238 - op_conv_loss: 0.1630 - avg_loss: 0.1832 - op_main_accuracy: 0.9183 - op_conv_accuracy: 0.9339 - avg_accuracy: 0.9334\n",
      "Epoch 00248: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7693 - op_main_loss: 0.2246 - op_conv_loss: 0.1641 - avg_loss: 0.1842 - op_main_accuracy: 0.9178 - op_conv_accuracy: 0.9331 - avg_accuracy: 0.9329 - val_loss: 1.0189 - val_op_main_loss: 0.2675 - val_op_conv_loss: 0.2994 - val_avg_loss: 0.2554 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9027\n",
      "Epoch 249/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7946 - op_main_loss: 0.2348 - op_conv_loss: 0.1712 - avg_loss: 0.1920 - op_main_accuracy: 0.9081 - op_conv_accuracy: 0.9329 - avg_accuracy: 0.9286\n",
      "Epoch 00249: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7946 - op_main_loss: 0.2348 - op_conv_loss: 0.1712 - avg_loss: 0.1920 - op_main_accuracy: 0.9081 - op_conv_accuracy: 0.9329 - avg_accuracy: 0.9286 - val_loss: 1.0113 - val_op_main_loss: 0.2724 - val_op_conv_loss: 0.2839 - val_avg_loss: 0.2586 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8999\n",
      "Epoch 250/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7664 - op_main_loss: 0.2281 - op_conv_loss: 0.1594 - avg_loss: 0.1825 - op_main_accuracy: 0.9115 - op_conv_accuracy: 0.9344 - avg_accuracy: 0.9335\n",
      "Epoch 00250: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7657 - op_main_loss: 0.2278 - op_conv_loss: 0.1591 - avg_loss: 0.1823 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9345 - avg_accuracy: 0.9336 - val_loss: 1.0649 - val_op_main_loss: 0.2775 - val_op_conv_loss: 0.3191 - val_avg_loss: 0.2719 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8942\n",
      "Epoch 251/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7546 - op_main_loss: 0.2221 - op_conv_loss: 0.1563 - avg_loss: 0.1795 - op_main_accuracy: 0.9169 - op_conv_accuracy: 0.9361 - avg_accuracy: 0.9339\n",
      "Epoch 00251: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7553 - op_main_loss: 0.2223 - op_conv_loss: 0.1566 - avg_loss: 0.1798 - op_main_accuracy: 0.9166 - op_conv_accuracy: 0.9360 - avg_accuracy: 0.9338 - val_loss: 1.0826 - val_op_main_loss: 0.2686 - val_op_conv_loss: 0.3475 - val_avg_loss: 0.2694 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8971\n",
      "Epoch 252/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7787 - op_main_loss: 0.2281 - op_conv_loss: 0.1663 - avg_loss: 0.1874 - op_main_accuracy: 0.9156 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9325\n",
      "Epoch 00252: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7800 - op_main_loss: 0.2283 - op_conv_loss: 0.1671 - avg_loss: 0.1877 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9315 - avg_accuracy: 0.9322 - val_loss: 1.0017 - val_op_main_loss: 0.2619 - val_op_conv_loss: 0.2922 - val_avg_loss: 0.2508 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 253/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7710 - op_main_loss: 0.2244 - op_conv_loss: 0.1652 - avg_loss: 0.1849 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9310\n",
      "Epoch 00253: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7710 - op_main_loss: 0.2244 - op_conv_loss: 0.1652 - avg_loss: 0.1849 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9310 - val_loss: 0.9994 - val_op_main_loss: 0.2641 - val_op_conv_loss: 0.2868 - val_avg_loss: 0.2521 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9018\n",
      "Epoch 254/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7728 - op_main_loss: 0.2299 - op_conv_loss: 0.1613 - avg_loss: 0.1852 - op_main_accuracy: 0.9151 - op_conv_accuracy: 0.9325 - avg_accuracy: 0.9332\n",
      "Epoch 00254: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7720 - op_main_loss: 0.2291 - op_conv_loss: 0.1616 - avg_loss: 0.1849 - op_main_accuracy: 0.9156 - op_conv_accuracy: 0.9327 - avg_accuracy: 0.9336 - val_loss: 1.0129 - val_op_main_loss: 0.2660 - val_op_conv_loss: 0.2960 - val_avg_loss: 0.2545 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9027\n",
      "Epoch 255/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7599 - op_main_loss: 0.2226 - op_conv_loss: 0.1598 - avg_loss: 0.1812 - op_main_accuracy: 0.9212 - op_conv_accuracy: 0.9361 - avg_accuracy: 0.9358\n",
      "Epoch 00255: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7599 - op_main_loss: 0.2221 - op_conv_loss: 0.1602 - avg_loss: 0.1812 - op_main_accuracy: 0.9218 - op_conv_accuracy: 0.9360 - avg_accuracy: 0.9355 - val_loss: 1.0086 - val_op_main_loss: 0.2656 - val_op_conv_loss: 0.2925 - val_avg_loss: 0.2543 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9027\n",
      "Epoch 256/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7598 - op_main_loss: 0.2240 - op_conv_loss: 0.1587 - avg_loss: 0.1807 - op_main_accuracy: 0.9163 - op_conv_accuracy: 0.9375 - avg_accuracy: 0.9361\n",
      "Epoch 00256: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7632 - op_main_loss: 0.2247 - op_conv_loss: 0.1605 - avg_loss: 0.1817 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9376 - avg_accuracy: 0.9362 - val_loss: 1.0649 - val_op_main_loss: 0.2678 - val_op_conv_loss: 0.3346 - val_avg_loss: 0.2663 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8971\n",
      "Epoch 257/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7618 - op_main_loss: 0.2242 - op_conv_loss: 0.1600 - avg_loss: 0.1816 - op_main_accuracy: 0.9162 - op_conv_accuracy: 0.9380 - avg_accuracy: 0.9399\n",
      "Epoch 00257: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7579 - op_main_loss: 0.2230 - op_conv_loss: 0.1585 - avg_loss: 0.1804 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9402 - val_loss: 0.9934 - val_op_main_loss: 0.2622 - val_op_conv_loss: 0.2841 - val_avg_loss: 0.2512 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9065\n",
      "Epoch 258/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.7855 - op_main_loss: 0.2319 - op_conv_loss: 0.1689 - avg_loss: 0.1889 - op_main_accuracy: 0.9118 - op_conv_accuracy: 0.9325 - avg_accuracy: 0.9293\n",
      "Epoch 00258: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7830 - op_main_loss: 0.2311 - op_conv_loss: 0.1679 - avg_loss: 0.1881 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9327 - avg_accuracy: 0.9296 - val_loss: 1.0575 - val_op_main_loss: 0.2743 - val_op_conv_loss: 0.3170 - val_avg_loss: 0.2705 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8961\n",
      "Epoch 259/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7651 - op_main_loss: 0.2290 - op_conv_loss: 0.1575 - avg_loss: 0.1827 - op_main_accuracy: 0.9087 - op_conv_accuracy: 0.9377 - avg_accuracy: 0.9337\n",
      "Epoch 00259: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7660 - op_main_loss: 0.2295 - op_conv_loss: 0.1576 - avg_loss: 0.1830 - op_main_accuracy: 0.9083 - op_conv_accuracy: 0.9379 - avg_accuracy: 0.9334 - val_loss: 1.0170 - val_op_main_loss: 0.2642 - val_op_conv_loss: 0.2993 - val_avg_loss: 0.2577 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8980\n",
      "Epoch 260/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7379 - op_main_loss: 0.2169 - op_conv_loss: 0.1510 - avg_loss: 0.1740 - op_main_accuracy: 0.9214 - op_conv_accuracy: 0.9399 - avg_accuracy: 0.9382\n",
      "Epoch 00260: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7425 - op_main_loss: 0.2186 - op_conv_loss: 0.1525 - avg_loss: 0.1755 - op_main_accuracy: 0.9204 - op_conv_accuracy: 0.9397 - avg_accuracy: 0.9376 - val_loss: 1.0179 - val_op_main_loss: 0.2686 - val_op_conv_loss: 0.2964 - val_avg_loss: 0.2569 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9018\n",
      "Epoch 261/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7608 - op_main_loss: 0.2262 - op_conv_loss: 0.1571 - avg_loss: 0.1816 - op_main_accuracy: 0.9130 - op_conv_accuracy: 0.9356 - avg_accuracy: 0.9334\n",
      "Epoch 00261: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7601 - op_main_loss: 0.2258 - op_conv_loss: 0.1570 - avg_loss: 0.1814 - op_main_accuracy: 0.9130 - op_conv_accuracy: 0.9355 - avg_accuracy: 0.9336 - val_loss: 1.0125 - val_op_main_loss: 0.2646 - val_op_conv_loss: 0.2975 - val_avg_loss: 0.2546 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9093 - val_avg_accuracy: 0.9046\n",
      "Epoch 262/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7615 - op_main_loss: 0.2260 - op_conv_loss: 0.1577 - avg_loss: 0.1820 - op_main_accuracy: 0.9119 - op_conv_accuracy: 0.9341 - avg_accuracy: 0.9360\n",
      "Epoch 00262: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7615 - op_main_loss: 0.2260 - op_conv_loss: 0.1577 - avg_loss: 0.1820 - op_main_accuracy: 0.9119 - op_conv_accuracy: 0.9341 - avg_accuracy: 0.9360 - val_loss: 1.0413 - val_op_main_loss: 0.2682 - val_op_conv_loss: 0.3160 - val_avg_loss: 0.2612 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 263/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7497 - op_main_loss: 0.2177 - op_conv_loss: 0.1582 - avg_loss: 0.1781 - op_main_accuracy: 0.9174 - op_conv_accuracy: 0.9339 - avg_accuracy: 0.9323\n",
      "Epoch 00263: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7494 - op_main_loss: 0.2176 - op_conv_loss: 0.1581 - avg_loss: 0.1780 - op_main_accuracy: 0.9173 - op_conv_accuracy: 0.9338 - avg_accuracy: 0.9322 - val_loss: 1.0192 - val_op_main_loss: 0.2644 - val_op_conv_loss: 0.3017 - val_avg_loss: 0.2572 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9056\n",
      "Epoch 264/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7718 - op_main_loss: 0.2274 - op_conv_loss: 0.1635 - avg_loss: 0.1850 - op_main_accuracy: 0.9067 - op_conv_accuracy: 0.9343 - avg_accuracy: 0.9319\n",
      "Epoch 00264: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7718 - op_main_loss: 0.2274 - op_conv_loss: 0.1635 - avg_loss: 0.1850 - op_main_accuracy: 0.9067 - op_conv_accuracy: 0.9343 - avg_accuracy: 0.9319 - val_loss: 1.0130 - val_op_main_loss: 0.2651 - val_op_conv_loss: 0.2962 - val_avg_loss: 0.2562 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9084\n",
      "Epoch 265/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7967 - op_main_loss: 0.2332 - op_conv_loss: 0.1750 - avg_loss: 0.1929 - op_main_accuracy: 0.9106 - op_conv_accuracy: 0.9264 - avg_accuracy: 0.9310\n",
      "Epoch 00265: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8010 - op_main_loss: 0.2347 - op_conv_loss: 0.1765 - avg_loss: 0.1943 - op_main_accuracy: 0.9095 - op_conv_accuracy: 0.9263 - avg_accuracy: 0.9303 - val_loss: 1.0535 - val_op_main_loss: 0.2713 - val_op_conv_loss: 0.3208 - val_avg_loss: 0.2659 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8990\n",
      "Epoch 266/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7715 - op_main_loss: 0.2266 - op_conv_loss: 0.1638 - avg_loss: 0.1857 - op_main_accuracy: 0.9144 - op_conv_accuracy: 0.9387 - avg_accuracy: 0.9346\n",
      "Epoch 00266: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7743 - op_main_loss: 0.2272 - op_conv_loss: 0.1653 - avg_loss: 0.1865 - op_main_accuracy: 0.9135 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9338 - val_loss: 1.0143 - val_op_main_loss: 0.2709 - val_op_conv_loss: 0.2896 - val_avg_loss: 0.2585 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 267/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7425 - op_main_loss: 0.2180 - op_conv_loss: 0.1532 - avg_loss: 0.1760 - op_main_accuracy: 0.9177 - op_conv_accuracy: 0.9361 - avg_accuracy: 0.9325\n",
      "Epoch 00267: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7411 - op_main_loss: 0.2177 - op_conv_loss: 0.1525 - avg_loss: 0.1755 - op_main_accuracy: 0.9180 - op_conv_accuracy: 0.9364 - avg_accuracy: 0.9329 - val_loss: 1.0236 - val_op_main_loss: 0.2655 - val_op_conv_loss: 0.3066 - val_avg_loss: 0.2560 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9122 - val_avg_accuracy: 0.9093\n",
      "Epoch 268/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7673 - op_main_loss: 0.2234 - op_conv_loss: 0.1656 - avg_loss: 0.1829 - op_main_accuracy: 0.9104 - op_conv_accuracy: 0.9322 - avg_accuracy: 0.9336\n",
      "Epoch 00268: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7649 - op_main_loss: 0.2230 - op_conv_loss: 0.1645 - avg_loss: 0.1821 - op_main_accuracy: 0.9104 - op_conv_accuracy: 0.9327 - avg_accuracy: 0.9341 - val_loss: 1.0600 - val_op_main_loss: 0.2680 - val_op_conv_loss: 0.3307 - val_avg_loss: 0.2660 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 269/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7255 - op_main_loss: 0.2106 - op_conv_loss: 0.1492 - avg_loss: 0.1705 - op_main_accuracy: 0.9222 - op_conv_accuracy: 0.9382 - avg_accuracy: 0.9382\n",
      "Epoch 00269: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7259 - op_main_loss: 0.2104 - op_conv_loss: 0.1496 - avg_loss: 0.1706 - op_main_accuracy: 0.9220 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9381 - val_loss: 1.0320 - val_op_main_loss: 0.2678 - val_op_conv_loss: 0.3111 - val_avg_loss: 0.2577 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9046\n",
      "Epoch 270/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.7599 - op_main_loss: 0.2235 - op_conv_loss: 0.1598 - avg_loss: 0.1813 - op_main_accuracy: 0.9113 - op_conv_accuracy: 0.9339 - avg_accuracy: 0.9297\n",
      "Epoch 00270: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7601 - op_main_loss: 0.2234 - op_conv_loss: 0.1600 - avg_loss: 0.1814 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9336 - avg_accuracy: 0.9293 - val_loss: 1.0270 - val_op_main_loss: 0.2687 - val_op_conv_loss: 0.3022 - val_avg_loss: 0.2610 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 271/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7649 - op_main_loss: 0.2239 - op_conv_loss: 0.1628 - avg_loss: 0.1834 - op_main_accuracy: 0.9174 - op_conv_accuracy: 0.9316 - avg_accuracy: 0.9309\n",
      "Epoch 00271: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7648 - op_main_loss: 0.2239 - op_conv_loss: 0.1626 - avg_loss: 0.1833 - op_main_accuracy: 0.9173 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9308 - val_loss: 1.0295 - val_op_main_loss: 0.2652 - val_op_conv_loss: 0.3116 - val_avg_loss: 0.2579 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9065\n",
      "Epoch 272/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7629 - op_main_loss: 0.2255 - op_conv_loss: 0.1596 - avg_loss: 0.1833 - op_main_accuracy: 0.9135 - op_conv_accuracy: 0.9361 - avg_accuracy: 0.9325\n",
      "Epoch 00272: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7619 - op_main_loss: 0.2256 - op_conv_loss: 0.1587 - avg_loss: 0.1830 - op_main_accuracy: 0.9140 - op_conv_accuracy: 0.9369 - avg_accuracy: 0.9334 - val_loss: 1.0102 - val_op_main_loss: 0.2597 - val_op_conv_loss: 0.3014 - val_avg_loss: 0.2547 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 273/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7448 - op_main_loss: 0.2198 - op_conv_loss: 0.1539 - avg_loss: 0.1766 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9392 - avg_accuracy: 0.9409\n",
      "Epoch 00273: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7432 - op_main_loss: 0.2194 - op_conv_loss: 0.1531 - avg_loss: 0.1761 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9395 - avg_accuracy: 0.9412 - val_loss: 1.0264 - val_op_main_loss: 0.2688 - val_op_conv_loss: 0.3028 - val_avg_loss: 0.2603 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8971\n",
      "Epoch 274/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7482 - op_main_loss: 0.2193 - op_conv_loss: 0.1565 - avg_loss: 0.1777 - op_main_accuracy: 0.9163 - op_conv_accuracy: 0.9382 - avg_accuracy: 0.9325\n",
      "Epoch 00274: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7474 - op_main_loss: 0.2187 - op_conv_loss: 0.1567 - avg_loss: 0.1773 - op_main_accuracy: 0.9166 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9324 - val_loss: 1.0135 - val_op_main_loss: 0.2672 - val_op_conv_loss: 0.2955 - val_avg_loss: 0.2561 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9008\n",
      "Epoch 275/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7553 - op_main_loss: 0.2223 - op_conv_loss: 0.1586 - avg_loss: 0.1797 - op_main_accuracy: 0.9156 - op_conv_accuracy: 0.9397 - avg_accuracy: 0.9361\n",
      "Epoch 00275: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7527 - op_main_loss: 0.2217 - op_conv_loss: 0.1573 - avg_loss: 0.1789 - op_main_accuracy: 0.9159 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9364 - val_loss: 1.1505 - val_op_main_loss: 0.2882 - val_op_conv_loss: 0.3703 - val_avg_loss: 0.2974 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8839\n",
      "Epoch 276/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7441 - op_main_loss: 0.2189 - op_conv_loss: 0.1540 - avg_loss: 0.1764 - op_main_accuracy: 0.9204 - op_conv_accuracy: 0.9385 - avg_accuracy: 0.9368\n",
      "Epoch 00276: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7447 - op_main_loss: 0.2190 - op_conv_loss: 0.1543 - avg_loss: 0.1766 - op_main_accuracy: 0.9204 - op_conv_accuracy: 0.9386 - avg_accuracy: 0.9367 - val_loss: 1.0137 - val_op_main_loss: 0.2646 - val_op_conv_loss: 0.2983 - val_avg_loss: 0.2557 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 277/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7543 - op_main_loss: 0.2195 - op_conv_loss: 0.1597 - avg_loss: 0.1802 - op_main_accuracy: 0.9123 - op_conv_accuracy: 0.9334 - avg_accuracy: 0.9329\n",
      "Epoch 00277: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7547 - op_main_loss: 0.2195 - op_conv_loss: 0.1600 - avg_loss: 0.1804 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9334 - avg_accuracy: 0.9329 - val_loss: 1.0399 - val_op_main_loss: 0.2680 - val_op_conv_loss: 0.3145 - val_avg_loss: 0.2626 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9018\n",
      "Epoch 278/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7432 - op_main_loss: 0.2174 - op_conv_loss: 0.1550 - avg_loss: 0.1759 - op_main_accuracy: 0.9167 - op_conv_accuracy: 0.9375 - avg_accuracy: 0.9366\n",
      "Epoch 00278: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7429 - op_main_loss: 0.2174 - op_conv_loss: 0.1548 - avg_loss: 0.1758 - op_main_accuracy: 0.9166 - op_conv_accuracy: 0.9376 - avg_accuracy: 0.9367 - val_loss: 1.0490 - val_op_main_loss: 0.2703 - val_op_conv_loss: 0.3208 - val_avg_loss: 0.2628 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9046\n",
      "Epoch 279/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7839 - op_main_loss: 0.2308 - op_conv_loss: 0.1685 - avg_loss: 0.1895 - op_main_accuracy: 0.9070 - op_conv_accuracy: 0.9334 - avg_accuracy: 0.9293\n",
      "Epoch 00279: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7878 - op_main_loss: 0.2316 - op_conv_loss: 0.1705 - avg_loss: 0.1907 - op_main_accuracy: 0.9064 - op_conv_accuracy: 0.9322 - avg_accuracy: 0.9284 - val_loss: 1.0225 - val_op_main_loss: 0.2664 - val_op_conv_loss: 0.3033 - val_avg_loss: 0.2580 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 280/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7376 - op_main_loss: 0.2173 - op_conv_loss: 0.1507 - avg_loss: 0.1747 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9387\n",
      "Epoch 00280: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7378 - op_main_loss: 0.2174 - op_conv_loss: 0.1507 - avg_loss: 0.1748 - op_main_accuracy: 0.9211 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9383 - val_loss: 1.0168 - val_op_main_loss: 0.2631 - val_op_conv_loss: 0.3037 - val_avg_loss: 0.2551 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 281/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7415 - op_main_loss: 0.2173 - op_conv_loss: 0.1541 - avg_loss: 0.1754 - op_main_accuracy: 0.9204 - op_conv_accuracy: 0.9377 - avg_accuracy: 0.9351\n",
      "Epoch 00281: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7422 - op_main_loss: 0.2176 - op_conv_loss: 0.1542 - avg_loss: 0.1756 - op_main_accuracy: 0.9206 - op_conv_accuracy: 0.9376 - avg_accuracy: 0.9350 - val_loss: 1.0364 - val_op_main_loss: 0.2670 - val_op_conv_loss: 0.3135 - val_avg_loss: 0.2611 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9027\n",
      "Epoch 282/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.7263 - op_main_loss: 0.2136 - op_conv_loss: 0.1471 - avg_loss: 0.1708 - op_main_accuracy: 0.9264 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9385\n",
      "Epoch 00282: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7228 - op_main_loss: 0.2123 - op_conv_loss: 0.1460 - avg_loss: 0.1697 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9393 - val_loss: 1.0079 - val_op_main_loss: 0.2628 - val_op_conv_loss: 0.2965 - val_avg_loss: 0.2538 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9065\n",
      "Epoch 283/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7601 - op_main_loss: 0.2236 - op_conv_loss: 0.1610 - avg_loss: 0.1809 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9346 - avg_accuracy: 0.9307\n",
      "Epoch 00283: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7611 - op_main_loss: 0.2241 - op_conv_loss: 0.1611 - avg_loss: 0.1813 - op_main_accuracy: 0.9166 - op_conv_accuracy: 0.9338 - avg_accuracy: 0.9303 - val_loss: 1.0184 - val_op_main_loss: 0.2666 - val_op_conv_loss: 0.2992 - val_avg_loss: 0.2582 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9018\n",
      "Epoch 284/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7437 - op_main_loss: 0.2197 - op_conv_loss: 0.1525 - avg_loss: 0.1769 - op_main_accuracy: 0.9204 - op_conv_accuracy: 0.9365 - avg_accuracy: 0.9370\n",
      "Epoch 00284: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7416 - op_main_loss: 0.2181 - op_conv_loss: 0.1529 - avg_loss: 0.1760 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9367 - avg_accuracy: 0.9371 - val_loss: 1.0095 - val_op_main_loss: 0.2644 - val_op_conv_loss: 0.2963 - val_avg_loss: 0.2543 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9093\n",
      "Epoch 285/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7377 - op_main_loss: 0.2147 - op_conv_loss: 0.1536 - avg_loss: 0.1750 - op_main_accuracy: 0.9210 - op_conv_accuracy: 0.9404 - avg_accuracy: 0.9392\n",
      "Epoch 00285: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7442 - op_main_loss: 0.2169 - op_conv_loss: 0.1558 - avg_loss: 0.1771 - op_main_accuracy: 0.9199 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9390 - val_loss: 1.0065 - val_op_main_loss: 0.2643 - val_op_conv_loss: 0.2938 - val_avg_loss: 0.2541 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9093\n",
      "Epoch 286/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7565 - op_main_loss: 0.2188 - op_conv_loss: 0.1626 - avg_loss: 0.1810 - op_main_accuracy: 0.9205 - op_conv_accuracy: 0.9373 - avg_accuracy: 0.9353\n",
      "Epoch 00286: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7537 - op_main_loss: 0.2173 - op_conv_loss: 0.1623 - avg_loss: 0.1799 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9355 - val_loss: 0.9968 - val_op_main_loss: 0.2601 - val_op_conv_loss: 0.2917 - val_avg_loss: 0.2510 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8990\n",
      "Epoch 287/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7623 - op_main_loss: 0.2233 - op_conv_loss: 0.1620 - avg_loss: 0.1828 - op_main_accuracy: 0.9147 - op_conv_accuracy: 0.9310 - avg_accuracy: 0.9298\n",
      "Epoch 00287: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7604 - op_main_loss: 0.2233 - op_conv_loss: 0.1608 - avg_loss: 0.1822 - op_main_accuracy: 0.9147 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9303 - val_loss: 1.0367 - val_op_main_loss: 0.2711 - val_op_conv_loss: 0.3088 - val_avg_loss: 0.2630 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9056\n",
      "Epoch 288/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7421 - op_main_loss: 0.2162 - op_conv_loss: 0.1565 - avg_loss: 0.1755 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9369 - avg_accuracy: 0.9383\n",
      "Epoch 00288: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7421 - op_main_loss: 0.2162 - op_conv_loss: 0.1565 - avg_loss: 0.1755 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9369 - avg_accuracy: 0.9383 - val_loss: 1.0031 - val_op_main_loss: 0.2619 - val_op_conv_loss: 0.2937 - val_avg_loss: 0.2538 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 289/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7338 - op_main_loss: 0.2147 - op_conv_loss: 0.1514 - avg_loss: 0.1739 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9387 - avg_accuracy: 0.9373\n",
      "Epoch 00289: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7405 - op_main_loss: 0.2166 - op_conv_loss: 0.1539 - avg_loss: 0.1762 - op_main_accuracy: 0.9201 - op_conv_accuracy: 0.9376 - avg_accuracy: 0.9360 - val_loss: 1.0139 - val_op_main_loss: 0.2625 - val_op_conv_loss: 0.3022 - val_avg_loss: 0.2553 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9027\n",
      "Epoch 290/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7457 - op_main_loss: 0.2165 - op_conv_loss: 0.1575 - avg_loss: 0.1778 - op_main_accuracy: 0.9148 - op_conv_accuracy: 0.9346 - avg_accuracy: 0.9318\n",
      "Epoch 00290: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7448 - op_main_loss: 0.2167 - op_conv_loss: 0.1567 - avg_loss: 0.1774 - op_main_accuracy: 0.9147 - op_conv_accuracy: 0.9353 - avg_accuracy: 0.9322 - val_loss: 1.0150 - val_op_main_loss: 0.2651 - val_op_conv_loss: 0.2969 - val_avg_loss: 0.2591 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8999\n",
      "Epoch 291/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7246 - op_main_loss: 0.2115 - op_conv_loss: 0.1482 - avg_loss: 0.1709 - op_main_accuracy: 0.9212 - op_conv_accuracy: 0.9429 - avg_accuracy: 0.9406\n",
      "Epoch 00291: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7238 - op_main_loss: 0.2113 - op_conv_loss: 0.1480 - avg_loss: 0.1706 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9407 - val_loss: 1.0232 - val_op_main_loss: 0.2701 - val_op_conv_loss: 0.3024 - val_avg_loss: 0.2568 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9027\n",
      "Epoch 292/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7469 - op_main_loss: 0.2152 - op_conv_loss: 0.1598 - avg_loss: 0.1781 - op_main_accuracy: 0.9133 - op_conv_accuracy: 0.9360 - avg_accuracy: 0.9331\n",
      "Epoch 00292: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7469 - op_main_loss: 0.2152 - op_conv_loss: 0.1598 - avg_loss: 0.1781 - op_main_accuracy: 0.9133 - op_conv_accuracy: 0.9360 - avg_accuracy: 0.9331 - val_loss: 1.1383 - val_op_main_loss: 0.2960 - val_op_conv_loss: 0.3504 - val_avg_loss: 0.2986 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8857\n",
      "Epoch 293/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7155 - op_main_loss: 0.2078 - op_conv_loss: 0.1463 - avg_loss: 0.1680 - op_main_accuracy: 0.9207 - op_conv_accuracy: 0.9401 - avg_accuracy: 0.9394\n",
      "Epoch 00293: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7165 - op_main_loss: 0.2079 - op_conv_loss: 0.1469 - avg_loss: 0.1684 - op_main_accuracy: 0.9208 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9393 - val_loss: 1.0557 - val_op_main_loss: 0.2766 - val_op_conv_loss: 0.3142 - val_avg_loss: 0.2715 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8971\n",
      "Epoch 294/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.7484 - op_main_loss: 0.2166 - op_conv_loss: 0.1595 - avg_loss: 0.1789 - op_main_accuracy: 0.9197 - op_conv_accuracy: 0.9361 - avg_accuracy: 0.9312\n",
      "Epoch 00294: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7483 - op_main_loss: 0.2169 - op_conv_loss: 0.1591 - avg_loss: 0.1789 - op_main_accuracy: 0.9190 - op_conv_accuracy: 0.9360 - avg_accuracy: 0.9310 - val_loss: 1.0111 - val_op_main_loss: 0.2625 - val_op_conv_loss: 0.3016 - val_avg_loss: 0.2539 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9037\n",
      "Epoch 295/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7599 - op_main_loss: 0.2201 - op_conv_loss: 0.1651 - avg_loss: 0.1821 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9327 - avg_accuracy: 0.9317\n",
      "Epoch 00295: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7599 - op_main_loss: 0.2201 - op_conv_loss: 0.1651 - avg_loss: 0.1821 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9327 - avg_accuracy: 0.9317 - val_loss: 1.0118 - val_op_main_loss: 0.2701 - val_op_conv_loss: 0.2913 - val_avg_loss: 0.2580 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8980\n",
      "Epoch 296/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7407 - op_main_loss: 0.2190 - op_conv_loss: 0.1528 - avg_loss: 0.1763 - op_main_accuracy: 0.9172 - op_conv_accuracy: 0.9411 - avg_accuracy: 0.9327\n",
      "Epoch 00296: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7385 - op_main_loss: 0.2185 - op_conv_loss: 0.1519 - avg_loss: 0.1756 - op_main_accuracy: 0.9173 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9331 - val_loss: 1.0028 - val_op_main_loss: 0.2636 - val_op_conv_loss: 0.2920 - val_avg_loss: 0.2546 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9046\n",
      "Epoch 297/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7625 - op_main_loss: 0.2232 - op_conv_loss: 0.1634 - avg_loss: 0.1833 - op_main_accuracy: 0.9159 - op_conv_accuracy: 0.9308 - avg_accuracy: 0.9303\n",
      "Epoch 00297: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7597 - op_main_loss: 0.2222 - op_conv_loss: 0.1625 - avg_loss: 0.1825 - op_main_accuracy: 0.9161 - op_conv_accuracy: 0.9312 - avg_accuracy: 0.9305 - val_loss: 1.0760 - val_op_main_loss: 0.2615 - val_op_conv_loss: 0.3542 - val_avg_loss: 0.2678 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8924\n",
      "Epoch 298/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7391 - op_main_loss: 0.2152 - op_conv_loss: 0.1557 - avg_loss: 0.1758 - op_main_accuracy: 0.9165 - op_conv_accuracy: 0.9349 - avg_accuracy: 0.9334\n",
      "Epoch 00298: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7410 - op_main_loss: 0.2152 - op_conv_loss: 0.1572 - avg_loss: 0.1763 - op_main_accuracy: 0.9166 - op_conv_accuracy: 0.9341 - avg_accuracy: 0.9329 - val_loss: 1.0150 - val_op_main_loss: 0.2638 - val_op_conv_loss: 0.3017 - val_avg_loss: 0.2571 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 299/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7288 - op_main_loss: 0.2126 - op_conv_loss: 0.1515 - avg_loss: 0.1724 - op_main_accuracy: 0.9190 - op_conv_accuracy: 0.9423 - avg_accuracy: 0.9370\n",
      "Epoch 00299: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7273 - op_main_loss: 0.2124 - op_conv_loss: 0.1507 - avg_loss: 0.1719 - op_main_accuracy: 0.9197 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9374 - val_loss: 1.0037 - val_op_main_loss: 0.2613 - val_op_conv_loss: 0.2947 - val_avg_loss: 0.2555 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8980\n",
      "Epoch 300/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7246 - op_main_loss: 0.2119 - op_conv_loss: 0.1485 - avg_loss: 0.1718 - op_main_accuracy: 0.9202 - op_conv_accuracy: 0.9387 - avg_accuracy: 0.9385\n",
      "Epoch 00300: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7278 - op_main_loss: 0.2128 - op_conv_loss: 0.1497 - avg_loss: 0.1728 - op_main_accuracy: 0.9201 - op_conv_accuracy: 0.9386 - avg_accuracy: 0.9379 - val_loss: 1.0614 - val_op_main_loss: 0.2791 - val_op_conv_loss: 0.3159 - val_avg_loss: 0.2740 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8924\n",
      "Epoch 301/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7261 - op_main_loss: 0.2122 - op_conv_loss: 0.1505 - avg_loss: 0.1712 - op_main_accuracy: 0.9196 - op_conv_accuracy: 0.9399 - avg_accuracy: 0.9370\n",
      "Epoch 00301: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7288 - op_main_loss: 0.2129 - op_conv_loss: 0.1516 - avg_loss: 0.1721 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9393 - avg_accuracy: 0.9362 - val_loss: 1.0323 - val_op_main_loss: 0.2721 - val_op_conv_loss: 0.3033 - val_avg_loss: 0.2647 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8999\n",
      "Epoch 302/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7214 - op_main_loss: 0.2119 - op_conv_loss: 0.1466 - avg_loss: 0.1706 - op_main_accuracy: 0.9220 - op_conv_accuracy: 0.9387 - avg_accuracy: 0.9368\n",
      "Epoch 00302: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7217 - op_main_loss: 0.2120 - op_conv_loss: 0.1466 - avg_loss: 0.1707 - op_main_accuracy: 0.9220 - op_conv_accuracy: 0.9386 - avg_accuracy: 0.9364 - val_loss: 1.0366 - val_op_main_loss: 0.2751 - val_op_conv_loss: 0.3024 - val_avg_loss: 0.2668 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8990\n",
      "Epoch 303/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7380 - op_main_loss: 0.2174 - op_conv_loss: 0.1527 - avg_loss: 0.1754 - op_main_accuracy: 0.9189 - op_conv_accuracy: 0.9396 - avg_accuracy: 0.9368\n",
      "Epoch 00303: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7376 - op_main_loss: 0.2174 - op_conv_loss: 0.1524 - avg_loss: 0.1753 - op_main_accuracy: 0.9192 - op_conv_accuracy: 0.9395 - avg_accuracy: 0.9369 - val_loss: 1.0138 - val_op_main_loss: 0.2620 - val_op_conv_loss: 0.3029 - val_avg_loss: 0.2564 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.9027\n",
      "Epoch 304/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7329 - op_main_loss: 0.2089 - op_conv_loss: 0.1577 - avg_loss: 0.1739 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9365 - avg_accuracy: 0.9382\n",
      "Epoch 00304: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7349 - op_main_loss: 0.2095 - op_conv_loss: 0.1585 - avg_loss: 0.1746 - op_main_accuracy: 0.9220 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9381 - val_loss: 1.0308 - val_op_main_loss: 0.2642 - val_op_conv_loss: 0.3155 - val_avg_loss: 0.2587 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9046\n",
      "Epoch 305/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7470 - op_main_loss: 0.2162 - op_conv_loss: 0.1604 - avg_loss: 0.1782 - op_main_accuracy: 0.9185 - op_conv_accuracy: 0.9339 - avg_accuracy: 0.9315\n",
      "Epoch 00305: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7471 - op_main_loss: 0.2163 - op_conv_loss: 0.1604 - avg_loss: 0.1783 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9341 - avg_accuracy: 0.9315 - val_loss: 1.0547 - val_op_main_loss: 0.2683 - val_op_conv_loss: 0.3279 - val_avg_loss: 0.2663 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.7339 - op_main_loss: 0.2170 - op_conv_loss: 0.1507 - avg_loss: 0.1742 - op_main_accuracy: 0.9180 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9346\n",
      "Epoch 00306: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7367 - op_main_loss: 0.2175 - op_conv_loss: 0.1520 - avg_loss: 0.1751 - op_main_accuracy: 0.9182 - op_conv_accuracy: 0.9405 - avg_accuracy: 0.9343 - val_loss: 1.0184 - val_op_main_loss: 0.2660 - val_op_conv_loss: 0.3001 - val_avg_loss: 0.2602 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8952\n",
      "Epoch 307/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7408 - op_main_loss: 0.2173 - op_conv_loss: 0.1552 - avg_loss: 0.1764 - op_main_accuracy: 0.9163 - op_conv_accuracy: 0.9392 - avg_accuracy: 0.9368\n",
      "Epoch 00307: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7381 - op_main_loss: 0.2164 - op_conv_loss: 0.1543 - avg_loss: 0.1755 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9395 - avg_accuracy: 0.9371 - val_loss: 1.0009 - val_op_main_loss: 0.2607 - val_op_conv_loss: 0.2943 - val_avg_loss: 0.2541 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9056\n",
      "Epoch 308/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7329 - op_main_loss: 0.2147 - op_conv_loss: 0.1529 - avg_loss: 0.1735 - op_main_accuracy: 0.9157 - op_conv_accuracy: 0.9380 - avg_accuracy: 0.9365\n",
      "Epoch 00308: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7314 - op_main_loss: 0.2143 - op_conv_loss: 0.1523 - avg_loss: 0.1731 - op_main_accuracy: 0.9149 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9362 - val_loss: 1.0092 - val_op_main_loss: 0.2587 - val_op_conv_loss: 0.3050 - val_avg_loss: 0.2535 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9084\n",
      "Epoch 309/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7099 - op_main_loss: 0.2083 - op_conv_loss: 0.1425 - avg_loss: 0.1671 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9484 - avg_accuracy: 0.9423\n",
      "Epoch 00309: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7073 - op_main_loss: 0.2075 - op_conv_loss: 0.1416 - avg_loss: 0.1662 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9431 - val_loss: 1.0101 - val_op_main_loss: 0.2604 - val_op_conv_loss: 0.3015 - val_avg_loss: 0.2562 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9037\n",
      "Epoch 310/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7182 - op_main_loss: 0.2089 - op_conv_loss: 0.1477 - avg_loss: 0.1694 - op_main_accuracy: 0.9229 - op_conv_accuracy: 0.9396 - avg_accuracy: 0.9392\n",
      "Epoch 00310: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7185 - op_main_loss: 0.2087 - op_conv_loss: 0.1480 - avg_loss: 0.1696 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9395 - avg_accuracy: 0.9395 - val_loss: 1.0061 - val_op_main_loss: 0.2606 - val_op_conv_loss: 0.2982 - val_avg_loss: 0.2550 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8990\n",
      "Epoch 311/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7310 - op_main_loss: 0.2139 - op_conv_loss: 0.1516 - avg_loss: 0.1733 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9375\n",
      "Epoch 00311: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7302 - op_main_loss: 0.2140 - op_conv_loss: 0.1509 - avg_loss: 0.1731 - op_main_accuracy: 0.9190 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9376 - val_loss: 1.0315 - val_op_main_loss: 0.2756 - val_op_conv_loss: 0.2981 - val_avg_loss: 0.2654 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8952\n",
      "Epoch 312/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6980 - op_main_loss: 0.2012 - op_conv_loss: 0.1419 - avg_loss: 0.1622 - op_main_accuracy: 0.9273 - op_conv_accuracy: 0.9443 - avg_accuracy: 0.9423\n",
      "Epoch 00312: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6971 - op_main_loss: 0.2016 - op_conv_loss: 0.1410 - avg_loss: 0.1619 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9431 - val_loss: 1.0369 - val_op_main_loss: 0.2630 - val_op_conv_loss: 0.3212 - val_avg_loss: 0.2600 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9046\n",
      "Epoch 313/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7233 - op_main_loss: 0.2103 - op_conv_loss: 0.1497 - avg_loss: 0.1708 - op_main_accuracy: 0.9207 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9389\n",
      "Epoch 00313: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7241 - op_main_loss: 0.2106 - op_conv_loss: 0.1499 - avg_loss: 0.1710 - op_main_accuracy: 0.9204 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9386 - val_loss: 1.0020 - val_op_main_loss: 0.2593 - val_op_conv_loss: 0.2971 - val_avg_loss: 0.2532 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 314/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7037 - op_main_loss: 0.2060 - op_conv_loss: 0.1407 - avg_loss: 0.1646 - op_main_accuracy: 0.9222 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9411\n",
      "Epoch 00314: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7042 - op_main_loss: 0.2060 - op_conv_loss: 0.1411 - avg_loss: 0.1648 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9407 - val_loss: 1.0129 - val_op_main_loss: 0.2591 - val_op_conv_loss: 0.3062 - val_avg_loss: 0.2553 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9027\n",
      "Epoch 315/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7483 - op_main_loss: 0.2191 - op_conv_loss: 0.1592 - avg_loss: 0.1779 - op_main_accuracy: 0.9167 - op_conv_accuracy: 0.9382 - avg_accuracy: 0.9351\n",
      "Epoch 00315: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7485 - op_main_loss: 0.2190 - op_conv_loss: 0.1595 - avg_loss: 0.1780 - op_main_accuracy: 0.9168 - op_conv_accuracy: 0.9383 - avg_accuracy: 0.9353 - val_loss: 1.0141 - val_op_main_loss: 0.2634 - val_op_conv_loss: 0.3022 - val_avg_loss: 0.2567 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9103\n",
      "Epoch 316/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6953 - op_main_loss: 0.2014 - op_conv_loss: 0.1393 - avg_loss: 0.1626 - op_main_accuracy: 0.9264 - op_conv_accuracy: 0.9455 - avg_accuracy: 0.9433\n",
      "Epoch 00316: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6921 - op_main_loss: 0.2004 - op_conv_loss: 0.1382 - avg_loss: 0.1616 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9457 - avg_accuracy: 0.9435 - val_loss: 1.0376 - val_op_main_loss: 0.2611 - val_op_conv_loss: 0.3277 - val_avg_loss: 0.2569 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9075\n",
      "Epoch 317/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7442 - op_main_loss: 0.2164 - op_conv_loss: 0.1589 - avg_loss: 0.1774 - op_main_accuracy: 0.9167 - op_conv_accuracy: 0.9385 - avg_accuracy: 0.9348\n",
      "Epoch 00317: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7434 - op_main_loss: 0.2165 - op_conv_loss: 0.1582 - avg_loss: 0.1772 - op_main_accuracy: 0.9161 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9343 - val_loss: 1.0401 - val_op_main_loss: 0.2654 - val_op_conv_loss: 0.3223 - val_avg_loss: 0.2611 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9046\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.7317 - op_main_loss: 0.2132 - op_conv_loss: 0.1534 - avg_loss: 0.1736 - op_main_accuracy: 0.9158 - op_conv_accuracy: 0.9396 - avg_accuracy: 0.9334\n",
      "Epoch 00318: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7305 - op_main_loss: 0.2127 - op_conv_loss: 0.1532 - avg_loss: 0.1732 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9395 - avg_accuracy: 0.9336 - val_loss: 1.0101 - val_op_main_loss: 0.2611 - val_op_conv_loss: 0.3025 - val_avg_loss: 0.2551 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.8999\n",
      "Epoch 319/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7398 - op_main_loss: 0.2137 - op_conv_loss: 0.1589 - avg_loss: 0.1759 - op_main_accuracy: 0.9215 - op_conv_accuracy: 0.9370 - avg_accuracy: 0.9351\n",
      "Epoch 00319: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7370 - op_main_loss: 0.2128 - op_conv_loss: 0.1577 - avg_loss: 0.1751 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9355 - val_loss: 0.9944 - val_op_main_loss: 0.2582 - val_op_conv_loss: 0.2932 - val_avg_loss: 0.2518 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9037\n",
      "Epoch 320/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7284 - op_main_loss: 0.2101 - op_conv_loss: 0.1543 - avg_loss: 0.1728 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9350 - avg_accuracy: 0.9357\n",
      "Epoch 00320: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7284 - op_main_loss: 0.2101 - op_conv_loss: 0.1543 - avg_loss: 0.1728 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9350 - avg_accuracy: 0.9357 - val_loss: 1.0099 - val_op_main_loss: 0.2643 - val_op_conv_loss: 0.2955 - val_avg_loss: 0.2590 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8990\n",
      "Epoch 321/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7219 - op_main_loss: 0.2119 - op_conv_loss: 0.1479 - avg_loss: 0.1711 - op_main_accuracy: 0.9288 - op_conv_accuracy: 0.9397 - avg_accuracy: 0.9382\n",
      "Epoch 00321: val_avg_accuracy did not improve from 0.91124\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7247 - op_main_loss: 0.2127 - op_conv_loss: 0.1489 - avg_loss: 0.1720 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9390 - avg_accuracy: 0.9376 - val_loss: 1.0190 - val_op_main_loss: 0.2626 - val_op_conv_loss: 0.3099 - val_avg_loss: 0.2552 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9037\n",
      "Epoch 322/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7063 - op_main_loss: 0.2092 - op_conv_loss: 0.1407 - avg_loss: 0.1652 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9418 - avg_accuracy: 0.9404\n",
      "Epoch 00322: val_avg_accuracy improved from 0.91124 to 0.91218, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7090 - op_main_loss: 0.2099 - op_conv_loss: 0.1417 - avg_loss: 0.1661 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9395 - val_loss: 1.0145 - val_op_main_loss: 0.2585 - val_op_conv_loss: 0.3125 - val_avg_loss: 0.2522 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9122\n",
      "Epoch 323/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7244 - op_main_loss: 0.2095 - op_conv_loss: 0.1527 - avg_loss: 0.1707 - op_main_accuracy: 0.9208 - op_conv_accuracy: 0.9377 - avg_accuracy: 0.9370\n",
      "Epoch 00323: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7228 - op_main_loss: 0.2093 - op_conv_loss: 0.1517 - avg_loss: 0.1703 - op_main_accuracy: 0.9206 - op_conv_accuracy: 0.9379 - avg_accuracy: 0.9371 - val_loss: 1.0044 - val_op_main_loss: 0.2582 - val_op_conv_loss: 0.3018 - val_avg_loss: 0.2530 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9046\n",
      "Epoch 324/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7141 - op_main_loss: 0.2098 - op_conv_loss: 0.1452 - avg_loss: 0.1677 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9404\n",
      "Epoch 00324: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7140 - op_main_loss: 0.2100 - op_conv_loss: 0.1450 - avg_loss: 0.1676 - op_main_accuracy: 0.9230 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9402 - val_loss: 1.0147 - val_op_main_loss: 0.2605 - val_op_conv_loss: 0.3073 - val_avg_loss: 0.2555 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 325/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7077 - op_main_loss: 0.2072 - op_conv_loss: 0.1426 - avg_loss: 0.1664 - op_main_accuracy: 0.9198 - op_conv_accuracy: 0.9375 - avg_accuracy: 0.9358\n",
      "Epoch 00325: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7090 - op_main_loss: 0.2074 - op_conv_loss: 0.1433 - avg_loss: 0.1669 - op_main_accuracy: 0.9192 - op_conv_accuracy: 0.9371 - avg_accuracy: 0.9353 - val_loss: 1.0222 - val_op_main_loss: 0.2621 - val_op_conv_loss: 0.3110 - val_avg_loss: 0.2575 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9037\n",
      "Epoch 326/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7007 - op_main_loss: 0.2059 - op_conv_loss: 0.1392 - avg_loss: 0.1639 - op_main_accuracy: 0.9238 - op_conv_accuracy: 0.9430 - avg_accuracy: 0.9428\n",
      "Epoch 00326: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7004 - op_main_loss: 0.2060 - op_conv_loss: 0.1389 - avg_loss: 0.1638 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9431 - val_loss: 1.0446 - val_op_main_loss: 0.2614 - val_op_conv_loss: 0.3300 - val_avg_loss: 0.2615 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 327/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7190 - op_main_loss: 0.2083 - op_conv_loss: 0.1496 - avg_loss: 0.1695 - op_main_accuracy: 0.9256 - op_conv_accuracy: 0.9390 - avg_accuracy: 0.9387\n",
      "Epoch 00327: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7171 - op_main_loss: 0.2078 - op_conv_loss: 0.1488 - avg_loss: 0.1689 - op_main_accuracy: 0.9260 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9395 - val_loss: 1.0514 - val_op_main_loss: 0.2692 - val_op_conv_loss: 0.3218 - val_avg_loss: 0.2689 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8999\n",
      "Epoch 328/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7040 - op_main_loss: 0.2048 - op_conv_loss: 0.1426 - avg_loss: 0.1652 - op_main_accuracy: 0.9222 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9447\n",
      "Epoch 00328: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7016 - op_main_loss: 0.2040 - op_conv_loss: 0.1417 - avg_loss: 0.1645 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9452 - val_loss: 1.0331 - val_op_main_loss: 0.2612 - val_op_conv_loss: 0.3221 - val_avg_loss: 0.2584 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9056\n",
      "Epoch 329/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7241 - op_main_loss: 0.2090 - op_conv_loss: 0.1531 - avg_loss: 0.1708 - op_main_accuracy: 0.9235 - op_conv_accuracy: 0.9361 - avg_accuracy: 0.9361\n",
      "Epoch 00329: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7236 - op_main_loss: 0.2088 - op_conv_loss: 0.1528 - avg_loss: 0.1706 - op_main_accuracy: 0.9237 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9362 - val_loss: 1.0077 - val_op_main_loss: 0.2634 - val_op_conv_loss: 0.2972 - val_avg_loss: 0.2558 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9008\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.7080 - op_main_loss: 0.2062 - op_conv_loss: 0.1451 - avg_loss: 0.1654 - op_main_accuracy: 0.9181 - op_conv_accuracy: 0.9432 - avg_accuracy: 0.9415\n",
      "Epoch 00330: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7079 - op_main_loss: 0.2063 - op_conv_loss: 0.1450 - avg_loss: 0.1654 - op_main_accuracy: 0.9182 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9416 - val_loss: 1.0348 - val_op_main_loss: 0.2595 - val_op_conv_loss: 0.3250 - val_avg_loss: 0.2591 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8971\n",
      "Epoch 331/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7303 - op_main_loss: 0.2126 - op_conv_loss: 0.1535 - avg_loss: 0.1731 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9406 - avg_accuracy: 0.9373\n",
      "Epoch 00331: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7285 - op_main_loss: 0.2120 - op_conv_loss: 0.1530 - avg_loss: 0.1725 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9412 - avg_accuracy: 0.9376 - val_loss: 1.0138 - val_op_main_loss: 0.2615 - val_op_conv_loss: 0.3067 - val_avg_loss: 0.2546 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9037\n",
      "Epoch 332/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7327 - op_main_loss: 0.2150 - op_conv_loss: 0.1531 - avg_loss: 0.1736 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9386 - avg_accuracy: 0.9350\n",
      "Epoch 00332: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7327 - op_main_loss: 0.2150 - op_conv_loss: 0.1531 - avg_loss: 0.1736 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9386 - avg_accuracy: 0.9350 - val_loss: 1.0342 - val_op_main_loss: 0.2645 - val_op_conv_loss: 0.3186 - val_avg_loss: 0.2602 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.8999\n",
      "Epoch 333/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7090 - op_main_loss: 0.2037 - op_conv_loss: 0.1485 - avg_loss: 0.1660 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9370 - avg_accuracy: 0.9365\n",
      "Epoch 00333: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7077 - op_main_loss: 0.2039 - op_conv_loss: 0.1474 - avg_loss: 0.1656 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9364 - val_loss: 1.0386 - val_op_main_loss: 0.2648 - val_op_conv_loss: 0.3246 - val_avg_loss: 0.2584 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.8999\n",
      "Epoch 334/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7189 - op_main_loss: 0.2082 - op_conv_loss: 0.1494 - avg_loss: 0.1706 - op_main_accuracy: 0.9244 - op_conv_accuracy: 0.9389 - avg_accuracy: 0.9396\n",
      "Epoch 00334: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7199 - op_main_loss: 0.2082 - op_conv_loss: 0.1501 - avg_loss: 0.1709 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9388 - avg_accuracy: 0.9395 - val_loss: 1.0388 - val_op_main_loss: 0.2702 - val_op_conv_loss: 0.3150 - val_avg_loss: 0.2630 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8990\n",
      "Epoch 335/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7206 - op_main_loss: 0.2091 - op_conv_loss: 0.1503 - avg_loss: 0.1706 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9399\n",
      "Epoch 00335: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7199 - op_main_loss: 0.2090 - op_conv_loss: 0.1498 - avg_loss: 0.1704 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9400 - val_loss: 1.0221 - val_op_main_loss: 0.2689 - val_op_conv_loss: 0.2987 - val_avg_loss: 0.2641 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8961\n",
      "Epoch 336/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7127 - op_main_loss: 0.2069 - op_conv_loss: 0.1476 - avg_loss: 0.1680 - op_main_accuracy: 0.9295 - op_conv_accuracy: 0.9411 - avg_accuracy: 0.9392\n",
      "Epoch 00336: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7165 - op_main_loss: 0.2080 - op_conv_loss: 0.1491 - avg_loss: 0.1692 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9381 - val_loss: 1.0230 - val_op_main_loss: 0.2619 - val_op_conv_loss: 0.3117 - val_avg_loss: 0.2590 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9008\n",
      "Epoch 337/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7090 - op_main_loss: 0.2071 - op_conv_loss: 0.1447 - avg_loss: 0.1669 - op_main_accuracy: 0.9228 - op_conv_accuracy: 0.9392 - avg_accuracy: 0.9392\n",
      "Epoch 00337: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7094 - op_main_loss: 0.2072 - op_conv_loss: 0.1449 - avg_loss: 0.1670 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9388 - avg_accuracy: 0.9388 - val_loss: 1.0136 - val_op_main_loss: 0.2593 - val_op_conv_loss: 0.3071 - val_avg_loss: 0.2569 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9008\n",
      "Epoch 338/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7211 - op_main_loss: 0.2098 - op_conv_loss: 0.1504 - avg_loss: 0.1704 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9387 - avg_accuracy: 0.9380\n",
      "Epoch 00338: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7224 - op_main_loss: 0.2107 - op_conv_loss: 0.1505 - avg_loss: 0.1708 - op_main_accuracy: 0.9230 - op_conv_accuracy: 0.9383 - avg_accuracy: 0.9379 - val_loss: 1.0327 - val_op_main_loss: 0.2695 - val_op_conv_loss: 0.3065 - val_avg_loss: 0.2664 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9008\n",
      "Epoch 339/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7335 - op_main_loss: 0.2141 - op_conv_loss: 0.1542 - avg_loss: 0.1750 - op_main_accuracy: 0.9179 - op_conv_accuracy: 0.9339 - avg_accuracy: 0.9321\n",
      "Epoch 00339: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7332 - op_main_loss: 0.2140 - op_conv_loss: 0.1540 - avg_loss: 0.1749 - op_main_accuracy: 0.9180 - op_conv_accuracy: 0.9341 - avg_accuracy: 0.9322 - val_loss: 1.0406 - val_op_main_loss: 0.2629 - val_op_conv_loss: 0.3236 - val_avg_loss: 0.2637 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8999\n",
      "Epoch 340/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7116 - op_main_loss: 0.2054 - op_conv_loss: 0.1480 - avg_loss: 0.1677 - op_main_accuracy: 0.9256 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9381\n",
      "Epoch 00340: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7116 - op_main_loss: 0.2054 - op_conv_loss: 0.1480 - avg_loss: 0.1677 - op_main_accuracy: 0.9256 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9381 - val_loss: 1.0759 - val_op_main_loss: 0.2715 - val_op_conv_loss: 0.3438 - val_avg_loss: 0.2701 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8980\n",
      "Epoch 341/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7091 - op_main_loss: 0.2044 - op_conv_loss: 0.1472 - avg_loss: 0.1674 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9413 - avg_accuracy: 0.9411\n",
      "Epoch 00341: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7115 - op_main_loss: 0.2055 - op_conv_loss: 0.1478 - avg_loss: 0.1682 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9412 - avg_accuracy: 0.9409 - val_loss: 1.0325 - val_op_main_loss: 0.2827 - val_op_conv_loss: 0.2904 - val_avg_loss: 0.2693 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8952\n",
      "Epoch 342/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6958 - op_main_loss: 0.2017 - op_conv_loss: 0.1410 - avg_loss: 0.1630 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9459 - avg_accuracy: 0.9480\n",
      "Epoch 00342: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6958 - op_main_loss: 0.2017 - op_conv_loss: 0.1410 - avg_loss: 0.1630 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9459 - avg_accuracy: 0.9480 - val_loss: 1.0244 - val_op_main_loss: 0.2620 - val_op_conv_loss: 0.3114 - val_avg_loss: 0.2609 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8980\n",
      "Epoch 343/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7011 - op_main_loss: 0.2039 - op_conv_loss: 0.1424 - avg_loss: 0.1647 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9409\n",
      "Epoch 00343: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7011 - op_main_loss: 0.2039 - op_conv_loss: 0.1424 - avg_loss: 0.1647 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9409 - val_loss: 1.0145 - val_op_main_loss: 0.2613 - val_op_conv_loss: 0.3058 - val_avg_loss: 0.2573 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8990\n",
      "Epoch 344/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6979 - op_main_loss: 0.2009 - op_conv_loss: 0.1434 - avg_loss: 0.1634 - op_main_accuracy: 0.9251 - op_conv_accuracy: 0.9427 - avg_accuracy: 0.9382\n",
      "Epoch 00344: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6970 - op_main_loss: 0.2007 - op_conv_loss: 0.1430 - avg_loss: 0.1632 - op_main_accuracy: 0.9249 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9383 - val_loss: 1.0108 - val_op_main_loss: 0.2608 - val_op_conv_loss: 0.3031 - val_avg_loss: 0.2569 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9065\n",
      "Epoch 345/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6945 - op_main_loss: 0.2010 - op_conv_loss: 0.1408 - avg_loss: 0.1626 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9414\n",
      "Epoch 00345: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6945 - op_main_loss: 0.2010 - op_conv_loss: 0.1408 - avg_loss: 0.1626 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9414 - val_loss: 1.1231 - val_op_main_loss: 0.2898 - val_op_conv_loss: 0.3497 - val_avg_loss: 0.2938 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8895\n",
      "Epoch 346/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7167 - op_main_loss: 0.2067 - op_conv_loss: 0.1510 - avg_loss: 0.1690 - op_main_accuracy: 0.9250 - op_conv_accuracy: 0.9370 - avg_accuracy: 0.9380\n",
      "Epoch 00346: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7235 - op_main_loss: 0.2091 - op_conv_loss: 0.1532 - avg_loss: 0.1712 - op_main_accuracy: 0.9237 - op_conv_accuracy: 0.9364 - avg_accuracy: 0.9371 - val_loss: 1.0449 - val_op_main_loss: 0.2653 - val_op_conv_loss: 0.3267 - val_avg_loss: 0.2628 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9027\n",
      "Epoch 347/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7013 - op_main_loss: 0.2018 - op_conv_loss: 0.1451 - avg_loss: 0.1641 - op_main_accuracy: 0.9280 - op_conv_accuracy: 0.9423 - avg_accuracy: 0.9413\n",
      "Epoch 00347: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6995 - op_main_loss: 0.2012 - op_conv_loss: 0.1445 - avg_loss: 0.1635 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9416 - val_loss: 1.0673 - val_op_main_loss: 0.2731 - val_op_conv_loss: 0.3294 - val_avg_loss: 0.2747 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8961\n",
      "Epoch 348/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7077 - op_main_loss: 0.2079 - op_conv_loss: 0.1430 - avg_loss: 0.1667 - op_main_accuracy: 0.9218 - op_conv_accuracy: 0.9439 - avg_accuracy: 0.9385\n",
      "Epoch 00348: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7073 - op_main_loss: 0.2080 - op_conv_loss: 0.1427 - avg_loss: 0.1666 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9440 - avg_accuracy: 0.9386 - val_loss: 1.0481 - val_op_main_loss: 0.2676 - val_op_conv_loss: 0.3208 - val_avg_loss: 0.2697 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8952\n",
      "Epoch 349/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7068 - op_main_loss: 0.2041 - op_conv_loss: 0.1467 - avg_loss: 0.1659 - op_main_accuracy: 0.9269 - op_conv_accuracy: 0.9404 - avg_accuracy: 0.9416\n",
      "Epoch 00349: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7056 - op_main_loss: 0.2040 - op_conv_loss: 0.1460 - avg_loss: 0.1655 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9421 - val_loss: 1.0240 - val_op_main_loss: 0.2620 - val_op_conv_loss: 0.3147 - val_avg_loss: 0.2572 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 350/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7339 - op_main_loss: 0.2114 - op_conv_loss: 0.1581 - avg_loss: 0.1746 - op_main_accuracy: 0.9194 - op_conv_accuracy: 0.9373 - avg_accuracy: 0.9358\n",
      "Epoch 00350: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7331 - op_main_loss: 0.2115 - op_conv_loss: 0.1574 - avg_loss: 0.1744 - op_main_accuracy: 0.9192 - op_conv_accuracy: 0.9376 - avg_accuracy: 0.9360 - val_loss: 1.0392 - val_op_main_loss: 0.2698 - val_op_conv_loss: 0.3098 - val_avg_loss: 0.2700 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8980\n",
      "Epoch 351/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7056 - op_main_loss: 0.2047 - op_conv_loss: 0.1456 - avg_loss: 0.1656 - op_main_accuracy: 0.9215 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9382\n",
      "Epoch 00351: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7068 - op_main_loss: 0.2051 - op_conv_loss: 0.1460 - avg_loss: 0.1659 - op_main_accuracy: 0.9206 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9379 - val_loss: 1.0648 - val_op_main_loss: 0.2675 - val_op_conv_loss: 0.3384 - val_avg_loss: 0.2688 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8942\n",
      "Epoch 352/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6978 - op_main_loss: 0.2024 - op_conv_loss: 0.1417 - avg_loss: 0.1637 - op_main_accuracy: 0.9276 - op_conv_accuracy: 0.9425 - avg_accuracy: 0.9399\n",
      "Epoch 00352: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6971 - op_main_loss: 0.2021 - op_conv_loss: 0.1415 - avg_loss: 0.1635 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9400 - val_loss: 1.0005 - val_op_main_loss: 0.2594 - val_op_conv_loss: 0.2972 - val_avg_loss: 0.2538 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 353/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6899 - op_main_loss: 0.1997 - op_conv_loss: 0.1392 - avg_loss: 0.1609 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9461\n",
      "Epoch 00353: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6899 - op_main_loss: 0.1997 - op_conv_loss: 0.1392 - avg_loss: 0.1609 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9461 - val_loss: 0.9895 - val_op_main_loss: 0.2555 - val_op_conv_loss: 0.2945 - val_avg_loss: 0.2497 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9084\n",
      "Epoch 354/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6920 - op_main_loss: 0.1997 - op_conv_loss: 0.1410 - avg_loss: 0.1614 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9392\n",
      "Epoch 00354: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6944 - op_main_loss: 0.2008 - op_conv_loss: 0.1415 - avg_loss: 0.1622 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9395 - val_loss: 1.0300 - val_op_main_loss: 0.2636 - val_op_conv_loss: 0.3137 - val_avg_loss: 0.2627 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9037\n",
      "Epoch 355/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7329 - op_main_loss: 0.2109 - op_conv_loss: 0.1582 - avg_loss: 0.1737 - op_main_accuracy: 0.9194 - op_conv_accuracy: 0.9404 - avg_accuracy: 0.9358\n",
      "Epoch 00355: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7320 - op_main_loss: 0.2106 - op_conv_loss: 0.1579 - avg_loss: 0.1734 - op_main_accuracy: 0.9194 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9357 - val_loss: 1.0265 - val_op_main_loss: 0.2674 - val_op_conv_loss: 0.3050 - val_avg_loss: 0.2645 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8990\n",
      "Epoch 356/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6889 - op_main_loss: 0.1978 - op_conv_loss: 0.1410 - avg_loss: 0.1604 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9440 - avg_accuracy: 0.9423\n",
      "Epoch 00356: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6913 - op_main_loss: 0.1982 - op_conv_loss: 0.1422 - avg_loss: 0.1612 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9414 - val_loss: 1.0138 - val_op_main_loss: 0.2592 - val_op_conv_loss: 0.3095 - val_avg_loss: 0.2554 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9056\n",
      "Epoch 357/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7070 - op_main_loss: 0.2082 - op_conv_loss: 0.1427 - avg_loss: 0.1664 - op_main_accuracy: 0.9236 - op_conv_accuracy: 0.9450 - avg_accuracy: 0.9416\n",
      "Epoch 00357: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7075 - op_main_loss: 0.2080 - op_conv_loss: 0.1433 - avg_loss: 0.1666 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9440 - avg_accuracy: 0.9407 - val_loss: 1.0416 - val_op_main_loss: 0.2637 - val_op_conv_loss: 0.3250 - val_avg_loss: 0.2633 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 358/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7046 - op_main_loss: 0.2048 - op_conv_loss: 0.1449 - avg_loss: 0.1653 - op_main_accuracy: 0.9215 - op_conv_accuracy: 0.9440 - avg_accuracy: 0.9414\n",
      "Epoch 00358: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7132 - op_main_loss: 0.2070 - op_conv_loss: 0.1484 - avg_loss: 0.1682 - op_main_accuracy: 0.9211 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9402 - val_loss: 1.0144 - val_op_main_loss: 0.2670 - val_op_conv_loss: 0.2987 - val_avg_loss: 0.2594 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 359/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7197 - op_main_loss: 0.2110 - op_conv_loss: 0.1488 - avg_loss: 0.1704 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9414 - avg_accuracy: 0.9367\n",
      "Epoch 00359: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7197 - op_main_loss: 0.2110 - op_conv_loss: 0.1488 - avg_loss: 0.1704 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9414 - avg_accuracy: 0.9367 - val_loss: 1.0657 - val_op_main_loss: 0.2782 - val_op_conv_loss: 0.3215 - val_avg_loss: 0.2765 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8933\n",
      "Epoch 360/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6885 - op_main_loss: 0.1980 - op_conv_loss: 0.1407 - avg_loss: 0.1605 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9405\n",
      "Epoch 00360: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6885 - op_main_loss: 0.1980 - op_conv_loss: 0.1407 - avg_loss: 0.1605 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9405 - val_loss: 1.0123 - val_op_main_loss: 0.2612 - val_op_conv_loss: 0.3032 - val_avg_loss: 0.2587 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8999\n",
      "Epoch 361/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6890 - op_main_loss: 0.2005 - op_conv_loss: 0.1388 - avg_loss: 0.1606 - op_main_accuracy: 0.9283 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9440\n",
      "Epoch 00361: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6868 - op_main_loss: 0.1999 - op_conv_loss: 0.1380 - avg_loss: 0.1600 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9445 - val_loss: 1.0231 - val_op_main_loss: 0.2604 - val_op_conv_loss: 0.3161 - val_avg_loss: 0.2576 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 362/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6962 - op_main_loss: 0.2025 - op_conv_loss: 0.1411 - avg_loss: 0.1633 - op_main_accuracy: 0.9189 - op_conv_accuracy: 0.9392 - avg_accuracy: 0.9363\n",
      "Epoch 00362: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6984 - op_main_loss: 0.2032 - op_conv_loss: 0.1420 - avg_loss: 0.1640 - op_main_accuracy: 0.9185 - op_conv_accuracy: 0.9390 - avg_accuracy: 0.9362 - val_loss: 1.0467 - val_op_main_loss: 0.2680 - val_op_conv_loss: 0.3202 - val_avg_loss: 0.2692 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8971\n",
      "Epoch 363/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7022 - op_main_loss: 0.2016 - op_conv_loss: 0.1464 - avg_loss: 0.1648 - op_main_accuracy: 0.9286 - op_conv_accuracy: 0.9404 - avg_accuracy: 0.9404\n",
      "Epoch 00363: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7053 - op_main_loss: 0.2027 - op_conv_loss: 0.1473 - avg_loss: 0.1658 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9397 - avg_accuracy: 0.9393 - val_loss: 1.0022 - val_op_main_loss: 0.2606 - val_op_conv_loss: 0.2964 - val_avg_loss: 0.2558 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 364/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6845 - op_main_loss: 0.1960 - op_conv_loss: 0.1403 - avg_loss: 0.1589 - op_main_accuracy: 0.9298 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9438\n",
      "Epoch 00364: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6845 - op_main_loss: 0.1960 - op_conv_loss: 0.1403 - avg_loss: 0.1589 - op_main_accuracy: 0.9298 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9438 - val_loss: 1.0049 - val_op_main_loss: 0.2585 - val_op_conv_loss: 0.3031 - val_avg_loss: 0.2539 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 365/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6991 - op_main_loss: 0.1998 - op_conv_loss: 0.1462 - avg_loss: 0.1637 - op_main_accuracy: 0.9259 - op_conv_accuracy: 0.9411 - avg_accuracy: 0.9419\n",
      "Epoch 00365: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7021 - op_main_loss: 0.2005 - op_conv_loss: 0.1476 - avg_loss: 0.1646 - op_main_accuracy: 0.9258 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9412 - val_loss: 1.0101 - val_op_main_loss: 0.2595 - val_op_conv_loss: 0.3071 - val_avg_loss: 0.2541 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 366/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6747 - op_main_loss: 0.1953 - op_conv_loss: 0.1335 - avg_loss: 0.1564 - op_main_accuracy: 0.9342 - op_conv_accuracy: 0.9451 - avg_accuracy: 0.9456\n",
      "Epoch 00366: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6733 - op_main_loss: 0.1948 - op_conv_loss: 0.1330 - avg_loss: 0.1559 - op_main_accuracy: 0.9343 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9457 - val_loss: 1.0028 - val_op_main_loss: 0.2569 - val_op_conv_loss: 0.3025 - val_avg_loss: 0.2541 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 367/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7013 - op_main_loss: 0.2043 - op_conv_loss: 0.1429 - avg_loss: 0.1646 - op_main_accuracy: 0.9254 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9416\n",
      "Epoch 00367: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6952 - op_main_loss: 0.2024 - op_conv_loss: 0.1409 - avg_loss: 0.1626 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9428 - val_loss: 0.9927 - val_op_main_loss: 0.2554 - val_op_conv_loss: 0.2983 - val_avg_loss: 0.2496 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9065\n",
      "Epoch 368/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7091 - op_main_loss: 0.2060 - op_conv_loss: 0.1468 - avg_loss: 0.1672 - op_main_accuracy: 0.9207 - op_conv_accuracy: 0.9373 - avg_accuracy: 0.9385\n",
      "Epoch 00368: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7067 - op_main_loss: 0.2051 - op_conv_loss: 0.1460 - avg_loss: 0.1665 - op_main_accuracy: 0.9211 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9388 - val_loss: 1.0014 - val_op_main_loss: 0.2529 - val_op_conv_loss: 0.3098 - val_avg_loss: 0.2496 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9103\n",
      "Epoch 369/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6904 - op_main_loss: 0.2014 - op_conv_loss: 0.1385 - avg_loss: 0.1614 - op_main_accuracy: 0.9276 - op_conv_accuracy: 0.9441 - avg_accuracy: 0.9411\n",
      "Epoch 00369: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6905 - op_main_loss: 0.2014 - op_conv_loss: 0.1385 - avg_loss: 0.1614 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9440 - avg_accuracy: 0.9409 - val_loss: 1.0062 - val_op_main_loss: 0.2557 - val_op_conv_loss: 0.3097 - val_avg_loss: 0.2515 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9093\n",
      "Epoch 370/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7006 - op_main_loss: 0.2026 - op_conv_loss: 0.1447 - avg_loss: 0.1642 - op_main_accuracy: 0.9240 - op_conv_accuracy: 0.9446 - avg_accuracy: 0.9437\n",
      "Epoch 00370: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7006 - op_main_loss: 0.2027 - op_conv_loss: 0.1446 - avg_loss: 0.1642 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9435 - val_loss: 1.0046 - val_op_main_loss: 0.2572 - val_op_conv_loss: 0.3050 - val_avg_loss: 0.2533 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9056\n",
      "Epoch 371/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6906 - op_main_loss: 0.1994 - op_conv_loss: 0.1404 - avg_loss: 0.1616 - op_main_accuracy: 0.9236 - op_conv_accuracy: 0.9413 - avg_accuracy: 0.9423\n",
      "Epoch 00371: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6892 - op_main_loss: 0.1988 - op_conv_loss: 0.1400 - avg_loss: 0.1611 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9414 - avg_accuracy: 0.9428 - val_loss: 1.0502 - val_op_main_loss: 0.2651 - val_op_conv_loss: 0.3296 - val_avg_loss: 0.2662 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9018\n",
      "Epoch 372/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6829 - op_main_loss: 0.1972 - op_conv_loss: 0.1380 - avg_loss: 0.1583 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9433\n",
      "Epoch 00372: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6829 - op_main_loss: 0.1972 - op_conv_loss: 0.1380 - avg_loss: 0.1583 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9433 - val_loss: 1.0198 - val_op_main_loss: 0.2638 - val_op_conv_loss: 0.3067 - val_avg_loss: 0.2598 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9037\n",
      "Epoch 373/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6919 - op_main_loss: 0.1996 - op_conv_loss: 0.1415 - avg_loss: 0.1614 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9432 - avg_accuracy: 0.9423\n",
      "Epoch 00373: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6938 - op_main_loss: 0.1998 - op_conv_loss: 0.1424 - avg_loss: 0.1620 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9419 - val_loss: 1.0587 - val_op_main_loss: 0.2694 - val_op_conv_loss: 0.3285 - val_avg_loss: 0.2716 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8952\n",
      "Epoch 374/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6970 - op_main_loss: 0.1991 - op_conv_loss: 0.1452 - avg_loss: 0.1635 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9394 - avg_accuracy: 0.9377\n",
      "Epoch 00374: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6982 - op_main_loss: 0.1998 - op_conv_loss: 0.1454 - avg_loss: 0.1639 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9393 - avg_accuracy: 0.9376 - val_loss: 1.0249 - val_op_main_loss: 0.2658 - val_op_conv_loss: 0.3122 - val_avg_loss: 0.2577 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9084\n",
      "Epoch 375/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6701 - op_main_loss: 0.1930 - op_conv_loss: 0.1331 - avg_loss: 0.1547 - op_main_accuracy: 0.9300 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9453\n",
      "Epoch 00375: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6678 - op_main_loss: 0.1923 - op_conv_loss: 0.1322 - avg_loss: 0.1539 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9457 - val_loss: 1.0149 - val_op_main_loss: 0.2586 - val_op_conv_loss: 0.3116 - val_avg_loss: 0.2551 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9075\n",
      "Epoch 376/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6837 - op_main_loss: 0.1968 - op_conv_loss: 0.1386 - avg_loss: 0.1592 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9423\n",
      "Epoch 00376: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6837 - op_main_loss: 0.1968 - op_conv_loss: 0.1386 - avg_loss: 0.1592 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9423 - val_loss: 1.0297 - val_op_main_loss: 0.2605 - val_op_conv_loss: 0.3245 - val_avg_loss: 0.2556 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9037\n",
      "Epoch 377/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6862 - op_main_loss: 0.1967 - op_conv_loss: 0.1404 - avg_loss: 0.1599 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9439 - avg_accuracy: 0.9411\n",
      "Epoch 00377: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6842 - op_main_loss: 0.1961 - op_conv_loss: 0.1396 - avg_loss: 0.1593 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9442 - avg_accuracy: 0.9414 - val_loss: 1.0240 - val_op_main_loss: 0.2619 - val_op_conv_loss: 0.3134 - val_avg_loss: 0.2595 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9008\n",
      "Epoch 378/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6882 - op_main_loss: 0.1972 - op_conv_loss: 0.1420 - avg_loss: 0.1601 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9432\n",
      "Epoch 00378: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6880 - op_main_loss: 0.1973 - op_conv_loss: 0.1418 - avg_loss: 0.1600 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9435 - val_loss: 1.0360 - val_op_main_loss: 0.2632 - val_op_conv_loss: 0.3202 - val_avg_loss: 0.2638 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8961\n",
      "Epoch 379/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7050 - op_main_loss: 0.2078 - op_conv_loss: 0.1425 - avg_loss: 0.1660 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9389\n",
      "Epoch 00379: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7082 - op_main_loss: 0.2084 - op_conv_loss: 0.1442 - avg_loss: 0.1670 - op_main_accuracy: 0.9230 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9386 - val_loss: 1.0483 - val_op_main_loss: 0.2666 - val_op_conv_loss: 0.3252 - val_avg_loss: 0.2682 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8980\n",
      "Epoch 380/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6835 - op_main_loss: 0.2001 - op_conv_loss: 0.1356 - avg_loss: 0.1593 - op_main_accuracy: 0.9222 - op_conv_accuracy: 0.9479 - avg_accuracy: 0.9406\n",
      "Epoch 00380: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6870 - op_main_loss: 0.2018 - op_conv_loss: 0.1363 - avg_loss: 0.1604 - op_main_accuracy: 0.9208 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9405 - val_loss: 1.0029 - val_op_main_loss: 0.2572 - val_op_conv_loss: 0.3043 - val_avg_loss: 0.2526 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9056\n",
      "Epoch 381/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6868 - op_main_loss: 0.1969 - op_conv_loss: 0.1409 - avg_loss: 0.1601 - op_main_accuracy: 0.9280 - op_conv_accuracy: 0.9437 - avg_accuracy: 0.9425\n",
      "Epoch 00381: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6854 - op_main_loss: 0.1967 - op_conv_loss: 0.1402 - avg_loss: 0.1597 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9442 - avg_accuracy: 0.9431 - val_loss: 1.0142 - val_op_main_loss: 0.2567 - val_op_conv_loss: 0.3135 - val_avg_loss: 0.2550 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9103\n",
      "Epoch 382/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6850 - op_main_loss: 0.1984 - op_conv_loss: 0.1390 - avg_loss: 0.1587 - op_main_accuracy: 0.9268 - op_conv_accuracy: 0.9451 - avg_accuracy: 0.9425\n",
      "Epoch 00382: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6841 - op_main_loss: 0.1982 - op_conv_loss: 0.1386 - avg_loss: 0.1584 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9423 - val_loss: 1.0154 - val_op_main_loss: 0.2607 - val_op_conv_loss: 0.3075 - val_avg_loss: 0.2583 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8990\n",
      "Epoch 383/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6778 - op_main_loss: 0.1964 - op_conv_loss: 0.1359 - avg_loss: 0.1568 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9409\n",
      "Epoch 00383: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6778 - op_main_loss: 0.1964 - op_conv_loss: 0.1359 - avg_loss: 0.1568 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9409 - val_loss: 1.0129 - val_op_main_loss: 0.2579 - val_op_conv_loss: 0.3108 - val_avg_loss: 0.2554 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9018\n",
      "Epoch 384/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6844 - op_main_loss: 0.1989 - op_conv_loss: 0.1368 - avg_loss: 0.1597 - op_main_accuracy: 0.9292 - op_conv_accuracy: 0.9439 - avg_accuracy: 0.9432\n",
      "Epoch 00384: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6833 - op_main_loss: 0.1987 - op_conv_loss: 0.1364 - avg_loss: 0.1594 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9442 - avg_accuracy: 0.9433 - val_loss: 1.0217 - val_op_main_loss: 0.2608 - val_op_conv_loss: 0.3121 - val_avg_loss: 0.2601 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9037\n",
      "Epoch 385/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6762 - op_main_loss: 0.1923 - op_conv_loss: 0.1384 - avg_loss: 0.1567 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9423\n",
      "Epoch 00385: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6825 - op_main_loss: 0.1943 - op_conv_loss: 0.1408 - avg_loss: 0.1586 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9442 - avg_accuracy: 0.9409 - val_loss: 1.0749 - val_op_main_loss: 0.2802 - val_op_conv_loss: 0.3268 - val_avg_loss: 0.2791 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8961\n",
      "Epoch 386/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6799 - op_main_loss: 0.1967 - op_conv_loss: 0.1369 - avg_loss: 0.1579 - op_main_accuracy: 0.9249 - op_conv_accuracy: 0.9465 - avg_accuracy: 0.9423\n",
      "Epoch 00386: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6832 - op_main_loss: 0.1977 - op_conv_loss: 0.1382 - avg_loss: 0.1590 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9426 - val_loss: 1.0291 - val_op_main_loss: 0.2616 - val_op_conv_loss: 0.3193 - val_avg_loss: 0.2597 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8980\n",
      "Epoch 387/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6737 - op_main_loss: 0.1957 - op_conv_loss: 0.1333 - avg_loss: 0.1563 - op_main_accuracy: 0.9256 - op_conv_accuracy: 0.9467 - avg_accuracy: 0.9448\n",
      "Epoch 00387: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6711 - op_main_loss: 0.1948 - op_conv_loss: 0.1324 - avg_loss: 0.1555 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9447 - val_loss: 1.0472 - val_op_main_loss: 0.2596 - val_op_conv_loss: 0.3408 - val_avg_loss: 0.2583 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8971\n",
      "Epoch 388/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6665 - op_main_loss: 0.1907 - op_conv_loss: 0.1342 - avg_loss: 0.1531 - op_main_accuracy: 0.9285 - op_conv_accuracy: 0.9469 - avg_accuracy: 0.9460\n",
      "Epoch 00388: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6737 - op_main_loss: 0.1932 - op_conv_loss: 0.1366 - avg_loss: 0.1555 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9452 - val_loss: 1.0138 - val_op_main_loss: 0.2572 - val_op_conv_loss: 0.3121 - val_avg_loss: 0.2562 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8971\n",
      "Epoch 389/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6666 - op_main_loss: 0.1921 - op_conv_loss: 0.1328 - avg_loss: 0.1534 - op_main_accuracy: 0.9330 - op_conv_accuracy: 0.9485 - avg_accuracy: 0.9466\n",
      "Epoch 00389: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6675 - op_main_loss: 0.1924 - op_conv_loss: 0.1330 - avg_loss: 0.1537 - op_main_accuracy: 0.9327 - op_conv_accuracy: 0.9485 - avg_accuracy: 0.9466 - val_loss: 1.0266 - val_op_main_loss: 0.2613 - val_op_conv_loss: 0.3180 - val_avg_loss: 0.2586 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9065\n",
      "Epoch 390/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6772 - op_main_loss: 0.1923 - op_conv_loss: 0.1397 - avg_loss: 0.1566 - op_main_accuracy: 0.9339 - op_conv_accuracy: 0.9477 - avg_accuracy: 0.9469\n",
      "Epoch 00390: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6804 - op_main_loss: 0.1936 - op_conv_loss: 0.1407 - avg_loss: 0.1576 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9464 - val_loss: 1.0201 - val_op_main_loss: 0.2573 - val_op_conv_loss: 0.3170 - val_avg_loss: 0.2575 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9027\n",
      "Epoch 391/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6683 - op_main_loss: 0.1937 - op_conv_loss: 0.1325 - avg_loss: 0.1541 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9439\n",
      "Epoch 00391: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6662 - op_main_loss: 0.1930 - op_conv_loss: 0.1317 - avg_loss: 0.1535 - op_main_accuracy: 0.9237 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9445 - val_loss: 0.9966 - val_op_main_loss: 0.2520 - val_op_conv_loss: 0.3051 - val_avg_loss: 0.2513 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9027\n",
      "Epoch 392/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6809 - op_main_loss: 0.1974 - op_conv_loss: 0.1377 - avg_loss: 0.1577 - op_main_accuracy: 0.9238 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9433\n",
      "Epoch 00392: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6842 - op_main_loss: 0.1986 - op_conv_loss: 0.1387 - avg_loss: 0.1588 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9426 - val_loss: 1.0384 - val_op_main_loss: 0.2625 - val_op_conv_loss: 0.3260 - val_avg_loss: 0.2618 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9056\n",
      "Epoch 393/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6820 - op_main_loss: 0.1946 - op_conv_loss: 0.1407 - avg_loss: 0.1584 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9443 - avg_accuracy: 0.9426\n",
      "Epoch 00393: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6842 - op_main_loss: 0.1959 - op_conv_loss: 0.1408 - avg_loss: 0.1592 - op_main_accuracy: 0.9286 - op_conv_accuracy: 0.9442 - avg_accuracy: 0.9428 - val_loss: 1.0331 - val_op_main_loss: 0.2639 - val_op_conv_loss: 0.3180 - val_avg_loss: 0.2627 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9027\n",
      "Epoch 394/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6863 - op_main_loss: 0.1942 - op_conv_loss: 0.1437 - avg_loss: 0.1599 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9408 - avg_accuracy: 0.9435\n",
      "Epoch 00394: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6884 - op_main_loss: 0.1955 - op_conv_loss: 0.1438 - avg_loss: 0.1606 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9412 - avg_accuracy: 0.9435 - val_loss: 1.0516 - val_op_main_loss: 0.2813 - val_op_conv_loss: 0.3076 - val_avg_loss: 0.2743 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8905\n",
      "Epoch 395/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6890 - op_main_loss: 0.1978 - op_conv_loss: 0.1420 - avg_loss: 0.1610 - op_main_accuracy: 0.9247 - op_conv_accuracy: 0.9455 - avg_accuracy: 0.9439\n",
      "Epoch 00395: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6883 - op_main_loss: 0.1976 - op_conv_loss: 0.1418 - avg_loss: 0.1608 - op_main_accuracy: 0.9249 - op_conv_accuracy: 0.9457 - avg_accuracy: 0.9440 - val_loss: 1.0391 - val_op_main_loss: 0.2696 - val_op_conv_loss: 0.3151 - val_avg_loss: 0.2664 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9046\n",
      "Epoch 396/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6824 - op_main_loss: 0.1966 - op_conv_loss: 0.1386 - avg_loss: 0.1592 - op_main_accuracy: 0.9260 - op_conv_accuracy: 0.9420 - avg_accuracy: 0.9401\n",
      "Epoch 00396: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6815 - op_main_loss: 0.1962 - op_conv_loss: 0.1384 - avg_loss: 0.1590 - op_main_accuracy: 0.9265 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9405 - val_loss: 1.0311 - val_op_main_loss: 0.2649 - val_op_conv_loss: 0.3141 - val_avg_loss: 0.2644 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9008\n",
      "Epoch 397/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6706 - op_main_loss: 0.1896 - op_conv_loss: 0.1384 - avg_loss: 0.1551 - op_main_accuracy: 0.9325 - op_conv_accuracy: 0.9430 - avg_accuracy: 0.9433\n",
      "Epoch 00397: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6690 - op_main_loss: 0.1892 - op_conv_loss: 0.1377 - avg_loss: 0.1546 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9435 - val_loss: 1.0233 - val_op_main_loss: 0.2588 - val_op_conv_loss: 0.3193 - val_avg_loss: 0.2578 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9046\n",
      "Epoch 398/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6949 - op_main_loss: 0.2006 - op_conv_loss: 0.1437 - avg_loss: 0.1631 - op_main_accuracy: 0.9247 - op_conv_accuracy: 0.9432 - avg_accuracy: 0.9411\n",
      "Epoch 00398: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6956 - op_main_loss: 0.2005 - op_conv_loss: 0.1442 - avg_loss: 0.1632 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9409 - val_loss: 1.0380 - val_op_main_loss: 0.2619 - val_op_conv_loss: 0.3246 - val_avg_loss: 0.2642 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9075\n",
      "Epoch 399/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6638 - op_main_loss: 0.1888 - op_conv_loss: 0.1342 - avg_loss: 0.1535 - op_main_accuracy: 0.9356 - op_conv_accuracy: 0.9450 - avg_accuracy: 0.9450\n",
      "Epoch 00399: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6635 - op_main_loss: 0.1887 - op_conv_loss: 0.1341 - avg_loss: 0.1534 - op_main_accuracy: 0.9355 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9447 - val_loss: 1.0408 - val_op_main_loss: 0.2622 - val_op_conv_loss: 0.3269 - val_avg_loss: 0.2643 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8952\n",
      "Epoch 400/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7011 - op_main_loss: 0.2023 - op_conv_loss: 0.1467 - avg_loss: 0.1649 - op_main_accuracy: 0.9281 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9397\n",
      "Epoch 00400: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7021 - op_main_loss: 0.2029 - op_conv_loss: 0.1468 - avg_loss: 0.1653 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9414 - avg_accuracy: 0.9395 - val_loss: 1.1524 - val_op_main_loss: 0.2949 - val_op_conv_loss: 0.3731 - val_avg_loss: 0.2975 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8848\n",
      "Epoch 401/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6887 - op_main_loss: 0.2006 - op_conv_loss: 0.1406 - avg_loss: 0.1604 - op_main_accuracy: 0.9256 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9433\n",
      "Epoch 00401: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6853 - op_main_loss: 0.1994 - op_conv_loss: 0.1396 - avg_loss: 0.1594 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9438 - avg_accuracy: 0.9440 - val_loss: 1.0067 - val_op_main_loss: 0.2591 - val_op_conv_loss: 0.3036 - val_avg_loss: 0.2570 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9018\n",
      "Epoch 402/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6708 - op_main_loss: 0.1910 - op_conv_loss: 0.1371 - avg_loss: 0.1556 - op_main_accuracy: 0.9302 - op_conv_accuracy: 0.9436 - avg_accuracy: 0.9436\n",
      "Epoch 00402: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6687 - op_main_loss: 0.1900 - op_conv_loss: 0.1366 - avg_loss: 0.1550 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9435 - val_loss: 1.0310 - val_op_main_loss: 0.2626 - val_op_conv_loss: 0.3186 - val_avg_loss: 0.2624 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8990\n",
      "Epoch 403/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6665 - op_main_loss: 0.1924 - op_conv_loss: 0.1329 - avg_loss: 0.1538 - op_main_accuracy: 0.9273 - op_conv_accuracy: 0.9482 - avg_accuracy: 0.9443\n",
      "Epoch 00403: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6667 - op_main_loss: 0.1922 - op_conv_loss: 0.1332 - avg_loss: 0.1539 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9440 - val_loss: 1.0813 - val_op_main_loss: 0.2631 - val_op_conv_loss: 0.3571 - val_avg_loss: 0.2739 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 404/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6687 - op_main_loss: 0.1896 - op_conv_loss: 0.1376 - avg_loss: 0.1542 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9418\n",
      "Epoch 00404: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6708 - op_main_loss: 0.1907 - op_conv_loss: 0.1379 - avg_loss: 0.1549 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9421 - val_loss: 1.0742 - val_op_main_loss: 0.2721 - val_op_conv_loss: 0.3448 - val_avg_loss: 0.2702 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9027\n",
      "Epoch 405/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6729 - op_main_loss: 0.1933 - op_conv_loss: 0.1363 - avg_loss: 0.1566 - op_main_accuracy: 0.9298 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9407\n",
      "Epoch 00405: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6729 - op_main_loss: 0.1933 - op_conv_loss: 0.1363 - avg_loss: 0.1566 - op_main_accuracy: 0.9298 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9407 - val_loss: 1.0222 - val_op_main_loss: 0.2602 - val_op_conv_loss: 0.3170 - val_avg_loss: 0.2584 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8971\n",
      "Epoch 406/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6713 - op_main_loss: 0.1934 - op_conv_loss: 0.1352 - avg_loss: 0.1559 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9445\n",
      "Epoch 00406: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6713 - op_main_loss: 0.1934 - op_conv_loss: 0.1352 - avg_loss: 0.1559 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9445 - val_loss: 1.0410 - val_op_main_loss: 0.2641 - val_op_conv_loss: 0.3269 - val_avg_loss: 0.2632 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9056\n",
      "Epoch 407/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6515 - op_main_loss: 0.1867 - op_conv_loss: 0.1287 - avg_loss: 0.1493 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9457 - avg_accuracy: 0.9426\n",
      "Epoch 00407: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6549 - op_main_loss: 0.1883 - op_conv_loss: 0.1294 - avg_loss: 0.1504 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9428 - val_loss: 1.0365 - val_op_main_loss: 0.2607 - val_op_conv_loss: 0.3276 - val_avg_loss: 0.2613 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9018\n",
      "Epoch 408/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6654 - op_main_loss: 0.1932 - op_conv_loss: 0.1316 - avg_loss: 0.1537 - op_main_accuracy: 0.9320 - op_conv_accuracy: 0.9516 - avg_accuracy: 0.9490\n",
      "Epoch 00408: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6656 - op_main_loss: 0.1931 - op_conv_loss: 0.1318 - avg_loss: 0.1537 - op_main_accuracy: 0.9319 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9487 - val_loss: 1.0196 - val_op_main_loss: 0.2603 - val_op_conv_loss: 0.3149 - val_avg_loss: 0.2578 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8990\n",
      "Epoch 409/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6670 - op_main_loss: 0.1945 - op_conv_loss: 0.1314 - avg_loss: 0.1547 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9483 - avg_accuracy: 0.9449\n",
      "Epoch 00409: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6670 - op_main_loss: 0.1945 - op_conv_loss: 0.1314 - avg_loss: 0.1547 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9483 - avg_accuracy: 0.9449 - val_loss: 1.1162 - val_op_main_loss: 0.2782 - val_op_conv_loss: 0.3615 - val_avg_loss: 0.2902 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8886\n",
      "Epoch 410/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6734 - op_main_loss: 0.1953 - op_conv_loss: 0.1353 - avg_loss: 0.1564 - op_main_accuracy: 0.9325 - op_conv_accuracy: 0.9470 - avg_accuracy: 0.9461\n",
      "Epoch 00410: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6728 - op_main_loss: 0.1953 - op_conv_loss: 0.1349 - avg_loss: 0.1562 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9459 - val_loss: 1.1452 - val_op_main_loss: 0.2899 - val_op_conv_loss: 0.3798 - val_avg_loss: 0.2891 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9018\n",
      "Epoch 411/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6692 - op_main_loss: 0.1958 - op_conv_loss: 0.1317 - avg_loss: 0.1552 - op_main_accuracy: 0.9247 - op_conv_accuracy: 0.9484 - avg_accuracy: 0.9416\n",
      "Epoch 00411: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6707 - op_main_loss: 0.1963 - op_conv_loss: 0.1322 - avg_loss: 0.1558 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9483 - avg_accuracy: 0.9419 - val_loss: 1.0316 - val_op_main_loss: 0.2630 - val_op_conv_loss: 0.3186 - val_avg_loss: 0.2634 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 412/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6572 - op_main_loss: 0.1923 - op_conv_loss: 0.1266 - avg_loss: 0.1516 - op_main_accuracy: 0.9280 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9456\n",
      "Epoch 00412: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6565 - op_main_loss: 0.1921 - op_conv_loss: 0.1264 - avg_loss: 0.1514 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9457 - val_loss: 1.0319 - val_op_main_loss: 0.2602 - val_op_conv_loss: 0.3263 - val_avg_loss: 0.2586 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8980\n",
      "Epoch 413/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6752 - op_main_loss: 0.1926 - op_conv_loss: 0.1388 - avg_loss: 0.1568 - op_main_accuracy: 0.9290 - op_conv_accuracy: 0.9380 - avg_accuracy: 0.9358\n",
      "Epoch 00413: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6747 - op_main_loss: 0.1925 - op_conv_loss: 0.1386 - avg_loss: 0.1567 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9360 - val_loss: 1.0989 - val_op_main_loss: 0.2892 - val_op_conv_loss: 0.3344 - val_avg_loss: 0.2882 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8876\n",
      "Epoch 414/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.7047 - op_main_loss: 0.2053 - op_conv_loss: 0.1468 - avg_loss: 0.1657 - op_main_accuracy: 0.9228 - op_conv_accuracy: 0.9427 - avg_accuracy: 0.9399\n",
      "Epoch 00414: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7038 - op_main_loss: 0.2049 - op_conv_loss: 0.1466 - avg_loss: 0.1654 - op_main_accuracy: 0.9230 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9400 - val_loss: 1.0103 - val_op_main_loss: 0.2586 - val_op_conv_loss: 0.3100 - val_avg_loss: 0.2550 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 415/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6833 - op_main_loss: 0.1996 - op_conv_loss: 0.1381 - avg_loss: 0.1591 - op_main_accuracy: 0.9237 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9439\n",
      "Epoch 00415: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6826 - op_main_loss: 0.1993 - op_conv_loss: 0.1378 - avg_loss: 0.1589 - op_main_accuracy: 0.9241 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9442 - val_loss: 0.9897 - val_op_main_loss: 0.2506 - val_op_conv_loss: 0.3049 - val_avg_loss: 0.2477 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9018\n",
      "Epoch 416/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6699 - op_main_loss: 0.1938 - op_conv_loss: 0.1347 - avg_loss: 0.1549 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9452\n",
      "Epoch 00416: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6699 - op_main_loss: 0.1938 - op_conv_loss: 0.1347 - avg_loss: 0.1549 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9452 - val_loss: 1.0024 - val_op_main_loss: 0.2546 - val_op_conv_loss: 0.3068 - val_avg_loss: 0.2543 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 417/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6847 - op_main_loss: 0.1974 - op_conv_loss: 0.1411 - avg_loss: 0.1594 - op_main_accuracy: 0.9264 - op_conv_accuracy: 0.9438 - avg_accuracy: 0.9445\n",
      "Epoch 00417: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6856 - op_main_loss: 0.1976 - op_conv_loss: 0.1415 - avg_loss: 0.1598 - op_main_accuracy: 0.9258 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9438 - val_loss: 1.0208 - val_op_main_loss: 0.2566 - val_op_conv_loss: 0.3189 - val_avg_loss: 0.2588 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9008\n",
      "Epoch 418/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6583 - op_main_loss: 0.1893 - op_conv_loss: 0.1309 - avg_loss: 0.1516 - op_main_accuracy: 0.9325 - op_conv_accuracy: 0.9498 - avg_accuracy: 0.9474\n",
      "Epoch 00418: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6550 - op_main_loss: 0.1886 - op_conv_loss: 0.1293 - avg_loss: 0.1506 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9480 - val_loss: 1.0009 - val_op_main_loss: 0.2562 - val_op_conv_loss: 0.3055 - val_avg_loss: 0.2526 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9065\n",
      "Epoch 419/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6818 - op_main_loss: 0.1957 - op_conv_loss: 0.1405 - avg_loss: 0.1589 - op_main_accuracy: 0.9240 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9389\n",
      "Epoch 00419: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6799 - op_main_loss: 0.1946 - op_conv_loss: 0.1403 - avg_loss: 0.1583 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9390 - val_loss: 1.0077 - val_op_main_loss: 0.2548 - val_op_conv_loss: 0.3098 - val_avg_loss: 0.2563 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9018\n",
      "Epoch 420/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6748 - op_main_loss: 0.1943 - op_conv_loss: 0.1370 - avg_loss: 0.1568 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9450 - avg_accuracy: 0.9428\n",
      "Epoch 00420: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6765 - op_main_loss: 0.1945 - op_conv_loss: 0.1379 - avg_loss: 0.1573 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9423 - val_loss: 1.0405 - val_op_main_loss: 0.2589 - val_op_conv_loss: 0.3322 - val_avg_loss: 0.2627 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8999\n",
      "Epoch 421/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6816 - op_main_loss: 0.1944 - op_conv_loss: 0.1416 - avg_loss: 0.1590 - op_main_accuracy: 0.9214 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9399\n",
      "Epoch 00421: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6786 - op_main_loss: 0.1933 - op_conv_loss: 0.1407 - avg_loss: 0.1580 - op_main_accuracy: 0.9218 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9405 - val_loss: 1.0430 - val_op_main_loss: 0.2757 - val_op_conv_loss: 0.3089 - val_avg_loss: 0.2720 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8914\n",
      "Epoch 422/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6677 - op_main_loss: 0.1928 - op_conv_loss: 0.1336 - avg_loss: 0.1549 - op_main_accuracy: 0.9295 - op_conv_accuracy: 0.9462 - avg_accuracy: 0.9448\n",
      "Epoch 00422: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6706 - op_main_loss: 0.1942 - op_conv_loss: 0.1341 - avg_loss: 0.1559 - op_main_accuracy: 0.9286 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9440 - val_loss: 1.0629 - val_op_main_loss: 0.2622 - val_op_conv_loss: 0.3486 - val_avg_loss: 0.2656 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 423/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6545 - op_main_loss: 0.1880 - op_conv_loss: 0.1306 - avg_loss: 0.1493 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9467\n",
      "Epoch 00423: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6536 - op_main_loss: 0.1875 - op_conv_loss: 0.1305 - avg_loss: 0.1491 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9471 - val_loss: 1.0408 - val_op_main_loss: 0.2634 - val_op_conv_loss: 0.3268 - val_avg_loss: 0.2638 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9027\n",
      "Epoch 424/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6665 - op_main_loss: 0.1937 - op_conv_loss: 0.1319 - avg_loss: 0.1544 - op_main_accuracy: 0.9242 - op_conv_accuracy: 0.9467 - avg_accuracy: 0.9443\n",
      "Epoch 00424: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6629 - op_main_loss: 0.1920 - op_conv_loss: 0.1312 - avg_loss: 0.1532 - op_main_accuracy: 0.9258 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9447 - val_loss: 1.0454 - val_op_main_loss: 0.2714 - val_op_conv_loss: 0.3202 - val_avg_loss: 0.2674 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8999\n",
      "Epoch 425/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6520 - op_main_loss: 0.1891 - op_conv_loss: 0.1265 - avg_loss: 0.1499 - op_main_accuracy: 0.9233 - op_conv_accuracy: 0.9453 - avg_accuracy: 0.9401\n",
      "Epoch 00425: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6533 - op_main_loss: 0.1896 - op_conv_loss: 0.1270 - avg_loss: 0.1503 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9397 - val_loss: 1.0556 - val_op_main_loss: 0.2617 - val_op_conv_loss: 0.3446 - val_avg_loss: 0.2629 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9065\n",
      "Epoch 426/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.6465 - op_main_loss: 0.1853 - op_conv_loss: 0.1276 - avg_loss: 0.1474 - op_main_accuracy: 0.9356 - op_conv_accuracy: 0.9481 - avg_accuracy: 0.9478\n",
      "Epoch 00426: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6506 - op_main_loss: 0.1860 - op_conv_loss: 0.1299 - avg_loss: 0.1485 - op_main_accuracy: 0.9357 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9475 - val_loss: 1.0187 - val_op_main_loss: 0.2546 - val_op_conv_loss: 0.3227 - val_avg_loss: 0.2551 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 427/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6581 - op_main_loss: 0.1895 - op_conv_loss: 0.1306 - avg_loss: 0.1516 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9469\n",
      "Epoch 00427: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6623 - op_main_loss: 0.1906 - op_conv_loss: 0.1323 - avg_loss: 0.1530 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9461 - val_loss: 1.0262 - val_op_main_loss: 0.2581 - val_op_conv_loss: 0.3225 - val_avg_loss: 0.2591 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 428/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6714 - op_main_loss: 0.1971 - op_conv_loss: 0.1326 - avg_loss: 0.1552 - op_main_accuracy: 0.9244 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9445\n",
      "Epoch 00428: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6714 - op_main_loss: 0.1971 - op_conv_loss: 0.1326 - avg_loss: 0.1552 - op_main_accuracy: 0.9244 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9445 - val_loss: 0.9874 - val_op_main_loss: 0.2508 - val_op_conv_loss: 0.2999 - val_avg_loss: 0.2503 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9027\n",
      "Epoch 429/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6675 - op_main_loss: 0.1936 - op_conv_loss: 0.1335 - avg_loss: 0.1540 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9471\n",
      "Epoch 00429: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6712 - op_main_loss: 0.1948 - op_conv_loss: 0.1349 - avg_loss: 0.1551 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9466 - val_loss: 1.0179 - val_op_main_loss: 0.2554 - val_op_conv_loss: 0.3207 - val_avg_loss: 0.2554 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9075\n",
      "Epoch 430/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6595 - op_main_loss: 0.1907 - op_conv_loss: 0.1306 - avg_loss: 0.1519 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9472 - avg_accuracy: 0.9455\n",
      "Epoch 00430: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6582 - op_main_loss: 0.1901 - op_conv_loss: 0.1302 - avg_loss: 0.1515 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9452 - val_loss: 1.0225 - val_op_main_loss: 0.2586 - val_op_conv_loss: 0.3169 - val_avg_loss: 0.2607 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8999\n",
      "Epoch 431/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6656 - op_main_loss: 0.1923 - op_conv_loss: 0.1334 - avg_loss: 0.1534 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9433\n",
      "Epoch 00431: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6656 - op_main_loss: 0.1923 - op_conv_loss: 0.1334 - avg_loss: 0.1534 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9433 - val_loss: 1.0379 - val_op_main_loss: 0.2637 - val_op_conv_loss: 0.3213 - val_avg_loss: 0.2665 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8990\n",
      "Epoch 432/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6589 - op_main_loss: 0.1901 - op_conv_loss: 0.1305 - avg_loss: 0.1519 - op_main_accuracy: 0.9300 - op_conv_accuracy: 0.9438 - avg_accuracy: 0.9452\n",
      "Epoch 00432: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6623 - op_main_loss: 0.1916 - op_conv_loss: 0.1313 - avg_loss: 0.1529 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9438 - avg_accuracy: 0.9449 - val_loss: 1.0224 - val_op_main_loss: 0.2573 - val_op_conv_loss: 0.3215 - val_avg_loss: 0.2571 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9046\n",
      "Epoch 433/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6685 - op_main_loss: 0.1951 - op_conv_loss: 0.1320 - avg_loss: 0.1549 - op_main_accuracy: 0.9257 - op_conv_accuracy: 0.9474 - avg_accuracy: 0.9463\n",
      "Epoch 00433: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6699 - op_main_loss: 0.1954 - op_conv_loss: 0.1327 - avg_loss: 0.1553 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9459 - val_loss: 1.0464 - val_op_main_loss: 0.2619 - val_op_conv_loss: 0.3351 - val_avg_loss: 0.2630 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9027\n",
      "Epoch 434/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6436 - op_main_loss: 0.1843 - op_conv_loss: 0.1261 - avg_loss: 0.1467 - op_main_accuracy: 0.9319 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9478\n",
      "Epoch 00434: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6436 - op_main_loss: 0.1843 - op_conv_loss: 0.1261 - avg_loss: 0.1467 - op_main_accuracy: 0.9319 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9478 - val_loss: 1.0939 - val_op_main_loss: 0.2699 - val_op_conv_loss: 0.3630 - val_avg_loss: 0.2742 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 435/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6831 - op_main_loss: 0.1949 - op_conv_loss: 0.1430 - avg_loss: 0.1586 - op_main_accuracy: 0.9292 - op_conv_accuracy: 0.9427 - avg_accuracy: 0.9408\n",
      "Epoch 00435: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6808 - op_main_loss: 0.1942 - op_conv_loss: 0.1421 - avg_loss: 0.1579 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9414 - val_loss: 1.1271 - val_op_main_loss: 0.2883 - val_op_conv_loss: 0.3568 - val_avg_loss: 0.2958 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8905\n",
      "Epoch 436/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6867 - op_main_loss: 0.1993 - op_conv_loss: 0.1420 - avg_loss: 0.1592 - op_main_accuracy: 0.9209 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9447\n",
      "Epoch 00436: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6854 - op_main_loss: 0.1988 - op_conv_loss: 0.1416 - avg_loss: 0.1587 - op_main_accuracy: 0.9211 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9449 - val_loss: 1.0331 - val_op_main_loss: 0.2631 - val_op_conv_loss: 0.3217 - val_avg_loss: 0.2621 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9056\n",
      "Epoch 437/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6475 - op_main_loss: 0.1846 - op_conv_loss: 0.1297 - avg_loss: 0.1471 - op_main_accuracy: 0.9332 - op_conv_accuracy: 0.9510 - avg_accuracy: 0.9488\n",
      "Epoch 00437: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6492 - op_main_loss: 0.1848 - op_conv_loss: 0.1305 - avg_loss: 0.1476 - op_main_accuracy: 0.9327 - op_conv_accuracy: 0.9501 - avg_accuracy: 0.9483 - val_loss: 1.0485 - val_op_main_loss: 0.2671 - val_op_conv_loss: 0.3276 - val_avg_loss: 0.2677 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9018\n",
      "Epoch 438/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6655 - op_main_loss: 0.1907 - op_conv_loss: 0.1351 - avg_loss: 0.1536 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9430\n",
      "Epoch 00438: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6655 - op_main_loss: 0.1905 - op_conv_loss: 0.1353 - avg_loss: 0.1536 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9431 - val_loss: 1.0204 - val_op_main_loss: 0.2606 - val_op_conv_loss: 0.3142 - val_avg_loss: 0.2596 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9075\n",
      "Epoch 439/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6763 - op_main_loss: 0.1942 - op_conv_loss: 0.1390 - avg_loss: 0.1574 - op_main_accuracy: 0.9287 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9447\n",
      "Epoch 00439: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6751 - op_main_loss: 0.1939 - op_conv_loss: 0.1384 - avg_loss: 0.1570 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9452 - val_loss: 1.0218 - val_op_main_loss: 0.2640 - val_op_conv_loss: 0.3117 - val_avg_loss: 0.2606 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9027\n",
      "Epoch 440/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6575 - op_main_loss: 0.1904 - op_conv_loss: 0.1295 - avg_loss: 0.1520 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9479 - avg_accuracy: 0.9453\n",
      "Epoch 00440: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6600 - op_main_loss: 0.1912 - op_conv_loss: 0.1303 - avg_loss: 0.1528 - op_main_accuracy: 0.9327 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9454 - val_loss: 1.0291 - val_op_main_loss: 0.2623 - val_op_conv_loss: 0.3179 - val_avg_loss: 0.2630 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9018\n",
      "Epoch 441/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6603 - op_main_loss: 0.1903 - op_conv_loss: 0.1321 - avg_loss: 0.1518 - op_main_accuracy: 0.9287 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9458\n",
      "Epoch 00441: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6579 - op_main_loss: 0.1898 - op_conv_loss: 0.1310 - avg_loss: 0.1510 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9464 - val_loss: 1.0159 - val_op_main_loss: 0.2578 - val_op_conv_loss: 0.3140 - val_avg_loss: 0.2582 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 442/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6566 - op_main_loss: 0.1923 - op_conv_loss: 0.1273 - avg_loss: 0.1508 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9478\n",
      "Epoch 00442: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6566 - op_main_loss: 0.1923 - op_conv_loss: 0.1273 - avg_loss: 0.1508 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9478 - val_loss: 1.0295 - val_op_main_loss: 0.2586 - val_op_conv_loss: 0.3233 - val_avg_loss: 0.2614 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9037\n",
      "Epoch 443/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6715 - op_main_loss: 0.1941 - op_conv_loss: 0.1353 - avg_loss: 0.1560 - op_main_accuracy: 0.9228 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9404\n",
      "Epoch 00443: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6707 - op_main_loss: 0.1944 - op_conv_loss: 0.1344 - avg_loss: 0.1557 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9459 - avg_accuracy: 0.9405 - val_loss: 0.9869 - val_op_main_loss: 0.2534 - val_op_conv_loss: 0.2969 - val_avg_loss: 0.2508 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9027\n",
      "Epoch 444/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6476 - op_main_loss: 0.1856 - op_conv_loss: 0.1275 - avg_loss: 0.1487 - op_main_accuracy: 0.9308 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9483\n",
      "Epoch 00444: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6476 - op_main_loss: 0.1856 - op_conv_loss: 0.1275 - avg_loss: 0.1487 - op_main_accuracy: 0.9308 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9483 - val_loss: 1.0031 - val_op_main_loss: 0.2587 - val_op_conv_loss: 0.3008 - val_avg_loss: 0.2577 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 445/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6733 - op_main_loss: 0.1903 - op_conv_loss: 0.1410 - avg_loss: 0.1561 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9431\n",
      "Epoch 00445: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6733 - op_main_loss: 0.1903 - op_conv_loss: 0.1410 - avg_loss: 0.1561 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9431 - val_loss: 1.0223 - val_op_main_loss: 0.2643 - val_op_conv_loss: 0.3106 - val_avg_loss: 0.2617 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8980\n",
      "Epoch 446/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6600 - op_main_loss: 0.1934 - op_conv_loss: 0.1296 - avg_loss: 0.1512 - op_main_accuracy: 0.9274 - op_conv_accuracy: 0.9505 - avg_accuracy: 0.9471\n",
      "Epoch 00446: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6579 - op_main_loss: 0.1929 - op_conv_loss: 0.1288 - avg_loss: 0.1505 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9473 - val_loss: 1.0380 - val_op_main_loss: 0.2613 - val_op_conv_loss: 0.3266 - val_avg_loss: 0.2643 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8999\n",
      "Epoch 447/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6464 - op_main_loss: 0.1864 - op_conv_loss: 0.1262 - avg_loss: 0.1478 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9474 - avg_accuracy: 0.9469\n",
      "Epoch 00447: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6487 - op_main_loss: 0.1871 - op_conv_loss: 0.1272 - avg_loss: 0.1485 - op_main_accuracy: 0.9286 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9457 - val_loss: 1.0301 - val_op_main_loss: 0.2595 - val_op_conv_loss: 0.3253 - val_avg_loss: 0.2594 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9046\n",
      "Epoch 448/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7026 - op_main_loss: 0.2007 - op_conv_loss: 0.1511 - avg_loss: 0.1652 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9412 - avg_accuracy: 0.9395\n",
      "Epoch 00448: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7026 - op_main_loss: 0.2007 - op_conv_loss: 0.1511 - avg_loss: 0.1652 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9412 - avg_accuracy: 0.9395 - val_loss: 1.0052 - val_op_main_loss: 0.2566 - val_op_conv_loss: 0.3080 - val_avg_loss: 0.2551 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 449/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6517 - op_main_loss: 0.1878 - op_conv_loss: 0.1284 - avg_loss: 0.1500 - op_main_accuracy: 0.9325 - op_conv_accuracy: 0.9480 - avg_accuracy: 0.9451\n",
      "Epoch 00449: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6514 - op_main_loss: 0.1874 - op_conv_loss: 0.1285 - avg_loss: 0.1499 - op_main_accuracy: 0.9327 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9449 - val_loss: 1.0247 - val_op_main_loss: 0.2586 - val_op_conv_loss: 0.3220 - val_avg_loss: 0.2585 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9065\n",
      "Epoch 450/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6558 - op_main_loss: 0.1857 - op_conv_loss: 0.1343 - avg_loss: 0.1502 - op_main_accuracy: 0.9315 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9449\n",
      "Epoch 00450: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6545 - op_main_loss: 0.1855 - op_conv_loss: 0.1336 - avg_loss: 0.1498 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9452 - val_loss: 0.9941 - val_op_main_loss: 0.2554 - val_op_conv_loss: 0.3008 - val_avg_loss: 0.2523 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9046\n",
      "Epoch 451/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6819 - op_main_loss: 0.1960 - op_conv_loss: 0.1418 - avg_loss: 0.1583 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9426\n",
      "Epoch 00451: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6819 - op_main_loss: 0.1960 - op_conv_loss: 0.1418 - avg_loss: 0.1583 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9426 - val_loss: 1.0343 - val_op_main_loss: 0.2614 - val_op_conv_loss: 0.3242 - val_avg_loss: 0.2628 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 452/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6501 - op_main_loss: 0.1862 - op_conv_loss: 0.1292 - avg_loss: 0.1489 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9447\n",
      "Epoch 00452: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6538 - op_main_loss: 0.1873 - op_conv_loss: 0.1305 - avg_loss: 0.1501 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9438 - val_loss: 1.0027 - val_op_main_loss: 0.2553 - val_op_conv_loss: 0.3051 - val_avg_loss: 0.2563 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 453/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6795 - op_main_loss: 0.1946 - op_conv_loss: 0.1405 - avg_loss: 0.1585 - op_main_accuracy: 0.9236 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9438\n",
      "Epoch 00453: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6781 - op_main_loss: 0.1944 - op_conv_loss: 0.1398 - avg_loss: 0.1581 - op_main_accuracy: 0.9241 - op_conv_accuracy: 0.9459 - avg_accuracy: 0.9445 - val_loss: 1.0503 - val_op_main_loss: 0.2586 - val_op_conv_loss: 0.3380 - val_avg_loss: 0.2681 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8971\n",
      "Epoch 454/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6527 - op_main_loss: 0.1894 - op_conv_loss: 0.1277 - avg_loss: 0.1500 - op_main_accuracy: 0.9281 - op_conv_accuracy: 0.9510 - avg_accuracy: 0.9505\n",
      "Epoch 00454: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6538 - op_main_loss: 0.1894 - op_conv_loss: 0.1284 - avg_loss: 0.1502 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9506 - val_loss: 0.9895 - val_op_main_loss: 0.2509 - val_op_conv_loss: 0.3028 - val_avg_loss: 0.2501 - val_op_main_accuracy: 0.9131 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9093\n",
      "Epoch 455/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6643 - op_main_loss: 0.1906 - op_conv_loss: 0.1344 - avg_loss: 0.1536 - op_main_accuracy: 0.9349 - op_conv_accuracy: 0.9486 - avg_accuracy: 0.9467\n",
      "Epoch 00455: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6635 - op_main_loss: 0.1903 - op_conv_loss: 0.1342 - avg_loss: 0.1533 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9487 - avg_accuracy: 0.9468 - val_loss: 0.9934 - val_op_main_loss: 0.2543 - val_op_conv_loss: 0.3010 - val_avg_loss: 0.2523 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9112\n",
      "Epoch 456/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6497 - op_main_loss: 0.1870 - op_conv_loss: 0.1278 - avg_loss: 0.1489 - op_main_accuracy: 0.9306 - op_conv_accuracy: 0.9470 - avg_accuracy: 0.9482\n",
      "Epoch 00456: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6491 - op_main_loss: 0.1868 - op_conv_loss: 0.1276 - avg_loss: 0.1487 - op_main_accuracy: 0.9308 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9483 - val_loss: 1.0151 - val_op_main_loss: 0.2565 - val_op_conv_loss: 0.3158 - val_avg_loss: 0.2566 - val_op_main_accuracy: 0.9112 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9093\n",
      "Epoch 457/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6633 - op_main_loss: 0.1886 - op_conv_loss: 0.1358 - avg_loss: 0.1528 - op_main_accuracy: 0.9264 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9445\n",
      "Epoch 00457: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6663 - op_main_loss: 0.1894 - op_conv_loss: 0.1370 - avg_loss: 0.1538 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9445 - val_loss: 0.9980 - val_op_main_loss: 0.2550 - val_op_conv_loss: 0.3034 - val_avg_loss: 0.2536 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9084\n",
      "Epoch 458/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6517 - op_main_loss: 0.1855 - op_conv_loss: 0.1313 - avg_loss: 0.1490 - op_main_accuracy: 0.9297 - op_conv_accuracy: 0.9474 - avg_accuracy: 0.9440\n",
      "Epoch 00458: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6510 - op_main_loss: 0.1856 - op_conv_loss: 0.1308 - avg_loss: 0.1488 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9483 - avg_accuracy: 0.9447 - val_loss: 1.0313 - val_op_main_loss: 0.2570 - val_op_conv_loss: 0.3304 - val_avg_loss: 0.2581 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9084\n",
      "Epoch 459/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6413 - op_main_loss: 0.1829 - op_conv_loss: 0.1263 - avg_loss: 0.1462 - op_main_accuracy: 0.9356 - op_conv_accuracy: 0.9484 - avg_accuracy: 0.9491\n",
      "Epoch 00459: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6442 - op_main_loss: 0.1833 - op_conv_loss: 0.1281 - avg_loss: 0.1471 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9483 - val_loss: 0.9992 - val_op_main_loss: 0.2522 - val_op_conv_loss: 0.3095 - val_avg_loss: 0.2515 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9084\n",
      "Epoch 460/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6441 - op_main_loss: 0.1819 - op_conv_loss: 0.1293 - avg_loss: 0.1471 - op_main_accuracy: 0.9373 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9478\n",
      "Epoch 00460: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6453 - op_main_loss: 0.1822 - op_conv_loss: 0.1298 - avg_loss: 0.1475 - op_main_accuracy: 0.9374 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9478 - val_loss: 1.0021 - val_op_main_loss: 0.2516 - val_op_conv_loss: 0.3133 - val_avg_loss: 0.2515 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 461/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6491 - op_main_loss: 0.1858 - op_conv_loss: 0.1279 - avg_loss: 0.1496 - op_main_accuracy: 0.9299 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9473\n",
      "Epoch 00461: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6480 - op_main_loss: 0.1853 - op_conv_loss: 0.1277 - avg_loss: 0.1493 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9475 - val_loss: 1.0576 - val_op_main_loss: 0.2663 - val_op_conv_loss: 0.3385 - val_avg_loss: 0.2668 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 462/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6606 - op_main_loss: 0.1886 - op_conv_loss: 0.1342 - avg_loss: 0.1522 - op_main_accuracy: 0.9318 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9442\n",
      "Epoch 00462: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6598 - op_main_loss: 0.1882 - op_conv_loss: 0.1341 - avg_loss: 0.1519 - op_main_accuracy: 0.9319 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9445 - val_loss: 0.9899 - val_op_main_loss: 0.2521 - val_op_conv_loss: 0.3003 - val_avg_loss: 0.2522 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9046\n",
      "Epoch 463/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6475 - op_main_loss: 0.1840 - op_conv_loss: 0.1297 - avg_loss: 0.1484 - op_main_accuracy: 0.9346 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9482\n",
      "Epoch 00463: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6464 - op_main_loss: 0.1836 - op_conv_loss: 0.1294 - avg_loss: 0.1480 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9483 - val_loss: 1.0364 - val_op_main_loss: 0.2688 - val_op_conv_loss: 0.3145 - val_avg_loss: 0.2677 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8990\n",
      "Epoch 464/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6410 - op_main_loss: 0.1843 - op_conv_loss: 0.1246 - avg_loss: 0.1467 - op_main_accuracy: 0.9323 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9482\n",
      "Epoch 00464: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6401 - op_main_loss: 0.1837 - op_conv_loss: 0.1245 - avg_loss: 0.1464 - op_main_accuracy: 0.9327 - op_conv_accuracy: 0.9492 - avg_accuracy: 0.9485 - val_loss: 1.0750 - val_op_main_loss: 0.2725 - val_op_conv_loss: 0.3385 - val_avg_loss: 0.2786 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8905\n",
      "Epoch 465/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6429 - op_main_loss: 0.1844 - op_conv_loss: 0.1262 - avg_loss: 0.1470 - op_main_accuracy: 0.9299 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9480\n",
      "Epoch 00465: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6426 - op_main_loss: 0.1843 - op_conv_loss: 0.1261 - avg_loss: 0.1469 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9480 - val_loss: 1.0167 - val_op_main_loss: 0.2589 - val_op_conv_loss: 0.3137 - val_avg_loss: 0.2587 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9027\n",
      "Epoch 466/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6379 - op_main_loss: 0.1853 - op_conv_loss: 0.1220 - avg_loss: 0.1454 - op_main_accuracy: 0.9311 - op_conv_accuracy: 0.9521 - avg_accuracy: 0.9480\n",
      "Epoch 00466: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6378 - op_main_loss: 0.1849 - op_conv_loss: 0.1223 - avg_loss: 0.1453 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9520 - avg_accuracy: 0.9480 - val_loss: 1.0214 - val_op_main_loss: 0.2576 - val_op_conv_loss: 0.3174 - val_avg_loss: 0.2612 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8971\n",
      "Epoch 467/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6522 - op_main_loss: 0.1840 - op_conv_loss: 0.1328 - avg_loss: 0.1502 - op_main_accuracy: 0.9339 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9490\n",
      "Epoch 00467: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6505 - op_main_loss: 0.1834 - op_conv_loss: 0.1322 - avg_loss: 0.1496 - op_main_accuracy: 0.9343 - op_conv_accuracy: 0.9457 - avg_accuracy: 0.9492 - val_loss: 1.0010 - val_op_main_loss: 0.2535 - val_op_conv_loss: 0.3084 - val_avg_loss: 0.2538 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9056\n",
      "Epoch 468/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6570 - op_main_loss: 0.1866 - op_conv_loss: 0.1340 - avg_loss: 0.1511 - op_main_accuracy: 0.9318 - op_conv_accuracy: 0.9501 - avg_accuracy: 0.9475\n",
      "Epoch 00468: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6586 - op_main_loss: 0.1873 - op_conv_loss: 0.1343 - avg_loss: 0.1516 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9471 - val_loss: 0.9993 - val_op_main_loss: 0.2518 - val_op_conv_loss: 0.3101 - val_avg_loss: 0.2524 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9027\n",
      "Epoch 469/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6636 - op_main_loss: 0.1911 - op_conv_loss: 0.1345 - avg_loss: 0.1529 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9461\n",
      "Epoch 00469: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6631 - op_main_loss: 0.1908 - op_conv_loss: 0.1343 - avg_loss: 0.1528 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9459 - val_loss: 1.0099 - val_op_main_loss: 0.2536 - val_op_conv_loss: 0.3146 - val_avg_loss: 0.2567 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8980\n",
      "Epoch 470/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6694 - op_main_loss: 0.1933 - op_conv_loss: 0.1355 - avg_loss: 0.1554 - op_main_accuracy: 0.9258 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9432\n",
      "Epoch 00470: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6678 - op_main_loss: 0.1928 - op_conv_loss: 0.1350 - avg_loss: 0.1550 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9435 - val_loss: 1.0029 - val_op_main_loss: 0.2532 - val_op_conv_loss: 0.3086 - val_avg_loss: 0.2561 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 471/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6589 - op_main_loss: 0.1893 - op_conv_loss: 0.1319 - avg_loss: 0.1525 - op_main_accuracy: 0.9306 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9451\n",
      "Epoch 00471: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6590 - op_main_loss: 0.1894 - op_conv_loss: 0.1318 - avg_loss: 0.1525 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9447 - val_loss: 1.0053 - val_op_main_loss: 0.2535 - val_op_conv_loss: 0.3115 - val_avg_loss: 0.2551 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9065\n",
      "Epoch 472/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6401 - op_main_loss: 0.1831 - op_conv_loss: 0.1253 - avg_loss: 0.1464 - op_main_accuracy: 0.9346 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9521\n",
      "Epoch 00472: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6395 - op_main_loss: 0.1828 - op_conv_loss: 0.1252 - avg_loss: 0.1463 - op_main_accuracy: 0.9348 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9520 - val_loss: 1.0608 - val_op_main_loss: 0.2648 - val_op_conv_loss: 0.3370 - val_avg_loss: 0.2740 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8895\n",
      "Epoch 473/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6519 - op_main_loss: 0.1843 - op_conv_loss: 0.1336 - avg_loss: 0.1488 - op_main_accuracy: 0.9318 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9468\n",
      "Epoch 00473: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6522 - op_main_loss: 0.1843 - op_conv_loss: 0.1338 - avg_loss: 0.1489 - op_main_accuracy: 0.9322 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9466 - val_loss: 0.9993 - val_op_main_loss: 0.2502 - val_op_conv_loss: 0.3130 - val_avg_loss: 0.2508 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9093\n",
      "Epoch 474/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6498 - op_main_loss: 0.1866 - op_conv_loss: 0.1295 - avg_loss: 0.1485 - op_main_accuracy: 0.9299 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9451\n",
      "Epoch 00474: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6484 - op_main_loss: 0.1861 - op_conv_loss: 0.1289 - avg_loss: 0.1481 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9454 - val_loss: 1.0299 - val_op_main_loss: 0.2646 - val_op_conv_loss: 0.3143 - val_avg_loss: 0.2658 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8971\n",
      "Epoch 475/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6523 - op_main_loss: 0.1860 - op_conv_loss: 0.1312 - avg_loss: 0.1499 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9482 - avg_accuracy: 0.9454\n",
      "Epoch 00475: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6543 - op_main_loss: 0.1865 - op_conv_loss: 0.1321 - avg_loss: 0.1505 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9480 - avg_accuracy: 0.9449 - val_loss: 1.0214 - val_op_main_loss: 0.2572 - val_op_conv_loss: 0.3203 - val_avg_loss: 0.2588 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9056\n",
      "Epoch 476/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6621 - op_main_loss: 0.1864 - op_conv_loss: 0.1375 - avg_loss: 0.1531 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9444 - avg_accuracy: 0.9432\n",
      "Epoch 00476: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6606 - op_main_loss: 0.1860 - op_conv_loss: 0.1369 - avg_loss: 0.1526 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9435 - val_loss: 1.0269 - val_op_main_loss: 0.2622 - val_op_conv_loss: 0.3181 - val_avg_loss: 0.2617 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9037\n",
      "Epoch 477/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6516 - op_main_loss: 0.1892 - op_conv_loss: 0.1274 - avg_loss: 0.1500 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9458 - avg_accuracy: 0.9432\n",
      "Epoch 00477: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6507 - op_main_loss: 0.1892 - op_conv_loss: 0.1268 - avg_loss: 0.1498 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9438 - val_loss: 1.0241 - val_op_main_loss: 0.2574 - val_op_conv_loss: 0.3229 - val_avg_loss: 0.2586 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9065\n",
      "Epoch 478/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6672 - op_main_loss: 0.1914 - op_conv_loss: 0.1369 - avg_loss: 0.1538 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9449\n",
      "Epoch 00478: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6699 - op_main_loss: 0.1925 - op_conv_loss: 0.1377 - avg_loss: 0.1547 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9445 - val_loss: 0.9911 - val_op_main_loss: 0.2526 - val_op_conv_loss: 0.3011 - val_avg_loss: 0.2525 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8990\n",
      "Epoch 479/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6378 - op_main_loss: 0.1826 - op_conv_loss: 0.1245 - avg_loss: 0.1457 - op_main_accuracy: 0.9332 - op_conv_accuracy: 0.9480 - avg_accuracy: 0.9463\n",
      "Epoch 00479: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6382 - op_main_loss: 0.1831 - op_conv_loss: 0.1243 - avg_loss: 0.1457 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9480 - avg_accuracy: 0.9461 - val_loss: 1.0060 - val_op_main_loss: 0.2566 - val_op_conv_loss: 0.3105 - val_avg_loss: 0.2538 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.8952\n",
      "Epoch 480/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6244 - op_main_loss: 0.1779 - op_conv_loss: 0.1207 - avg_loss: 0.1407 - op_main_accuracy: 0.9308 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9516\n",
      "Epoch 00480: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6223 - op_main_loss: 0.1772 - op_conv_loss: 0.1199 - avg_loss: 0.1400 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9518 - avg_accuracy: 0.9520 - val_loss: 1.0408 - val_op_main_loss: 0.2648 - val_op_conv_loss: 0.3226 - val_avg_loss: 0.2682 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8980\n",
      "Epoch 481/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6357 - op_main_loss: 0.1812 - op_conv_loss: 0.1251 - avg_loss: 0.1442 - op_main_accuracy: 0.9315 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9449\n",
      "Epoch 00481: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6380 - op_main_loss: 0.1816 - op_conv_loss: 0.1264 - avg_loss: 0.1448 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9447 - val_loss: 1.0637 - val_op_main_loss: 0.2670 - val_op_conv_loss: 0.3428 - val_avg_loss: 0.2687 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9046\n",
      "Epoch 482/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6760 - op_main_loss: 0.1921 - op_conv_loss: 0.1423 - avg_loss: 0.1565 - op_main_accuracy: 0.9260 - op_conv_accuracy: 0.9432 - avg_accuracy: 0.9420\n",
      "Epoch 00482: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6799 - op_main_loss: 0.1940 - op_conv_loss: 0.1431 - avg_loss: 0.1577 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9419 - val_loss: 1.0373 - val_op_main_loss: 0.2626 - val_op_conv_loss: 0.3236 - val_avg_loss: 0.2661 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8971\n",
      "Epoch 483/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6494 - op_main_loss: 0.1825 - op_conv_loss: 0.1333 - avg_loss: 0.1485 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9454\n",
      "Epoch 00483: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6490 - op_main_loss: 0.1824 - op_conv_loss: 0.1332 - avg_loss: 0.1484 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9457 - val_loss: 1.0277 - val_op_main_loss: 0.2602 - val_op_conv_loss: 0.3209 - val_avg_loss: 0.2617 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9008\n",
      "Epoch 484/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6427 - op_main_loss: 0.1812 - op_conv_loss: 0.1295 - avg_loss: 0.1470 - op_main_accuracy: 0.9332 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9456\n",
      "Epoch 00484: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6418 - op_main_loss: 0.1810 - op_conv_loss: 0.1290 - avg_loss: 0.1467 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9457 - val_loss: 1.0117 - val_op_main_loss: 0.2569 - val_op_conv_loss: 0.3139 - val_avg_loss: 0.2559 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9093\n",
      "Epoch 485/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6289 - op_main_loss: 0.1781 - op_conv_loss: 0.1229 - avg_loss: 0.1428 - op_main_accuracy: 0.9363 - op_conv_accuracy: 0.9516 - avg_accuracy: 0.9485\n",
      "Epoch 00485: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6298 - op_main_loss: 0.1786 - op_conv_loss: 0.1230 - avg_loss: 0.1431 - op_main_accuracy: 0.9357 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9480 - val_loss: 1.1185 - val_op_main_loss: 0.2778 - val_op_conv_loss: 0.3690 - val_avg_loss: 0.2867 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8952\n",
      "Epoch 486/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6454 - op_main_loss: 0.1836 - op_conv_loss: 0.1284 - avg_loss: 0.1484 - op_main_accuracy: 0.9327 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9470\n",
      "Epoch 00486: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6448 - op_main_loss: 0.1832 - op_conv_loss: 0.1283 - avg_loss: 0.1482 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9459 - avg_accuracy: 0.9473 - val_loss: 1.0563 - val_op_main_loss: 0.2639 - val_op_conv_loss: 0.3405 - val_avg_loss: 0.2667 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 487/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6604 - op_main_loss: 0.1899 - op_conv_loss: 0.1336 - avg_loss: 0.1520 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9487\n",
      "Epoch 00487: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6593 - op_main_loss: 0.1897 - op_conv_loss: 0.1331 - avg_loss: 0.1516 - op_main_accuracy: 0.9265 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9490 - val_loss: 1.0209 - val_op_main_loss: 0.2597 - val_op_conv_loss: 0.3153 - val_avg_loss: 0.2610 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8999\n",
      "Epoch 488/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6480 - op_main_loss: 0.1852 - op_conv_loss: 0.1289 - avg_loss: 0.1489 - op_main_accuracy: 0.9294 - op_conv_accuracy: 0.9492 - avg_accuracy: 0.9466\n",
      "Epoch 00488: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6458 - op_main_loss: 0.1844 - op_conv_loss: 0.1283 - avg_loss: 0.1482 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9471 - val_loss: 1.0481 - val_op_main_loss: 0.2614 - val_op_conv_loss: 0.3393 - val_avg_loss: 0.2624 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9037\n",
      "Epoch 489/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6422 - op_main_loss: 0.1826 - op_conv_loss: 0.1284 - avg_loss: 0.1462 - op_main_accuracy: 0.9313 - op_conv_accuracy: 0.9524 - avg_accuracy: 0.9477\n",
      "Epoch 00489: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6415 - op_main_loss: 0.1823 - op_conv_loss: 0.1282 - avg_loss: 0.1460 - op_main_accuracy: 0.9315 - op_conv_accuracy: 0.9525 - avg_accuracy: 0.9478 - val_loss: 0.9997 - val_op_main_loss: 0.2541 - val_op_conv_loss: 0.3073 - val_avg_loss: 0.2533 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8999\n",
      "Epoch 490/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6323 - op_main_loss: 0.1808 - op_conv_loss: 0.1234 - avg_loss: 0.1431 - op_main_accuracy: 0.9383 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9520\n",
      "Epoch 00490: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6323 - op_main_loss: 0.1808 - op_conv_loss: 0.1234 - avg_loss: 0.1431 - op_main_accuracy: 0.9383 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9520 - val_loss: 1.0139 - val_op_main_loss: 0.2542 - val_op_conv_loss: 0.3195 - val_avg_loss: 0.2551 - val_op_main_accuracy: 0.9112 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9046\n",
      "Epoch 491/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6323 - op_main_loss: 0.1802 - op_conv_loss: 0.1236 - avg_loss: 0.1435 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9487\n",
      "Epoch 00491: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6323 - op_main_loss: 0.1802 - op_conv_loss: 0.1236 - avg_loss: 0.1435 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9487 - val_loss: 1.0118 - val_op_main_loss: 0.2539 - val_op_conv_loss: 0.3195 - val_avg_loss: 0.2534 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9018\n",
      "Epoch 492/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6372 - op_main_loss: 0.1794 - op_conv_loss: 0.1279 - avg_loss: 0.1451 - op_main_accuracy: 0.9339 - op_conv_accuracy: 0.9504 - avg_accuracy: 0.9499\n",
      "Epoch 00492: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6377 - op_main_loss: 0.1793 - op_conv_loss: 0.1282 - avg_loss: 0.1453 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9501 - avg_accuracy: 0.9497 - val_loss: 1.0509 - val_op_main_loss: 0.2666 - val_op_conv_loss: 0.3284 - val_avg_loss: 0.2715 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8942\n",
      "Epoch 493/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6313 - op_main_loss: 0.1803 - op_conv_loss: 0.1231 - avg_loss: 0.1435 - op_main_accuracy: 0.9358 - op_conv_accuracy: 0.9522 - avg_accuracy: 0.9500\n",
      "Epoch 00493: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6307 - op_main_loss: 0.1801 - op_conv_loss: 0.1229 - avg_loss: 0.1432 - op_main_accuracy: 0.9360 - op_conv_accuracy: 0.9523 - avg_accuracy: 0.9501 - val_loss: 1.0185 - val_op_main_loss: 0.2544 - val_op_conv_loss: 0.3235 - val_avg_loss: 0.2563 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9027\n",
      "Epoch 494/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6587 - op_main_loss: 0.1904 - op_conv_loss: 0.1315 - avg_loss: 0.1525 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9412\n",
      "Epoch 00494: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6587 - op_main_loss: 0.1904 - op_conv_loss: 0.1315 - avg_loss: 0.1525 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9412 - val_loss: 1.0078 - val_op_main_loss: 0.2525 - val_op_conv_loss: 0.3169 - val_avg_loss: 0.2541 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 495/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6476 - op_main_loss: 0.1805 - op_conv_loss: 0.1347 - avg_loss: 0.1480 - op_main_accuracy: 0.9358 - op_conv_accuracy: 0.9474 - avg_accuracy: 0.9484\n",
      "Epoch 00495: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6477 - op_main_loss: 0.1809 - op_conv_loss: 0.1345 - avg_loss: 0.1480 - op_main_accuracy: 0.9355 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9478 - val_loss: 1.0528 - val_op_main_loss: 0.2633 - val_op_conv_loss: 0.3390 - val_avg_loss: 0.2661 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9027\n",
      "Epoch 496/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6509 - op_main_loss: 0.1885 - op_conv_loss: 0.1285 - avg_loss: 0.1496 - op_main_accuracy: 0.9292 - op_conv_accuracy: 0.9504 - avg_accuracy: 0.9480\n",
      "Epoch 00496: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6491 - op_main_loss: 0.1879 - op_conv_loss: 0.1279 - avg_loss: 0.1490 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9504 - avg_accuracy: 0.9480 - val_loss: 1.0518 - val_op_main_loss: 0.2638 - val_op_conv_loss: 0.3380 - val_avg_loss: 0.2657 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9046\n",
      "Epoch 497/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6358 - op_main_loss: 0.1833 - op_conv_loss: 0.1237 - avg_loss: 0.1443 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9492 - avg_accuracy: 0.9485\n",
      "Epoch 00497: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6371 - op_main_loss: 0.1842 - op_conv_loss: 0.1237 - avg_loss: 0.1446 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9492 - avg_accuracy: 0.9483 - val_loss: 1.0459 - val_op_main_loss: 0.2593 - val_op_conv_loss: 0.3394 - val_avg_loss: 0.2625 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9046\n",
      "Epoch 498/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6284 - op_main_loss: 0.1796 - op_conv_loss: 0.1219 - avg_loss: 0.1422 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9504\n",
      "Epoch 00498: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6284 - op_main_loss: 0.1796 - op_conv_loss: 0.1219 - avg_loss: 0.1422 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9504 - val_loss: 1.0236 - val_op_main_loss: 0.2592 - val_op_conv_loss: 0.3210 - val_avg_loss: 0.2587 - val_op_main_accuracy: 0.9112 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9046\n",
      "Epoch 499/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.6853 - op_main_loss: 0.1957 - op_conv_loss: 0.1445 - avg_loss: 0.1604 - op_main_accuracy: 0.9290 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9395\n",
      "Epoch 00499: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6875 - op_main_loss: 0.1955 - op_conv_loss: 0.1465 - avg_loss: 0.1609 - op_main_accuracy: 0.9286 - op_conv_accuracy: 0.9397 - avg_accuracy: 0.9388 - val_loss: 1.0344 - val_op_main_loss: 0.2605 - val_op_conv_loss: 0.3282 - val_avg_loss: 0.2612 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9037\n",
      "Epoch 500/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6413 - op_main_loss: 0.1805 - op_conv_loss: 0.1297 - avg_loss: 0.1467 - op_main_accuracy: 0.9314 - op_conv_accuracy: 0.9479 - avg_accuracy: 0.9462\n",
      "Epoch 00500: val_avg_accuracy did not improve from 0.91218\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6397 - op_main_loss: 0.1801 - op_conv_loss: 0.1290 - avg_loss: 0.1462 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9459 - val_loss: 1.0213 - val_op_main_loss: 0.2582 - val_op_conv_loss: 0.3192 - val_avg_loss: 0.2593 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9065\n"
     ]
    }
   ],
   "source": [
    "def nlp_lstm2(w2v):\n",
    "    inputs = Input(shape=(X_train[0].shape[-1],))\n",
    "\n",
    "    embedding_layer = gensim_to_keras_embedding(w2v)\n",
    "    \n",
    "    embedding = embedding_layer(inputs)\n",
    "\n",
    "    lstm1 = LSTM(lstm_units,return_sequences=True, return_state=True, kernel_regularizer=l2(w_decay),recurrent_regularizer=l2(w_decay), dropout=dropout_rate)(embedding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='op_main')(lstm1[1])\n",
    "    \n",
    "\n",
    "    output_td_gap = GlobalAveragePooling1D(data_format='channels_first')(lstm1[0])\n",
    "    \n",
    "    output_td = TimeDistributed(Dense(units=1, activation='sigmoid'))(lstm1[0])\n",
    "    output_td = Flatten()(output_td)\n",
    "    \n",
    "    output_td = Multiply()([output_td_gap, output_td])\n",
    "    \n",
    "    output_td = Activation('relu', name='before_split')(output_td)\n",
    "    \n",
    "    output_td_splits = tf.split(output_td, 10, axis=-1)\n",
    "    \n",
    "    features = concatenate([output_td_splits[0], output_td_splits[1], output_td_splits[-2], output_td_splits[-1]])\n",
    "    \n",
    "    print(features.shape)\n",
    "    \n",
    "    output_td = Reshape((8, 10, 1))(features)\n",
    "    \n",
    "    output_td = Conv2D(2, 8, padding='same', strides=1, activation='relu', kernel_regularizer=l2(w_decay))(output_td)\n",
    "    output_td = BatchNormalization()(output_td)\n",
    "    output_td = Flatten()(output_td)\n",
    "   \n",
    "\n",
    "    output_td = Dense(units=1, activation='sigmoid', name='op_conv')(output_td)\n",
    "    \n",
    "    \n",
    "    \n",
    "    avg = tf.keras.layers.Average(name='avg')([output, output_td])\n",
    "    \n",
    "\n",
    "    model = Model(inputs, [output, output_td, avg])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = nlp_lstm2(w2v_model)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./weight_cp/weight_lstm2.hdf5', save_freq=\"epoch\",  verbose=1, monitor='val_avg_accuracy', save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(0.0001)\n",
    "model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=epochs_to_run, validation_data=(X_val, y_val), callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7013a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['avg_accuracy'])\n",
    "plt.plot(history.history['val_avg_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11c0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./weight_cp/weight_lstm2.hdf5')\n",
    "predictionss = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2963b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF Multi-Supervised LSTM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89       661\n",
      "           1       0.90      0.88      0.89       662\n",
      "\n",
      "    accuracy                           0.89      1323\n",
      "   macro avg       0.89      0.89      0.89      1323\n",
      "weighted avg       0.89      0.89      0.89      1323\n",
      "\n",
      "0.891912320483749\n"
     ]
    }
   ],
   "source": [
    "predictions = np.where(predictionss[-1] > 0.5, 1, 0)\n",
    "y_pred = []\n",
    "for p in predictions:\n",
    "    y_pred.append(p[0])\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"CLASSIFICATION REPORT OF Multi-Supervised LSTM\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8777138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2 0\n",
      "embedding_1 1\n",
      "lstm_1 2\n",
      "time_distributed 3\n",
      "global_average_pooling1d 4\n",
      "flatten 5\n",
      "multiply 6\n",
      "before_split 7\n",
      "tf_op_layer_split 8\n",
      "concatenate 9\n",
      "reshape 10\n",
      "conv2d 11\n",
      "batch_normalization 12\n",
      "flatten_1 13\n",
      "op_main 14\n",
      "op_conv 15\n",
      "avg 16\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(layer.name, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c7d38a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prr = 71\n",
    "f = Model(model.input, model.layers[7].output)\n",
    "predictions = f.predict(X_test)\n",
    "plt.imshow(predictions[prr].reshape((20, 10)), cmap='viridis', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecf8872e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAGdCAYAAADkLYEYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAX70lEQVR4nO3df2zV9b348ddpaw9M204REL4cFN0PBMRfRb7I5o+JGq6aaRa3GcwYLktmqoLNlsGWzSVOi0tmWNShGIdLlOGWDXXmq0bZhDllFhiLzM0f02mVAbpoD+DdwbXn+8fNem8nlZ5DT889bx6P5PPH+fA+vF8eap/9nJ6eZorFYjEAgJpWV+0BAIADJ+gAkABBB4AECDoAJEDQASABgg4ACRB0AEiAoANAAhqGe8Pe3t7Ytm1bNDU1RSaTGe7tAaCmFIvF2LVrV4wfPz7q6ga+Dh/2oG/bti1yudxwbwsANa2rqysmTJgw4J8Pe9CbmpoiIuIT8R/REIcM9/YwPGr02adMY2O1RyhLprE2P5dkDqnNuaNh2NMxZDIN9dUeoWT/7N0bT2xf2dfPgQz7v8q/nmZviEOiIVOjH8ywP7Ua9Br9fzKTqdEvROpq8/GOuhoOei3Pvp/PK14UBwAJEHQASICgA0ACBB0AEiDoAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABZQX9tttui2OOOSZGjBgRM2fOjGeeeWao5wIASlBy0O+7775ob2+P6667LjZv3hwnnnhinH/++bFz585KzAcADELJQb/55pvjy1/+cixYsCCmTJkSt99+e3zoQx+KH/3oR5WYDwAYhJKCvnfv3ti0aVPMmTPnv/+CurqYM2dOPP300/u8T6FQiHw+3+8AAIZWSUF/6623oqenJ8aOHdvv/NixY2P79u37vE9HR0e0tLT0HblcrvxpAYB9qvir3JcsWRLd3d19R1dXV6W3BICDTkMpi4888sior6+PHTt29Du/Y8eOOOqoo/Z5n2w2G9lstvwJAYD9KukKvbGxMU499dRYu3Zt37ne3t5Yu3ZtzJo1a8iHAwAGp6Qr9IiI9vb2mD9/frS2tsZpp50Wy5Ytiz179sSCBQsqMR8AMAglB/1zn/tcvPnmm/Htb387tm/fHieddFI88sgj73uhHAAwfDLFYrE4nBvm8/loaWmJs+LT0ZA5ZDi3huGTyVR7grJkGhurPUJZanfuGv0c2FDyteD/GpkanP2fvYV4fNsd0d3dHc3NzQOu817uAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEiAoANAAqr2i2EbJv6faKjLVmv78tTV5tc/xfranLtWH++IqNnfhx51tTp3DX+s1KBirX58R0SxBj9UensKEdv2v64G/9MAgH8n6ACQAEEHgAQIOgAkQNABIAGCDgAJEHQASICgA0ACBB0AEiDoAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AElBz09evXx0UXXRTjx4+PTCYT999/fwXGAgBKUXLQ9+zZEyeeeGLcdtttlZgHAChDQ6l3mDt3bsydO7cSswAAZSo56KUqFApRKBT6bufz+UpvCQAHnYq/KK6joyNaWlr6jlwuV+ktAeCgU/GgL1myJLq7u/uOrq6uSm8JAAedij/lns1mI5vNVnobADio+Tl0AEhAyVfou3fvjpdeeqnv9iuvvBJbtmyJI444IiZOnDikwwEAg1Ny0Ddu3Bhnn3123+329vaIiJg/f37cfffdQzYYADB4JQf9rLPOimKxWIlZAIAy+R46ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQgJJ/H/pQ6W05LHrrs9Xa/uCSyVR7goNO0WNOylwKDqtiXf2g1vlnAYAECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQAEEHgAQIOgAkQNABIAGCDgAJEHQASICgA0ACBB0AEiDoAJAAQQeABAg6ACRA0AEgASUFvaOjI2bMmBFNTU0xZsyYuPjii+P555+v1GwAwCCVFPR169ZFW1tbbNiwIR577LF477334rzzzos9e/ZUaj4AYBAaSln8yCOP9Lt99913x5gxY2LTpk1xxhlnDOlgAMDglRT0f9fd3R0REUccccSAawqFQhQKhb7b+Xz+QLYEAPah7BfF9fb2xqJFi2L27Nkxbdq0Add1dHRES0tL35HL5crdEgAYQNlBb2tri61bt8bq1as/cN2SJUuiu7u77+jq6ip3SwBgAGU95X7VVVfFQw89FOvXr48JEyZ84NpsNhvZbLas4QCAwSkp6MViMa6++upYs2ZNPPHEEzFp0qRKzQUAlKCkoLe1tcWqVavigQceiKampti+fXtERLS0tMTIkSMrMiAAsH8lfQ99+fLl0d3dHWeddVaMGzeu77jvvvsqNR8AMAglP+UOAPzv473cASABgg4ACRB0AEiAoANAAgQdABIg6ACQAEEHgAQIOgAkQNABIAGCDgAJEHQASICgA0ACBB0AEiDoAJAAQQeABAg6ACSgoVobFxvqoljv6wlgCGQy1Z4AKmeQH96KCgAJEHQASICgA0ACBB0AEiDoAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEhASUFfvnx5TJ8+PZqbm6O5uTlmzZoVDz/8cKVmAwAGqaSgT5gwIZYuXRqbNm2KjRs3xqc+9an49Kc/HX/84x8rNR8AMAgNpSy+6KKL+t2+4YYbYvny5bFhw4aYOnXqkA4GAAxeSUH/n3p6euJnP/tZ7NmzJ2bNmjXgukKhEIVCoe92Pp8vd0sAYAAlvyju2WefjcMOOyyy2Wx85StfiTVr1sSUKVMGXN/R0REtLS19Ry6XO6CBAYD3KznoH//4x2PLli3xu9/9Lq688sqYP39+PPfccwOuX7JkSXR3d/cdXV1dBzQwAPB+JT/l3tjYGB/5yEciIuLUU0+Nzs7O+MEPfhB33HHHPtdns9nIZrMHNiUA8IEO+OfQe3t7+32PHAAYfiVdoS9ZsiTmzp0bEydOjF27dsWqVaviiSeeiEcffbRS8wEAg1BS0Hfu3Blf+MIX4m9/+1u0tLTE9OnT49FHH41zzz23UvMBAINQUtDvuuuuSs0BABwA7+UOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQAEEHgAQIOgAkQNABIAENVds5ExGZTNW2B4CUuEIHgAQIOgAkQNABIAGCDgAJEHQASICgA0ACBB0AEiDoAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJOCAgr506dLIZDKxaNGiIRoHAChH2UHv7OyMO+64I6ZPnz6U8wAAZSgr6Lt374558+bFnXfeGYcffvhQzwQAlKisoLe1tcUFF1wQc+bMGep5AIAyNJR6h9WrV8fmzZujs7NzUOsLhUIUCoW+2/l8vtQtAYD9KOkKvaurKxYuXBj33ntvjBgxYlD36ejoiJaWlr4jl8uVNSgAMLBMsVgsDnbx/fffH5dccknU19f3nevp6YlMJhN1dXVRKBT6/VnEvq/Qc7lcnH3K4mioH9wXBQBwsPpnzz/i15uXRnd3dzQ3Nw+4rqSn3M8555x49tln+51bsGBBTJ48Ob7+9a+/L+YREdlsNrLZbCnbAAAlKinoTU1NMW3atH7nDj300Bg1atT7zgMAw8c7xQFAAkp+lfu/e+KJJ4ZgDADgQLhCB4AECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQAEEHgAQIOgAkQNABIAGCDgAJEHQASICgA0ACBB0AEiDoAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQAEEHgAQIOgAkQNABIAGCDgAJKCno3/nOdyKTyfQ7Jk+eXKnZAIBBaij1DlOnTo3HH3/8v/+ChpL/CgBgiJVc44aGhjjqqKMqMQsAUKaSv4f+4osvxvjx4+PYY4+NefPmxWuvvfaB6wuFQuTz+X4HADC0Sgr6zJkz4+67745HHnkkli9fHq+88kp88pOfjF27dg14n46Ojmhpaek7crncAQ8NAPSXKRaLxXLv/M4778TRRx8dN998c3zpS1/a55pCoRCFQqHvdj6fj1wuF2efsjga6keUuzUAHBT+2fOP+PXmpdHd3R3Nzc0DrjugV7R9+MMfjo997GPx0ksvDbgmm81GNps9kG0AgP04oJ9D3717d/zlL3+JcePGDdU8AEAZSgr6V7/61Vi3bl389a9/jaeeeiouueSSqK+vj8suu6xS8wEAg1DSU+6vv/56XHbZZfH3v/89Ro8eHZ/4xCdiw4YNMXr06ErNBwAMQklBX716daXmAAAOgPdyB4AECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQAEEHgAQIOgAkQNABIAGCDgAJEHQASICgA0ACBB0AEiDoAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQAEEHgAQIOgAkQNABIAElB/2NN96Iyy+/PEaNGhUjR46ME044ITZu3FiJ2QCAQWooZfHbb78ds2fPjrPPPjsefvjhGD16dLz44otx+OGHV2o+AGAQSgr6TTfdFLlcLlauXNl3btKkSUM+FABQmpKecn/wwQejtbU1Lr300hgzZkycfPLJceedd37gfQqFQuTz+X4HADC0Sgr6yy+/HMuXL4+PfvSj8eijj8aVV14Z11xzTfz4xz8e8D4dHR3R0tLSd+RyuQMeGgDoL1MsFouDXdzY2Bitra3x1FNP9Z275pprorOzM55++ul93qdQKEShUOi7nc/nI5fLxdmnLI6G+hEHMDoApO+fPf+IX29eGt3d3dHc3DzgupKu0MeNGxdTpkzpd+7444+P1157bcD7ZLPZaG5u7ncAAEOrpKDPnj07nn/++X7nXnjhhTj66KOHdCgAoDQlBf3aa6+NDRs2xI033hgvvfRSrFq1KlasWBFtbW2Vmg8AGISSgj5jxoxYs2ZN/OQnP4lp06bF9ddfH8uWLYt58+ZVaj4AYBBK+jn0iIgLL7wwLrzwwkrMAgCUyXu5A0ACBB0AEiDoAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEhAQ7U2zk86LBoOGVGt7cvSk81Ue4Sy7J5Qm3O/e/R71R6hbBeduqXaI5Tl/z0/rdojlKUnf0i1RyjLZ//vM9UeoSw3jd1S7RHKVijW3ueV/K7eOOrj+1/nCh0AEiDoAJAAQQeABAg6ACRA0AEgAYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQgJKCfswxx0Qmk3nf0dbWVqn5AIBBaChlcWdnZ/T09PTd3rp1a5x77rlx6aWXDvlgAMDglRT00aNH97u9dOnSOO644+LMM88c0qEAgNKUFPT/ae/evXHPPfdEe3t7ZDKZAdcVCoUoFAp9t/P5fLlbAgADKPtFcffff3+888478cUvfvED13V0dERLS0vfkcvlyt0SABhA2UG/6667Yu7cuTF+/PgPXLdkyZLo7u7uO7q6usrdEgAYQFlPub/66qvx+OOPxy9+8Yv9rs1ms5HNZsvZBgAYpLKu0FeuXBljxoyJCy64YKjnAQDKUHLQe3t7Y+XKlTF//vxoaCj7NXUAwBAqOeiPP/54vPbaa3HFFVdUYh4AoAwlX2Kfd955USwWKzELAFAm7+UOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEiAoANAAgQdABIg6ACQAEEHgAQIOgAkoOTfh36g/vW71Hve+8dwb33AejOZao9Qlp5Cbc7d+5/vVXuEsu3dXZuz975be/9fRkT0/mdPtUcoS6FGP07yH+qt9ghlKxRrb/Zdu/9r5n/1cyCZ4v5WDLHXX389crnccG4JADWvq6srJkyYMOCfD3vQe3t7Y9u2bdHU1BSZIb7izefzkcvloqurK5qbm4f07+b9PN7Dy+M9/Dzmw8vjvW/FYjF27doV48ePj7q6gb9TPuxPudfV1X3gVxhDobm52QfDMPJ4Dy+P9/DzmA8vj/f7tbS07HeNF8UBQAIEHQASkFTQs9lsXHfddZHNZqs9ykHB4z28PN7Dz2M+vDzeB2bYXxQHAAy9pK7QAeBgJegAkABBB4AECDoAJCCZoN92221xzDHHxIgRI2LmzJnxzDPPVHukZHV0dMSMGTOiqakpxowZExdffHE8//zz1R7roLF06dLIZDKxaNGiao+SrDfeeCMuv/zyGDVqVIwcOTJOOOGE2LhxY7XHSlZPT09861vfikmTJsXIkSPjuOOOi+uvv36/711Of0kE/b777ov29va47rrrYvPmzXHiiSfG+eefHzt37qz2aElat25dtLW1xYYNG+Kxxx6L9957L84777zYs2dPtUdLXmdnZ9xxxx0xffr0ao+SrLfffjtmz54dhxxySDz88MPx3HPPxfe///04/PDDqz1asm666aZYvnx53HrrrfGnP/0pbrrppvje974Xt9xyS7VHqylJ/NjazJkzY8aMGXHrrbdGxH+9X3wul4urr746Fi9eXOXp0vfmm2/GmDFjYt26dXHGGWdUe5xk7d69O0455ZT44Q9/GN/97nfjpJNOimXLllV7rOQsXrw4fvvb38ZvfvObao9y0Ljwwgtj7Nixcdddd/Wd+8xnPhMjR46Me+65p4qT1Zaav0Lfu3dvbNq0KebMmdN3rq6uLubMmRNPP/10FSc7eHR3d0dExBFHHFHlSdLW1tYWF1xwQb+PdYbegw8+GK2trXHppZfGmDFj4uSTT44777yz2mMl7fTTT4+1a9fGCy+8EBERf/jDH+LJJ5+MuXPnVnmy2jLsv5xlqL311lvR09MTY8eO7Xd+7Nix8ec//7lKUx08ent7Y9GiRTF79uyYNm1atcdJ1urVq2Pz5s3R2dlZ7VGS9/LLL8fy5cujvb09vvGNb0RnZ2dcc8010djYGPPnz6/2eElavHhx5PP5mDx5ctTX10dPT0/ccMMNMW/evGqPVlNqPuhUV1tbW2zdujWefPLJao+SrK6urli4cGE89thjMWLEiGqPk7ze3t5obW2NG2+8MSIiTj755Ni6dWvcfvvtgl4hP/3pT+Pee++NVatWxdSpU2PLli2xaNGiGD9+vMe8BDUf9COPPDLq6+tjx44d/c7v2LEjjjrqqCpNdXC46qqr4qGHHor169dX/FfiHsw2bdoUO3fujFNOOaXvXE9PT6xfvz5uvfXWKBQKUV9fX8UJ0zJu3LiYMmVKv3PHH398/PznP6/SROn72te+FosXL47Pf/7zERFxwgknxKuvvhodHR2CXoKa/x56Y2NjnHrqqbF27dq+c729vbF27dqYNWtWFSdLV7FYjKuuuirWrFkTv/rVr2LSpEnVHilp55xzTjz77LOxZcuWvqO1tTXmzZsXW7ZsEfMhNnv27Pf9GOYLL7wQRx99dJUmSt+7774bdXX9c1RfXx+9vb1Vmqg21fwVekREe3t7zJ8/P1pbW+O0006LZcuWxZ49e2LBggXVHi1JbW1tsWrVqnjggQeiqakptm/fHhERLS0tMXLkyCpPl56mpqb3vT7h0EMPjVGjRnndQgVce+21cfrpp8eNN94Yn/3sZ+OZZ56JFStWxIoVK6o9WrIuuuiiuOGGG2LixIkxderU+P3vfx8333xzXHHFFdUerbYUE3HLLbcUJ06cWGxsbCyedtppxQ0bNlR7pGRFxD6PlStXVnu0g8aZZ55ZXLhwYbXHSNYvf/nL4rRp04rZbLY4efLk4ooVK6o9UtLy+Xxx4cKFxYkTJxZHjBhRPPbYY4vf/OY3i4VCodqj1ZQkfg4dAA52Nf89dABA0AEgCYIOAAkQdABIgKADQAIEHQASIOgAkABBB4AECDoAJEDQASABgg4ACRB0AEjA/wfYzK2ZUrmciwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = Model(model.input, model.layers[10].output)\n",
    "predictions = f.predict(X_test)\n",
    "plt.imshow(predictions[prr], cmap='viridis', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b2d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
