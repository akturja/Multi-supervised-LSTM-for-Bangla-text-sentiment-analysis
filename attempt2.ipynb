{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f933f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset (Bangla ( Bengali ) sentiment analysis classification benchmark dataset corpus) : https://data.mendeley.com/datasets/p6zc7krs37/4\n",
    "\"\"\"\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import *\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc3c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 50\n",
    "w_decay = 0.05\n",
    "dropout_rate = 0.2\n",
    "epochs_to_run = 500\n",
    "sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8966bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 positive sentences\n",
      "3307 negative sentences\n",
      "3307 positive sentences\n",
      "3307 negative sentences\n"
     ]
    }
   ],
   "source": [
    "# Loading Bangla ( Bengali ) sentiment analysis classification benchmark dataset\n",
    "positive_sentences = []\n",
    "f = open('../datasets/all_positive_8500.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    positive_sentences.append(line.strip())\n",
    "\n",
    "negative_sentences = []\n",
    "f = open('../datasets/all_negative_3307.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    negative_sentences.append(line.strip())\n",
    "    \n",
    "print(len(positive_sentences), 'positive sentences')\n",
    "print(len(negative_sentences), 'negative sentences')\n",
    "\n",
    "import random\n",
    "random.shuffle(positive_sentences)\n",
    "\n",
    "for i in range(len(positive_sentences)-len(negative_sentences)):\n",
    "    positive_sentences.pop(0)\n",
    "\n",
    "print(len(positive_sentences), 'positive sentences')\n",
    "print(len(negative_sentences), 'negative sentences')\n",
    "\n",
    "\n",
    "y_pos = [1 for i in range(len(positive_sentences))]\n",
    "y_neg = [0 for i in range(len(negative_sentences))]\n",
    "\n",
    "X = positive_sentences + negative_sentences\n",
    "y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c24841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141148\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4113e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(X)\n",
    "y = np.array(y)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "X_train_val = X_train\n",
    "y_train_val = y_train\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_val, test_size=0.2, random_state=1, stratify=y_train_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747457da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856b4b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 100)          14114800  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 200, 50), (None,  30200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 14,145,051\n",
      "Trainable params: 30,251\n",
      "Non-trainable params: 14,114,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 8.7831 - accuracy: 0.5505\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.57129, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 8.7651 - accuracy: 0.5515 - val_loss: 7.6906 - val_accuracy: 0.5713\n",
      "Epoch 2/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 6.8470 - accuracy: 0.5891\n",
      "Epoch 00002: val_accuracy improved from 0.57129 to 0.63173, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 6.8268 - accuracy: 0.5914 - val_loss: 5.9962 - val_accuracy: 0.6317\n",
      "Epoch 3/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 5.3373 - accuracy: 0.6205\n",
      "Epoch 00003: val_accuracy improved from 0.63173 to 0.64967, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 5.3316 - accuracy: 0.6205 - val_loss: 4.6885 - val_accuracy: 0.6497\n",
      "Epoch 4/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 4.1835 - accuracy: 0.6560\n",
      "Epoch 00004: val_accuracy improved from 0.64967 to 0.67233, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 4.1754 - accuracy: 0.6564 - val_loss: 3.6800 - val_accuracy: 0.6723\n",
      "Epoch 5/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 3.2891 - accuracy: 0.6834\n",
      "Epoch 00005: val_accuracy improved from 0.67233 to 0.68650, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 3.2800 - accuracy: 0.6834 - val_loss: 2.9035 - val_accuracy: 0.6865\n",
      "Epoch 6/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.5972 - accuracy: 0.7143\n",
      "Epoch 00006: val_accuracy improved from 0.68650 to 0.69405, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 2.5972 - accuracy: 0.7143 - val_loss: 2.3098 - val_accuracy: 0.6941\n",
      "Epoch 7/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 2.0743 - accuracy: 0.7218\n",
      "Epoch 00007: val_accuracy improved from 0.69405 to 0.71010, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 2.0738 - accuracy: 0.7221 - val_loss: 1.8577 - val_accuracy: 0.7101\n",
      "Epoch 8/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.6773 - accuracy: 0.7389\n",
      "Epoch 00008: val_accuracy improved from 0.71010 to 0.71294, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.6736 - accuracy: 0.7389 - val_loss: 1.5157 - val_accuracy: 0.7129\n",
      "Epoch 9/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.3770 - accuracy: 0.7483\n",
      "Epoch 00009: val_accuracy improved from 0.71294 to 0.72427, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.3756 - accuracy: 0.7486 - val_loss: 1.2572 - val_accuracy: 0.7243\n",
      "Epoch 10/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.1506 - accuracy: 0.7611\n",
      "Epoch 00010: val_accuracy improved from 0.72427 to 0.73749, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 1.1504 - accuracy: 0.7613 - val_loss: 1.0661 - val_accuracy: 0.7375\n",
      "Epoch 11/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9881 - accuracy: 0.7654\n",
      "Epoch 00011: val_accuracy improved from 0.73749 to 0.75826, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9869 - accuracy: 0.7658 - val_loss: 0.9199 - val_accuracy: 0.7583\n",
      "Epoch 12/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8672 - accuracy: 0.7725\n",
      "Epoch 00012: val_accuracy improved from 0.75826 to 0.77148, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8666 - accuracy: 0.7722 - val_loss: 0.8189 - val_accuracy: 0.7715\n",
      "Epoch 13/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7779 - accuracy: 0.7706\n",
      "Epoch 00013: val_accuracy improved from 0.77148 to 0.77337, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7778 - accuracy: 0.7706 - val_loss: 0.7431 - val_accuracy: 0.7734\n",
      "Epoch 14/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7138 - accuracy: 0.7836\n",
      "Epoch 00014: val_accuracy improved from 0.77337 to 0.77904, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.7138 - accuracy: 0.7836 - val_loss: 0.6878 - val_accuracy: 0.7790\n",
      "Epoch 15/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6693 - accuracy: 0.7867\n",
      "Epoch 00015: val_accuracy did not improve from 0.77904\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6692 - accuracy: 0.7866 - val_loss: 0.6718 - val_accuracy: 0.7639\n",
      "Epoch 16/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6385 - accuracy: 0.7822\n",
      "Epoch 00016: val_accuracy improved from 0.77904 to 0.78470, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6389 - accuracy: 0.7817 - val_loss: 0.6405 - val_accuracy: 0.7847\n",
      "Epoch 17/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6123 - accuracy: 0.7929\n",
      "Epoch 00017: val_accuracy improved from 0.78470 to 0.78848, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6130 - accuracy: 0.7923 - val_loss: 0.6138 - val_accuracy: 0.7885\n",
      "Epoch 18/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5989 - accuracy: 0.7872\n",
      "Epoch 00018: val_accuracy improved from 0.78848 to 0.79887, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5989 - accuracy: 0.7869 - val_loss: 0.5862 - val_accuracy: 0.7989\n",
      "Epoch 19/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5811 - accuracy: 0.7956\n",
      "Epoch 00019: val_accuracy did not improve from 0.79887\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5810 - accuracy: 0.7951 - val_loss: 0.5817 - val_accuracy: 0.7941\n",
      "Epoch 20/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7891\n",
      "Epoch 00020: val_accuracy did not improve from 0.79887\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5717 - accuracy: 0.7883 - val_loss: 0.5731 - val_accuracy: 0.7885\n",
      "Epoch 21/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5597 - accuracy: 0.7975\n",
      "Epoch 00021: val_accuracy did not improve from 0.79887\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5603 - accuracy: 0.7961 - val_loss: 0.5582 - val_accuracy: 0.7941\n",
      "Epoch 22/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5553 - accuracy: 0.7999\n",
      "Epoch 00022: val_accuracy did not improve from 0.79887\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5547 - accuracy: 0.8010 - val_loss: 0.5510 - val_accuracy: 0.7932\n",
      "Epoch 23/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.8020\n",
      "Epoch 00023: val_accuracy improved from 0.79887 to 0.80359, saving model to ./weight_cp\\weight_lstm1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5427 - accuracy: 0.8020 - val_loss: 0.5467 - val_accuracy: 0.8036\n",
      "Epoch 24/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5426 - accuracy: 0.7965\n",
      "Epoch 00024: val_accuracy did not improve from 0.80359\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5430 - accuracy: 0.7961 - val_loss: 0.5439 - val_accuracy: 0.7941\n",
      "Epoch 25/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5383 - accuracy: 0.8044\n",
      "Epoch 00025: val_accuracy improved from 0.80359 to 0.80548, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5382 - accuracy: 0.8048 - val_loss: 0.5307 - val_accuracy: 0.8055\n",
      "Epoch 26/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5341 - accuracy: 0.8035\n",
      "Epoch 00026: val_accuracy did not improve from 0.80548\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5347 - accuracy: 0.8032 - val_loss: 0.5538 - val_accuracy: 0.7838\n",
      "Epoch 27/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5292 - accuracy: 0.8026\n",
      "Epoch 00027: val_accuracy improved from 0.80548 to 0.80925, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5299 - accuracy: 0.8029 - val_loss: 0.5245 - val_accuracy: 0.8093\n",
      "Epoch 28/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5269 - accuracy: 0.8073\n",
      "Epoch 00028: val_accuracy did not improve from 0.80925\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5266 - accuracy: 0.8074 - val_loss: 0.5285 - val_accuracy: 0.7989\n",
      "Epoch 29/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5239 - accuracy: 0.8065\n",
      "Epoch 00029: val_accuracy improved from 0.80925 to 0.81114, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5249 - accuracy: 0.8053 - val_loss: 0.5190 - val_accuracy: 0.8111\n",
      "Epoch 30/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5221 - accuracy: 0.8018\n",
      "Epoch 00030: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5226 - accuracy: 0.8020 - val_loss: 0.5268 - val_accuracy: 0.8045\n",
      "Epoch 31/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5231 - accuracy: 0.8087\n",
      "Epoch 00031: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5218 - accuracy: 0.8098 - val_loss: 0.5139 - val_accuracy: 0.8093\n",
      "Epoch 32/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5173 - accuracy: 0.8108\n",
      "Epoch 00032: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5180 - accuracy: 0.8095 - val_loss: 0.5113 - val_accuracy: 0.8045\n",
      "Epoch 33/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5113 - accuracy: 0.8115\n",
      "Epoch 00033: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5106 - accuracy: 0.8121 - val_loss: 0.5103 - val_accuracy: 0.8102\n",
      "Epoch 34/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.8145\n",
      "Epoch 00034: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5046 - accuracy: 0.8145 - val_loss: 0.5105 - val_accuracy: 0.8102\n",
      "Epoch 35/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5024 - accuracy: 0.8240\n",
      "Epoch 00035: val_accuracy improved from 0.81114 to 0.81492, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5026 - accuracy: 0.8237 - val_loss: 0.5044 - val_accuracy: 0.8149\n",
      "Epoch 36/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.8201\n",
      "Epoch 00036: val_accuracy did not improve from 0.81492\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5026 - accuracy: 0.8197 - val_loss: 0.5063 - val_accuracy: 0.8017\n",
      "Epoch 37/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5011 - accuracy: 0.8173\n",
      "Epoch 00037: val_accuracy did not improve from 0.81492\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5015 - accuracy: 0.8169 - val_loss: 0.4998 - val_accuracy: 0.8111\n",
      "Epoch 38/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4995 - accuracy: 0.8188\n",
      "Epoch 00038: val_accuracy did not improve from 0.81492\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4995 - accuracy: 0.8188 - val_loss: 0.4980 - val_accuracy: 0.8140\n",
      "Epoch 39/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4983 - accuracy: 0.8159\n",
      "Epoch 00039: val_accuracy did not improve from 0.81492\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4978 - accuracy: 0.8169 - val_loss: 0.4981 - val_accuracy: 0.8064\n",
      "Epoch 40/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.8248\n",
      "Epoch 00040: val_accuracy did not improve from 0.81492\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4893 - accuracy: 0.8251 - val_loss: 0.5033 - val_accuracy: 0.8111\n",
      "Epoch 41/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4913 - accuracy: 0.8213\n",
      "Epoch 00041: val_accuracy did not improve from 0.81492\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4918 - accuracy: 0.8211 - val_loss: 0.4969 - val_accuracy: 0.8121\n",
      "Epoch 42/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.8290\n",
      "Epoch 00042: val_accuracy improved from 0.81492 to 0.82059, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4839 - accuracy: 0.8294 - val_loss: 0.4882 - val_accuracy: 0.8206\n",
      "Epoch 43/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4902 - accuracy: 0.8302\n",
      "Epoch 00043: val_accuracy did not improve from 0.82059\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4901 - accuracy: 0.8299 - val_loss: 0.4870 - val_accuracy: 0.8140\n",
      "Epoch 44/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4885 - accuracy: 0.8255\n",
      "Epoch 00044: val_accuracy did not improve from 0.82059\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4879 - accuracy: 0.8259 - val_loss: 0.4863 - val_accuracy: 0.8178\n",
      "Epoch 45/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4850 - accuracy: 0.8297\n",
      "Epoch 00045: val_accuracy did not improve from 0.82059\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4858 - accuracy: 0.8294 - val_loss: 0.4915 - val_accuracy: 0.8149\n",
      "Epoch 46/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4885 - accuracy: 0.8253\n",
      "Epoch 00046: val_accuracy did not improve from 0.82059\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4883 - accuracy: 0.8256 - val_loss: 0.4853 - val_accuracy: 0.8121\n",
      "Epoch 47/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4879 - accuracy: 0.8251\n",
      "Epoch 00047: val_accuracy did not improve from 0.82059\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4879 - accuracy: 0.8251 - val_loss: 0.4934 - val_accuracy: 0.8206\n",
      "Epoch 48/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4780 - accuracy: 0.8296\n",
      "Epoch 00048: val_accuracy did not improve from 0.82059\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4780 - accuracy: 0.8296 - val_loss: 0.4875 - val_accuracy: 0.8196\n",
      "Epoch 49/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4790 - accuracy: 0.8268\n",
      "Epoch 00049: val_accuracy did not improve from 0.82059\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4794 - accuracy: 0.8268 - val_loss: 0.4848 - val_accuracy: 0.8178\n",
      "Epoch 50/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4739 - accuracy: 0.8315\n",
      "Epoch 00050: val_accuracy improved from 0.82059 to 0.82153, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4748 - accuracy: 0.8310 - val_loss: 0.4847 - val_accuracy: 0.8215\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.4794 - accuracy: 0.8292\n",
      "Epoch 00051: val_accuracy did not improve from 0.82153\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4789 - accuracy: 0.8292 - val_loss: 0.4801 - val_accuracy: 0.8187\n",
      "Epoch 52/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.8341\n",
      "Epoch 00052: val_accuracy did not improve from 0.82153\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4745 - accuracy: 0.8336 - val_loss: 0.4867 - val_accuracy: 0.8121\n",
      "Epoch 53/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4811 - accuracy: 0.8317\n",
      "Epoch 00053: val_accuracy did not improve from 0.82153\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4798 - accuracy: 0.8327 - val_loss: 0.4801 - val_accuracy: 0.8178\n",
      "Epoch 54/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4754 - accuracy: 0.8266\n",
      "Epoch 00054: val_accuracy did not improve from 0.82153\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4737 - accuracy: 0.8282 - val_loss: 0.4898 - val_accuracy: 0.8187\n",
      "Epoch 55/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4692 - accuracy: 0.8399\n",
      "Epoch 00055: val_accuracy did not improve from 0.82153\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4695 - accuracy: 0.8400 - val_loss: 0.4803 - val_accuracy: 0.8206\n",
      "Epoch 56/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4672 - accuracy: 0.8367\n",
      "Epoch 00056: val_accuracy improved from 0.82153 to 0.82436, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4672 - accuracy: 0.8367 - val_loss: 0.4725 - val_accuracy: 0.8244\n",
      "Epoch 57/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4722 - accuracy: 0.8308\n",
      "Epoch 00057: val_accuracy did not improve from 0.82436\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4735 - accuracy: 0.8294 - val_loss: 0.4841 - val_accuracy: 0.8178\n",
      "Epoch 58/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4670 - accuracy: 0.8392\n",
      "Epoch 00058: val_accuracy did not improve from 0.82436\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4662 - accuracy: 0.8398 - val_loss: 0.4862 - val_accuracy: 0.8121\n",
      "Epoch 59/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4582 - accuracy: 0.8452\n",
      "Epoch 00059: val_accuracy did not improve from 0.82436\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4568 - accuracy: 0.8459 - val_loss: 0.4851 - val_accuracy: 0.8093\n",
      "Epoch 60/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4682 - accuracy: 0.8325\n",
      "Epoch 00060: val_accuracy did not improve from 0.82436\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4674 - accuracy: 0.8329 - val_loss: 0.4780 - val_accuracy: 0.8196\n",
      "Epoch 61/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4596 - accuracy: 0.8418\n",
      "Epoch 00061: val_accuracy improved from 0.82436 to 0.82814, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4584 - accuracy: 0.8422 - val_loss: 0.4685 - val_accuracy: 0.8281\n",
      "Epoch 62/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4644 - accuracy: 0.8353\n",
      "Epoch 00062: val_accuracy did not improve from 0.82814\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4634 - accuracy: 0.8367 - val_loss: 0.4741 - val_accuracy: 0.8281\n",
      "Epoch 63/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4507 - accuracy: 0.8447\n",
      "Epoch 00063: val_accuracy did not improve from 0.82814\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4517 - accuracy: 0.8438 - val_loss: 0.4683 - val_accuracy: 0.8263\n",
      "Epoch 64/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.8430\n",
      "Epoch 00064: val_accuracy improved from 0.82814 to 0.83003, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4521 - accuracy: 0.8436 - val_loss: 0.4636 - val_accuracy: 0.8300\n",
      "Epoch 65/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4524 - accuracy: 0.8423\n",
      "Epoch 00065: val_accuracy improved from 0.83003 to 0.83569, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4511 - accuracy: 0.8433 - val_loss: 0.4637 - val_accuracy: 0.8357\n",
      "Epoch 66/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4563 - accuracy: 0.8402\n",
      "Epoch 00066: val_accuracy did not improve from 0.83569\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4556 - accuracy: 0.8403 - val_loss: 0.4615 - val_accuracy: 0.8300\n",
      "Epoch 67/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4476 - accuracy: 0.8519\n",
      "Epoch 00067: val_accuracy did not improve from 0.83569\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4485 - accuracy: 0.8514 - val_loss: 0.4654 - val_accuracy: 0.8291\n",
      "Epoch 68/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4521 - accuracy: 0.8423\n",
      "Epoch 00068: val_accuracy did not improve from 0.83569\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4519 - accuracy: 0.8424 - val_loss: 0.4632 - val_accuracy: 0.8319\n",
      "Epoch 69/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4519 - accuracy: 0.8430\n",
      "Epoch 00069: val_accuracy improved from 0.83569 to 0.84042, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4514 - accuracy: 0.8433 - val_loss: 0.4573 - val_accuracy: 0.8404\n",
      "Epoch 70/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4454 - accuracy: 0.8527\n",
      "Epoch 00070: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4449 - accuracy: 0.8533 - val_loss: 0.4695 - val_accuracy: 0.8225\n",
      "Epoch 71/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4458 - accuracy: 0.8485\n",
      "Epoch 00071: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4458 - accuracy: 0.8485 - val_loss: 0.4575 - val_accuracy: 0.8376\n",
      "Epoch 72/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4449 - accuracy: 0.8486\n",
      "Epoch 00072: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4456 - accuracy: 0.8481 - val_loss: 0.4666 - val_accuracy: 0.8253\n",
      "Epoch 73/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4417 - accuracy: 0.8487\n",
      "Epoch 00073: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4414 - accuracy: 0.8490 - val_loss: 0.4635 - val_accuracy: 0.8281\n",
      "Epoch 74/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4437 - accuracy: 0.8471\n",
      "Epoch 00074: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4456 - accuracy: 0.8457 - val_loss: 0.4585 - val_accuracy: 0.8366\n",
      "Epoch 75/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4403 - accuracy: 0.8492\n",
      "Epoch 00075: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4409 - accuracy: 0.8492 - val_loss: 0.4505 - val_accuracy: 0.8385\n",
      "Epoch 76/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4396 - accuracy: 0.8536\n",
      "Epoch 00076: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4416 - accuracy: 0.8528 - val_loss: 0.4490 - val_accuracy: 0.8366\n",
      "Epoch 77/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4430 - accuracy: 0.8542\n",
      "Epoch 00077: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4429 - accuracy: 0.8544 - val_loss: 0.4764 - val_accuracy: 0.8225\n",
      "Epoch 78/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.8438\n",
      "Epoch 00078: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4446 - accuracy: 0.8440 - val_loss: 0.4562 - val_accuracy: 0.8253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.8573\n",
      "Epoch 00079: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4420 - accuracy: 0.8568 - val_loss: 0.4538 - val_accuracy: 0.8319\n",
      "Epoch 80/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4312 - accuracy: 0.8511\n",
      "Epoch 00080: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4338 - accuracy: 0.8500 - val_loss: 0.4541 - val_accuracy: 0.8338\n",
      "Epoch 81/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4395 - accuracy: 0.8581\n",
      "Epoch 00081: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4405 - accuracy: 0.8570 - val_loss: 0.4635 - val_accuracy: 0.8300\n",
      "Epoch 82/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4328 - accuracy: 0.8547\n",
      "Epoch 00082: val_accuracy improved from 0.84042 to 0.84419, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4327 - accuracy: 0.8542 - val_loss: 0.4450 - val_accuracy: 0.8442\n",
      "Epoch 83/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4334 - accuracy: 0.8504\n",
      "Epoch 00083: val_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4340 - accuracy: 0.8500 - val_loss: 0.4492 - val_accuracy: 0.8404\n",
      "Epoch 84/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4358 - accuracy: 0.8533\n",
      "Epoch 00084: val_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4348 - accuracy: 0.8535 - val_loss: 0.4496 - val_accuracy: 0.8395\n",
      "Epoch 85/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4304 - accuracy: 0.8525\n",
      "Epoch 00085: val_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4301 - accuracy: 0.8528 - val_loss: 0.4480 - val_accuracy: 0.8310\n",
      "Epoch 86/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4319 - accuracy: 0.8547\n",
      "Epoch 00086: val_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4333 - accuracy: 0.8540 - val_loss: 0.4439 - val_accuracy: 0.8432\n",
      "Epoch 87/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4241 - accuracy: 0.8582\n",
      "Epoch 00087: val_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4242 - accuracy: 0.8585 - val_loss: 0.4470 - val_accuracy: 0.8347\n",
      "Epoch 88/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4336 - accuracy: 0.8535\n",
      "Epoch 00088: val_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4337 - accuracy: 0.8533 - val_loss: 0.4449 - val_accuracy: 0.8329\n",
      "Epoch 89/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4301 - accuracy: 0.8566\n",
      "Epoch 00089: val_accuracy improved from 0.84419 to 0.84514, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4293 - accuracy: 0.8568 - val_loss: 0.4415 - val_accuracy: 0.8451\n",
      "Epoch 90/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4236 - accuracy: 0.8551\n",
      "Epoch 00090: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4243 - accuracy: 0.8542 - val_loss: 0.4445 - val_accuracy: 0.8404\n",
      "Epoch 91/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4276 - accuracy: 0.8576\n",
      "Epoch 00091: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4266 - accuracy: 0.8582 - val_loss: 0.4599 - val_accuracy: 0.8300\n",
      "Epoch 92/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4257 - accuracy: 0.8575\n",
      "Epoch 00092: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4259 - accuracy: 0.8573 - val_loss: 0.4580 - val_accuracy: 0.8272\n",
      "Epoch 93/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.8596\n",
      "Epoch 00093: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4294 - accuracy: 0.8596 - val_loss: 0.4438 - val_accuracy: 0.8423\n",
      "Epoch 94/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.8490\n",
      "Epoch 00094: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4306 - accuracy: 0.8490 - val_loss: 0.4581 - val_accuracy: 0.8253\n",
      "Epoch 95/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8615\n",
      "Epoch 00095: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4218 - accuracy: 0.8613 - val_loss: 0.4591 - val_accuracy: 0.8291\n",
      "Epoch 96/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.8627\n",
      "Epoch 00096: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4185 - accuracy: 0.8627 - val_loss: 0.4468 - val_accuracy: 0.8395\n",
      "Epoch 97/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8554\n",
      "Epoch 00097: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4280 - accuracy: 0.8540 - val_loss: 0.4438 - val_accuracy: 0.8347\n",
      "Epoch 98/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4234 - accuracy: 0.8616\n",
      "Epoch 00098: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4238 - accuracy: 0.8615 - val_loss: 0.4401 - val_accuracy: 0.8395\n",
      "Epoch 99/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8659\n",
      "Epoch 00099: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4156 - accuracy: 0.8663 - val_loss: 0.4428 - val_accuracy: 0.8432\n",
      "Epoch 100/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8680\n",
      "Epoch 00100: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4171 - accuracy: 0.8677 - val_loss: 0.4368 - val_accuracy: 0.8451\n",
      "Epoch 101/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.8595\n",
      "Epoch 00101: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4231 - accuracy: 0.8594 - val_loss: 0.4603 - val_accuracy: 0.8310\n",
      "Epoch 102/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8641\n",
      "Epoch 00102: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4158 - accuracy: 0.8641 - val_loss: 0.4337 - val_accuracy: 0.8432\n",
      "Epoch 103/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4146 - accuracy: 0.8659\n",
      "Epoch 00103: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4151 - accuracy: 0.8660 - val_loss: 0.4451 - val_accuracy: 0.8414\n",
      "Epoch 104/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8684\n",
      "Epoch 00104: val_accuracy did not improve from 0.84514\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4135 - accuracy: 0.8686 - val_loss: 0.4396 - val_accuracy: 0.8414\n",
      "Epoch 105/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8672\n",
      "Epoch 00105: val_accuracy improved from 0.84514 to 0.85080, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4114 - accuracy: 0.8672 - val_loss: 0.4333 - val_accuracy: 0.8508\n",
      "Epoch 106/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8672\n",
      "Epoch 00106: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4101 - accuracy: 0.8674 - val_loss: 0.4499 - val_accuracy: 0.8338\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.4086 - accuracy: 0.8694\n",
      "Epoch 00107: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4098 - accuracy: 0.8686 - val_loss: 0.4342 - val_accuracy: 0.8404\n",
      "Epoch 108/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8721\n",
      "Epoch 00108: val_accuracy improved from 0.85080 to 0.85175, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4076 - accuracy: 0.8726 - val_loss: 0.4484 - val_accuracy: 0.8517\n",
      "Epoch 109/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4058 - accuracy: 0.8700\n",
      "Epoch 00109: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4058 - accuracy: 0.8700 - val_loss: 0.4311 - val_accuracy: 0.8480\n",
      "Epoch 110/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8669\n",
      "Epoch 00110: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4100 - accuracy: 0.8667 - val_loss: 0.4303 - val_accuracy: 0.8480\n",
      "Epoch 111/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4094 - accuracy: 0.8659\n",
      "Epoch 00111: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4089 - accuracy: 0.8658 - val_loss: 0.4377 - val_accuracy: 0.8423\n",
      "Epoch 112/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4019 - accuracy: 0.8699\n",
      "Epoch 00112: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4019 - accuracy: 0.8698 - val_loss: 0.4431 - val_accuracy: 0.8347\n",
      "Epoch 113/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8714\n",
      "Epoch 00113: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4030 - accuracy: 0.8715 - val_loss: 0.4343 - val_accuracy: 0.8470\n",
      "Epoch 114/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4039 - accuracy: 0.8647\n",
      "Epoch 00114: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4036 - accuracy: 0.8648 - val_loss: 0.4292 - val_accuracy: 0.8451\n",
      "Epoch 115/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4066 - accuracy: 0.8671\n",
      "Epoch 00115: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4060 - accuracy: 0.8672 - val_loss: 0.4326 - val_accuracy: 0.8451\n",
      "Epoch 116/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4004 - accuracy: 0.8723\n",
      "Epoch 00116: val_accuracy improved from 0.85175 to 0.85364, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.4021 - accuracy: 0.8705 - val_loss: 0.4274 - val_accuracy: 0.8536\n",
      "Epoch 117/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8689\n",
      "Epoch 00117: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3990 - accuracy: 0.8693 - val_loss: 0.4803 - val_accuracy: 0.8319\n",
      "Epoch 118/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3999 - accuracy: 0.8714\n",
      "Epoch 00118: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3996 - accuracy: 0.8717 - val_loss: 0.4323 - val_accuracy: 0.8470\n",
      "Epoch 119/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8724\n",
      "Epoch 00119: val_accuracy improved from 0.85364 to 0.85552, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.4002 - accuracy: 0.8724 - val_loss: 0.4285 - val_accuracy: 0.8555\n",
      "Epoch 120/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8724\n",
      "Epoch 00120: val_accuracy did not improve from 0.85552\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3985 - accuracy: 0.8722 - val_loss: 0.4261 - val_accuracy: 0.8517\n",
      "Epoch 121/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8704\n",
      "Epoch 00121: val_accuracy did not improve from 0.85552\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4002 - accuracy: 0.8710 - val_loss: 0.4339 - val_accuracy: 0.8499\n",
      "Epoch 122/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8741\n",
      "Epoch 00122: val_accuracy did not improve from 0.85552\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3958 - accuracy: 0.8741 - val_loss: 0.4267 - val_accuracy: 0.8461\n",
      "Epoch 123/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8733\n",
      "Epoch 00123: val_accuracy did not improve from 0.85552\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3958 - accuracy: 0.8733 - val_loss: 0.4284 - val_accuracy: 0.8432\n",
      "Epoch 124/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3999 - accuracy: 0.8703\n",
      "Epoch 00124: val_accuracy improved from 0.85552 to 0.86119, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3996 - accuracy: 0.8705 - val_loss: 0.4248 - val_accuracy: 0.8612\n",
      "Epoch 125/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3886 - accuracy: 0.8819\n",
      "Epoch 00125: val_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3893 - accuracy: 0.8819 - val_loss: 0.4425 - val_accuracy: 0.8432\n",
      "Epoch 126/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.8743\n",
      "Epoch 00126: val_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3949 - accuracy: 0.8743 - val_loss: 0.4280 - val_accuracy: 0.8451\n",
      "Epoch 127/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3914 - accuracy: 0.8752\n",
      "Epoch 00127: val_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3919 - accuracy: 0.8752 - val_loss: 0.4303 - val_accuracy: 0.8432\n",
      "Epoch 128/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.8731\n",
      "Epoch 00128: val_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3964 - accuracy: 0.8726 - val_loss: 0.4229 - val_accuracy: 0.8584\n",
      "Epoch 129/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8699\n",
      "Epoch 00129: val_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3995 - accuracy: 0.8703 - val_loss: 0.4215 - val_accuracy: 0.8584\n",
      "Epoch 130/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.8760\n",
      "Epoch 00130: val_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3897 - accuracy: 0.8762 - val_loss: 0.4233 - val_accuracy: 0.8461\n",
      "Epoch 131/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3951 - accuracy: 0.8750\n",
      "Epoch 00131: val_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3953 - accuracy: 0.8748 - val_loss: 0.4207 - val_accuracy: 0.8546\n",
      "Epoch 132/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3912 - accuracy: 0.8757\n",
      "Epoch 00132: val_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3912 - accuracy: 0.8757 - val_loss: 0.4169 - val_accuracy: 0.8555\n",
      "Epoch 133/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3838 - accuracy: 0.8814\n",
      "Epoch 00133: val_accuracy improved from 0.86119 to 0.86308, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3835 - accuracy: 0.8816 - val_loss: 0.4310 - val_accuracy: 0.8631\n",
      "Epoch 134/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3873 - accuracy: 0.8848\n",
      "Epoch 00134: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3881 - accuracy: 0.8840 - val_loss: 0.4203 - val_accuracy: 0.8574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3857 - accuracy: 0.8838\n",
      "Epoch 00135: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3854 - accuracy: 0.8842 - val_loss: 0.4223 - val_accuracy: 0.8527\n",
      "Epoch 136/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3850 - accuracy: 0.8882\n",
      "Epoch 00136: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3842 - accuracy: 0.8887 - val_loss: 0.4254 - val_accuracy: 0.8470\n",
      "Epoch 137/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.8719\n",
      "Epoch 00137: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3909 - accuracy: 0.8710 - val_loss: 0.4237 - val_accuracy: 0.8565\n",
      "Epoch 138/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3838 - accuracy: 0.8786\n",
      "Epoch 00138: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3832 - accuracy: 0.8788 - val_loss: 0.4379 - val_accuracy: 0.8376\n",
      "Epoch 139/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.8841\n",
      "Epoch 00139: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3836 - accuracy: 0.8835 - val_loss: 0.4181 - val_accuracy: 0.8555\n",
      "Epoch 140/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3878 - accuracy: 0.8743\n",
      "Epoch 00140: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3891 - accuracy: 0.8731 - val_loss: 0.4203 - val_accuracy: 0.8602\n",
      "Epoch 141/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.8857\n",
      "Epoch 00141: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3816 - accuracy: 0.8854 - val_loss: 0.4220 - val_accuracy: 0.8536\n",
      "Epoch 142/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.8743\n",
      "Epoch 00142: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3951 - accuracy: 0.8738 - val_loss: 0.4281 - val_accuracy: 0.8499\n",
      "Epoch 143/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3878 - accuracy: 0.8802\n",
      "Epoch 00143: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3878 - accuracy: 0.8797 - val_loss: 0.4223 - val_accuracy: 0.8480\n",
      "Epoch 144/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3843 - accuracy: 0.8805\n",
      "Epoch 00144: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3832 - accuracy: 0.8811 - val_loss: 0.4388 - val_accuracy: 0.8451\n",
      "Epoch 145/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.8814\n",
      "Epoch 00145: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3801 - accuracy: 0.8811 - val_loss: 0.4165 - val_accuracy: 0.8527\n",
      "Epoch 146/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8841\n",
      "Epoch 00146: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3828 - accuracy: 0.8837 - val_loss: 0.4191 - val_accuracy: 0.8621\n",
      "Epoch 147/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3832 - accuracy: 0.8825\n",
      "Epoch 00147: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3830 - accuracy: 0.8821 - val_loss: 0.4195 - val_accuracy: 0.8517\n",
      "Epoch 148/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8837\n",
      "Epoch 00148: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3775 - accuracy: 0.8837 - val_loss: 0.4191 - val_accuracy: 0.8621\n",
      "Epoch 149/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3821 - accuracy: 0.8823\n",
      "Epoch 00149: val_accuracy improved from 0.86308 to 0.86402, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3821 - accuracy: 0.8823 - val_loss: 0.4213 - val_accuracy: 0.8640\n",
      "Epoch 150/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8846\n",
      "Epoch 00150: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3771 - accuracy: 0.8856 - val_loss: 0.4157 - val_accuracy: 0.8584\n",
      "Epoch 151/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8884\n",
      "Epoch 00151: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3756 - accuracy: 0.8882 - val_loss: 0.4162 - val_accuracy: 0.8508\n",
      "Epoch 152/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8896\n",
      "Epoch 00152: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3722 - accuracy: 0.8904 - val_loss: 0.4143 - val_accuracy: 0.8555\n",
      "Epoch 153/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3825 - accuracy: 0.8813\n",
      "Epoch 00153: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3812 - accuracy: 0.8816 - val_loss: 0.4300 - val_accuracy: 0.8536\n",
      "Epoch 154/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8852\n",
      "Epoch 00154: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3775 - accuracy: 0.8849 - val_loss: 0.4298 - val_accuracy: 0.8489\n",
      "Epoch 155/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3768 - accuracy: 0.8889\n",
      "Epoch 00155: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3761 - accuracy: 0.8894 - val_loss: 0.4240 - val_accuracy: 0.8574\n",
      "Epoch 156/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8861\n",
      "Epoch 00156: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3720 - accuracy: 0.8866 - val_loss: 0.4375 - val_accuracy: 0.8489\n",
      "Epoch 157/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.8800\n",
      "Epoch 00157: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3731 - accuracy: 0.8807 - val_loss: 0.4150 - val_accuracy: 0.8565\n",
      "Epoch 158/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.8844\n",
      "Epoch 00158: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3823 - accuracy: 0.8837 - val_loss: 0.4217 - val_accuracy: 0.8584\n",
      "Epoch 159/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8808\n",
      "Epoch 00159: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3821 - accuracy: 0.8819 - val_loss: 0.4121 - val_accuracy: 0.8631\n",
      "Epoch 160/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.8889\n",
      "Epoch 00160: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3686 - accuracy: 0.8882 - val_loss: 0.4137 - val_accuracy: 0.8602\n",
      "Epoch 161/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8878\n",
      "Epoch 00161: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3737 - accuracy: 0.8875 - val_loss: 0.4134 - val_accuracy: 0.8640\n",
      "Epoch 162/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8814\n",
      "Epoch 00162: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3774 - accuracy: 0.8811 - val_loss: 0.4471 - val_accuracy: 0.8546\n",
      "Epoch 163/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3812 - accuracy: 0.8786\n",
      "Epoch 00163: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3813 - accuracy: 0.8785 - val_loss: 0.4112 - val_accuracy: 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3680 - accuracy: 0.8880\n",
      "Epoch 00164: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3680 - accuracy: 0.8880 - val_loss: 0.4178 - val_accuracy: 0.8593\n",
      "Epoch 165/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8869\n",
      "Epoch 00165: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3659 - accuracy: 0.8878 - val_loss: 0.4228 - val_accuracy: 0.8612\n",
      "Epoch 166/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8912\n",
      "Epoch 00166: val_accuracy improved from 0.86402 to 0.86497, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3724 - accuracy: 0.8915 - val_loss: 0.4121 - val_accuracy: 0.8650\n",
      "Epoch 167/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8829\n",
      "Epoch 00167: val_accuracy did not improve from 0.86497\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3758 - accuracy: 0.8823 - val_loss: 0.4113 - val_accuracy: 0.8621\n",
      "Epoch 168/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8905\n",
      "Epoch 00168: val_accuracy improved from 0.86497 to 0.86591, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3629 - accuracy: 0.8904 - val_loss: 0.4093 - val_accuracy: 0.8659\n",
      "Epoch 169/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8946\n",
      "Epoch 00169: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3596 - accuracy: 0.8946 - val_loss: 0.4150 - val_accuracy: 0.8621\n",
      "Epoch 170/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3690 - accuracy: 0.8861\n",
      "Epoch 00170: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3696 - accuracy: 0.8854 - val_loss: 0.4163 - val_accuracy: 0.8621\n",
      "Epoch 171/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3676 - accuracy: 0.8917\n",
      "Epoch 00171: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3671 - accuracy: 0.8930 - val_loss: 0.4569 - val_accuracy: 0.8385\n",
      "Epoch 172/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3698 - accuracy: 0.8907\n",
      "Epoch 00172: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3696 - accuracy: 0.8908 - val_loss: 0.4081 - val_accuracy: 0.8659\n",
      "Epoch 173/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8881\n",
      "Epoch 00173: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3655 - accuracy: 0.8882 - val_loss: 0.4047 - val_accuracy: 0.8659\n",
      "Epoch 174/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3722 - accuracy: 0.8848\n",
      "Epoch 00174: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3721 - accuracy: 0.8852 - val_loss: 0.4550 - val_accuracy: 0.8404\n",
      "Epoch 175/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8843\n",
      "Epoch 00175: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3706 - accuracy: 0.8849 - val_loss: 0.4574 - val_accuracy: 0.8366\n",
      "Epoch 176/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3699 - accuracy: 0.8856\n",
      "Epoch 00176: val_accuracy improved from 0.86591 to 0.86969, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3699 - accuracy: 0.8856 - val_loss: 0.4077 - val_accuracy: 0.8697\n",
      "Epoch 177/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3619 - accuracy: 0.8968\n",
      "Epoch 00177: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3617 - accuracy: 0.8970 - val_loss: 0.4086 - val_accuracy: 0.8602\n",
      "Epoch 178/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3628 - accuracy: 0.8923\n",
      "Epoch 00178: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3628 - accuracy: 0.8922 - val_loss: 0.4095 - val_accuracy: 0.8697\n",
      "Epoch 179/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8886\n",
      "Epoch 00179: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3641 - accuracy: 0.8885 - val_loss: 0.4101 - val_accuracy: 0.8650\n",
      "Epoch 180/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.8955\n",
      "Epoch 00180: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3626 - accuracy: 0.8953 - val_loss: 0.4619 - val_accuracy: 0.8395\n",
      "Epoch 181/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3682 - accuracy: 0.8874\n",
      "Epoch 00181: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3686 - accuracy: 0.8871 - val_loss: 0.4105 - val_accuracy: 0.8687\n",
      "Epoch 182/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.8931\n",
      "Epoch 00182: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3631 - accuracy: 0.8934 - val_loss: 0.4243 - val_accuracy: 0.8517\n",
      "Epoch 183/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3610 - accuracy: 0.8970\n",
      "Epoch 00183: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3610 - accuracy: 0.8970 - val_loss: 0.4090 - val_accuracy: 0.8697\n",
      "Epoch 184/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.8869\n",
      "Epoch 00184: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3688 - accuracy: 0.8868 - val_loss: 0.4175 - val_accuracy: 0.8584\n",
      "Epoch 185/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8891\n",
      "Epoch 00185: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3604 - accuracy: 0.8894 - val_loss: 0.4335 - val_accuracy: 0.8584\n",
      "Epoch 186/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3549 - accuracy: 0.8988\n",
      "Epoch 00186: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3545 - accuracy: 0.8986 - val_loss: 0.4111 - val_accuracy: 0.8650\n",
      "Epoch 187/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8925\n",
      "Epoch 00187: val_accuracy did not improve from 0.86969\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3619 - accuracy: 0.8925 - val_loss: 0.4365 - val_accuracy: 0.8527\n",
      "Epoch 188/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3542 - accuracy: 0.8956\n",
      "Epoch 00188: val_accuracy improved from 0.86969 to 0.87252, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3542 - accuracy: 0.8956 - val_loss: 0.4172 - val_accuracy: 0.8725\n",
      "Epoch 189/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3651 - accuracy: 0.8923\n",
      "Epoch 00189: val_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3651 - accuracy: 0.8925 - val_loss: 0.4212 - val_accuracy: 0.8621\n",
      "Epoch 190/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3545 - accuracy: 0.8944\n",
      "Epoch 00190: val_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3541 - accuracy: 0.8946 - val_loss: 0.4282 - val_accuracy: 0.8584\n",
      "Epoch 191/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3488 - accuracy: 0.9012\n",
      "Epoch 00191: val_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3485 - accuracy: 0.9017 - val_loss: 0.4396 - val_accuracy: 0.8489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.8948\n",
      "Epoch 00192: val_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3609 - accuracy: 0.8946 - val_loss: 0.4056 - val_accuracy: 0.8706\n",
      "Epoch 193/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.8948\n",
      "Epoch 00193: val_accuracy improved from 0.87252 to 0.87535, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3573 - accuracy: 0.8948 - val_loss: 0.4019 - val_accuracy: 0.8754\n",
      "Epoch 194/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3535 - accuracy: 0.8950\n",
      "Epoch 00194: val_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3540 - accuracy: 0.8948 - val_loss: 0.4209 - val_accuracy: 0.8555\n",
      "Epoch 195/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8977\n",
      "Epoch 00195: val_accuracy improved from 0.87535 to 0.87819, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3525 - accuracy: 0.8977 - val_loss: 0.4056 - val_accuracy: 0.8782\n",
      "Epoch 196/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3563 - accuracy: 0.8872\n",
      "Epoch 00196: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3557 - accuracy: 0.8873 - val_loss: 0.4116 - val_accuracy: 0.8716\n",
      "Epoch 197/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3613 - accuracy: 0.8896\n",
      "Epoch 00197: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3605 - accuracy: 0.8904 - val_loss: 0.4108 - val_accuracy: 0.8631\n",
      "Epoch 198/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8948\n",
      "Epoch 00198: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3593 - accuracy: 0.8946 - val_loss: 0.4022 - val_accuracy: 0.8678\n",
      "Epoch 199/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3589 - accuracy: 0.8941\n",
      "Epoch 00199: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3593 - accuracy: 0.8934 - val_loss: 0.4040 - val_accuracy: 0.8716\n",
      "Epoch 200/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8980\n",
      "Epoch 00200: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3517 - accuracy: 0.8982 - val_loss: 0.4328 - val_accuracy: 0.8517\n",
      "Epoch 201/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8958\n",
      "Epoch 00201: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3578 - accuracy: 0.8960 - val_loss: 0.4048 - val_accuracy: 0.8697\n",
      "Epoch 202/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8970\n",
      "Epoch 00202: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3527 - accuracy: 0.8970 - val_loss: 0.4066 - val_accuracy: 0.8706\n",
      "Epoch 203/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3509 - accuracy: 0.8962\n",
      "Epoch 00203: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3510 - accuracy: 0.8963 - val_loss: 0.4062 - val_accuracy: 0.8697\n",
      "Epoch 204/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3476 - accuracy: 0.9010\n",
      "Epoch 00204: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3474 - accuracy: 0.9008 - val_loss: 0.4064 - val_accuracy: 0.8735\n",
      "Epoch 205/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3589 - accuracy: 0.8946\n",
      "Epoch 00205: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3577 - accuracy: 0.8951 - val_loss: 0.4262 - val_accuracy: 0.8650\n",
      "Epoch 206/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.9007\n",
      "Epoch 00206: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3508 - accuracy: 0.9003 - val_loss: 0.4111 - val_accuracy: 0.8697\n",
      "Epoch 207/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3472 - accuracy: 0.9007\n",
      "Epoch 00207: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3463 - accuracy: 0.9017 - val_loss: 0.4155 - val_accuracy: 0.8659\n",
      "Epoch 208/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.8977\n",
      "Epoch 00208: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3460 - accuracy: 0.8977 - val_loss: 0.4075 - val_accuracy: 0.8706\n",
      "Epoch 209/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8998\n",
      "Epoch 00209: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3459 - accuracy: 0.8993 - val_loss: 0.4218 - val_accuracy: 0.8612\n",
      "Epoch 210/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3491 - accuracy: 0.8953\n",
      "Epoch 00210: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3496 - accuracy: 0.8951 - val_loss: 0.4045 - val_accuracy: 0.8706\n",
      "Epoch 211/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.9041\n",
      "Epoch 00211: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3495 - accuracy: 0.9036 - val_loss: 0.4182 - val_accuracy: 0.8621\n",
      "Epoch 212/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.8941\n",
      "Epoch 00212: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3506 - accuracy: 0.8944 - val_loss: 0.4029 - val_accuracy: 0.8725\n",
      "Epoch 213/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3508 - accuracy: 0.8959\n",
      "Epoch 00213: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3502 - accuracy: 0.8963 - val_loss: 0.4056 - val_accuracy: 0.8725\n",
      "Epoch 214/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.9020\n",
      "Epoch 00214: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3481 - accuracy: 0.9022 - val_loss: 0.4192 - val_accuracy: 0.8602\n",
      "Epoch 215/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8988\n",
      "Epoch 00215: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3487 - accuracy: 0.8979 - val_loss: 0.4175 - val_accuracy: 0.8697\n",
      "Epoch 216/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3490 - accuracy: 0.8998\n",
      "Epoch 00216: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3492 - accuracy: 0.8993 - val_loss: 0.4028 - val_accuracy: 0.8706\n",
      "Epoch 217/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3541 - accuracy: 0.8919\n",
      "Epoch 00217: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3552 - accuracy: 0.8913 - val_loss: 0.3997 - val_accuracy: 0.8763\n",
      "Epoch 218/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3575 - accuracy: 0.8953\n",
      "Epoch 00218: val_accuracy improved from 0.87819 to 0.87913, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3573 - accuracy: 0.8953 - val_loss: 0.3997 - val_accuracy: 0.8791\n",
      "Epoch 219/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8960\n",
      "Epoch 00219: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3516 - accuracy: 0.8967 - val_loss: 0.4284 - val_accuracy: 0.8565\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8991\n",
      "Epoch 00220: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3493 - accuracy: 0.8984 - val_loss: 0.3998 - val_accuracy: 0.8735\n",
      "Epoch 221/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8929\n",
      "Epoch 00221: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3562 - accuracy: 0.8937 - val_loss: 0.4034 - val_accuracy: 0.8706\n",
      "Epoch 222/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.9048\n",
      "Epoch 00222: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3370 - accuracy: 0.9052 - val_loss: 0.4116 - val_accuracy: 0.8631\n",
      "Epoch 223/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9029\n",
      "Epoch 00223: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3442 - accuracy: 0.9024 - val_loss: 0.4608 - val_accuracy: 0.8517\n",
      "Epoch 224/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3398 - accuracy: 0.9065\n",
      "Epoch 00224: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3396 - accuracy: 0.9060 - val_loss: 0.4086 - val_accuracy: 0.8735\n",
      "Epoch 225/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3384 - accuracy: 0.9048\n",
      "Epoch 00225: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3390 - accuracy: 0.9045 - val_loss: 0.4002 - val_accuracy: 0.8697\n",
      "Epoch 226/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8967\n",
      "Epoch 00226: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3461 - accuracy: 0.8967 - val_loss: 0.4030 - val_accuracy: 0.8735\n",
      "Epoch 227/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.9020\n",
      "Epoch 00227: val_accuracy did not improve from 0.87913\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3384 - accuracy: 0.9022 - val_loss: 0.4056 - val_accuracy: 0.8706\n",
      "Epoch 228/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8958\n",
      "Epoch 00228: val_accuracy improved from 0.87913 to 0.88008, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3434 - accuracy: 0.8960 - val_loss: 0.3981 - val_accuracy: 0.8801\n",
      "Epoch 229/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.9041\n",
      "Epoch 00229: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3416 - accuracy: 0.9034 - val_loss: 0.3957 - val_accuracy: 0.8763\n",
      "Epoch 230/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3455 - accuracy: 0.8996\n",
      "Epoch 00230: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3464 - accuracy: 0.8979 - val_loss: 0.4032 - val_accuracy: 0.8716\n",
      "Epoch 231/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.9031\n",
      "Epoch 00231: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3420 - accuracy: 0.9031 - val_loss: 0.4018 - val_accuracy: 0.8725\n",
      "Epoch 232/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy: 0.8977\n",
      "Epoch 00232: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3443 - accuracy: 0.8979 - val_loss: 0.4238 - val_accuracy: 0.8678\n",
      "Epoch 233/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3465 - accuracy: 0.8969\n",
      "Epoch 00233: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3466 - accuracy: 0.8967 - val_loss: 0.4061 - val_accuracy: 0.8744\n",
      "Epoch 234/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3372 - accuracy: 0.9048\n",
      "Epoch 00234: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3372 - accuracy: 0.9050 - val_loss: 0.4131 - val_accuracy: 0.8687\n",
      "Epoch 235/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8991\n",
      "Epoch 00235: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3414 - accuracy: 0.8998 - val_loss: 0.4290 - val_accuracy: 0.8659\n",
      "Epoch 236/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.9079\n",
      "Epoch 00236: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3357 - accuracy: 0.9083 - val_loss: 0.3990 - val_accuracy: 0.8801\n",
      "Epoch 237/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.8972\n",
      "Epoch 00237: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3486 - accuracy: 0.8977 - val_loss: 0.3973 - val_accuracy: 0.8791\n",
      "Epoch 238/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3352 - accuracy: 0.9098\n",
      "Epoch 00238: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3361 - accuracy: 0.9088 - val_loss: 0.4191 - val_accuracy: 0.8669\n",
      "Epoch 239/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy: 0.8996\n",
      "Epoch 00239: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3444 - accuracy: 0.8996 - val_loss: 0.3950 - val_accuracy: 0.8801\n",
      "Epoch 240/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.9051\n",
      "Epoch 00240: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3359 - accuracy: 0.9050 - val_loss: 0.4021 - val_accuracy: 0.8782\n",
      "Epoch 241/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8958\n",
      "Epoch 00241: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3407 - accuracy: 0.8958 - val_loss: 0.4189 - val_accuracy: 0.8754\n",
      "Epoch 242/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.9017\n",
      "Epoch 00242: val_accuracy did not improve from 0.88008\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3390 - accuracy: 0.9019 - val_loss: 0.4017 - val_accuracy: 0.8706\n",
      "Epoch 243/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.9060\n",
      "Epoch 00243: val_accuracy improved from 0.88008 to 0.88102, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3345 - accuracy: 0.9055 - val_loss: 0.4002 - val_accuracy: 0.8810\n",
      "Epoch 244/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.8986\n",
      "Epoch 00244: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3414 - accuracy: 0.8979 - val_loss: 0.4019 - val_accuracy: 0.8754\n",
      "Epoch 245/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3367 - accuracy: 0.9058\n",
      "Epoch 00245: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3372 - accuracy: 0.9055 - val_loss: 0.4063 - val_accuracy: 0.8801\n",
      "Epoch 246/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3347 - accuracy: 0.9020\n",
      "Epoch 00246: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3354 - accuracy: 0.9017 - val_loss: 0.4033 - val_accuracy: 0.8744\n",
      "Epoch 247/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8977\n",
      "Epoch 00247: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3441 - accuracy: 0.8979 - val_loss: 0.4021 - val_accuracy: 0.8744\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.9027\n",
      "Epoch 00248: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3375 - accuracy: 0.9031 - val_loss: 0.3949 - val_accuracy: 0.8754\n",
      "Epoch 249/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.9020\n",
      "Epoch 00249: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3375 - accuracy: 0.9026 - val_loss: 0.4026 - val_accuracy: 0.8763\n",
      "Epoch 250/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.9036\n",
      "Epoch 00250: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3329 - accuracy: 0.9038 - val_loss: 0.3992 - val_accuracy: 0.8791\n",
      "Epoch 251/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.9012\n",
      "Epoch 00251: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3395 - accuracy: 0.9015 - val_loss: 0.3928 - val_accuracy: 0.8801\n",
      "Epoch 252/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3385 - accuracy: 0.9079\n",
      "Epoch 00252: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3382 - accuracy: 0.9083 - val_loss: 0.4035 - val_accuracy: 0.8735\n",
      "Epoch 253/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.9074\n",
      "Epoch 00253: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3291 - accuracy: 0.9081 - val_loss: 0.4255 - val_accuracy: 0.8650\n",
      "Epoch 254/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.9000\n",
      "Epoch 00254: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3381 - accuracy: 0.9005 - val_loss: 0.4013 - val_accuracy: 0.8772\n",
      "Epoch 255/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.9072\n",
      "Epoch 00255: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3331 - accuracy: 0.9074 - val_loss: 0.4250 - val_accuracy: 0.8650\n",
      "Epoch 256/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3351 - accuracy: 0.9053\n",
      "Epoch 00256: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3350 - accuracy: 0.9055 - val_loss: 0.4041 - val_accuracy: 0.8763\n",
      "Epoch 257/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.9103\n",
      "Epoch 00257: val_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3291 - accuracy: 0.9100 - val_loss: 0.4118 - val_accuracy: 0.8735\n",
      "Epoch 258/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.9034\n",
      "Epoch 00258: val_accuracy improved from 0.88102 to 0.88480, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3369 - accuracy: 0.9031 - val_loss: 0.3916 - val_accuracy: 0.8848\n",
      "Epoch 259/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.9062\n",
      "Epoch 00259: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3316 - accuracy: 0.9069 - val_loss: 0.4000 - val_accuracy: 0.8763\n",
      "Epoch 260/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.9065\n",
      "Epoch 00260: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3307 - accuracy: 0.9067 - val_loss: 0.4348 - val_accuracy: 0.8602\n",
      "Epoch 261/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3416 - accuracy: 0.9012\n",
      "Epoch 00261: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3410 - accuracy: 0.9019 - val_loss: 0.4082 - val_accuracy: 0.8650\n",
      "Epoch 262/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.9113\n",
      "Epoch 00262: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3288 - accuracy: 0.9109 - val_loss: 0.4065 - val_accuracy: 0.8820\n",
      "Epoch 263/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3278 - accuracy: 0.9105\n",
      "Epoch 00263: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3283 - accuracy: 0.9100 - val_loss: 0.4165 - val_accuracy: 0.8669\n",
      "Epoch 264/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.9036\n",
      "Epoch 00264: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3322 - accuracy: 0.9036 - val_loss: 0.4004 - val_accuracy: 0.8763\n",
      "Epoch 265/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.9084\n",
      "Epoch 00265: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3312 - accuracy: 0.9078 - val_loss: 0.4027 - val_accuracy: 0.8725\n",
      "Epoch 266/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3321 - accuracy: 0.9086\n",
      "Epoch 00266: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3326 - accuracy: 0.9088 - val_loss: 0.4008 - val_accuracy: 0.8772\n",
      "Epoch 267/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3343 - accuracy: 0.9046\n",
      "Epoch 00267: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3338 - accuracy: 0.9052 - val_loss: 0.3969 - val_accuracy: 0.8810\n",
      "Epoch 268/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.9096\n",
      "Epoch 00268: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3282 - accuracy: 0.9097 - val_loss: 0.3931 - val_accuracy: 0.8725\n",
      "Epoch 269/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.9070\n",
      "Epoch 00269: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3289 - accuracy: 0.9078 - val_loss: 0.4005 - val_accuracy: 0.8820\n",
      "Epoch 270/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3330 - accuracy: 0.9060\n",
      "Epoch 00270: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3331 - accuracy: 0.9062 - val_loss: 0.4150 - val_accuracy: 0.8669\n",
      "Epoch 271/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.9117\n",
      "Epoch 00271: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3247 - accuracy: 0.9123 - val_loss: 0.4039 - val_accuracy: 0.8725\n",
      "Epoch 272/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.9125\n",
      "Epoch 00272: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3226 - accuracy: 0.9121 - val_loss: 0.3952 - val_accuracy: 0.8839\n",
      "Epoch 273/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.9074\n",
      "Epoch 00273: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3316 - accuracy: 0.9067 - val_loss: 0.3948 - val_accuracy: 0.8801\n",
      "Epoch 274/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.9010\n",
      "Epoch 00274: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3317 - accuracy: 0.9012 - val_loss: 0.3958 - val_accuracy: 0.8791\n",
      "Epoch 275/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.9065\n",
      "Epoch 00275: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3339 - accuracy: 0.9060 - val_loss: 0.3949 - val_accuracy: 0.8763\n",
      "Epoch 276/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3277 - accuracy: 0.9084\n",
      "Epoch 00276: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3270 - accuracy: 0.9088 - val_loss: 0.3928 - val_accuracy: 0.8782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.9031\n",
      "Epoch 00277: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3357 - accuracy: 0.9031 - val_loss: 0.4178 - val_accuracy: 0.8725\n",
      "Epoch 278/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.9093\n",
      "Epoch 00278: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3298 - accuracy: 0.9095 - val_loss: 0.4049 - val_accuracy: 0.8735\n",
      "Epoch 279/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.9070\n",
      "Epoch 00279: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3289 - accuracy: 0.9069 - val_loss: 0.3973 - val_accuracy: 0.8820\n",
      "Epoch 280/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.9115\n",
      "Epoch 00280: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3248 - accuracy: 0.9116 - val_loss: 0.4169 - val_accuracy: 0.8782\n",
      "Epoch 281/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.9022\n",
      "Epoch 00281: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3284 - accuracy: 0.9019 - val_loss: 0.3929 - val_accuracy: 0.8839\n",
      "Epoch 282/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3285 - accuracy: 0.9075\n",
      "Epoch 00282: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3279 - accuracy: 0.9071 - val_loss: 0.4255 - val_accuracy: 0.8584\n",
      "Epoch 283/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.9082\n",
      "Epoch 00283: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3276 - accuracy: 0.9076 - val_loss: 0.4273 - val_accuracy: 0.8735\n",
      "Epoch 284/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.9079\n",
      "Epoch 00284: val_accuracy did not improve from 0.88480\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3190 - accuracy: 0.9078 - val_loss: 0.4058 - val_accuracy: 0.8839\n",
      "Epoch 285/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3291 - accuracy: 0.9072\n",
      "Epoch 00285: val_accuracy improved from 0.88480 to 0.89141, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3285 - accuracy: 0.9076 - val_loss: 0.3939 - val_accuracy: 0.8914\n",
      "Epoch 286/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.9105\n",
      "Epoch 00286: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3203 - accuracy: 0.9104 - val_loss: 0.4311 - val_accuracy: 0.8678\n",
      "Epoch 287/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.9098\n",
      "Epoch 00287: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3271 - accuracy: 0.9097 - val_loss: 0.4229 - val_accuracy: 0.8735\n",
      "Epoch 288/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.9165\n",
      "Epoch 00288: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3182 - accuracy: 0.9159 - val_loss: 0.4002 - val_accuracy: 0.8820\n",
      "Epoch 289/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3306 - accuracy: 0.9077\n",
      "Epoch 00289: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3304 - accuracy: 0.9081 - val_loss: 0.3923 - val_accuracy: 0.8848\n",
      "Epoch 290/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.9082\n",
      "Epoch 00290: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3322 - accuracy: 0.9071 - val_loss: 0.3975 - val_accuracy: 0.8744\n",
      "Epoch 291/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.9070\n",
      "Epoch 00291: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3223 - accuracy: 0.9071 - val_loss: 0.4017 - val_accuracy: 0.8829\n",
      "Epoch 292/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.9089\n",
      "Epoch 00292: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3252 - accuracy: 0.9086 - val_loss: 0.3937 - val_accuracy: 0.8829\n",
      "Epoch 293/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.9065\n",
      "Epoch 00293: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3282 - accuracy: 0.9067 - val_loss: 0.3953 - val_accuracy: 0.8848\n",
      "Epoch 294/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.9136\n",
      "Epoch 00294: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3232 - accuracy: 0.9135 - val_loss: 0.4129 - val_accuracy: 0.8772\n",
      "Epoch 295/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3297 - accuracy: 0.9046\n",
      "Epoch 00295: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3298 - accuracy: 0.9041 - val_loss: 0.3984 - val_accuracy: 0.8801\n",
      "Epoch 296/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.9110\n",
      "Epoch 00296: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3235 - accuracy: 0.9109 - val_loss: 0.4011 - val_accuracy: 0.8782\n",
      "Epoch 297/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.9120\n",
      "Epoch 00297: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3201 - accuracy: 0.9116 - val_loss: 0.4085 - val_accuracy: 0.8829\n",
      "Epoch 298/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.9067\n",
      "Epoch 00298: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3273 - accuracy: 0.9069 - val_loss: 0.4073 - val_accuracy: 0.8735\n",
      "Epoch 299/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.9117\n",
      "Epoch 00299: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3224 - accuracy: 0.9116 - val_loss: 0.3946 - val_accuracy: 0.8820\n",
      "Epoch 300/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3190 - accuracy: 0.9122\n",
      "Epoch 00300: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3193 - accuracy: 0.9119 - val_loss: 0.3981 - val_accuracy: 0.8801\n",
      "Epoch 301/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.9070\n",
      "Epoch 00301: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3251 - accuracy: 0.9071 - val_loss: 0.4024 - val_accuracy: 0.8706\n",
      "Epoch 302/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.9072\n",
      "Epoch 00302: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3232 - accuracy: 0.9078 - val_loss: 0.3897 - val_accuracy: 0.8791\n",
      "Epoch 303/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3172 - accuracy: 0.9086\n",
      "Epoch 00303: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3179 - accuracy: 0.9083 - val_loss: 0.3959 - val_accuracy: 0.8867\n",
      "Epoch 304/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.9129\n",
      "Epoch 00304: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3229 - accuracy: 0.9123 - val_loss: 0.4086 - val_accuracy: 0.8772\n",
      "Epoch 305/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.9132\n",
      "Epoch 00305: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3213 - accuracy: 0.9138 - val_loss: 0.4479 - val_accuracy: 0.8546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3298 - accuracy: 0.9053\n",
      "Epoch 00306: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3293 - accuracy: 0.9055 - val_loss: 0.4108 - val_accuracy: 0.8716\n",
      "Epoch 307/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.9148\n",
      "Epoch 00307: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3201 - accuracy: 0.9149 - val_loss: 0.4227 - val_accuracy: 0.8725\n",
      "Epoch 308/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.9177\n",
      "Epoch 00308: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3147 - accuracy: 0.9175 - val_loss: 0.4029 - val_accuracy: 0.8820\n",
      "Epoch 309/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3128 - accuracy: 0.9153\n",
      "Epoch 00309: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3139 - accuracy: 0.9145 - val_loss: 0.3969 - val_accuracy: 0.8857\n",
      "Epoch 310/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.9077\n",
      "Epoch 00310: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3246 - accuracy: 0.9083 - val_loss: 0.4291 - val_accuracy: 0.8650\n",
      "Epoch 311/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.9136\n",
      "Epoch 00311: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3127 - accuracy: 0.9138 - val_loss: 0.3919 - val_accuracy: 0.8857\n",
      "Epoch 312/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3141 - accuracy: 0.9167\n",
      "Epoch 00312: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3144 - accuracy: 0.9159 - val_loss: 0.4097 - val_accuracy: 0.8716\n",
      "Epoch 313/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.9079\n",
      "Epoch 00313: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3245 - accuracy: 0.9074 - val_loss: 0.4194 - val_accuracy: 0.8754\n",
      "Epoch 314/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.9136\n",
      "Epoch 00314: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3236 - accuracy: 0.9140 - val_loss: 0.4516 - val_accuracy: 0.8584\n",
      "Epoch 315/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.9125\n",
      "Epoch 00315: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3232 - accuracy: 0.9130 - val_loss: 0.3897 - val_accuracy: 0.8839\n",
      "Epoch 316/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.9177\n",
      "Epoch 00316: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3135 - accuracy: 0.9175 - val_loss: 0.3921 - val_accuracy: 0.8801\n",
      "Epoch 317/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.9146\n",
      "Epoch 00317: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3160 - accuracy: 0.9149 - val_loss: 0.3844 - val_accuracy: 0.8867\n",
      "Epoch 318/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.9096\n",
      "Epoch 00318: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3158 - accuracy: 0.9100 - val_loss: 0.3970 - val_accuracy: 0.8829\n",
      "Epoch 319/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.9058\n",
      "Epoch 00319: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3293 - accuracy: 0.9057 - val_loss: 0.4036 - val_accuracy: 0.8782\n",
      "Epoch 320/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.9170\n",
      "Epoch 00320: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3148 - accuracy: 0.9171 - val_loss: 0.3931 - val_accuracy: 0.8839\n",
      "Epoch 321/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.9103\n",
      "Epoch 00321: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3204 - accuracy: 0.9100 - val_loss: 0.4003 - val_accuracy: 0.8791\n",
      "Epoch 322/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3128 - accuracy: 0.9125\n",
      "Epoch 00322: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3133 - accuracy: 0.9116 - val_loss: 0.4184 - val_accuracy: 0.8754\n",
      "Epoch 323/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.9141\n",
      "Epoch 00323: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3091 - accuracy: 0.9145 - val_loss: 0.4057 - val_accuracy: 0.8801\n",
      "Epoch 324/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.9139\n",
      "Epoch 00324: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3095 - accuracy: 0.9140 - val_loss: 0.3905 - val_accuracy: 0.8848\n",
      "Epoch 325/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.9160\n",
      "Epoch 00325: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3143 - accuracy: 0.9161 - val_loss: 0.3890 - val_accuracy: 0.8839\n",
      "Epoch 326/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.9117\n",
      "Epoch 00326: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3155 - accuracy: 0.9121 - val_loss: 0.3856 - val_accuracy: 0.8848\n",
      "Epoch 327/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.9213\n",
      "Epoch 00327: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3087 - accuracy: 0.9213 - val_loss: 0.4036 - val_accuracy: 0.8754\n",
      "Epoch 328/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3232 - accuracy: 0.9096\n",
      "Epoch 00328: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3223 - accuracy: 0.9102 - val_loss: 0.4288 - val_accuracy: 0.8631\n",
      "Epoch 329/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.9103\n",
      "Epoch 00329: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3196 - accuracy: 0.9104 - val_loss: 0.3979 - val_accuracy: 0.8754\n",
      "Epoch 330/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.9086\n",
      "Epoch 00330: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3188 - accuracy: 0.9093 - val_loss: 0.4132 - val_accuracy: 0.8772\n",
      "Epoch 331/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.9170\n",
      "Epoch 00331: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3134 - accuracy: 0.9168 - val_loss: 0.3940 - val_accuracy: 0.8801\n",
      "Epoch 332/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3186 - accuracy: 0.9136\n",
      "Epoch 00332: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3191 - accuracy: 0.9133 - val_loss: 0.4281 - val_accuracy: 0.8678\n",
      "Epoch 333/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.9187\n",
      "Epoch 00333: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3075 - accuracy: 0.9185 - val_loss: 0.3923 - val_accuracy: 0.8914\n",
      "Epoch 334/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.9187\n",
      "Epoch 00334: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3092 - accuracy: 0.9187 - val_loss: 0.4090 - val_accuracy: 0.8754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3161 - accuracy: 0.9101\n",
      "Epoch 00335: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3156 - accuracy: 0.9104 - val_loss: 0.4024 - val_accuracy: 0.8763\n",
      "Epoch 336/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.9148\n",
      "Epoch 00336: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3093 - accuracy: 0.9149 - val_loss: 0.4243 - val_accuracy: 0.8697\n",
      "Epoch 337/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3177 - accuracy: 0.9108\n",
      "Epoch 00337: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3174 - accuracy: 0.9112 - val_loss: 0.3980 - val_accuracy: 0.8810\n",
      "Epoch 338/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3140 - accuracy: 0.9103\n",
      "Epoch 00338: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3142 - accuracy: 0.9100 - val_loss: 0.3919 - val_accuracy: 0.8829\n",
      "Epoch 339/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.9189\n",
      "Epoch 00339: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3116 - accuracy: 0.9194 - val_loss: 0.4014 - val_accuracy: 0.8810\n",
      "Epoch 340/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.9165\n",
      "Epoch 00340: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3115 - accuracy: 0.9154 - val_loss: 0.3850 - val_accuracy: 0.8848\n",
      "Epoch 341/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.9134\n",
      "Epoch 00341: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3168 - accuracy: 0.9138 - val_loss: 0.4078 - val_accuracy: 0.8791\n",
      "Epoch 342/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.9141\n",
      "Epoch 00342: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3103 - accuracy: 0.9142 - val_loss: 0.3916 - val_accuracy: 0.8791\n",
      "Epoch 343/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.9163\n",
      "Epoch 00343: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3071 - accuracy: 0.9166 - val_loss: 0.3906 - val_accuracy: 0.8829\n",
      "Epoch 344/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.9251\n",
      "Epoch 00344: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3039 - accuracy: 0.9251 - val_loss: 0.4208 - val_accuracy: 0.8716\n",
      "Epoch 345/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3145 - accuracy: 0.9146\n",
      "Epoch 00345: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3141 - accuracy: 0.9149 - val_loss: 0.4031 - val_accuracy: 0.8801\n",
      "Epoch 346/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.9167\n",
      "Epoch 00346: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3075 - accuracy: 0.9166 - val_loss: 0.3951 - val_accuracy: 0.8782\n",
      "Epoch 347/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.9129\n",
      "Epoch 00347: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3062 - accuracy: 0.9135 - val_loss: 0.4108 - val_accuracy: 0.8782\n",
      "Epoch 348/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.9170\n",
      "Epoch 00348: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3121 - accuracy: 0.9175 - val_loss: 0.4217 - val_accuracy: 0.8763\n",
      "Epoch 349/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.9158\n",
      "Epoch 00349: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3097 - accuracy: 0.9159 - val_loss: 0.3910 - val_accuracy: 0.8867\n",
      "Epoch 350/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.9134\n",
      "Epoch 00350: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3097 - accuracy: 0.9135 - val_loss: 0.3874 - val_accuracy: 0.8848\n",
      "Epoch 351/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.9156\n",
      "Epoch 00351: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3101 - accuracy: 0.9161 - val_loss: 0.3835 - val_accuracy: 0.8839\n",
      "Epoch 352/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.9239\n",
      "Epoch 00352: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3025 - accuracy: 0.9239 - val_loss: 0.4257 - val_accuracy: 0.8735\n",
      "Epoch 353/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.9127\n",
      "Epoch 00353: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3167 - accuracy: 0.9130 - val_loss: 0.3960 - val_accuracy: 0.8829\n",
      "Epoch 354/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.9191\n",
      "Epoch 00354: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3109 - accuracy: 0.9182 - val_loss: 0.4088 - val_accuracy: 0.8763\n",
      "Epoch 355/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.9153\n",
      "Epoch 00355: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3115 - accuracy: 0.9149 - val_loss: 0.4111 - val_accuracy: 0.8820\n",
      "Epoch 356/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.9079\n",
      "Epoch 00356: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3155 - accuracy: 0.9081 - val_loss: 0.3926 - val_accuracy: 0.8820\n",
      "Epoch 357/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.9189\n",
      "Epoch 00357: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3086 - accuracy: 0.9187 - val_loss: 0.3911 - val_accuracy: 0.8820\n",
      "Epoch 358/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.9163\n",
      "Epoch 00358: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3059 - accuracy: 0.9168 - val_loss: 0.4048 - val_accuracy: 0.8829\n",
      "Epoch 359/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3124 - accuracy: 0.9108\n",
      "Epoch 00359: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3135 - accuracy: 0.9102 - val_loss: 0.4091 - val_accuracy: 0.8782\n",
      "Epoch 360/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.9175\n",
      "Epoch 00360: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3093 - accuracy: 0.9173 - val_loss: 0.4044 - val_accuracy: 0.8886\n",
      "Epoch 361/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.9201\n",
      "Epoch 00361: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3069 - accuracy: 0.9204 - val_loss: 0.4290 - val_accuracy: 0.8754\n",
      "Epoch 362/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.9167\n",
      "Epoch 00362: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3062 - accuracy: 0.9161 - val_loss: 0.3994 - val_accuracy: 0.8801\n",
      "Epoch 363/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.9196\n",
      "Epoch 00363: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3012 - accuracy: 0.9194 - val_loss: 0.3976 - val_accuracy: 0.8782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.9120\n",
      "Epoch 00364: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3129 - accuracy: 0.9126 - val_loss: 0.4245 - val_accuracy: 0.8697\n",
      "Epoch 365/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3211 - accuracy: 0.9113\n",
      "Epoch 00365: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3217 - accuracy: 0.9107 - val_loss: 0.4301 - val_accuracy: 0.8744\n",
      "Epoch 366/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.9153\n",
      "Epoch 00366: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3097 - accuracy: 0.9156 - val_loss: 0.4030 - val_accuracy: 0.8791\n",
      "Epoch 367/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.9144\n",
      "Epoch 00367: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3133 - accuracy: 0.9138 - val_loss: 0.3891 - val_accuracy: 0.8791\n",
      "Epoch 368/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.9139\n",
      "Epoch 00368: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3132 - accuracy: 0.9133 - val_loss: 0.4140 - val_accuracy: 0.8791\n",
      "Epoch 369/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.9222\n",
      "Epoch 00369: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3050 - accuracy: 0.9227 - val_loss: 0.4026 - val_accuracy: 0.8791\n",
      "Epoch 370/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.9225\n",
      "Epoch 00370: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3035 - accuracy: 0.9223 - val_loss: 0.4002 - val_accuracy: 0.8744\n",
      "Epoch 371/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.9165\n",
      "Epoch 00371: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3069 - accuracy: 0.9168 - val_loss: 0.3930 - val_accuracy: 0.8839\n",
      "Epoch 372/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.9182\n",
      "Epoch 00372: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3075 - accuracy: 0.9185 - val_loss: 0.3980 - val_accuracy: 0.8820\n",
      "Epoch 373/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.9201\n",
      "Epoch 00373: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3003 - accuracy: 0.9204 - val_loss: 0.4441 - val_accuracy: 0.8697\n",
      "Epoch 374/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.9175\n",
      "Epoch 00374: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3071 - accuracy: 0.9178 - val_loss: 0.4022 - val_accuracy: 0.8848\n",
      "Epoch 375/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3117 - accuracy: 0.9134\n",
      "Epoch 00375: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3107 - accuracy: 0.9140 - val_loss: 0.3943 - val_accuracy: 0.8839\n",
      "Epoch 376/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3124 - accuracy: 0.9096\n",
      "Epoch 00376: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3132 - accuracy: 0.9088 - val_loss: 0.4039 - val_accuracy: 0.8791\n",
      "Epoch 377/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.9191\n",
      "Epoch 00377: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2976 - accuracy: 0.9197 - val_loss: 0.4271 - val_accuracy: 0.8716\n",
      "Epoch 378/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3080 - accuracy: 0.9213\n",
      "Epoch 00378: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3080 - accuracy: 0.9213 - val_loss: 0.4037 - val_accuracy: 0.8839\n",
      "Epoch 379/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3147 - accuracy: 0.9127\n",
      "Epoch 00379: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3144 - accuracy: 0.9128 - val_loss: 0.4333 - val_accuracy: 0.8763\n",
      "Epoch 380/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.9177\n",
      "Epoch 00380: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3052 - accuracy: 0.9180 - val_loss: 0.3861 - val_accuracy: 0.8876\n",
      "Epoch 381/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3116 - accuracy: 0.9132\n",
      "Epoch 00381: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3116 - accuracy: 0.9130 - val_loss: 0.3931 - val_accuracy: 0.8839\n",
      "Epoch 382/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.9203\n",
      "Epoch 00382: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3024 - accuracy: 0.9204 - val_loss: 0.3933 - val_accuracy: 0.8820\n",
      "Epoch 383/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.9153\n",
      "Epoch 00383: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3097 - accuracy: 0.9156 - val_loss: 0.3918 - val_accuracy: 0.8839\n",
      "Epoch 384/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.9253\n",
      "Epoch 00384: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2960 - accuracy: 0.9256 - val_loss: 0.3941 - val_accuracy: 0.8857\n",
      "Epoch 385/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.9134\n",
      "Epoch 00385: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3081 - accuracy: 0.9135 - val_loss: 0.4167 - val_accuracy: 0.8810\n",
      "Epoch 386/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.9177\n",
      "Epoch 00386: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3022 - accuracy: 0.9182 - val_loss: 0.3972 - val_accuracy: 0.8857\n",
      "Epoch 387/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.9187\n",
      "Epoch 00387: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3014 - accuracy: 0.9187 - val_loss: 0.3897 - val_accuracy: 0.8876\n",
      "Epoch 388/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.9198\n",
      "Epoch 00388: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3027 - accuracy: 0.9204 - val_loss: 0.4862 - val_accuracy: 0.8461\n",
      "Epoch 389/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.9163\n",
      "Epoch 00389: val_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3027 - accuracy: 0.9161 - val_loss: 0.4322 - val_accuracy: 0.8735\n",
      "Epoch 390/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.9198\n",
      "Epoch 00390: val_accuracy improved from 0.89141 to 0.89518, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3079 - accuracy: 0.9197 - val_loss: 0.3986 - val_accuracy: 0.8952\n",
      "Epoch 391/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.9265\n",
      "Epoch 00391: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2930 - accuracy: 0.9267 - val_loss: 0.3973 - val_accuracy: 0.8839\n",
      "Epoch 392/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.9189\n",
      "Epoch 00392: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3031 - accuracy: 0.9187 - val_loss: 0.3952 - val_accuracy: 0.8848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.9220\n",
      "Epoch 00393: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2920 - accuracy: 0.9225 - val_loss: 0.4193 - val_accuracy: 0.8791\n",
      "Epoch 394/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.9198\n",
      "Epoch 00394: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2997 - accuracy: 0.9204 - val_loss: 0.3886 - val_accuracy: 0.8933\n",
      "Epoch 395/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.9232\n",
      "Epoch 00395: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2982 - accuracy: 0.9218 - val_loss: 0.3903 - val_accuracy: 0.8895\n",
      "Epoch 396/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.9234\n",
      "Epoch 00396: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3021 - accuracy: 0.9237 - val_loss: 0.3898 - val_accuracy: 0.8886\n",
      "Epoch 397/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.9182\n",
      "Epoch 00397: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3032 - accuracy: 0.9180 - val_loss: 0.4248 - val_accuracy: 0.8829\n",
      "Epoch 398/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.9163\n",
      "Epoch 00398: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3048 - accuracy: 0.9161 - val_loss: 0.4625 - val_accuracy: 0.8527\n",
      "Epoch 399/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.9158\n",
      "Epoch 00399: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3082 - accuracy: 0.9161 - val_loss: 0.3965 - val_accuracy: 0.8914\n",
      "Epoch 400/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.9177\n",
      "Epoch 00400: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3028 - accuracy: 0.9180 - val_loss: 0.3894 - val_accuracy: 0.8876\n",
      "Epoch 401/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.9148\n",
      "Epoch 00401: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3030 - accuracy: 0.9152 - val_loss: 0.4020 - val_accuracy: 0.8857\n",
      "Epoch 402/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.9237\n",
      "Epoch 00402: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2983 - accuracy: 0.9230 - val_loss: 0.3958 - val_accuracy: 0.8829\n",
      "Epoch 403/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.9197\n",
      "Epoch 00403: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3004 - accuracy: 0.9197 - val_loss: 0.4231 - val_accuracy: 0.8725\n",
      "Epoch 404/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.9162\n",
      "Epoch 00404: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3004 - accuracy: 0.9164 - val_loss: 0.3980 - val_accuracy: 0.8791\n",
      "Epoch 405/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.9204\n",
      "Epoch 00405: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.2994 - accuracy: 0.9216 - val_loss: 0.4388 - val_accuracy: 0.8716\n",
      "Epoch 406/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.9207\n",
      "Epoch 00406: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2978 - accuracy: 0.9206 - val_loss: 0.3900 - val_accuracy: 0.8867\n",
      "Epoch 407/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.9190\n",
      "Epoch 00407: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2970 - accuracy: 0.9190 - val_loss: 0.3928 - val_accuracy: 0.8876\n",
      "Epoch 408/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9168\n",
      "Epoch 00408: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3075 - accuracy: 0.9168 - val_loss: 0.3873 - val_accuracy: 0.8820\n",
      "Epoch 409/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.9231\n",
      "Epoch 00409: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.2965 - accuracy: 0.9230 - val_loss: 0.4126 - val_accuracy: 0.8810\n",
      "Epoch 410/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.9115\n",
      "Epoch 00410: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3094 - accuracy: 0.9114 - val_loss: 0.4342 - val_accuracy: 0.8659\n",
      "Epoch 411/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.9241\n",
      "Epoch 00411: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2929 - accuracy: 0.9246 - val_loss: 0.4116 - val_accuracy: 0.8857\n",
      "Epoch 412/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.9206\n",
      "Epoch 00412: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2999 - accuracy: 0.9204 - val_loss: 0.3999 - val_accuracy: 0.8876\n",
      "Epoch 413/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.9229\n",
      "Epoch 00413: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2939 - accuracy: 0.9230 - val_loss: 0.3964 - val_accuracy: 0.8876\n",
      "Epoch 414/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.9201\n",
      "Epoch 00414: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3033 - accuracy: 0.9204 - val_loss: 0.3892 - val_accuracy: 0.8848\n",
      "Epoch 415/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.9222\n",
      "Epoch 00415: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3026 - accuracy: 0.9218 - val_loss: 0.3956 - val_accuracy: 0.8829\n",
      "Epoch 416/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9275\n",
      "Epoch 00416: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2913 - accuracy: 0.9275 - val_loss: 0.4168 - val_accuracy: 0.8810\n",
      "Epoch 417/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.9237\n",
      "Epoch 00417: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2922 - accuracy: 0.9241 - val_loss: 0.3955 - val_accuracy: 0.8829\n",
      "Epoch 418/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.9218\n",
      "Epoch 00418: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2978 - accuracy: 0.9223 - val_loss: 0.3866 - val_accuracy: 0.8848\n",
      "Epoch 419/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.9249\n",
      "Epoch 00419: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2936 - accuracy: 0.9251 - val_loss: 0.4053 - val_accuracy: 0.8791\n",
      "Epoch 420/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.9234\n",
      "Epoch 00420: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2955 - accuracy: 0.9230 - val_loss: 0.3868 - val_accuracy: 0.8848\n",
      "Epoch 421/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.9213\n",
      "Epoch 00421: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3007 - accuracy: 0.9206 - val_loss: 0.3983 - val_accuracy: 0.8829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 422/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.9141\n",
      "Epoch 00422: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3080 - accuracy: 0.9138 - val_loss: 0.4057 - val_accuracy: 0.8876\n",
      "Epoch 423/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9253\n",
      "Epoch 00423: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2924 - accuracy: 0.9253 - val_loss: 0.3884 - val_accuracy: 0.8857\n",
      "Epoch 424/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9196\n",
      "Epoch 00424: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2989 - accuracy: 0.9201 - val_loss: 0.3879 - val_accuracy: 0.8876\n",
      "Epoch 425/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.9189\n",
      "Epoch 00425: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3021 - accuracy: 0.9187 - val_loss: 0.4307 - val_accuracy: 0.8612\n",
      "Epoch 426/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3080 - accuracy: 0.9146\n",
      "Epoch 00426: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3071 - accuracy: 0.9152 - val_loss: 0.4292 - val_accuracy: 0.8621\n",
      "Epoch 427/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.9220\n",
      "Epoch 00427: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2937 - accuracy: 0.9223 - val_loss: 0.3930 - val_accuracy: 0.8839\n",
      "Epoch 428/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.9175\n",
      "Epoch 00428: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2991 - accuracy: 0.9175 - val_loss: 0.4249 - val_accuracy: 0.8763\n",
      "Epoch 429/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.9184\n",
      "Epoch 00429: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2998 - accuracy: 0.9190 - val_loss: 0.3822 - val_accuracy: 0.8895\n",
      "Epoch 430/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.9234\n",
      "Epoch 00430: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2972 - accuracy: 0.9232 - val_loss: 0.3789 - val_accuracy: 0.8848\n",
      "Epoch 431/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.9232\n",
      "Epoch 00431: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2968 - accuracy: 0.9234 - val_loss: 0.4200 - val_accuracy: 0.8706\n",
      "Epoch 432/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9244\n",
      "Epoch 00432: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2902 - accuracy: 0.9246 - val_loss: 0.3840 - val_accuracy: 0.8895\n",
      "Epoch 433/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9253\n",
      "Epoch 00433: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2863 - accuracy: 0.9253 - val_loss: 0.3900 - val_accuracy: 0.8820\n",
      "Epoch 434/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.9177\n",
      "Epoch 00434: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2977 - accuracy: 0.9178 - val_loss: 0.4874 - val_accuracy: 0.8451\n",
      "Epoch 435/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.9127\n",
      "Epoch 00435: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3022 - accuracy: 0.9121 - val_loss: 0.3867 - val_accuracy: 0.8857\n",
      "Epoch 436/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.9232\n",
      "Epoch 00436: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2938 - accuracy: 0.9237 - val_loss: 0.3813 - val_accuracy: 0.8914\n",
      "Epoch 437/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9241\n",
      "Epoch 00437: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2933 - accuracy: 0.9246 - val_loss: 0.3812 - val_accuracy: 0.8895\n",
      "Epoch 438/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9246\n",
      "Epoch 00438: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2884 - accuracy: 0.9244 - val_loss: 0.4364 - val_accuracy: 0.8735\n",
      "Epoch 439/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9249\n",
      "Epoch 00439: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2928 - accuracy: 0.9246 - val_loss: 0.4132 - val_accuracy: 0.8744\n",
      "Epoch 440/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9256\n",
      "Epoch 00440: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2902 - accuracy: 0.9251 - val_loss: 0.4136 - val_accuracy: 0.8725\n",
      "Epoch 441/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9249\n",
      "Epoch 00441: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2915 - accuracy: 0.9253 - val_loss: 0.3877 - val_accuracy: 0.8839\n",
      "Epoch 442/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.9218\n",
      "Epoch 00442: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2921 - accuracy: 0.9220 - val_loss: 0.3829 - val_accuracy: 0.8876\n",
      "Epoch 443/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.9277\n",
      "Epoch 00443: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2849 - accuracy: 0.9277 - val_loss: 0.4186 - val_accuracy: 0.8744\n",
      "Epoch 444/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.9196\n",
      "Epoch 00444: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2960 - accuracy: 0.9192 - val_loss: 0.3968 - val_accuracy: 0.8867\n",
      "Epoch 445/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.9246\n",
      "Epoch 00445: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2913 - accuracy: 0.9251 - val_loss: 0.4019 - val_accuracy: 0.8791\n",
      "Epoch 446/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9225\n",
      "Epoch 00446: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2875 - accuracy: 0.9225 - val_loss: 0.4338 - val_accuracy: 0.8772\n",
      "Epoch 447/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9244\n",
      "Epoch 00447: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2905 - accuracy: 0.9239 - val_loss: 0.4433 - val_accuracy: 0.8763\n",
      "Epoch 448/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.9241\n",
      "Epoch 00448: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2962 - accuracy: 0.9246 - val_loss: 0.4014 - val_accuracy: 0.8857\n",
      "Epoch 449/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9225\n",
      "Epoch 00449: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2905 - accuracy: 0.9223 - val_loss: 0.3888 - val_accuracy: 0.8857\n",
      "Epoch 450/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9301\n",
      "Epoch 00450: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2828 - accuracy: 0.9303 - val_loss: 0.3909 - val_accuracy: 0.8895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.9241\n",
      "Epoch 00451: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2945 - accuracy: 0.9241 - val_loss: 0.3869 - val_accuracy: 0.8876\n",
      "Epoch 452/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.9239\n",
      "Epoch 00452: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2926 - accuracy: 0.9239 - val_loss: 0.3931 - val_accuracy: 0.8867\n",
      "Epoch 453/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.9170\n",
      "Epoch 00453: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3002 - accuracy: 0.9168 - val_loss: 0.4002 - val_accuracy: 0.8829\n",
      "Epoch 454/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.9215\n",
      "Epoch 00454: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2933 - accuracy: 0.9218 - val_loss: 0.3977 - val_accuracy: 0.8876\n",
      "Epoch 455/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.9179\n",
      "Epoch 00455: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2967 - accuracy: 0.9173 - val_loss: 0.3874 - val_accuracy: 0.8839\n",
      "Epoch 456/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9260\n",
      "Epoch 00456: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2866 - accuracy: 0.9263 - val_loss: 0.3946 - val_accuracy: 0.8876\n",
      "Epoch 457/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.9189\n",
      "Epoch 00457: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2964 - accuracy: 0.9185 - val_loss: 0.3973 - val_accuracy: 0.8839\n",
      "Epoch 458/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9256\n",
      "Epoch 00458: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2877 - accuracy: 0.9251 - val_loss: 0.3840 - val_accuracy: 0.8867\n",
      "Epoch 459/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.9241\n",
      "Epoch 00459: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2935 - accuracy: 0.9244 - val_loss: 0.3909 - val_accuracy: 0.8848\n",
      "Epoch 460/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9250\n",
      "Epoch 00460: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2885 - accuracy: 0.9256 - val_loss: 0.3970 - val_accuracy: 0.8886\n",
      "Epoch 461/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.9261\n",
      "Epoch 00461: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2912 - accuracy: 0.9265 - val_loss: 0.3795 - val_accuracy: 0.8905\n",
      "Epoch 462/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.9229\n",
      "Epoch 00462: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2963 - accuracy: 0.9232 - val_loss: 0.3791 - val_accuracy: 0.8924\n",
      "Epoch 463/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9239\n",
      "Epoch 00463: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2974 - accuracy: 0.9237 - val_loss: 0.3865 - val_accuracy: 0.8867\n",
      "Epoch 464/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.9327\n",
      "Epoch 00464: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2795 - accuracy: 0.9329 - val_loss: 0.4019 - val_accuracy: 0.8857\n",
      "Epoch 465/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.9241\n",
      "Epoch 00465: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2908 - accuracy: 0.9239 - val_loss: 0.4207 - val_accuracy: 0.8848\n",
      "Epoch 466/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9296\n",
      "Epoch 00466: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2859 - accuracy: 0.9293 - val_loss: 0.3917 - val_accuracy: 0.8876\n",
      "Epoch 467/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9232\n",
      "Epoch 00467: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2881 - accuracy: 0.9234 - val_loss: 0.4184 - val_accuracy: 0.8744\n",
      "Epoch 468/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9249\n",
      "Epoch 00468: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2893 - accuracy: 0.9249 - val_loss: 0.4324 - val_accuracy: 0.8716\n",
      "Epoch 469/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.9184\n",
      "Epoch 00469: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2906 - accuracy: 0.9187 - val_loss: 0.3907 - val_accuracy: 0.8876\n",
      "Epoch 470/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9203\n",
      "Epoch 00470: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2964 - accuracy: 0.9206 - val_loss: 0.4147 - val_accuracy: 0.8763\n",
      "Epoch 471/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9258\n",
      "Epoch 00471: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2878 - accuracy: 0.9263 - val_loss: 0.4115 - val_accuracy: 0.8791\n",
      "Epoch 472/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9227\n",
      "Epoch 00472: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2911 - accuracy: 0.9225 - val_loss: 0.4035 - val_accuracy: 0.8905\n",
      "Epoch 473/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9292\n",
      "Epoch 00473: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2850 - accuracy: 0.9296 - val_loss: 0.4407 - val_accuracy: 0.8716\n",
      "Epoch 474/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9289\n",
      "Epoch 00474: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2848 - accuracy: 0.9284 - val_loss: 0.4289 - val_accuracy: 0.8782\n",
      "Epoch 475/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.9225\n",
      "Epoch 00475: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2941 - accuracy: 0.9232 - val_loss: 0.3886 - val_accuracy: 0.8867\n",
      "Epoch 476/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.9265\n",
      "Epoch 00476: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2917 - accuracy: 0.9258 - val_loss: 0.3903 - val_accuracy: 0.8886\n",
      "Epoch 477/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.9281\n",
      "Epoch 00477: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2833 - accuracy: 0.9275 - val_loss: 0.3895 - val_accuracy: 0.8820\n",
      "Epoch 478/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.9206\n",
      "Epoch 00478: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2956 - accuracy: 0.9206 - val_loss: 0.4045 - val_accuracy: 0.8782\n",
      "Epoch 479/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9280\n",
      "Epoch 00479: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2875 - accuracy: 0.9284 - val_loss: 0.3844 - val_accuracy: 0.8829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.9210\n",
      "Epoch 00480: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2951 - accuracy: 0.9211 - val_loss: 0.4053 - val_accuracy: 0.8839\n",
      "Epoch 481/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.9227\n",
      "Epoch 00481: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2959 - accuracy: 0.9223 - val_loss: 0.3778 - val_accuracy: 0.8848\n",
      "Epoch 482/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9299\n",
      "Epoch 00482: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2833 - accuracy: 0.9301 - val_loss: 0.4009 - val_accuracy: 0.8848\n",
      "Epoch 483/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9253\n",
      "Epoch 00483: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2881 - accuracy: 0.9256 - val_loss: 0.4014 - val_accuracy: 0.8914\n",
      "Epoch 484/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9268\n",
      "Epoch 00484: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2888 - accuracy: 0.9260 - val_loss: 0.4019 - val_accuracy: 0.8810\n",
      "Epoch 485/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9265\n",
      "Epoch 00485: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2851 - accuracy: 0.9263 - val_loss: 0.4175 - val_accuracy: 0.8725\n",
      "Epoch 486/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9258\n",
      "Epoch 00486: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2881 - accuracy: 0.9256 - val_loss: 0.3794 - val_accuracy: 0.8867\n",
      "Epoch 487/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9296\n",
      "Epoch 00487: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2862 - accuracy: 0.9293 - val_loss: 0.3835 - val_accuracy: 0.8839\n",
      "Epoch 488/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9260\n",
      "Epoch 00488: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2840 - accuracy: 0.9263 - val_loss: 0.4283 - val_accuracy: 0.8772\n",
      "Epoch 489/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9241\n",
      "Epoch 00489: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2862 - accuracy: 0.9239 - val_loss: 0.3826 - val_accuracy: 0.8867\n",
      "Epoch 490/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.9282\n",
      "Epoch 00490: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2846 - accuracy: 0.9270 - val_loss: 0.3830 - val_accuracy: 0.8876\n",
      "Epoch 491/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.9272\n",
      "Epoch 00491: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2838 - accuracy: 0.9272 - val_loss: 0.3773 - val_accuracy: 0.8876\n",
      "Epoch 492/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9232\n",
      "Epoch 00492: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2917 - accuracy: 0.9223 - val_loss: 0.4285 - val_accuracy: 0.8801\n",
      "Epoch 493/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9249\n",
      "Epoch 00493: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2856 - accuracy: 0.9241 - val_loss: 0.3944 - val_accuracy: 0.8886\n",
      "Epoch 494/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2781 - accuracy: 0.9337\n",
      "Epoch 00494: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2781 - accuracy: 0.9336 - val_loss: 0.4354 - val_accuracy: 0.8782\n",
      "Epoch 495/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2792 - accuracy: 0.9306\n",
      "Epoch 00495: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2799 - accuracy: 0.9298 - val_loss: 0.4434 - val_accuracy: 0.8650\n",
      "Epoch 496/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9244\n",
      "Epoch 00496: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2889 - accuracy: 0.9241 - val_loss: 0.3894 - val_accuracy: 0.8876\n",
      "Epoch 497/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9232\n",
      "Epoch 00497: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2874 - accuracy: 0.9234 - val_loss: 0.3977 - val_accuracy: 0.8867\n",
      "Epoch 498/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.9337\n",
      "Epoch 00498: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2824 - accuracy: 0.9329 - val_loss: 0.4255 - val_accuracy: 0.8791\n",
      "Epoch 499/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9268\n",
      "Epoch 00499: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2831 - accuracy: 0.9270 - val_loss: 0.4108 - val_accuracy: 0.8820\n",
      "Epoch 500/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9253\n",
      "Epoch 00500: val_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2844 - accuracy: 0.9249 - val_loss: 0.3828 - val_accuracy: 0.8876\n"
     ]
    }
   ],
   "source": [
    "def nlp_lstm(w2v):\n",
    "    inputs = Input(shape=(X_train[0].shape[-1],))\n",
    "\n",
    "    embedding_layer = gensim_to_keras_embedding(w2v)\n",
    "    \n",
    "    embedding = embedding_layer(inputs)\n",
    "\n",
    "    lstm1 = LSTM(lstm_units,return_sequences=True, return_state=True, kernel_regularizer=l2(w_decay),recurrent_regularizer=l2(w_decay), dropout=dropout_rate)(embedding)\n",
    "    output = Dense(units=1, activation='sigmoid')(lstm1[1])\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = nlp_lstm(w2v_model)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./weight_cp/weight_lstm1.hdf5', save_freq=\"epoch\",  verbose=1, monitor='val_accuracy', save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(0.0001)\n",
    "model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=epochs_to_run, validation_data=(X_val, y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd2d3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97d4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF LSTM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89       661\n",
      "           1       0.90      0.87      0.89       662\n",
      "\n",
      "    accuracy                           0.89      1323\n",
      "   macro avg       0.89      0.89      0.89      1323\n",
      "weighted avg       0.89      0.89      0.89      1323\n",
      "\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./weight_cp/weight_lstm1.hdf5')\n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "y_pred = []\n",
    "for p in predictions:\n",
    "    y_pred.append(p[0])\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"CLASSIFICATION REPORT OF LSTM\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41679010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 80)\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     14114800    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200, 50), (N 30200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 200, 1)       51          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 200)          0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 200)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "before_split (Activation)       (None, 200)          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 20), (None,  0           before_split[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 80)           0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][1]          \n",
      "                                                                 tf_op_layer_split[0][8]          \n",
      "                                                                 tf_op_layer_split[0][9]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 8, 10, 1)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 8, 10, 2)     130         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 8, 10, 2)     8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 160)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "op_main (Dense)                 (None, 1)            51          lstm_1[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "op_conv (Dense)                 (None, 1)            161         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "avg (Average)                   (None, 1)            0           op_main[0][0]                    \n",
      "                                                                 op_conv[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,145,401\n",
      "Trainable params: 30,597\n",
      "Non-trainable params: 14,114,804\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 10.3484 - op_main_loss: 0.6989 - op_conv_loss: 0.6938 - avg_loss: 0.6916 - op_main_accuracy: 0.5302 - op_conv_accuracy: 0.4974 - avg_accuracy: 0.5357\n",
      "Epoch 00001: val_avg_accuracy improved from -inf to 0.55902, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 3s 20ms/step - loss: 10.3484 - op_main_loss: 0.6989 - op_conv_loss: 0.6938 - avg_loss: 0.6916 - op_main_accuracy: 0.5302 - op_conv_accuracy: 0.4974 - avg_accuracy: 0.5357 - val_loss: 9.3605 - val_op_main_loss: 0.6829 - val_op_conv_loss: 0.6931 - val_avg_loss: 0.6842 - val_op_main_accuracy: 0.5609 - val_op_conv_accuracy: 0.5014 - val_avg_accuracy: 0.5590\n",
      "Epoch 2/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 8.5184 - op_main_loss: 0.6624 - op_conv_loss: 0.6930 - avg_loss: 0.6738 - op_main_accuracy: 0.6004 - op_conv_accuracy: 0.4950 - avg_accuracy: 0.6013\n",
      "Epoch 00002: val_avg_accuracy improved from 0.55902 to 0.61095, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 8.5168 - op_main_loss: 0.6622 - op_conv_loss: 0.6930 - avg_loss: 0.6738 - op_main_accuracy: 0.6004 - op_conv_accuracy: 0.4950 - avg_accuracy: 0.6016 - val_loss: 7.7278 - val_op_main_loss: 0.6519 - val_op_conv_loss: 0.6931 - val_avg_loss: 0.6679 - val_op_main_accuracy: 0.6100 - val_op_conv_accuracy: 0.5042 - val_avg_accuracy: 0.6110\n",
      "Epoch 3/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 7.0566 - op_main_loss: 0.6346 - op_conv_loss: 0.6929 - avg_loss: 0.6592 - op_main_accuracy: 0.6517 - op_conv_accuracy: 0.4939 - avg_accuracy: 0.6534\n",
      "Epoch 00003: val_avg_accuracy improved from 0.61095 to 0.65628, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 7.0566 - op_main_loss: 0.6346 - op_conv_loss: 0.6929 - avg_loss: 0.6592 - op_main_accuracy: 0.6517 - op_conv_accuracy: 0.4939 - avg_accuracy: 0.6534 - val_loss: 6.4292 - val_op_main_loss: 0.6250 - val_op_conv_loss: 0.6931 - val_avg_loss: 0.6529 - val_op_main_accuracy: 0.6591 - val_op_conv_accuracy: 0.5042 - val_avg_accuracy: 0.6563\n",
      "Epoch 4/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 5.8924 - op_main_loss: 0.6053 - op_conv_loss: 0.6930 - avg_loss: 0.6431 - op_main_accuracy: 0.6883 - op_conv_accuracy: 0.5005 - avg_accuracy: 0.6890\n",
      "Epoch 00004: val_avg_accuracy improved from 0.65628 to 0.68272, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 5.8924 - op_main_loss: 0.6053 - op_conv_loss: 0.6930 - avg_loss: 0.6431 - op_main_accuracy: 0.6883 - op_conv_accuracy: 0.5005 - avg_accuracy: 0.6890 - val_loss: 5.4039 - val_op_main_loss: 0.6015 - val_op_conv_loss: 0.6932 - val_avg_loss: 0.6388 - val_op_main_accuracy: 0.6799 - val_op_conv_accuracy: 0.4967 - val_avg_accuracy: 0.6827\n",
      "Epoch 5/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 4.9782 - op_main_loss: 0.5821 - op_conv_loss: 0.6929 - avg_loss: 0.6295 - op_main_accuracy: 0.7134 - op_conv_accuracy: 0.4983 - avg_accuracy: 0.7160\n",
      "Epoch 00005: val_avg_accuracy improved from 0.68272 to 0.70727, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 4.9782 - op_main_loss: 0.5821 - op_conv_loss: 0.6929 - avg_loss: 0.6295 - op_main_accuracy: 0.7134 - op_conv_accuracy: 0.4983 - avg_accuracy: 0.7160 - val_loss: 4.5927 - val_op_main_loss: 0.5762 - val_op_conv_loss: 0.6932 - val_avg_loss: 0.6242 - val_op_main_accuracy: 0.7073 - val_op_conv_accuracy: 0.4948 - val_avg_accuracy: 0.7073\n",
      "Epoch 6/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 4.2549 - op_main_loss: 0.5561 - op_conv_loss: 0.6928 - avg_loss: 0.6140 - op_main_accuracy: 0.7415 - op_conv_accuracy: 0.5095 - avg_accuracy: 0.7450\n",
      "Epoch 00006: val_avg_accuracy improved from 0.70727 to 0.72238, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 4.2549 - op_main_loss: 0.5561 - op_conv_loss: 0.6928 - avg_loss: 0.6140 - op_main_accuracy: 0.7415 - op_conv_accuracy: 0.5095 - avg_accuracy: 0.7450 - val_loss: 3.9565 - val_op_main_loss: 0.5519 - val_op_conv_loss: 0.6932 - val_avg_loss: 0.6090 - val_op_main_accuracy: 0.7224 - val_op_conv_accuracy: 0.4958 - val_avg_accuracy: 0.7224\n",
      "Epoch 7/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 3.6897 - op_main_loss: 0.5319 - op_conv_loss: 0.6931 - avg_loss: 0.5990 - op_main_accuracy: 0.7521 - op_conv_accuracy: 0.4858 - avg_accuracy: 0.7521\n",
      "Epoch 00007: val_avg_accuracy improved from 0.72238 to 0.73182, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 3.6897 - op_main_loss: 0.5319 - op_conv_loss: 0.6931 - avg_loss: 0.5990 - op_main_accuracy: 0.7521 - op_conv_accuracy: 0.4858 - avg_accuracy: 0.7521 - val_loss: 3.4560 - val_op_main_loss: 0.5268 - val_op_conv_loss: 0.6932 - val_avg_loss: 0.5933 - val_op_main_accuracy: 0.7356 - val_op_conv_accuracy: 0.4939 - val_avg_accuracy: 0.7318\n",
      "Epoch 8/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 3.2529 - op_main_loss: 0.5118 - op_conv_loss: 0.6929 - avg_loss: 0.5861 - op_main_accuracy: 0.7609 - op_conv_accuracy: 0.5028 - avg_accuracy: 0.7625\n",
      "Epoch 00008: val_avg_accuracy improved from 0.73182 to 0.74788, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 3.2529 - op_main_loss: 0.5118 - op_conv_loss: 0.6929 - avg_loss: 0.5861 - op_main_accuracy: 0.7609 - op_conv_accuracy: 0.5028 - avg_accuracy: 0.7625 - val_loss: 3.0721 - val_op_main_loss: 0.5064 - val_op_conv_loss: 0.6932 - val_avg_loss: 0.5792 - val_op_main_accuracy: 0.7479 - val_op_conv_accuracy: 0.4929 - val_avg_accuracy: 0.7479\n",
      "Epoch 9/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.9124 - op_main_loss: 0.4897 - op_conv_loss: 0.6929 - avg_loss: 0.5718 - op_main_accuracy: 0.7672 - op_conv_accuracy: 0.4962 - avg_accuracy: 0.7663\n",
      "Epoch 00009: val_avg_accuracy improved from 0.74788 to 0.76393, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 2.9124 - op_main_loss: 0.4897 - op_conv_loss: 0.6929 - avg_loss: 0.5718 - op_main_accuracy: 0.7672 - op_conv_accuracy: 0.4962 - avg_accuracy: 0.7663 - val_loss: 2.7735 - val_op_main_loss: 0.4840 - val_op_conv_loss: 0.6931 - val_avg_loss: 0.5654 - val_op_main_accuracy: 0.7630 - val_op_conv_accuracy: 0.4873 - val_avg_accuracy: 0.7639\n",
      "Epoch 10/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.6571 - op_main_loss: 0.4741 - op_conv_loss: 0.6928 - avg_loss: 0.5610 - op_main_accuracy: 0.7753 - op_conv_accuracy: 0.4962 - avg_accuracy: 0.7758\n",
      "Epoch 00010: val_avg_accuracy improved from 0.76393 to 0.76771, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 2.6571 - op_main_loss: 0.4741 - op_conv_loss: 0.6928 - avg_loss: 0.5610 - op_main_accuracy: 0.7753 - op_conv_accuracy: 0.4962 - avg_accuracy: 0.7758 - val_loss: 2.5699 - val_op_main_loss: 0.4841 - val_op_conv_loss: 0.6930 - val_avg_loss: 0.5591 - val_op_main_accuracy: 0.7686 - val_op_conv_accuracy: 0.4835 - val_avg_accuracy: 0.7677\n",
      "Epoch 11/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.4592 - op_main_loss: 0.4597 - op_conv_loss: 0.6916 - avg_loss: 0.5505 - op_main_accuracy: 0.7885 - op_conv_accuracy: 0.5328 - avg_accuracy: 0.7878\n",
      "Epoch 00011: val_avg_accuracy did not improve from 0.76771\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.4592 - op_main_loss: 0.4597 - op_conv_loss: 0.6916 - avg_loss: 0.5505 - op_main_accuracy: 0.7885 - op_conv_accuracy: 0.5328 - avg_accuracy: 0.7878 - val_loss: 2.4277 - val_op_main_loss: 0.4922 - val_op_conv_loss: 0.6914 - val_avg_loss: 0.5561 - val_op_main_accuracy: 0.7535 - val_op_conv_accuracy: 0.4929 - val_avg_accuracy: 0.7583\n",
      "Epoch 12/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.3036 - op_main_loss: 0.4500 - op_conv_loss: 0.6819 - avg_loss: 0.5400 - op_main_accuracy: 0.7970 - op_conv_accuracy: 0.5912 - avg_accuracy: 0.7970\n",
      "Epoch 00012: val_avg_accuracy improved from 0.76771 to 0.79037, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 2.3036 - op_main_loss: 0.4500 - op_conv_loss: 0.6819 - avg_loss: 0.5400 - op_main_accuracy: 0.7970 - op_conv_accuracy: 0.5912 - avg_accuracy: 0.7970 - val_loss: 2.2161 - val_op_main_loss: 0.4461 - val_op_conv_loss: 0.6608 - val_avg_loss: 0.5274 - val_op_main_accuracy: 0.7951 - val_op_conv_accuracy: 0.7129 - val_avg_accuracy: 0.7904\n",
      "Epoch 13/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.1013 - op_main_loss: 0.4375 - op_conv_loss: 0.6107 - avg_loss: 0.5055 - op_main_accuracy: 0.7991 - op_conv_accuracy: 0.6959 - avg_accuracy: 0.7975\n",
      "Epoch 00013: val_avg_accuracy improved from 0.79037 to 0.79320, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 2.1013 - op_main_loss: 0.4375 - op_conv_loss: 0.6107 - avg_loss: 0.5055 - op_main_accuracy: 0.7991 - op_conv_accuracy: 0.6959 - avg_accuracy: 0.7975 - val_loss: 1.9792 - val_op_main_loss: 0.4358 - val_op_conv_loss: 0.5462 - val_avg_loss: 0.4795 - val_op_main_accuracy: 0.7989 - val_op_conv_accuracy: 0.7479 - val_avg_accuracy: 0.7932\n",
      "Epoch 14/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.9168 - op_main_loss: 0.4305 - op_conv_loss: 0.5251 - avg_loss: 0.4670 - op_main_accuracy: 0.8062 - op_conv_accuracy: 0.7654 - avg_accuracy: 0.8010\n",
      "Epoch 00014: val_avg_accuracy improved from 0.79320 to 0.79887, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.9168 - op_main_loss: 0.4305 - op_conv_loss: 0.5251 - avg_loss: 0.4670 - op_main_accuracy: 0.8062 - op_conv_accuracy: 0.7654 - avg_accuracy: 0.8010 - val_loss: 1.8375 - val_op_main_loss: 0.4211 - val_op_conv_loss: 0.4943 - val_avg_loss: 0.4493 - val_op_main_accuracy: 0.8074 - val_op_conv_accuracy: 0.7800 - val_avg_accuracy: 0.7989\n",
      "Epoch 15/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.7945 - op_main_loss: 0.4173 - op_conv_loss: 0.4817 - avg_loss: 0.4413 - op_main_accuracy: 0.8152 - op_conv_accuracy: 0.7814 - avg_accuracy: 0.8072\n",
      "Epoch 00015: val_avg_accuracy improved from 0.79887 to 0.80264, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.7945 - op_main_loss: 0.4173 - op_conv_loss: 0.4817 - avg_loss: 0.4413 - op_main_accuracy: 0.8152 - op_conv_accuracy: 0.7814 - avg_accuracy: 0.8072 - val_loss: 1.7657 - val_op_main_loss: 0.4181 - val_op_conv_loss: 0.4724 - val_avg_loss: 0.4388 - val_op_main_accuracy: 0.8064 - val_op_conv_accuracy: 0.7885 - val_avg_accuracy: 0.8026\n",
      "Epoch 16/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.7387 - op_main_loss: 0.4155 - op_conv_loss: 0.4675 - avg_loss: 0.4336 - op_main_accuracy: 0.8133 - op_conv_accuracy: 0.7866 - avg_accuracy: 0.8072\n",
      "Epoch 00016: val_avg_accuracy did not improve from 0.80264\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.7387 - op_main_loss: 0.4155 - op_conv_loss: 0.4675 - avg_loss: 0.4336 - op_main_accuracy: 0.8133 - op_conv_accuracy: 0.7866 - avg_accuracy: 0.8072 - val_loss: 1.7135 - val_op_main_loss: 0.4174 - val_op_conv_loss: 0.4563 - val_avg_loss: 0.4312 - val_op_main_accuracy: 0.8111 - val_op_conv_accuracy: 0.7932 - val_avg_accuracy: 0.7989\n",
      "Epoch 17/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.6679 - op_main_loss: 0.4042 - op_conv_loss: 0.4463 - avg_loss: 0.4180 - op_main_accuracy: 0.8225 - op_conv_accuracy: 0.7963 - avg_accuracy: 0.8166\n",
      "Epoch 00017: val_avg_accuracy improved from 0.80264 to 0.81398, saving model to ./weight_cp\\weight_lstm2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 18ms/step - loss: 1.6679 - op_main_loss: 0.4042 - op_conv_loss: 0.4463 - avg_loss: 0.4180 - op_main_accuracy: 0.8225 - op_conv_accuracy: 0.7963 - avg_accuracy: 0.8166 - val_loss: 1.6454 - val_op_main_loss: 0.4025 - val_op_conv_loss: 0.4386 - val_avg_loss: 0.4149 - val_op_main_accuracy: 0.8178 - val_op_conv_accuracy: 0.8017 - val_avg_accuracy: 0.8140\n",
      "Epoch 18/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.6103 - op_main_loss: 0.3958 - op_conv_loss: 0.4272 - avg_loss: 0.4054 - op_main_accuracy: 0.8336 - op_conv_accuracy: 0.8110 - avg_accuracy: 0.8268\n",
      "Epoch 00018: val_avg_accuracy improved from 0.81398 to 0.82247, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.6103 - op_main_loss: 0.3958 - op_conv_loss: 0.4272 - avg_loss: 0.4054 - op_main_accuracy: 0.8336 - op_conv_accuracy: 0.8110 - avg_accuracy: 0.8268 - val_loss: 1.6093 - val_op_main_loss: 0.3970 - val_op_conv_loss: 0.4284 - val_avg_loss: 0.4080 - val_op_main_accuracy: 0.8196 - val_op_conv_accuracy: 0.8074 - val_avg_accuracy: 0.8225\n",
      "Epoch 19/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.5548 - op_main_loss: 0.3845 - op_conv_loss: 0.4085 - avg_loss: 0.3909 - op_main_accuracy: 0.8339 - op_conv_accuracy: 0.8204 - avg_accuracy: 0.8348\n",
      "Epoch 00019: val_avg_accuracy improved from 0.82247 to 0.83003, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.5548 - op_main_loss: 0.3845 - op_conv_loss: 0.4085 - avg_loss: 0.3909 - op_main_accuracy: 0.8339 - op_conv_accuracy: 0.8204 - avg_accuracy: 0.8348 - val_loss: 1.5602 - val_op_main_loss: 0.3879 - val_op_conv_loss: 0.4131 - val_avg_loss: 0.3946 - val_op_main_accuracy: 0.8329 - val_op_conv_accuracy: 0.8215 - val_avg_accuracy: 0.8300\n",
      "Epoch 20/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.5368 - op_main_loss: 0.3853 - op_conv_loss: 0.4042 - avg_loss: 0.3885 - op_main_accuracy: 0.8353 - op_conv_accuracy: 0.8230 - avg_accuracy: 0.8320\n",
      "Epoch 00020: val_avg_accuracy improved from 0.83003 to 0.83286, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.5368 - op_main_loss: 0.3853 - op_conv_loss: 0.4042 - avg_loss: 0.3885 - op_main_accuracy: 0.8353 - op_conv_accuracy: 0.8230 - avg_accuracy: 0.8320 - val_loss: 1.5286 - val_op_main_loss: 0.3829 - val_op_conv_loss: 0.4035 - val_avg_loss: 0.3881 - val_op_main_accuracy: 0.8347 - val_op_conv_accuracy: 0.8196 - val_avg_accuracy: 0.8329\n",
      "Epoch 21/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.5077 - op_main_loss: 0.3801 - op_conv_loss: 0.3948 - avg_loss: 0.3817 - op_main_accuracy: 0.8360 - op_conv_accuracy: 0.8233 - avg_accuracy: 0.8336\n",
      "Epoch 00021: val_avg_accuracy improved from 0.83286 to 0.83475, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.5077 - op_main_loss: 0.3801 - op_conv_loss: 0.3948 - avg_loss: 0.3817 - op_main_accuracy: 0.8360 - op_conv_accuracy: 0.8233 - avg_accuracy: 0.8336 - val_loss: 1.4965 - val_op_main_loss: 0.3767 - val_op_conv_loss: 0.3924 - val_avg_loss: 0.3798 - val_op_main_accuracy: 0.8404 - val_op_conv_accuracy: 0.8263 - val_avg_accuracy: 0.8347\n",
      "Epoch 22/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.4655 - op_main_loss: 0.3713 - op_conv_loss: 0.3798 - avg_loss: 0.3700 - op_main_accuracy: 0.8405 - op_conv_accuracy: 0.8334 - avg_accuracy: 0.8398\n",
      "Epoch 00022: val_avg_accuracy improved from 0.83475 to 0.84042, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.4655 - op_main_loss: 0.3713 - op_conv_loss: 0.3798 - avg_loss: 0.3700 - op_main_accuracy: 0.8405 - op_conv_accuracy: 0.8334 - avg_accuracy: 0.8398 - val_loss: 1.4747 - val_op_main_loss: 0.3732 - val_op_conv_loss: 0.3861 - val_avg_loss: 0.3744 - val_op_main_accuracy: 0.8423 - val_op_conv_accuracy: 0.8310 - val_avg_accuracy: 0.8404\n",
      "Epoch 23/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.4560 - op_main_loss: 0.3711 - op_conv_loss: 0.3778 - avg_loss: 0.3688 - op_main_accuracy: 0.8410 - op_conv_accuracy: 0.8341 - avg_accuracy: 0.8396\n",
      "Epoch 00023: val_avg_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4560 - op_main_loss: 0.3711 - op_conv_loss: 0.3778 - avg_loss: 0.3688 - op_main_accuracy: 0.8410 - op_conv_accuracy: 0.8341 - avg_accuracy: 0.8396 - val_loss: 1.4728 - val_op_main_loss: 0.3758 - val_op_conv_loss: 0.3867 - val_avg_loss: 0.3763 - val_op_main_accuracy: 0.8281 - val_op_conv_accuracy: 0.8291 - val_avg_accuracy: 0.8272\n",
      "Epoch 24/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.4351 - op_main_loss: 0.3687 - op_conv_loss: 0.3700 - avg_loss: 0.3636 - op_main_accuracy: 0.8443 - op_conv_accuracy: 0.8396 - avg_accuracy: 0.8459\n",
      "Epoch 00024: val_avg_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4351 - op_main_loss: 0.3687 - op_conv_loss: 0.3700 - avg_loss: 0.3636 - op_main_accuracy: 0.8443 - op_conv_accuracy: 0.8396 - avg_accuracy: 0.8459 - val_loss: 1.4461 - val_op_main_loss: 0.3681 - val_op_conv_loss: 0.3787 - val_avg_loss: 0.3683 - val_op_main_accuracy: 0.8423 - val_op_conv_accuracy: 0.8253 - val_avg_accuracy: 0.8404\n",
      "Epoch 25/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.4059 - op_main_loss: 0.3630 - op_conv_loss: 0.3577 - avg_loss: 0.3548 - op_main_accuracy: 0.8452 - op_conv_accuracy: 0.8478 - avg_accuracy: 0.8540\n",
      "Epoch 00025: val_avg_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4059 - op_main_loss: 0.3630 - op_conv_loss: 0.3577 - avg_loss: 0.3548 - op_main_accuracy: 0.8452 - op_conv_accuracy: 0.8478 - avg_accuracy: 0.8540 - val_loss: 1.4767 - val_op_main_loss: 0.3744 - val_op_conv_loss: 0.3937 - val_avg_loss: 0.3782 - val_op_main_accuracy: 0.8385 - val_op_conv_accuracy: 0.8291 - val_avg_accuracy: 0.8347\n",
      "Epoch 26/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.4073 - op_main_loss: 0.3651 - op_conv_loss: 0.3580 - avg_loss: 0.3556 - op_main_accuracy: 0.8455 - op_conv_accuracy: 0.8459 - avg_accuracy: 0.8483\n",
      "Epoch 00026: val_avg_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4073 - op_main_loss: 0.3651 - op_conv_loss: 0.3580 - avg_loss: 0.3556 - op_main_accuracy: 0.8455 - op_conv_accuracy: 0.8459 - avg_accuracy: 0.8483 - val_loss: 1.4292 - val_op_main_loss: 0.3665 - val_op_conv_loss: 0.3726 - val_avg_loss: 0.3635 - val_op_main_accuracy: 0.8442 - val_op_conv_accuracy: 0.8385 - val_avg_accuracy: 0.8404\n",
      "Epoch 27/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.3611 - op_main_loss: 0.3533 - op_conv_loss: 0.3405 - avg_loss: 0.3415 - op_main_accuracy: 0.8500 - op_conv_accuracy: 0.8483 - avg_accuracy: 0.8526\n",
      "Epoch 00027: val_avg_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3611 - op_main_loss: 0.3533 - op_conv_loss: 0.3405 - avg_loss: 0.3415 - op_main_accuracy: 0.8500 - op_conv_accuracy: 0.8483 - avg_accuracy: 0.8526 - val_loss: 1.4124 - val_op_main_loss: 0.3633 - val_op_conv_loss: 0.3653 - val_avg_loss: 0.3588 - val_op_main_accuracy: 0.8357 - val_op_conv_accuracy: 0.8395 - val_avg_accuracy: 0.8404\n",
      "Epoch 28/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.3461 - op_main_loss: 0.3494 - op_conv_loss: 0.3357 - avg_loss: 0.3371 - op_main_accuracy: 0.8533 - op_conv_accuracy: 0.8535 - avg_accuracy: 0.8573\n",
      "Epoch 00028: val_avg_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3461 - op_main_loss: 0.3494 - op_conv_loss: 0.3357 - avg_loss: 0.3371 - op_main_accuracy: 0.8533 - op_conv_accuracy: 0.8535 - avg_accuracy: 0.8573 - val_loss: 1.3905 - val_op_main_loss: 0.3581 - val_op_conv_loss: 0.3576 - val_avg_loss: 0.3519 - val_op_main_accuracy: 0.8423 - val_op_conv_accuracy: 0.8329 - val_avg_accuracy: 0.8395\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 1.3354 - op_main_loss: 0.3485 - op_conv_loss: 0.3310 - avg_loss: 0.3339 - op_main_accuracy: 0.8552 - op_conv_accuracy: 0.8533 - avg_accuracy: 0.8603\n",
      "Epoch 00029: val_avg_accuracy improved from 0.84042 to 0.85080, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.3354 - op_main_loss: 0.3485 - op_conv_loss: 0.3310 - avg_loss: 0.3339 - op_main_accuracy: 0.8552 - op_conv_accuracy: 0.8533 - avg_accuracy: 0.8603 - val_loss: 1.3799 - val_op_main_loss: 0.3568 - val_op_conv_loss: 0.3523 - val_avg_loss: 0.3491 - val_op_main_accuracy: 0.8414 - val_op_conv_accuracy: 0.8404 - val_avg_accuracy: 0.8508\n",
      "Epoch 30/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.3065 - op_main_loss: 0.3407 - op_conv_loss: 0.3201 - avg_loss: 0.3248 - op_main_accuracy: 0.8629 - op_conv_accuracy: 0.8648 - avg_accuracy: 0.8665\n",
      "Epoch 00030: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3065 - op_main_loss: 0.3407 - op_conv_loss: 0.3201 - avg_loss: 0.3248 - op_main_accuracy: 0.8629 - op_conv_accuracy: 0.8648 - avg_accuracy: 0.8665 - val_loss: 1.3654 - val_op_main_loss: 0.3531 - val_op_conv_loss: 0.3484 - val_avg_loss: 0.3448 - val_op_main_accuracy: 0.8461 - val_op_conv_accuracy: 0.8423 - val_avg_accuracy: 0.8499\n",
      "Epoch 31/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2979 - op_main_loss: 0.3399 - op_conv_loss: 0.3168 - avg_loss: 0.3223 - op_main_accuracy: 0.8601 - op_conv_accuracy: 0.8601 - avg_accuracy: 0.8648\n",
      "Epoch 00031: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2979 - op_main_loss: 0.3399 - op_conv_loss: 0.3168 - avg_loss: 0.3223 - op_main_accuracy: 0.8601 - op_conv_accuracy: 0.8601 - avg_accuracy: 0.8648 - val_loss: 1.3556 - val_op_main_loss: 0.3502 - val_op_conv_loss: 0.3459 - val_avg_loss: 0.3416 - val_op_main_accuracy: 0.8451 - val_op_conv_accuracy: 0.8404 - val_avg_accuracy: 0.8461\n",
      "Epoch 32/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2880 - op_main_loss: 0.3390 - op_conv_loss: 0.3120 - avg_loss: 0.3191 - op_main_accuracy: 0.8547 - op_conv_accuracy: 0.8686 - avg_accuracy: 0.8644\n",
      "Epoch 00032: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2880 - op_main_loss: 0.3390 - op_conv_loss: 0.3120 - avg_loss: 0.3191 - op_main_accuracy: 0.8547 - op_conv_accuracy: 0.8686 - avg_accuracy: 0.8644 - val_loss: 1.3556 - val_op_main_loss: 0.3480 - val_op_conv_loss: 0.3494 - val_avg_loss: 0.3412 - val_op_main_accuracy: 0.8442 - val_op_conv_accuracy: 0.8376 - val_avg_accuracy: 0.8508\n",
      "Epoch 33/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2658 - op_main_loss: 0.3342 - op_conv_loss: 0.3024 - avg_loss: 0.3119 - op_main_accuracy: 0.8606 - op_conv_accuracy: 0.8738 - avg_accuracy: 0.8679\n",
      "Epoch 00033: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2658 - op_main_loss: 0.3342 - op_conv_loss: 0.3024 - avg_loss: 0.3119 - op_main_accuracy: 0.8606 - op_conv_accuracy: 0.8738 - avg_accuracy: 0.8679 - val_loss: 1.4425 - val_op_main_loss: 0.3753 - val_op_conv_loss: 0.3808 - val_avg_loss: 0.3712 - val_op_main_accuracy: 0.8366 - val_op_conv_accuracy: 0.8291 - val_avg_accuracy: 0.8310\n",
      "Epoch 34/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2486 - op_main_loss: 0.3275 - op_conv_loss: 0.2985 - avg_loss: 0.3065 - op_main_accuracy: 0.8646 - op_conv_accuracy: 0.8736 - avg_accuracy: 0.8745\n",
      "Epoch 00034: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2486 - op_main_loss: 0.3275 - op_conv_loss: 0.2985 - avg_loss: 0.3065 - op_main_accuracy: 0.8646 - op_conv_accuracy: 0.8736 - avg_accuracy: 0.8745 - val_loss: 1.3468 - val_op_main_loss: 0.3495 - val_op_conv_loss: 0.3425 - val_avg_loss: 0.3391 - val_op_main_accuracy: 0.8480 - val_op_conv_accuracy: 0.8442 - val_avg_accuracy: 0.8489\n",
      "Epoch 35/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2425 - op_main_loss: 0.3276 - op_conv_loss: 0.2944 - avg_loss: 0.3047 - op_main_accuracy: 0.8627 - op_conv_accuracy: 0.8764 - avg_accuracy: 0.8741\n",
      "Epoch 00035: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2425 - op_main_loss: 0.3276 - op_conv_loss: 0.2944 - avg_loss: 0.3047 - op_main_accuracy: 0.8627 - op_conv_accuracy: 0.8764 - avg_accuracy: 0.8741 - val_loss: 1.3837 - val_op_main_loss: 0.3558 - val_op_conv_loss: 0.3612 - val_avg_loss: 0.3506 - val_op_main_accuracy: 0.8461 - val_op_conv_accuracy: 0.8357 - val_avg_accuracy: 0.8480\n",
      "Epoch 36/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2447 - op_main_loss: 0.3268 - op_conv_loss: 0.2977 - avg_loss: 0.3049 - op_main_accuracy: 0.8691 - op_conv_accuracy: 0.8736 - avg_accuracy: 0.8736\n",
      "Epoch 00036: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2447 - op_main_loss: 0.3268 - op_conv_loss: 0.2977 - avg_loss: 0.3049 - op_main_accuracy: 0.8691 - op_conv_accuracy: 0.8736 - avg_accuracy: 0.8736 - val_loss: 1.3183 - val_op_main_loss: 0.3423 - val_op_conv_loss: 0.3317 - val_avg_loss: 0.3298 - val_op_main_accuracy: 0.8480 - val_op_conv_accuracy: 0.8489 - val_avg_accuracy: 0.8461\n",
      "Epoch 37/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2268 - op_main_loss: 0.3252 - op_conv_loss: 0.2876 - avg_loss: 0.2996 - op_main_accuracy: 0.8663 - op_conv_accuracy: 0.8748 - avg_accuracy: 0.8755\n",
      "Epoch 00037: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2268 - op_main_loss: 0.3252 - op_conv_loss: 0.2876 - avg_loss: 0.2996 - op_main_accuracy: 0.8663 - op_conv_accuracy: 0.8748 - avg_accuracy: 0.8755 - val_loss: 1.3105 - val_op_main_loss: 0.3442 - val_op_conv_loss: 0.3253 - val_avg_loss: 0.3272 - val_op_main_accuracy: 0.8442 - val_op_conv_accuracy: 0.8602 - val_avg_accuracy: 0.8499\n",
      "Epoch 38/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2068 - op_main_loss: 0.3191 - op_conv_loss: 0.2806 - avg_loss: 0.2935 - op_main_accuracy: 0.8752 - op_conv_accuracy: 0.8823 - avg_accuracy: 0.8821\n",
      "Epoch 00038: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2068 - op_main_loss: 0.3191 - op_conv_loss: 0.2806 - avg_loss: 0.2935 - op_main_accuracy: 0.8752 - op_conv_accuracy: 0.8823 - avg_accuracy: 0.8821 - val_loss: 1.3030 - val_op_main_loss: 0.3383 - val_op_conv_loss: 0.3269 - val_avg_loss: 0.3238 - val_op_main_accuracy: 0.8489 - val_op_conv_accuracy: 0.8546 - val_avg_accuracy: 0.8508\n",
      "Epoch 39/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2053 - op_main_loss: 0.3192 - op_conv_loss: 0.2790 - avg_loss: 0.2921 - op_main_accuracy: 0.8637 - op_conv_accuracy: 0.8833 - avg_accuracy: 0.8781\n",
      "Epoch 00039: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2053 - op_main_loss: 0.3192 - op_conv_loss: 0.2790 - avg_loss: 0.2921 - op_main_accuracy: 0.8637 - op_conv_accuracy: 0.8833 - avg_accuracy: 0.8781 - val_loss: 1.2973 - val_op_main_loss: 0.3395 - val_op_conv_loss: 0.3211 - val_avg_loss: 0.3221 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8536 - val_avg_accuracy: 0.8499\n",
      "Epoch 40/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1850 - op_main_loss: 0.3138 - op_conv_loss: 0.2717 - avg_loss: 0.2856 - op_main_accuracy: 0.8712 - op_conv_accuracy: 0.8854 - avg_accuracy: 0.8800\n",
      "Epoch 00040: val_avg_accuracy improved from 0.85080 to 0.85175, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.1850 - op_main_loss: 0.3138 - op_conv_loss: 0.2717 - avg_loss: 0.2856 - op_main_accuracy: 0.8712 - op_conv_accuracy: 0.8854 - avg_accuracy: 0.8800 - val_loss: 1.3352 - val_op_main_loss: 0.3410 - val_op_conv_loss: 0.3461 - val_avg_loss: 0.3340 - val_op_main_accuracy: 0.8470 - val_op_conv_accuracy: 0.8432 - val_avg_accuracy: 0.8517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1781 - op_main_loss: 0.3134 - op_conv_loss: 0.2673 - avg_loss: 0.2832 - op_main_accuracy: 0.8691 - op_conv_accuracy: 0.8868 - avg_accuracy: 0.8835\n",
      "Epoch 00041: val_avg_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1781 - op_main_loss: 0.3134 - op_conv_loss: 0.2673 - avg_loss: 0.2832 - op_main_accuracy: 0.8691 - op_conv_accuracy: 0.8868 - avg_accuracy: 0.8835 - val_loss: 1.3438 - val_op_main_loss: 0.3520 - val_op_conv_loss: 0.3400 - val_avg_loss: 0.3381 - val_op_main_accuracy: 0.8442 - val_op_conv_accuracy: 0.8442 - val_avg_accuracy: 0.8489\n",
      "Epoch 42/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1538 - op_main_loss: 0.3037 - op_conv_loss: 0.2613 - avg_loss: 0.2747 - op_main_accuracy: 0.8823 - op_conv_accuracy: 0.8911 - avg_accuracy: 0.8927\n",
      "Epoch 00042: val_avg_accuracy improved from 0.85175 to 0.85458, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.1538 - op_main_loss: 0.3037 - op_conv_loss: 0.2613 - avg_loss: 0.2747 - op_main_accuracy: 0.8823 - op_conv_accuracy: 0.8911 - avg_accuracy: 0.8927 - val_loss: 1.2945 - val_op_main_loss: 0.3363 - val_op_conv_loss: 0.3232 - val_avg_loss: 0.3215 - val_op_main_accuracy: 0.8451 - val_op_conv_accuracy: 0.8527 - val_avg_accuracy: 0.8546\n",
      "Epoch 43/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1469 - op_main_loss: 0.3035 - op_conv_loss: 0.2562 - avg_loss: 0.2727 - op_main_accuracy: 0.8809 - op_conv_accuracy: 0.8956 - avg_accuracy: 0.8927\n",
      "Epoch 00043: val_avg_accuracy improved from 0.85458 to 0.86119, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.1469 - op_main_loss: 0.3035 - op_conv_loss: 0.2562 - avg_loss: 0.2727 - op_main_accuracy: 0.8809 - op_conv_accuracy: 0.8956 - avg_accuracy: 0.8927 - val_loss: 1.2974 - val_op_main_loss: 0.3376 - val_op_conv_loss: 0.3233 - val_avg_loss: 0.3222 - val_op_main_accuracy: 0.8508 - val_op_conv_accuracy: 0.8565 - val_avg_accuracy: 0.8612\n",
      "Epoch 44/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1530 - op_main_loss: 0.3055 - op_conv_loss: 0.2587 - avg_loss: 0.2747 - op_main_accuracy: 0.8778 - op_conv_accuracy: 0.8941 - avg_accuracy: 0.8932\n",
      "Epoch 00044: val_avg_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1530 - op_main_loss: 0.3055 - op_conv_loss: 0.2587 - avg_loss: 0.2747 - op_main_accuracy: 0.8778 - op_conv_accuracy: 0.8941 - avg_accuracy: 0.8932 - val_loss: 1.4138 - val_op_main_loss: 0.3669 - val_op_conv_loss: 0.3727 - val_avg_loss: 0.3605 - val_op_main_accuracy: 0.8470 - val_op_conv_accuracy: 0.8404 - val_avg_accuracy: 0.8451\n",
      "Epoch 45/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1340 - op_main_loss: 0.3038 - op_conv_loss: 0.2481 - avg_loss: 0.2684 - op_main_accuracy: 0.8778 - op_conv_accuracy: 0.8986 - avg_accuracy: 0.8946\n",
      "Epoch 00045: val_avg_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1340 - op_main_loss: 0.3038 - op_conv_loss: 0.2481 - avg_loss: 0.2684 - op_main_accuracy: 0.8778 - op_conv_accuracy: 0.8986 - avg_accuracy: 0.8946 - val_loss: 1.3701 - val_op_main_loss: 0.3470 - val_op_conv_loss: 0.3644 - val_avg_loss: 0.3446 - val_op_main_accuracy: 0.8489 - val_op_conv_accuracy: 0.8432 - val_avg_accuracy: 0.8461\n",
      "Epoch 46/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1505 - op_main_loss: 0.3047 - op_conv_loss: 0.2586 - avg_loss: 0.2733 - op_main_accuracy: 0.8750 - op_conv_accuracy: 0.8918 - avg_accuracy: 0.8894\n",
      "Epoch 00046: val_avg_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1505 - op_main_loss: 0.3047 - op_conv_loss: 0.2586 - avg_loss: 0.2733 - op_main_accuracy: 0.8750 - op_conv_accuracy: 0.8918 - avg_accuracy: 0.8894 - val_loss: 1.3299 - val_op_main_loss: 0.3501 - val_op_conv_loss: 0.3328 - val_avg_loss: 0.3329 - val_op_main_accuracy: 0.8470 - val_op_conv_accuracy: 0.8584 - val_avg_accuracy: 0.8536\n",
      "Epoch 47/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1352 - op_main_loss: 0.3004 - op_conv_loss: 0.2528 - avg_loss: 0.2687 - op_main_accuracy: 0.8811 - op_conv_accuracy: 0.8972 - avg_accuracy: 0.8972\n",
      "Epoch 00047: val_avg_accuracy improved from 0.86119 to 0.86402, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.1352 - op_main_loss: 0.3004 - op_conv_loss: 0.2528 - avg_loss: 0.2687 - op_main_accuracy: 0.8811 - op_conv_accuracy: 0.8972 - avg_accuracy: 0.8972 - val_loss: 1.2941 - val_op_main_loss: 0.3462 - val_op_conv_loss: 0.3162 - val_avg_loss: 0.3192 - val_op_main_accuracy: 0.8508 - val_op_conv_accuracy: 0.8612 - val_avg_accuracy: 0.8640\n",
      "Epoch 48/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1147 - op_main_loss: 0.2949 - op_conv_loss: 0.2446 - avg_loss: 0.2623 - op_main_accuracy: 0.8759 - op_conv_accuracy: 0.8956 - avg_accuracy: 0.8932\n",
      "Epoch 00048: val_avg_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1147 - op_main_loss: 0.2949 - op_conv_loss: 0.2446 - avg_loss: 0.2623 - op_main_accuracy: 0.8759 - op_conv_accuracy: 0.8956 - avg_accuracy: 0.8932 - val_loss: 1.3215 - val_op_main_loss: 0.3467 - val_op_conv_loss: 0.3321 - val_avg_loss: 0.3301 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8555 - val_avg_accuracy: 0.8536\n",
      "Epoch 49/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1303 - op_main_loss: 0.2994 - op_conv_loss: 0.2513 - avg_loss: 0.2667 - op_main_accuracy: 0.8785 - op_conv_accuracy: 0.8913 - avg_accuracy: 0.8915\n",
      "Epoch 00049: val_avg_accuracy improved from 0.86402 to 0.86591, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.1303 - op_main_loss: 0.2994 - op_conv_loss: 0.2513 - avg_loss: 0.2667 - op_main_accuracy: 0.8785 - op_conv_accuracy: 0.8913 - avg_accuracy: 0.8915 - val_loss: 1.2519 - val_op_main_loss: 0.3247 - val_op_conv_loss: 0.3084 - val_avg_loss: 0.3059 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8659\n",
      "Epoch 50/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1025 - op_main_loss: 0.2917 - op_conv_loss: 0.2398 - avg_loss: 0.2584 - op_main_accuracy: 0.8845 - op_conv_accuracy: 0.9000 - avg_accuracy: 0.8970\n",
      "Epoch 00050: val_avg_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1025 - op_main_loss: 0.2917 - op_conv_loss: 0.2398 - avg_loss: 0.2584 - op_main_accuracy: 0.8845 - op_conv_accuracy: 0.9000 - avg_accuracy: 0.8970 - val_loss: 1.2656 - val_op_main_loss: 0.3241 - val_op_conv_loss: 0.3181 - val_avg_loss: 0.3107 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8640\n",
      "Epoch 51/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0843 - op_main_loss: 0.2868 - op_conv_loss: 0.2326 - avg_loss: 0.2527 - op_main_accuracy: 0.8882 - op_conv_accuracy: 0.9036 - avg_accuracy: 0.9031\n",
      "Epoch 00051: val_avg_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0843 - op_main_loss: 0.2868 - op_conv_loss: 0.2326 - avg_loss: 0.2527 - op_main_accuracy: 0.8882 - op_conv_accuracy: 0.9036 - avg_accuracy: 0.9031 - val_loss: 1.3874 - val_op_main_loss: 0.3594 - val_op_conv_loss: 0.3642 - val_avg_loss: 0.3515 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8470 - val_avg_accuracy: 0.8517\n",
      "Epoch 52/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0833 - op_main_loss: 0.2859 - op_conv_loss: 0.2335 - avg_loss: 0.2517 - op_main_accuracy: 0.8873 - op_conv_accuracy: 0.9041 - avg_accuracy: 0.9008\n",
      "Epoch 00052: val_avg_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0833 - op_main_loss: 0.2859 - op_conv_loss: 0.2335 - avg_loss: 0.2517 - op_main_accuracy: 0.8873 - op_conv_accuracy: 0.9041 - avg_accuracy: 0.9008 - val_loss: 1.3747 - val_op_main_loss: 0.3572 - val_op_conv_loss: 0.3580 - val_avg_loss: 0.3469 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8536 - val_avg_accuracy: 0.8584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0931 - op_main_loss: 0.2918 - op_conv_loss: 0.2347 - avg_loss: 0.2547 - op_main_accuracy: 0.8816 - op_conv_accuracy: 0.9050 - avg_accuracy: 0.8996\n",
      "Epoch 00053: val_avg_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0931 - op_main_loss: 0.2918 - op_conv_loss: 0.2347 - avg_loss: 0.2547 - op_main_accuracy: 0.8816 - op_conv_accuracy: 0.9050 - avg_accuracy: 0.8996 - val_loss: 1.2548 - val_op_main_loss: 0.3241 - val_op_conv_loss: 0.3114 - val_avg_loss: 0.3078 - val_op_main_accuracy: 0.8565 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8621\n",
      "Epoch 54/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0756 - op_main_loss: 0.2855 - op_conv_loss: 0.2294 - avg_loss: 0.2493 - op_main_accuracy: 0.8885 - op_conv_accuracy: 0.9086 - avg_accuracy: 0.9017\n",
      "Epoch 00054: val_avg_accuracy improved from 0.86591 to 0.86780, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.0756 - op_main_loss: 0.2855 - op_conv_loss: 0.2294 - avg_loss: 0.2493 - op_main_accuracy: 0.8885 - op_conv_accuracy: 0.9086 - avg_accuracy: 0.9017 - val_loss: 1.2595 - val_op_main_loss: 0.3290 - val_op_conv_loss: 0.3100 - val_avg_loss: 0.3090 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8678\n",
      "Epoch 55/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0644 - op_main_loss: 0.2821 - op_conv_loss: 0.2249 - avg_loss: 0.2456 - op_main_accuracy: 0.8882 - op_conv_accuracy: 0.9123 - avg_accuracy: 0.9081\n",
      "Epoch 00055: val_avg_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0644 - op_main_loss: 0.2821 - op_conv_loss: 0.2249 - avg_loss: 0.2456 - op_main_accuracy: 0.8882 - op_conv_accuracy: 0.9123 - avg_accuracy: 0.9081 - val_loss: 1.2704 - val_op_main_loss: 0.3215 - val_op_conv_loss: 0.3257 - val_avg_loss: 0.3111 - val_op_main_accuracy: 0.8499 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8602\n",
      "Epoch 56/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0392 - op_main_loss: 0.2747 - op_conv_loss: 0.2147 - avg_loss: 0.2369 - op_main_accuracy: 0.8889 - op_conv_accuracy: 0.9119 - avg_accuracy: 0.9076\n",
      "Epoch 00056: val_avg_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0392 - op_main_loss: 0.2747 - op_conv_loss: 0.2147 - avg_loss: 0.2369 - op_main_accuracy: 0.8889 - op_conv_accuracy: 0.9119 - avg_accuracy: 0.9076 - val_loss: 1.3528 - val_op_main_loss: 0.3301 - val_op_conv_loss: 0.3789 - val_avg_loss: 0.3314 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8414 - val_avg_accuracy: 0.8489\n",
      "Epoch 57/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0699 - op_main_loss: 0.2876 - op_conv_loss: 0.2233 - avg_loss: 0.2469 - op_main_accuracy: 0.8840 - op_conv_accuracy: 0.9064 - avg_accuracy: 0.9026\n",
      "Epoch 00057: val_avg_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0699 - op_main_loss: 0.2876 - op_conv_loss: 0.2233 - avg_loss: 0.2469 - op_main_accuracy: 0.8840 - op_conv_accuracy: 0.9064 - avg_accuracy: 0.9026 - val_loss: 1.2610 - val_op_main_loss: 0.3258 - val_op_conv_loss: 0.3145 - val_avg_loss: 0.3093 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8621\n",
      "Epoch 58/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0311 - op_main_loss: 0.2727 - op_conv_loss: 0.2119 - avg_loss: 0.2347 - op_main_accuracy: 0.8937 - op_conv_accuracy: 0.9149 - avg_accuracy: 0.9100\n",
      "Epoch 00058: val_avg_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0311 - op_main_loss: 0.2727 - op_conv_loss: 0.2119 - avg_loss: 0.2347 - op_main_accuracy: 0.8937 - op_conv_accuracy: 0.9149 - avg_accuracy: 0.9100 - val_loss: 1.2709 - val_op_main_loss: 0.3246 - val_op_conv_loss: 0.3224 - val_avg_loss: 0.3119 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8574\n",
      "Epoch 59/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0390 - op_main_loss: 0.2730 - op_conv_loss: 0.2169 - avg_loss: 0.2362 - op_main_accuracy: 0.8937 - op_conv_accuracy: 0.9126 - avg_accuracy: 0.9119\n",
      "Epoch 00059: val_avg_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0390 - op_main_loss: 0.2730 - op_conv_loss: 0.2169 - avg_loss: 0.2362 - op_main_accuracy: 0.8937 - op_conv_accuracy: 0.9126 - avg_accuracy: 0.9119 - val_loss: 1.3118 - val_op_main_loss: 0.3513 - val_op_conv_loss: 0.3224 - val_avg_loss: 0.3251 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8659\n",
      "Epoch 60/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0343 - op_main_loss: 0.2734 - op_conv_loss: 0.2134 - avg_loss: 0.2349 - op_main_accuracy: 0.8948 - op_conv_accuracy: 0.9138 - avg_accuracy: 0.9109\n",
      "Epoch 00060: val_avg_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0343 - op_main_loss: 0.2734 - op_conv_loss: 0.2134 - avg_loss: 0.2349 - op_main_accuracy: 0.8948 - op_conv_accuracy: 0.9138 - avg_accuracy: 0.9109 - val_loss: 1.3576 - val_op_main_loss: 0.3545 - val_op_conv_loss: 0.3504 - val_avg_loss: 0.3404 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8574 - val_avg_accuracy: 0.8555\n",
      "Epoch 61/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0434 - op_main_loss: 0.2779 - op_conv_loss: 0.2159 - avg_loss: 0.2379 - op_main_accuracy: 0.8889 - op_conv_accuracy: 0.9147 - avg_accuracy: 0.9102\n",
      "Epoch 00061: val_avg_accuracy did not improve from 0.86780\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0434 - op_main_loss: 0.2779 - op_conv_loss: 0.2159 - avg_loss: 0.2379 - op_main_accuracy: 0.8889 - op_conv_accuracy: 0.9147 - avg_accuracy: 0.9102 - val_loss: 1.3316 - val_op_main_loss: 0.3481 - val_op_conv_loss: 0.3403 - val_avg_loss: 0.3324 - val_op_main_accuracy: 0.8536 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8555\n",
      "Epoch 62/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0319 - op_main_loss: 0.2736 - op_conv_loss: 0.2125 - avg_loss: 0.2349 - op_main_accuracy: 0.8915 - op_conv_accuracy: 0.9135 - avg_accuracy: 0.9112\n",
      "Epoch 00062: val_avg_accuracy improved from 0.86780 to 0.88196, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.0319 - op_main_loss: 0.2736 - op_conv_loss: 0.2125 - avg_loss: 0.2349 - op_main_accuracy: 0.8915 - op_conv_accuracy: 0.9135 - avg_accuracy: 0.9112 - val_loss: 1.2252 - val_op_main_loss: 0.3137 - val_op_conv_loss: 0.3065 - val_avg_loss: 0.2939 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8820\n",
      "Epoch 63/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0238 - op_main_loss: 0.2704 - op_conv_loss: 0.2099 - avg_loss: 0.2318 - op_main_accuracy: 0.8904 - op_conv_accuracy: 0.9123 - avg_accuracy: 0.9095\n",
      "Epoch 00063: val_avg_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0238 - op_main_loss: 0.2704 - op_conv_loss: 0.2099 - avg_loss: 0.2318 - op_main_accuracy: 0.8904 - op_conv_accuracy: 0.9123 - avg_accuracy: 0.9095 - val_loss: 1.3700 - val_op_main_loss: 0.3629 - val_op_conv_loss: 0.3505 - val_avg_loss: 0.3448 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8536\n",
      "Epoch 64/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0258 - op_main_loss: 0.2720 - op_conv_loss: 0.2100 - avg_loss: 0.2321 - op_main_accuracy: 0.8892 - op_conv_accuracy: 0.9121 - avg_accuracy: 0.9100\n",
      "Epoch 00064: val_avg_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0258 - op_main_loss: 0.2720 - op_conv_loss: 0.2100 - avg_loss: 0.2321 - op_main_accuracy: 0.8892 - op_conv_accuracy: 0.9121 - avg_accuracy: 0.9100 - val_loss: 1.2546 - val_op_main_loss: 0.3237 - val_op_conv_loss: 0.3134 - val_avg_loss: 0.3068 - val_op_main_accuracy: 0.8555 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9948 - op_main_loss: 0.2629 - op_conv_loss: 0.1978 - avg_loss: 0.2223 - op_main_accuracy: 0.8948 - op_conv_accuracy: 0.9204 - avg_accuracy: 0.9154\n",
      "Epoch 00065: val_avg_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9948 - op_main_loss: 0.2629 - op_conv_loss: 0.1978 - avg_loss: 0.2223 - op_main_accuracy: 0.8948 - op_conv_accuracy: 0.9204 - avg_accuracy: 0.9154 - val_loss: 1.2294 - val_op_main_loss: 0.3136 - val_op_conv_loss: 0.3072 - val_avg_loss: 0.2957 - val_op_main_accuracy: 0.8565 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8772\n",
      "Epoch 66/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9976 - op_main_loss: 0.2637 - op_conv_loss: 0.1983 - avg_loss: 0.2236 - op_main_accuracy: 0.9003 - op_conv_accuracy: 0.9168 - avg_accuracy: 0.9138\n",
      "Epoch 00066: val_avg_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9976 - op_main_loss: 0.2637 - op_conv_loss: 0.1983 - avg_loss: 0.2236 - op_main_accuracy: 0.9003 - op_conv_accuracy: 0.9168 - avg_accuracy: 0.9138 - val_loss: 1.2374 - val_op_main_loss: 0.3180 - val_op_conv_loss: 0.3081 - val_avg_loss: 0.2997 - val_op_main_accuracy: 0.8593 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8754\n",
      "Epoch 67/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0199 - op_main_loss: 0.2705 - op_conv_loss: 0.2090 - avg_loss: 0.2295 - op_main_accuracy: 0.8932 - op_conv_accuracy: 0.9159 - avg_accuracy: 0.9107\n",
      "Epoch 00067: val_avg_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0199 - op_main_loss: 0.2705 - op_conv_loss: 0.2090 - avg_loss: 0.2295 - op_main_accuracy: 0.8932 - op_conv_accuracy: 0.9159 - avg_accuracy: 0.9107 - val_loss: 1.2335 - val_op_main_loss: 0.3106 - val_op_conv_loss: 0.3141 - val_avg_loss: 0.2980 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8687\n",
      "Epoch 68/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9775 - op_main_loss: 0.2584 - op_conv_loss: 0.1909 - avg_loss: 0.2167 - op_main_accuracy: 0.8989 - op_conv_accuracy: 0.9241 - avg_accuracy: 0.9178\n",
      "Epoch 00068: val_avg_accuracy improved from 0.88196 to 0.88291, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.9775 - op_main_loss: 0.2584 - op_conv_loss: 0.1909 - avg_loss: 0.2167 - op_main_accuracy: 0.8989 - op_conv_accuracy: 0.9241 - avg_accuracy: 0.9178 - val_loss: 1.2230 - val_op_main_loss: 0.3101 - val_op_conv_loss: 0.3090 - val_avg_loss: 0.2924 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8829\n",
      "Epoch 69/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0085 - op_main_loss: 0.2677 - op_conv_loss: 0.2031 - avg_loss: 0.2266 - op_main_accuracy: 0.8897 - op_conv_accuracy: 0.9159 - avg_accuracy: 0.9107\n",
      "Epoch 00069: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0085 - op_main_loss: 0.2677 - op_conv_loss: 0.2031 - avg_loss: 0.2266 - op_main_accuracy: 0.8897 - op_conv_accuracy: 0.9159 - avg_accuracy: 0.9107 - val_loss: 1.2298 - val_op_main_loss: 0.3099 - val_op_conv_loss: 0.3132 - val_avg_loss: 0.2962 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8706\n",
      "Epoch 70/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0458 - op_main_loss: 0.2846 - op_conv_loss: 0.2126 - avg_loss: 0.2384 - op_main_accuracy: 0.8866 - op_conv_accuracy: 0.9121 - avg_accuracy: 0.9100\n",
      "Epoch 00070: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0458 - op_main_loss: 0.2846 - op_conv_loss: 0.2126 - avg_loss: 0.2384 - op_main_accuracy: 0.8866 - op_conv_accuracy: 0.9121 - avg_accuracy: 0.9100 - val_loss: 1.3276 - val_op_main_loss: 0.3343 - val_op_conv_loss: 0.3542 - val_avg_loss: 0.3278 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.8612 - val_avg_accuracy: 0.8621\n",
      "Epoch 71/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9865 - op_main_loss: 0.2621 - op_conv_loss: 0.1944 - avg_loss: 0.2190 - op_main_accuracy: 0.8953 - op_conv_accuracy: 0.9220 - avg_accuracy: 0.9185\n",
      "Epoch 00071: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9865 - op_main_loss: 0.2621 - op_conv_loss: 0.1944 - avg_loss: 0.2190 - op_main_accuracy: 0.8953 - op_conv_accuracy: 0.9220 - avg_accuracy: 0.9185 - val_loss: 1.3170 - val_op_main_loss: 0.3279 - val_op_conv_loss: 0.3534 - val_avg_loss: 0.3242 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8517 - val_avg_accuracy: 0.8584\n",
      "Epoch 72/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9880 - op_main_loss: 0.2613 - op_conv_loss: 0.1960 - avg_loss: 0.2192 - op_main_accuracy: 0.8979 - op_conv_accuracy: 0.9180 - avg_accuracy: 0.9185\n",
      "Epoch 00072: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9880 - op_main_loss: 0.2613 - op_conv_loss: 0.1960 - avg_loss: 0.2192 - op_main_accuracy: 0.8979 - op_conv_accuracy: 0.9180 - avg_accuracy: 0.9185 - val_loss: 1.2485 - val_op_main_loss: 0.3210 - val_op_conv_loss: 0.3163 - val_avg_loss: 0.3005 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8735\n",
      "Epoch 73/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9803 - op_main_loss: 0.2603 - op_conv_loss: 0.1921 - avg_loss: 0.2172 - op_main_accuracy: 0.8998 - op_conv_accuracy: 0.9258 - avg_accuracy: 0.9230\n",
      "Epoch 00073: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9803 - op_main_loss: 0.2603 - op_conv_loss: 0.1921 - avg_loss: 0.2172 - op_main_accuracy: 0.8998 - op_conv_accuracy: 0.9258 - avg_accuracy: 0.9230 - val_loss: 1.4408 - val_op_main_loss: 0.3432 - val_op_conv_loss: 0.4262 - val_avg_loss: 0.3603 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8366 - val_avg_accuracy: 0.8470\n",
      "Epoch 74/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9735 - op_main_loss: 0.2564 - op_conv_loss: 0.1910 - avg_loss: 0.2152 - op_main_accuracy: 0.8974 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9213\n",
      "Epoch 00074: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9735 - op_main_loss: 0.2564 - op_conv_loss: 0.1910 - avg_loss: 0.2152 - op_main_accuracy: 0.8974 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9213 - val_loss: 1.2285 - val_op_main_loss: 0.3127 - val_op_conv_loss: 0.3090 - val_avg_loss: 0.2966 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8801\n",
      "Epoch 75/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9623 - op_main_loss: 0.2535 - op_conv_loss: 0.1866 - avg_loss: 0.2115 - op_main_accuracy: 0.9008 - op_conv_accuracy: 0.9239 - avg_accuracy: 0.9220\n",
      "Epoch 00075: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9623 - op_main_loss: 0.2535 - op_conv_loss: 0.1866 - avg_loss: 0.2115 - op_main_accuracy: 0.9008 - op_conv_accuracy: 0.9239 - avg_accuracy: 0.9220 - val_loss: 1.2858 - val_op_main_loss: 0.3276 - val_op_conv_loss: 0.3322 - val_avg_loss: 0.3144 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8735\n",
      "Epoch 76/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9644 - op_main_loss: 0.2540 - op_conv_loss: 0.1872 - avg_loss: 0.2119 - op_main_accuracy: 0.9045 - op_conv_accuracy: 0.9284 - avg_accuracy: 0.9234\n",
      "Epoch 00076: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9644 - op_main_loss: 0.2540 - op_conv_loss: 0.1872 - avg_loss: 0.2119 - op_main_accuracy: 0.9045 - op_conv_accuracy: 0.9284 - avg_accuracy: 0.9234 - val_loss: 1.3757 - val_op_main_loss: 0.3441 - val_op_conv_loss: 0.3771 - val_avg_loss: 0.3438 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8555 - val_avg_accuracy: 0.8602\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.9552 - op_main_loss: 0.2508 - op_conv_loss: 0.1852 - avg_loss: 0.2092 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9234 - avg_accuracy: 0.9211\n",
      "Epoch 00077: val_avg_accuracy improved from 0.88291 to 0.88669, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.9552 - op_main_loss: 0.2508 - op_conv_loss: 0.1852 - avg_loss: 0.2092 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9234 - avg_accuracy: 0.9211 - val_loss: 1.2025 - val_op_main_loss: 0.3032 - val_op_conv_loss: 0.3038 - val_avg_loss: 0.2855 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8867\n",
      "Epoch 78/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9570 - op_main_loss: 0.2497 - op_conv_loss: 0.1873 - avg_loss: 0.2100 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9267 - avg_accuracy: 0.9237\n",
      "Epoch 00078: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9570 - op_main_loss: 0.2497 - op_conv_loss: 0.1873 - avg_loss: 0.2100 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9267 - avg_accuracy: 0.9237 - val_loss: 1.2714 - val_op_main_loss: 0.3075 - val_op_conv_loss: 0.3506 - val_avg_loss: 0.3032 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8650\n",
      "Epoch 79/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9488 - op_main_loss: 0.2523 - op_conv_loss: 0.1795 - avg_loss: 0.2071 - op_main_accuracy: 0.9005 - op_conv_accuracy: 0.9298 - avg_accuracy: 0.9211\n",
      "Epoch 00079: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9488 - op_main_loss: 0.2523 - op_conv_loss: 0.1795 - avg_loss: 0.2071 - op_main_accuracy: 0.9005 - op_conv_accuracy: 0.9298 - avg_accuracy: 0.9211 - val_loss: 1.2270 - val_op_main_loss: 0.3097 - val_op_conv_loss: 0.3138 - val_avg_loss: 0.2940 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8782\n",
      "Epoch 80/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9608 - op_main_loss: 0.2553 - op_conv_loss: 0.1844 - avg_loss: 0.2107 - op_main_accuracy: 0.9003 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9230\n",
      "Epoch 00080: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9608 - op_main_loss: 0.2553 - op_conv_loss: 0.1844 - avg_loss: 0.2107 - op_main_accuracy: 0.9003 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9230 - val_loss: 1.2232 - val_op_main_loss: 0.3074 - val_op_conv_loss: 0.3123 - val_avg_loss: 0.2923 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8839\n",
      "Epoch 81/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9637 - op_main_loss: 0.2555 - op_conv_loss: 0.1858 - avg_loss: 0.2119 - op_main_accuracy: 0.8996 - op_conv_accuracy: 0.9227 - avg_accuracy: 0.9185\n",
      "Epoch 00081: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9637 - op_main_loss: 0.2555 - op_conv_loss: 0.1858 - avg_loss: 0.2119 - op_main_accuracy: 0.8996 - op_conv_accuracy: 0.9227 - avg_accuracy: 0.9185 - val_loss: 1.2699 - val_op_main_loss: 0.3287 - val_op_conv_loss: 0.3240 - val_avg_loss: 0.3077 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8829\n",
      "Epoch 82/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9454 - op_main_loss: 0.2508 - op_conv_loss: 0.1788 - avg_loss: 0.2062 - op_main_accuracy: 0.9022 - op_conv_accuracy: 0.9270 - avg_accuracy: 0.9237\n",
      "Epoch 00082: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9454 - op_main_loss: 0.2508 - op_conv_loss: 0.1788 - avg_loss: 0.2062 - op_main_accuracy: 0.9022 - op_conv_accuracy: 0.9270 - avg_accuracy: 0.9237 - val_loss: 1.2367 - val_op_main_loss: 0.3156 - val_op_conv_loss: 0.3139 - val_avg_loss: 0.2973 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8810\n",
      "Epoch 83/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9387 - op_main_loss: 0.2463 - op_conv_loss: 0.1787 - avg_loss: 0.2035 - op_main_accuracy: 0.9012 - op_conv_accuracy: 0.9265 - avg_accuracy: 0.9227\n",
      "Epoch 00083: val_avg_accuracy improved from 0.88669 to 0.89046, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.9387 - op_main_loss: 0.2463 - op_conv_loss: 0.1787 - avg_loss: 0.2035 - op_main_accuracy: 0.9012 - op_conv_accuracy: 0.9265 - avg_accuracy: 0.9227 - val_loss: 1.1984 - val_op_main_loss: 0.3031 - val_op_conv_loss: 0.3021 - val_avg_loss: 0.2837 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8905\n",
      "Epoch 84/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9402 - op_main_loss: 0.2483 - op_conv_loss: 0.1792 - avg_loss: 0.2038 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9279 - avg_accuracy: 0.9265\n",
      "Epoch 00084: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9402 - op_main_loss: 0.2483 - op_conv_loss: 0.1792 - avg_loss: 0.2038 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9279 - avg_accuracy: 0.9265 - val_loss: 1.2122 - val_op_main_loss: 0.3057 - val_op_conv_loss: 0.3094 - val_avg_loss: 0.2874 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8810\n",
      "Epoch 85/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9324 - op_main_loss: 0.2444 - op_conv_loss: 0.1768 - avg_loss: 0.2018 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9293\n",
      "Epoch 00085: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9324 - op_main_loss: 0.2444 - op_conv_loss: 0.1768 - avg_loss: 0.2018 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9293 - val_loss: 1.2660 - val_op_main_loss: 0.3270 - val_op_conv_loss: 0.3237 - val_avg_loss: 0.3071 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8857\n",
      "Epoch 86/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9313 - op_main_loss: 0.2448 - op_conv_loss: 0.1754 - avg_loss: 0.2022 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9282 - avg_accuracy: 0.9265\n",
      "Epoch 00086: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9313 - op_main_loss: 0.2448 - op_conv_loss: 0.1754 - avg_loss: 0.2022 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9282 - avg_accuracy: 0.9265 - val_loss: 1.2298 - val_op_main_loss: 0.3103 - val_op_conv_loss: 0.3168 - val_avg_loss: 0.2937 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8876\n",
      "Epoch 87/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9281 - op_main_loss: 0.2422 - op_conv_loss: 0.1769 - avg_loss: 0.1999 - op_main_accuracy: 0.9093 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9275\n",
      "Epoch 00087: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9281 - op_main_loss: 0.2422 - op_conv_loss: 0.1769 - avg_loss: 0.1999 - op_main_accuracy: 0.9093 - op_conv_accuracy: 0.9317 - avg_accuracy: 0.9275 - val_loss: 1.2167 - val_op_main_loss: 0.3041 - val_op_conv_loss: 0.3147 - val_avg_loss: 0.2889 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8820\n",
      "Epoch 88/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9070 - op_main_loss: 0.2362 - op_conv_loss: 0.1685 - avg_loss: 0.1933 - op_main_accuracy: 0.9123 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9324\n",
      "Epoch 00088: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9070 - op_main_loss: 0.2362 - op_conv_loss: 0.1685 - avg_loss: 0.1933 - op_main_accuracy: 0.9123 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9324 - val_loss: 1.2280 - val_op_main_loss: 0.3089 - val_op_conv_loss: 0.3180 - val_avg_loss: 0.2924 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9181 - op_main_loss: 0.2418 - op_conv_loss: 0.1705 - avg_loss: 0.1971 - op_main_accuracy: 0.9031 - op_conv_accuracy: 0.9324 - avg_accuracy: 0.9291\n",
      "Epoch 00089: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9181 - op_main_loss: 0.2418 - op_conv_loss: 0.1705 - avg_loss: 0.1971 - op_main_accuracy: 0.9031 - op_conv_accuracy: 0.9324 - avg_accuracy: 0.9291 - val_loss: 1.2705 - val_op_main_loss: 0.3261 - val_op_conv_loss: 0.3270 - val_avg_loss: 0.3089 - val_op_main_accuracy: 0.8593 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8744\n",
      "Epoch 90/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9122 - op_main_loss: 0.2441 - op_conv_loss: 0.1654 - avg_loss: 0.1946 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9315 - avg_accuracy: 0.9301\n",
      "Epoch 00090: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9122 - op_main_loss: 0.2441 - op_conv_loss: 0.1654 - avg_loss: 0.1946 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9315 - avg_accuracy: 0.9301 - val_loss: 1.2791 - val_op_main_loss: 0.3225 - val_op_conv_loss: 0.3372 - val_avg_loss: 0.3113 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8763\n",
      "Epoch 91/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9223 - op_main_loss: 0.2427 - op_conv_loss: 0.1727 - avg_loss: 0.1988 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9279\n",
      "Epoch 00091: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9223 - op_main_loss: 0.2427 - op_conv_loss: 0.1727 - avg_loss: 0.1988 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9279 - val_loss: 1.2285 - val_op_main_loss: 0.2987 - val_op_conv_loss: 0.3340 - val_avg_loss: 0.2883 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8754\n",
      "Epoch 92/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9265 - op_main_loss: 0.2426 - op_conv_loss: 0.1766 - avg_loss: 0.1998 - op_main_accuracy: 0.9062 - op_conv_accuracy: 0.9277 - avg_accuracy: 0.9263\n",
      "Epoch 00092: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9265 - op_main_loss: 0.2426 - op_conv_loss: 0.1766 - avg_loss: 0.1998 - op_main_accuracy: 0.9062 - op_conv_accuracy: 0.9277 - avg_accuracy: 0.9263 - val_loss: 1.2262 - val_op_main_loss: 0.3035 - val_op_conv_loss: 0.3259 - val_avg_loss: 0.2897 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8839\n",
      "Epoch 93/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9029 - op_main_loss: 0.2378 - op_conv_loss: 0.1647 - avg_loss: 0.1923 - op_main_accuracy: 0.9005 - op_conv_accuracy: 0.9336 - avg_accuracy: 0.9315\n",
      "Epoch 00093: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9029 - op_main_loss: 0.2378 - op_conv_loss: 0.1647 - avg_loss: 0.1923 - op_main_accuracy: 0.9005 - op_conv_accuracy: 0.9336 - avg_accuracy: 0.9315 - val_loss: 1.2090 - val_op_main_loss: 0.3016 - val_op_conv_loss: 0.3131 - val_avg_loss: 0.2864 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8905\n",
      "Epoch 94/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9068 - op_main_loss: 0.2401 - op_conv_loss: 0.1655 - avg_loss: 0.1931 - op_main_accuracy: 0.9052 - op_conv_accuracy: 0.9348 - avg_accuracy: 0.9303\n",
      "Epoch 00094: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9068 - op_main_loss: 0.2401 - op_conv_loss: 0.1655 - avg_loss: 0.1931 - op_main_accuracy: 0.9052 - op_conv_accuracy: 0.9348 - avg_accuracy: 0.9303 - val_loss: 1.2203 - val_op_main_loss: 0.3026 - val_op_conv_loss: 0.3218 - val_avg_loss: 0.2882 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8848\n",
      "Epoch 95/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9191 - op_main_loss: 0.2393 - op_conv_loss: 0.1744 - avg_loss: 0.1977 - op_main_accuracy: 0.9050 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9216\n",
      "Epoch 00095: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9191 - op_main_loss: 0.2393 - op_conv_loss: 0.1744 - avg_loss: 0.1977 - op_main_accuracy: 0.9050 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9216 - val_loss: 1.3056 - val_op_main_loss: 0.3143 - val_op_conv_loss: 0.3675 - val_avg_loss: 0.3165 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8669\n",
      "Epoch 96/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8921 - op_main_loss: 0.2344 - op_conv_loss: 0.1619 - avg_loss: 0.1891 - op_main_accuracy: 0.9081 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9319\n",
      "Epoch 00096: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8921 - op_main_loss: 0.2344 - op_conv_loss: 0.1619 - avg_loss: 0.1891 - op_main_accuracy: 0.9081 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9319 - val_loss: 1.2172 - val_op_main_loss: 0.3028 - val_op_conv_loss: 0.3203 - val_avg_loss: 0.2876 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8895\n",
      "Epoch 97/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8924 - op_main_loss: 0.2350 - op_conv_loss: 0.1614 - avg_loss: 0.1893 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9334\n",
      "Epoch 00097: val_avg_accuracy improved from 0.89046 to 0.89424, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.8924 - op_main_loss: 0.2350 - op_conv_loss: 0.1614 - avg_loss: 0.1893 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9334 - val_loss: 1.2162 - val_op_main_loss: 0.3008 - val_op_conv_loss: 0.3233 - val_avg_loss: 0.2853 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8942\n",
      "Epoch 98/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8831 - op_main_loss: 0.2284 - op_conv_loss: 0.1616 - avg_loss: 0.1856 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9303\n",
      "Epoch 00098: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8831 - op_main_loss: 0.2284 - op_conv_loss: 0.1616 - avg_loss: 0.1856 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9303 - val_loss: 1.2942 - val_op_main_loss: 0.3327 - val_op_conv_loss: 0.3389 - val_avg_loss: 0.3148 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8801\n",
      "Epoch 99/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9033 - op_main_loss: 0.2359 - op_conv_loss: 0.1677 - avg_loss: 0.1924 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9298\n",
      "Epoch 00099: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9033 - op_main_loss: 0.2359 - op_conv_loss: 0.1677 - avg_loss: 0.1924 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9298 - val_loss: 1.2075 - val_op_main_loss: 0.3000 - val_op_conv_loss: 0.3152 - val_avg_loss: 0.2856 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8942\n",
      "Epoch 100/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8849 - op_main_loss: 0.2293 - op_conv_loss: 0.1618 - avg_loss: 0.1868 - op_main_accuracy: 0.9130 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9355\n",
      "Epoch 00100: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8849 - op_main_loss: 0.2293 - op_conv_loss: 0.1618 - avg_loss: 0.1868 - op_main_accuracy: 0.9130 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9355 - val_loss: 1.2295 - val_op_main_loss: 0.3026 - val_op_conv_loss: 0.3299 - val_avg_loss: 0.2898 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8876\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.9480 - op_main_loss: 0.2563 - op_conv_loss: 0.1778 - avg_loss: 0.2066 - op_main_accuracy: 0.9026 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9237\n",
      "Epoch 00101: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9480 - op_main_loss: 0.2563 - op_conv_loss: 0.1778 - avg_loss: 0.2066 - op_main_accuracy: 0.9026 - op_conv_accuracy: 0.9319 - avg_accuracy: 0.9237 - val_loss: 1.2405 - val_op_main_loss: 0.3035 - val_op_conv_loss: 0.3337 - val_avg_loss: 0.2962 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8829\n",
      "Epoch 102/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8835 - op_main_loss: 0.2286 - op_conv_loss: 0.1611 - avg_loss: 0.1864 - op_main_accuracy: 0.9128 - op_conv_accuracy: 0.9369 - avg_accuracy: 0.9315\n",
      "Epoch 00102: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8835 - op_main_loss: 0.2286 - op_conv_loss: 0.1611 - avg_loss: 0.1864 - op_main_accuracy: 0.9128 - op_conv_accuracy: 0.9369 - avg_accuracy: 0.9315 - val_loss: 1.2558 - val_op_main_loss: 0.3122 - val_op_conv_loss: 0.3346 - val_avg_loss: 0.3021 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8820\n",
      "Epoch 103/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8684 - op_main_loss: 0.2272 - op_conv_loss: 0.1521 - avg_loss: 0.1819 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9362\n",
      "Epoch 00103: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8684 - op_main_loss: 0.2272 - op_conv_loss: 0.1521 - avg_loss: 0.1819 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9362 - val_loss: 1.2454 - val_op_main_loss: 0.3060 - val_op_conv_loss: 0.3363 - val_avg_loss: 0.2952 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8867\n",
      "Epoch 104/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8930 - op_main_loss: 0.2350 - op_conv_loss: 0.1619 - avg_loss: 0.1882 - op_main_accuracy: 0.9095 - op_conv_accuracy: 0.9364 - avg_accuracy: 0.9315\n",
      "Epoch 00104: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8930 - op_main_loss: 0.2350 - op_conv_loss: 0.1619 - avg_loss: 0.1882 - op_main_accuracy: 0.9095 - op_conv_accuracy: 0.9364 - avg_accuracy: 0.9315 - val_loss: 1.2989 - val_op_main_loss: 0.3219 - val_op_conv_loss: 0.3539 - val_avg_loss: 0.3157 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8744\n",
      "Epoch 105/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8643 - op_main_loss: 0.2246 - op_conv_loss: 0.1523 - avg_loss: 0.1803 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9397\n",
      "Epoch 00105: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8643 - op_main_loss: 0.2246 - op_conv_loss: 0.1523 - avg_loss: 0.1803 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9397 - val_loss: 1.3431 - val_op_main_loss: 0.3380 - val_op_conv_loss: 0.3682 - val_avg_loss: 0.3298 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8754\n",
      "Epoch 106/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8754 - op_main_loss: 0.2304 - op_conv_loss: 0.1547 - avg_loss: 0.1835 - op_main_accuracy: 0.9083 - op_conv_accuracy: 0.9393 - avg_accuracy: 0.9345\n",
      "Epoch 00106: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8754 - op_main_loss: 0.2304 - op_conv_loss: 0.1547 - avg_loss: 0.1835 - op_main_accuracy: 0.9083 - op_conv_accuracy: 0.9393 - avg_accuracy: 0.9345 - val_loss: 1.2887 - val_op_main_loss: 0.3220 - val_op_conv_loss: 0.3475 - val_avg_loss: 0.3123 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8716\n",
      "Epoch 107/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8443 - op_main_loss: 0.2205 - op_conv_loss: 0.1430 - avg_loss: 0.1732 - op_main_accuracy: 0.9175 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9419\n",
      "Epoch 00107: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8443 - op_main_loss: 0.2205 - op_conv_loss: 0.1430 - avg_loss: 0.1732 - op_main_accuracy: 0.9175 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9419 - val_loss: 1.2315 - val_op_main_loss: 0.3001 - val_op_conv_loss: 0.3344 - val_avg_loss: 0.2901 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8886\n",
      "Epoch 108/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8671 - op_main_loss: 0.2264 - op_conv_loss: 0.1542 - avg_loss: 0.1798 - op_main_accuracy: 0.9081 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9350\n",
      "Epoch 00108: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8671 - op_main_loss: 0.2264 - op_conv_loss: 0.1542 - avg_loss: 0.1798 - op_main_accuracy: 0.9081 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9350 - val_loss: 1.2597 - val_op_main_loss: 0.3123 - val_op_conv_loss: 0.3382 - val_avg_loss: 0.3021 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8848\n",
      "Epoch 109/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8605 - op_main_loss: 0.2262 - op_conv_loss: 0.1488 - avg_loss: 0.1785 - op_main_accuracy: 0.9147 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9364\n",
      "Epoch 00109: val_avg_accuracy improved from 0.89424 to 0.90274, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.8605 - op_main_loss: 0.2262 - op_conv_loss: 0.1488 - avg_loss: 0.1785 - op_main_accuracy: 0.9147 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9364 - val_loss: 1.2097 - val_op_main_loss: 0.2981 - val_op_conv_loss: 0.3231 - val_avg_loss: 0.2822 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9027\n",
      "Epoch 110/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8732 - op_main_loss: 0.2296 - op_conv_loss: 0.1542 - avg_loss: 0.1828 - op_main_accuracy: 0.9097 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9360\n",
      "Epoch 00110: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8732 - op_main_loss: 0.2296 - op_conv_loss: 0.1542 - avg_loss: 0.1828 - op_main_accuracy: 0.9097 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9360 - val_loss: 1.6292 - val_op_main_loss: 0.3752 - val_op_conv_loss: 0.5356 - val_avg_loss: 0.4114 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8263 - val_avg_accuracy: 0.8385\n",
      "Epoch 111/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8522 - op_main_loss: 0.2201 - op_conv_loss: 0.1502 - avg_loss: 0.1754 - op_main_accuracy: 0.9168 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9353\n",
      "Epoch 00111: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8522 - op_main_loss: 0.2201 - op_conv_loss: 0.1502 - avg_loss: 0.1754 - op_main_accuracy: 0.9168 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9353 - val_loss: 1.3783 - val_op_main_loss: 0.3315 - val_op_conv_loss: 0.4033 - val_avg_loss: 0.3371 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8687\n",
      "Epoch 112/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8668 - op_main_loss: 0.2290 - op_conv_loss: 0.1509 - avg_loss: 0.1806 - op_main_accuracy: 0.9119 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9386\n",
      "Epoch 00112: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8668 - op_main_loss: 0.2290 - op_conv_loss: 0.1509 - avg_loss: 0.1806 - op_main_accuracy: 0.9119 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9386 - val_loss: 1.3359 - val_op_main_loss: 0.3172 - val_op_conv_loss: 0.3872 - val_avg_loss: 0.3254 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8602 - val_avg_accuracy: 0.8640\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.8586 - op_main_loss: 0.2270 - op_conv_loss: 0.1476 - avg_loss: 0.1778 - op_main_accuracy: 0.9069 - op_conv_accuracy: 0.9405 - avg_accuracy: 0.9329\n",
      "Epoch 00113: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8586 - op_main_loss: 0.2270 - op_conv_loss: 0.1476 - avg_loss: 0.1778 - op_main_accuracy: 0.9069 - op_conv_accuracy: 0.9405 - avg_accuracy: 0.9329 - val_loss: 1.4714 - val_op_main_loss: 0.3310 - val_op_conv_loss: 0.4739 - val_avg_loss: 0.3599 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8385 - val_avg_accuracy: 0.8527\n",
      "Epoch 114/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8519 - op_main_loss: 0.2196 - op_conv_loss: 0.1505 - avg_loss: 0.1752 - op_main_accuracy: 0.9166 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9402\n",
      "Epoch 00114: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8519 - op_main_loss: 0.2196 - op_conv_loss: 0.1505 - avg_loss: 0.1752 - op_main_accuracy: 0.9166 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9402 - val_loss: 1.2201 - val_op_main_loss: 0.2985 - val_op_conv_loss: 0.3292 - val_avg_loss: 0.2858 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8895\n",
      "Epoch 115/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8501 - op_main_loss: 0.2225 - op_conv_loss: 0.1458 - avg_loss: 0.1756 - op_main_accuracy: 0.9190 - op_conv_accuracy: 0.9438 - avg_accuracy: 0.9393\n",
      "Epoch 00115: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8501 - op_main_loss: 0.2225 - op_conv_loss: 0.1458 - avg_loss: 0.1756 - op_main_accuracy: 0.9190 - op_conv_accuracy: 0.9438 - avg_accuracy: 0.9393 - val_loss: 1.5681 - val_op_main_loss: 0.3603 - val_op_conv_loss: 0.5108 - val_avg_loss: 0.3910 - val_op_main_accuracy: 0.8555 - val_op_conv_accuracy: 0.8366 - val_avg_accuracy: 0.8489\n",
      "Epoch 116/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8539 - op_main_loss: 0.2270 - op_conv_loss: 0.1450 - avg_loss: 0.1762 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9400\n",
      "Epoch 00116: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8539 - op_main_loss: 0.2270 - op_conv_loss: 0.1450 - avg_loss: 0.1762 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9400 - val_loss: 1.4322 - val_op_main_loss: 0.3642 - val_op_conv_loss: 0.4055 - val_avg_loss: 0.3581 - val_op_main_accuracy: 0.8499 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8659\n",
      "Epoch 117/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8583 - op_main_loss: 0.2240 - op_conv_loss: 0.1509 - avg_loss: 0.1789 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9357\n",
      "Epoch 00117: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8583 - op_main_loss: 0.2240 - op_conv_loss: 0.1509 - avg_loss: 0.1789 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9357 - val_loss: 1.2297 - val_op_main_loss: 0.3008 - val_op_conv_loss: 0.3349 - val_avg_loss: 0.2887 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8942\n",
      "Epoch 118/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8229 - op_main_loss: 0.2133 - op_conv_loss: 0.1376 - avg_loss: 0.1664 - op_main_accuracy: 0.9199 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9414\n",
      "Epoch 00118: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8229 - op_main_loss: 0.2133 - op_conv_loss: 0.1376 - avg_loss: 0.1664 - op_main_accuracy: 0.9199 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9414 - val_loss: 1.2426 - val_op_main_loss: 0.3138 - val_op_conv_loss: 0.3286 - val_avg_loss: 0.2940 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8933\n",
      "Epoch 119/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8212 - op_main_loss: 0.2134 - op_conv_loss: 0.1364 - avg_loss: 0.1657 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9407\n",
      "Epoch 00119: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8212 - op_main_loss: 0.2134 - op_conv_loss: 0.1364 - avg_loss: 0.1657 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9407 - val_loss: 1.2132 - val_op_main_loss: 0.2980 - val_op_conv_loss: 0.3257 - val_avg_loss: 0.2839 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8886\n",
      "Epoch 120/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8269 - op_main_loss: 0.2154 - op_conv_loss: 0.1382 - avg_loss: 0.1676 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9423\n",
      "Epoch 00120: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8269 - op_main_loss: 0.2154 - op_conv_loss: 0.1382 - avg_loss: 0.1676 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9423 - val_loss: 1.2117 - val_op_main_loss: 0.2972 - val_op_conv_loss: 0.3254 - val_avg_loss: 0.2839 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 121/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8250 - op_main_loss: 0.2134 - op_conv_loss: 0.1393 - avg_loss: 0.1668 - op_main_accuracy: 0.9168 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9402\n",
      "Epoch 00121: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8250 - op_main_loss: 0.2134 - op_conv_loss: 0.1393 - avg_loss: 0.1668 - op_main_accuracy: 0.9168 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9402 - val_loss: 1.2988 - val_op_main_loss: 0.3133 - val_op_conv_loss: 0.3675 - val_avg_loss: 0.3127 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8772\n",
      "Epoch 122/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8332 - op_main_loss: 0.2198 - op_conv_loss: 0.1385 - avg_loss: 0.1702 - op_main_accuracy: 0.9159 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9416\n",
      "Epoch 00122: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8332 - op_main_loss: 0.2198 - op_conv_loss: 0.1385 - avg_loss: 0.1702 - op_main_accuracy: 0.9159 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9416 - val_loss: 1.3000 - val_op_main_loss: 0.3159 - val_op_conv_loss: 0.3664 - val_avg_loss: 0.3135 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8791\n",
      "Epoch 123/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8210 - op_main_loss: 0.2109 - op_conv_loss: 0.1393 - avg_loss: 0.1663 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9419\n",
      "Epoch 00123: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8210 - op_main_loss: 0.2109 - op_conv_loss: 0.1393 - avg_loss: 0.1663 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9419 - val_loss: 1.2204 - val_op_main_loss: 0.2940 - val_op_conv_loss: 0.3405 - val_avg_loss: 0.2816 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8886\n",
      "Epoch 124/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8352 - op_main_loss: 0.2178 - op_conv_loss: 0.1423 - avg_loss: 0.1709 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9364\n",
      "Epoch 00124: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8352 - op_main_loss: 0.2178 - op_conv_loss: 0.1423 - avg_loss: 0.1709 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9364 - val_loss: 1.3233 - val_op_main_loss: 0.3190 - val_op_conv_loss: 0.3802 - val_avg_loss: 0.3196 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8782\n",
      "Epoch 125/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.8135 - op_main_loss: 0.2093 - op_conv_loss: 0.1353 - avg_loss: 0.1640 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9428\n",
      "Epoch 00125: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8135 - op_main_loss: 0.2093 - op_conv_loss: 0.1353 - avg_loss: 0.1640 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9428 - val_loss: 1.2232 - val_op_main_loss: 0.2992 - val_op_conv_loss: 0.3324 - val_avg_loss: 0.2867 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8933\n",
      "Epoch 126/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8478 - op_main_loss: 0.2220 - op_conv_loss: 0.1472 - avg_loss: 0.1737 - op_main_accuracy: 0.9102 - op_conv_accuracy: 0.9457 - avg_accuracy: 0.9390\n",
      "Epoch 00126: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8478 - op_main_loss: 0.2220 - op_conv_loss: 0.1472 - avg_loss: 0.1737 - op_main_accuracy: 0.9102 - op_conv_accuracy: 0.9457 - avg_accuracy: 0.9390 - val_loss: 1.4925 - val_op_main_loss: 0.3585 - val_op_conv_loss: 0.4569 - val_avg_loss: 0.3727 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8499 - val_avg_accuracy: 0.8555\n",
      "Epoch 127/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8359 - op_main_loss: 0.2177 - op_conv_loss: 0.1427 - avg_loss: 0.1716 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9383\n",
      "Epoch 00127: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8359 - op_main_loss: 0.2177 - op_conv_loss: 0.1427 - avg_loss: 0.1716 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9383 - val_loss: 1.3168 - val_op_main_loss: 0.3155 - val_op_conv_loss: 0.3802 - val_avg_loss: 0.3172 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8791\n",
      "Epoch 128/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8309 - op_main_loss: 0.2153 - op_conv_loss: 0.1422 - avg_loss: 0.1694 - op_main_accuracy: 0.9140 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9393\n",
      "Epoch 00128: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8309 - op_main_loss: 0.2153 - op_conv_loss: 0.1422 - avg_loss: 0.1694 - op_main_accuracy: 0.9140 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9393 - val_loss: 1.3807 - val_op_main_loss: 0.3372 - val_op_conv_loss: 0.3996 - val_avg_loss: 0.3405 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8612 - val_avg_accuracy: 0.8640\n",
      "Epoch 129/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7997 - op_main_loss: 0.2072 - op_conv_loss: 0.1296 - avg_loss: 0.1593 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9492 - avg_accuracy: 0.9442\n",
      "Epoch 00129: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7997 - op_main_loss: 0.2072 - op_conv_loss: 0.1296 - avg_loss: 0.1593 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9492 - avg_accuracy: 0.9442 - val_loss: 1.3062 - val_op_main_loss: 0.3368 - val_op_conv_loss: 0.3481 - val_avg_loss: 0.3174 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8782\n",
      "Epoch 130/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8114 - op_main_loss: 0.2092 - op_conv_loss: 0.1356 - avg_loss: 0.1632 - op_main_accuracy: 0.9201 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9431\n",
      "Epoch 00130: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8114 - op_main_loss: 0.2092 - op_conv_loss: 0.1356 - avg_loss: 0.1632 - op_main_accuracy: 0.9201 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9431 - val_loss: 1.3044 - val_op_main_loss: 0.3211 - val_op_conv_loss: 0.3669 - val_avg_loss: 0.3142 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8754\n",
      "Epoch 131/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8228 - op_main_loss: 0.2150 - op_conv_loss: 0.1384 - avg_loss: 0.1673 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9449\n",
      "Epoch 00131: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8228 - op_main_loss: 0.2150 - op_conv_loss: 0.1384 - avg_loss: 0.1673 - op_main_accuracy: 0.9223 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9449 - val_loss: 1.3470 - val_op_main_loss: 0.3262 - val_op_conv_loss: 0.3901 - val_avg_loss: 0.3282 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8735\n",
      "Epoch 132/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8172 - op_main_loss: 0.2134 - op_conv_loss: 0.1359 - avg_loss: 0.1650 - op_main_accuracy: 0.9178 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9400\n",
      "Epoch 00132: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8172 - op_main_loss: 0.2134 - op_conv_loss: 0.1359 - avg_loss: 0.1650 - op_main_accuracy: 0.9178 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9400 - val_loss: 1.2430 - val_op_main_loss: 0.2999 - val_op_conv_loss: 0.3477 - val_avg_loss: 0.2917 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8914\n",
      "Epoch 133/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8082 - op_main_loss: 0.2133 - op_conv_loss: 0.1293 - avg_loss: 0.1617 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9423\n",
      "Epoch 00133: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8082 - op_main_loss: 0.2133 - op_conv_loss: 0.1293 - avg_loss: 0.1617 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9423 - val_loss: 1.4243 - val_op_main_loss: 0.3331 - val_op_conv_loss: 0.4393 - val_avg_loss: 0.3480 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8584 - val_avg_accuracy: 0.8687\n",
      "Epoch 134/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8242 - op_main_loss: 0.2151 - op_conv_loss: 0.1379 - avg_loss: 0.1671 - op_main_accuracy: 0.9175 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9402\n",
      "Epoch 00134: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8242 - op_main_loss: 0.2151 - op_conv_loss: 0.1379 - avg_loss: 0.1671 - op_main_accuracy: 0.9175 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9402 - val_loss: 1.2672 - val_op_main_loss: 0.3013 - val_op_conv_loss: 0.3624 - val_avg_loss: 0.3002 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8839\n",
      "Epoch 135/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7901 - op_main_loss: 0.2023 - op_conv_loss: 0.1282 - avg_loss: 0.1563 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9442\n",
      "Epoch 00135: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7901 - op_main_loss: 0.2023 - op_conv_loss: 0.1282 - avg_loss: 0.1563 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9442 - val_loss: 1.2183 - val_op_main_loss: 0.2959 - val_op_conv_loss: 0.3345 - val_avg_loss: 0.2839 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 136/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7964 - op_main_loss: 0.2081 - op_conv_loss: 0.1267 - avg_loss: 0.1581 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9473\n",
      "Epoch 00136: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7964 - op_main_loss: 0.2081 - op_conv_loss: 0.1267 - avg_loss: 0.1581 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9473 - val_loss: 1.2273 - val_op_main_loss: 0.3010 - val_op_conv_loss: 0.3345 - val_avg_loss: 0.2883 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.7845 - op_main_loss: 0.2013 - op_conv_loss: 0.1254 - avg_loss: 0.1542 - op_main_accuracy: 0.9237 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9490\n",
      "Epoch 00137: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7845 - op_main_loss: 0.2013 - op_conv_loss: 0.1254 - avg_loss: 0.1542 - op_main_accuracy: 0.9237 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9490 - val_loss: 1.5640 - val_op_main_loss: 0.3853 - val_op_conv_loss: 0.4805 - val_avg_loss: 0.3948 - val_op_main_accuracy: 0.8470 - val_op_conv_accuracy: 0.8574 - val_avg_accuracy: 0.8565\n",
      "Epoch 138/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7987 - op_main_loss: 0.2051 - op_conv_loss: 0.1314 - avg_loss: 0.1593 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9473\n",
      "Epoch 00138: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7987 - op_main_loss: 0.2051 - op_conv_loss: 0.1314 - avg_loss: 0.1593 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9473 - val_loss: 1.2457 - val_op_main_loss: 0.3066 - val_op_conv_loss: 0.3450 - val_avg_loss: 0.2910 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8876\n",
      "Epoch 139/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7739 - op_main_loss: 0.1980 - op_conv_loss: 0.1211 - avg_loss: 0.1514 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9530 - avg_accuracy: 0.9475\n",
      "Epoch 00139: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7739 - op_main_loss: 0.1980 - op_conv_loss: 0.1211 - avg_loss: 0.1514 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9530 - avg_accuracy: 0.9475 - val_loss: 1.2609 - val_op_main_loss: 0.3121 - val_op_conv_loss: 0.3469 - val_avg_loss: 0.2990 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8886\n",
      "Epoch 140/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8029 - op_main_loss: 0.2077 - op_conv_loss: 0.1328 - avg_loss: 0.1598 - op_main_accuracy: 0.9185 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9433\n",
      "Epoch 00140: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8029 - op_main_loss: 0.2077 - op_conv_loss: 0.1328 - avg_loss: 0.1598 - op_main_accuracy: 0.9185 - op_conv_accuracy: 0.9461 - avg_accuracy: 0.9433 - val_loss: 1.2457 - val_op_main_loss: 0.2936 - val_op_conv_loss: 0.3581 - val_avg_loss: 0.2922 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8876\n",
      "Epoch 141/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7727 - op_main_loss: 0.1988 - op_conv_loss: 0.1199 - avg_loss: 0.1515 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9509\n",
      "Epoch 00141: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7727 - op_main_loss: 0.1988 - op_conv_loss: 0.1199 - avg_loss: 0.1515 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9509 - val_loss: 1.2432 - val_op_main_loss: 0.3036 - val_op_conv_loss: 0.3440 - val_avg_loss: 0.2929 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8905\n",
      "Epoch 142/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7728 - op_main_loss: 0.2018 - op_conv_loss: 0.1172 - avg_loss: 0.1513 - op_main_accuracy: 0.9233 - op_conv_accuracy: 0.9538 - avg_accuracy: 0.9517\n",
      "Epoch 00142: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7728 - op_main_loss: 0.2019 - op_conv_loss: 0.1172 - avg_loss: 0.1513 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9516 - val_loss: 1.4017 - val_op_main_loss: 0.3774 - val_op_conv_loss: 0.3753 - val_avg_loss: 0.3466 - val_op_main_accuracy: 0.8489 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8801\n",
      "Epoch 143/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8218 - op_main_loss: 0.2179 - op_conv_loss: 0.1353 - avg_loss: 0.1669 - op_main_accuracy: 0.9149 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9414\n",
      "Epoch 00143: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8218 - op_main_loss: 0.2179 - op_conv_loss: 0.1353 - avg_loss: 0.1669 - op_main_accuracy: 0.9149 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9414 - val_loss: 1.2937 - val_op_main_loss: 0.3041 - val_op_conv_loss: 0.3788 - val_avg_loss: 0.3093 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8763\n",
      "Epoch 144/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7844 - op_main_loss: 0.2024 - op_conv_loss: 0.1247 - avg_loss: 0.1553 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9473\n",
      "Epoch 00144: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7844 - op_main_loss: 0.2024 - op_conv_loss: 0.1247 - avg_loss: 0.1553 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9473 - val_loss: 1.2700 - val_op_main_loss: 0.3091 - val_op_conv_loss: 0.3574 - val_avg_loss: 0.3013 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8952\n",
      "Epoch 145/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7788 - op_main_loss: 0.2025 - op_conv_loss: 0.1210 - avg_loss: 0.1532 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9516\n",
      "Epoch 00145: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7788 - op_main_loss: 0.2025 - op_conv_loss: 0.1210 - avg_loss: 0.1532 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9516 - val_loss: 1.2129 - val_op_main_loss: 0.2932 - val_op_conv_loss: 0.3365 - val_avg_loss: 0.2816 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9018\n",
      "Epoch 146/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8102 - op_main_loss: 0.2095 - op_conv_loss: 0.1371 - avg_loss: 0.1621 - op_main_accuracy: 0.9175 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9402\n",
      "Epoch 00146: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8102 - op_main_loss: 0.2095 - op_conv_loss: 0.1371 - avg_loss: 0.1621 - op_main_accuracy: 0.9175 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9402 - val_loss: 1.2389 - val_op_main_loss: 0.3092 - val_op_conv_loss: 0.3346 - val_avg_loss: 0.2931 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8905\n",
      "Epoch 147/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7785 - op_main_loss: 0.1987 - op_conv_loss: 0.1249 - avg_loss: 0.1529 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9473\n",
      "Epoch 00147: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7785 - op_main_loss: 0.1987 - op_conv_loss: 0.1249 - avg_loss: 0.1529 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9473 - val_loss: 1.7372 - val_op_main_loss: 0.4334 - val_op_conv_loss: 0.5563 - val_avg_loss: 0.4452 - val_op_main_accuracy: 0.8366 - val_op_conv_accuracy: 0.8565 - val_avg_accuracy: 0.8508\n",
      "Epoch 148/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7807 - op_main_loss: 0.2012 - op_conv_loss: 0.1238 - avg_loss: 0.1535 - op_main_accuracy: 0.9251 - op_conv_accuracy: 0.9523 - avg_accuracy: 0.9473\n",
      "Epoch 00148: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7807 - op_main_loss: 0.2012 - op_conv_loss: 0.1238 - avg_loss: 0.1535 - op_main_accuracy: 0.9251 - op_conv_accuracy: 0.9523 - avg_accuracy: 0.9473 - val_loss: 1.3409 - val_op_main_loss: 0.3312 - val_op_conv_loss: 0.3811 - val_avg_loss: 0.3259 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8820\n",
      "Epoch 149/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.7929 - op_main_loss: 0.2086 - op_conv_loss: 0.1247 - avg_loss: 0.1572 - op_main_accuracy: 0.9201 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9487\n",
      "Epoch 00149: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7929 - op_main_loss: 0.2086 - op_conv_loss: 0.1247 - avg_loss: 0.1572 - op_main_accuracy: 0.9201 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9487 - val_loss: 1.3495 - val_op_main_loss: 0.2947 - val_op_conv_loss: 0.4381 - val_avg_loss: 0.3144 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8763\n",
      "Epoch 150/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7758 - op_main_loss: 0.1962 - op_conv_loss: 0.1246 - avg_loss: 0.1520 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9480\n",
      "Epoch 00150: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7758 - op_main_loss: 0.1962 - op_conv_loss: 0.1246 - avg_loss: 0.1520 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9480 - val_loss: 1.2429 - val_op_main_loss: 0.2946 - val_op_conv_loss: 0.3562 - val_avg_loss: 0.2894 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8961\n",
      "Epoch 151/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7661 - op_main_loss: 0.1957 - op_conv_loss: 0.1194 - avg_loss: 0.1488 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9518 - avg_accuracy: 0.9520\n",
      "Epoch 00151: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7661 - op_main_loss: 0.1957 - op_conv_loss: 0.1194 - avg_loss: 0.1488 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9518 - avg_accuracy: 0.9520 - val_loss: 1.2193 - val_op_main_loss: 0.2941 - val_op_conv_loss: 0.3395 - val_avg_loss: 0.2834 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8999\n",
      "Epoch 152/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7672 - op_main_loss: 0.1948 - op_conv_loss: 0.1209 - avg_loss: 0.1492 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9523 - avg_accuracy: 0.9518\n",
      "Epoch 00152: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7672 - op_main_loss: 0.1948 - op_conv_loss: 0.1209 - avg_loss: 0.1492 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9523 - avg_accuracy: 0.9518 - val_loss: 1.3699 - val_op_main_loss: 0.3196 - val_op_conv_loss: 0.4179 - val_avg_loss: 0.3306 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8735\n",
      "Epoch 153/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7634 - op_main_loss: 0.1961 - op_conv_loss: 0.1181 - avg_loss: 0.1474 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9527 - avg_accuracy: 0.9504\n",
      "Epoch 00153: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7634 - op_main_loss: 0.1961 - op_conv_loss: 0.1181 - avg_loss: 0.1474 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9527 - avg_accuracy: 0.9504 - val_loss: 1.2555 - val_op_main_loss: 0.3060 - val_op_conv_loss: 0.3500 - val_avg_loss: 0.2977 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8905\n",
      "Epoch 154/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7734 - op_main_loss: 0.1966 - op_conv_loss: 0.1235 - avg_loss: 0.1517 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9492\n",
      "Epoch 00154: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7734 - op_main_loss: 0.1966 - op_conv_loss: 0.1235 - avg_loss: 0.1517 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9492 - val_loss: 1.2803 - val_op_main_loss: 0.3146 - val_op_conv_loss: 0.3570 - val_avg_loss: 0.3076 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8857\n",
      "Epoch 155/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7913 - op_main_loss: 0.2047 - op_conv_loss: 0.1289 - avg_loss: 0.1571 - op_main_accuracy: 0.9208 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9475\n",
      "Epoch 00155: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7913 - op_main_loss: 0.2047 - op_conv_loss: 0.1289 - avg_loss: 0.1571 - op_main_accuracy: 0.9208 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9475 - val_loss: 1.2293 - val_op_main_loss: 0.2882 - val_op_conv_loss: 0.3516 - val_avg_loss: 0.2890 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8905\n",
      "Epoch 156/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7675 - op_main_loss: 0.1982 - op_conv_loss: 0.1184 - avg_loss: 0.1500 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9551 - avg_accuracy: 0.9530\n",
      "Epoch 00156: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7675 - op_main_loss: 0.1982 - op_conv_loss: 0.1184 - avg_loss: 0.1500 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9551 - avg_accuracy: 0.9530 - val_loss: 1.1883 - val_op_main_loss: 0.2874 - val_op_conv_loss: 0.3244 - val_avg_loss: 0.2758 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8980\n",
      "Epoch 157/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7502 - op_main_loss: 0.1910 - op_conv_loss: 0.1141 - avg_loss: 0.1442 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9558 - avg_accuracy: 0.9527\n",
      "Epoch 00157: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7502 - op_main_loss: 0.1910 - op_conv_loss: 0.1141 - avg_loss: 0.1442 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9558 - avg_accuracy: 0.9527 - val_loss: 1.4839 - val_op_main_loss: 0.3563 - val_op_conv_loss: 0.4589 - val_avg_loss: 0.3677 - val_op_main_accuracy: 0.8593 - val_op_conv_accuracy: 0.8631 - val_avg_accuracy: 0.8612\n",
      "Epoch 158/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7509 - op_main_loss: 0.1917 - op_conv_loss: 0.1145 - avg_loss: 0.1439 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9516\n",
      "Epoch 00158: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7509 - op_main_loss: 0.1917 - op_conv_loss: 0.1145 - avg_loss: 0.1439 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9516 - val_loss: 1.1937 - val_op_main_loss: 0.2862 - val_op_conv_loss: 0.3332 - val_avg_loss: 0.2736 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8971\n",
      "Epoch 159/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7547 - op_main_loss: 0.1927 - op_conv_loss: 0.1163 - avg_loss: 0.1444 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9532\n",
      "Epoch 00159: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7547 - op_main_loss: 0.1927 - op_conv_loss: 0.1163 - avg_loss: 0.1444 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9532 - val_loss: 1.7287 - val_op_main_loss: 0.3993 - val_op_conv_loss: 0.5895 - val_avg_loss: 0.4391 - val_op_main_accuracy: 0.8376 - val_op_conv_accuracy: 0.8310 - val_avg_accuracy: 0.8357\n",
      "Epoch 160/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7632 - op_main_loss: 0.1955 - op_conv_loss: 0.1192 - avg_loss: 0.1474 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9506\n",
      "Epoch 00160: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7632 - op_main_loss: 0.1955 - op_conv_loss: 0.1192 - avg_loss: 0.1474 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9506 - val_loss: 1.3343 - val_op_main_loss: 0.3193 - val_op_conv_loss: 0.3927 - val_avg_loss: 0.3213 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8810\n",
      "Epoch 161/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.7607 - op_main_loss: 0.1932 - op_conv_loss: 0.1190 - avg_loss: 0.1476 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9494\n",
      "Epoch 00161: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7607 - op_main_loss: 0.1932 - op_conv_loss: 0.1190 - avg_loss: 0.1476 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9494 - val_loss: 1.4513 - val_op_main_loss: 0.3521 - val_op_conv_loss: 0.4417 - val_avg_loss: 0.3564 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8735\n",
      "Epoch 162/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7603 - op_main_loss: 0.1931 - op_conv_loss: 0.1195 - avg_loss: 0.1464 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9532 - avg_accuracy: 0.9501\n",
      "Epoch 00162: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7603 - op_main_loss: 0.1931 - op_conv_loss: 0.1195 - avg_loss: 0.1464 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9532 - avg_accuracy: 0.9501 - val_loss: 1.6301 - val_op_main_loss: 0.3774 - val_op_conv_loss: 0.5457 - val_avg_loss: 0.4059 - val_op_main_accuracy: 0.8536 - val_op_conv_accuracy: 0.8517 - val_avg_accuracy: 0.8489\n",
      "Epoch 163/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7453 - op_main_loss: 0.1913 - op_conv_loss: 0.1100 - avg_loss: 0.1428 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9553 - avg_accuracy: 0.9568\n",
      "Epoch 00163: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7453 - op_main_loss: 0.1913 - op_conv_loss: 0.1100 - avg_loss: 0.1428 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9553 - avg_accuracy: 0.9568 - val_loss: 1.5021 - val_op_main_loss: 0.3715 - val_op_conv_loss: 0.4557 - val_avg_loss: 0.3732 - val_op_main_accuracy: 0.8536 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8650\n",
      "Epoch 164/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7545 - op_main_loss: 0.1932 - op_conv_loss: 0.1150 - avg_loss: 0.1452 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9516 - avg_accuracy: 0.9499\n",
      "Epoch 00164: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7545 - op_main_loss: 0.1932 - op_conv_loss: 0.1150 - avg_loss: 0.1452 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9516 - avg_accuracy: 0.9499 - val_loss: 1.2277 - val_op_main_loss: 0.2923 - val_op_conv_loss: 0.3502 - val_avg_loss: 0.2836 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9018\n",
      "Epoch 165/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7610 - op_main_loss: 0.1951 - op_conv_loss: 0.1173 - avg_loss: 0.1470 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9534 - avg_accuracy: 0.9499\n",
      "Epoch 00165: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7610 - op_main_loss: 0.1951 - op_conv_loss: 0.1173 - avg_loss: 0.1470 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9534 - avg_accuracy: 0.9499 - val_loss: 1.3717 - val_op_main_loss: 0.3160 - val_op_conv_loss: 0.4263 - val_avg_loss: 0.3274 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8810\n",
      "Epoch 166/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7733 - op_main_loss: 0.2030 - op_conv_loss: 0.1186 - avg_loss: 0.1504 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9511\n",
      "Epoch 00166: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7733 - op_main_loss: 0.2030 - op_conv_loss: 0.1186 - avg_loss: 0.1504 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9511 - val_loss: 1.2760 - val_op_main_loss: 0.2969 - val_op_conv_loss: 0.3835 - val_avg_loss: 0.2947 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8867\n",
      "Epoch 167/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7650 - op_main_loss: 0.1984 - op_conv_loss: 0.1175 - avg_loss: 0.1477 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9518 - avg_accuracy: 0.9499\n",
      "Epoch 00167: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7650 - op_main_loss: 0.1984 - op_conv_loss: 0.1175 - avg_loss: 0.1477 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9518 - avg_accuracy: 0.9499 - val_loss: 1.2257 - val_op_main_loss: 0.2926 - val_op_conv_loss: 0.3479 - val_avg_loss: 0.2836 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8961\n",
      "Epoch 168/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7448 - op_main_loss: 0.1886 - op_conv_loss: 0.1122 - avg_loss: 0.1420 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9575 - avg_accuracy: 0.9532\n",
      "Epoch 00168: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7448 - op_main_loss: 0.1886 - op_conv_loss: 0.1122 - avg_loss: 0.1420 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9575 - avg_accuracy: 0.9532 - val_loss: 1.4789 - val_op_main_loss: 0.3412 - val_op_conv_loss: 0.4757 - val_avg_loss: 0.3603 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8612 - val_avg_accuracy: 0.8621\n",
      "Epoch 169/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7508 - op_main_loss: 0.1898 - op_conv_loss: 0.1161 - avg_loss: 0.1430 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9565 - avg_accuracy: 0.9556\n",
      "Epoch 00169: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7508 - op_main_loss: 0.1898 - op_conv_loss: 0.1161 - avg_loss: 0.1430 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9565 - avg_accuracy: 0.9556 - val_loss: 1.3734 - val_op_main_loss: 0.3198 - val_op_conv_loss: 0.4238 - val_avg_loss: 0.3283 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8810\n",
      "Epoch 170/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7391 - op_main_loss: 0.1854 - op_conv_loss: 0.1127 - avg_loss: 0.1398 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9527\n",
      "Epoch 00170: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7391 - op_main_loss: 0.1854 - op_conv_loss: 0.1127 - avg_loss: 0.1398 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9527 - val_loss: 1.3995 - val_op_main_loss: 0.3310 - val_op_conv_loss: 0.4296 - val_avg_loss: 0.3380 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8810\n",
      "Epoch 171/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7365 - op_main_loss: 0.1848 - op_conv_loss: 0.1115 - avg_loss: 0.1397 - op_main_accuracy: 0.9319 - op_conv_accuracy: 0.9579 - avg_accuracy: 0.9539\n",
      "Epoch 00171: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7365 - op_main_loss: 0.1848 - op_conv_loss: 0.1115 - avg_loss: 0.1397 - op_main_accuracy: 0.9319 - op_conv_accuracy: 0.9579 - avg_accuracy: 0.9539 - val_loss: 1.3441 - val_op_main_loss: 0.3303 - val_op_conv_loss: 0.3896 - val_avg_loss: 0.3241 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8876\n",
      "Epoch 172/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7337 - op_main_loss: 0.1852 - op_conv_loss: 0.1096 - avg_loss: 0.1386 - op_main_accuracy: 0.9343 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9584\n",
      "Epoch 00172: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7337 - op_main_loss: 0.1852 - op_conv_loss: 0.1096 - avg_loss: 0.1386 - op_main_accuracy: 0.9343 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9584 - val_loss: 1.2330 - val_op_main_loss: 0.2919 - val_op_conv_loss: 0.3541 - val_avg_loss: 0.2866 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8971\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.7205 - op_main_loss: 0.1816 - op_conv_loss: 0.1041 - avg_loss: 0.1345 - op_main_accuracy: 0.9379 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9579\n",
      "Epoch 00173: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7205 - op_main_loss: 0.1816 - op_conv_loss: 0.1041 - avg_loss: 0.1345 - op_main_accuracy: 0.9379 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9579 - val_loss: 1.3870 - val_op_main_loss: 0.3431 - val_op_conv_loss: 0.4049 - val_avg_loss: 0.3387 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8744\n",
      "Epoch 174/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7304 - op_main_loss: 0.1848 - op_conv_loss: 0.1078 - avg_loss: 0.1379 - op_main_accuracy: 0.9322 - op_conv_accuracy: 0.9577 - avg_accuracy: 0.9558\n",
      "Epoch 00174: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7304 - op_main_loss: 0.1848 - op_conv_loss: 0.1078 - avg_loss: 0.1379 - op_main_accuracy: 0.9322 - op_conv_accuracy: 0.9577 - avg_accuracy: 0.9558 - val_loss: 1.2342 - val_op_main_loss: 0.2963 - val_op_conv_loss: 0.3493 - val_avg_loss: 0.2895 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8961\n",
      "Epoch 175/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7244 - op_main_loss: 0.1842 - op_conv_loss: 0.1053 - avg_loss: 0.1359 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9553\n",
      "Epoch 00175: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7244 - op_main_loss: 0.1842 - op_conv_loss: 0.1053 - avg_loss: 0.1359 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9553 - val_loss: 1.4078 - val_op_main_loss: 0.3506 - val_op_conv_loss: 0.4118 - val_avg_loss: 0.3464 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8772\n",
      "Epoch 176/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7438 - op_main_loss: 0.1875 - op_conv_loss: 0.1145 - avg_loss: 0.1428 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9530 - avg_accuracy: 0.9509\n",
      "Epoch 00176: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7438 - op_main_loss: 0.1875 - op_conv_loss: 0.1145 - avg_loss: 0.1428 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9530 - avg_accuracy: 0.9509 - val_loss: 1.4340 - val_op_main_loss: 0.3254 - val_op_conv_loss: 0.4649 - val_avg_loss: 0.3455 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8584 - val_avg_accuracy: 0.8612\n",
      "Epoch 177/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7271 - op_main_loss: 0.1848 - op_conv_loss: 0.1067 - avg_loss: 0.1372 - op_main_accuracy: 0.9338 - op_conv_accuracy: 0.9568 - avg_accuracy: 0.9546\n",
      "Epoch 00177: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7271 - op_main_loss: 0.1848 - op_conv_loss: 0.1067 - avg_loss: 0.1372 - op_main_accuracy: 0.9338 - op_conv_accuracy: 0.9568 - avg_accuracy: 0.9546 - val_loss: 1.2166 - val_op_main_loss: 0.2862 - val_op_conv_loss: 0.3509 - val_avg_loss: 0.2811 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8990\n",
      "Epoch 178/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7474 - op_main_loss: 0.1934 - op_conv_loss: 0.1120 - avg_loss: 0.1435 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9560 - avg_accuracy: 0.9539\n",
      "Epoch 00178: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7474 - op_main_loss: 0.1934 - op_conv_loss: 0.1120 - avg_loss: 0.1435 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9560 - avg_accuracy: 0.9539 - val_loss: 1.2344 - val_op_main_loss: 0.2936 - val_op_conv_loss: 0.3523 - val_avg_loss: 0.2895 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 179/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7445 - op_main_loss: 0.1863 - op_conv_loss: 0.1178 - avg_loss: 0.1413 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9537 - avg_accuracy: 0.9518\n",
      "Epoch 00179: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7445 - op_main_loss: 0.1863 - op_conv_loss: 0.1178 - avg_loss: 0.1413 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9537 - avg_accuracy: 0.9518 - val_loss: 1.3079 - val_op_main_loss: 0.3161 - val_op_conv_loss: 0.3806 - val_avg_loss: 0.3116 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8971\n",
      "Epoch 180/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7052 - op_main_loss: 0.1757 - op_conv_loss: 0.0996 - avg_loss: 0.1303 - op_main_accuracy: 0.9400 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9603\n",
      "Epoch 00180: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7052 - op_main_loss: 0.1757 - op_conv_loss: 0.0996 - avg_loss: 0.1303 - op_main_accuracy: 0.9400 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9603 - val_loss: 1.2657 - val_op_main_loss: 0.2966 - val_op_conv_loss: 0.3719 - val_avg_loss: 0.2974 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8952\n",
      "Epoch 181/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7371 - op_main_loss: 0.1849 - op_conv_loss: 0.1137 - avg_loss: 0.1391 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9563 - avg_accuracy: 0.9523\n",
      "Epoch 00181: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7371 - op_main_loss: 0.1849 - op_conv_loss: 0.1137 - avg_loss: 0.1391 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9563 - avg_accuracy: 0.9523 - val_loss: 1.2157 - val_op_main_loss: 0.2930 - val_op_conv_loss: 0.3383 - val_avg_loss: 0.2860 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8952\n",
      "Epoch 182/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7101 - op_main_loss: 0.1789 - op_conv_loss: 0.1009 - avg_loss: 0.1318 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9570\n",
      "Epoch 00182: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7101 - op_main_loss: 0.1789 - op_conv_loss: 0.1009 - avg_loss: 0.1318 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9570 - val_loss: 1.2391 - val_op_main_loss: 0.2888 - val_op_conv_loss: 0.3667 - val_avg_loss: 0.2850 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8895\n",
      "Epoch 183/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7333 - op_main_loss: 0.1818 - op_conv_loss: 0.1142 - avg_loss: 0.1383 - op_main_accuracy: 0.9381 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9551\n",
      "Epoch 00183: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7333 - op_main_loss: 0.1818 - op_conv_loss: 0.1142 - avg_loss: 0.1383 - op_main_accuracy: 0.9381 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9551 - val_loss: 1.4236 - val_op_main_loss: 0.3439 - val_op_conv_loss: 0.4306 - val_avg_loss: 0.3505 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8744\n",
      "Epoch 184/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7196 - op_main_loss: 0.1815 - op_conv_loss: 0.1054 - avg_loss: 0.1352 - op_main_accuracy: 0.9362 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9556\n",
      "Epoch 00184: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7196 - op_main_loss: 0.1815 - op_conv_loss: 0.1054 - avg_loss: 0.1352 - op_main_accuracy: 0.9362 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9556 - val_loss: 1.2299 - val_op_main_loss: 0.2901 - val_op_conv_loss: 0.3552 - val_avg_loss: 0.2879 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8924\n",
      "Epoch 185/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.7231 - op_main_loss: 0.1797 - op_conv_loss: 0.1103 - avg_loss: 0.1362 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9591 - avg_accuracy: 0.9584\n",
      "Epoch 00185: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7231 - op_main_loss: 0.1797 - op_conv_loss: 0.1103 - avg_loss: 0.1362 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9591 - avg_accuracy: 0.9584 - val_loss: 1.3448 - val_op_main_loss: 0.3062 - val_op_conv_loss: 0.4211 - val_avg_loss: 0.3196 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8857\n",
      "Epoch 186/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7546 - op_main_loss: 0.1915 - op_conv_loss: 0.1199 - avg_loss: 0.1459 - op_main_accuracy: 0.9265 - op_conv_accuracy: 0.9485 - avg_accuracy: 0.9461\n",
      "Epoch 00186: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7546 - op_main_loss: 0.1915 - op_conv_loss: 0.1199 - avg_loss: 0.1459 - op_main_accuracy: 0.9265 - op_conv_accuracy: 0.9485 - avg_accuracy: 0.9461 - val_loss: 1.2002 - val_op_main_loss: 0.2882 - val_op_conv_loss: 0.3362 - val_avg_loss: 0.2787 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 187/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7299 - op_main_loss: 0.1873 - op_conv_loss: 0.1071 - avg_loss: 0.1382 - op_main_accuracy: 0.9322 - op_conv_accuracy: 0.9558 - avg_accuracy: 0.9560\n",
      "Epoch 00187: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7299 - op_main_loss: 0.1873 - op_conv_loss: 0.1071 - avg_loss: 0.1382 - op_main_accuracy: 0.9322 - op_conv_accuracy: 0.9558 - avg_accuracy: 0.9560 - val_loss: 1.2581 - val_op_main_loss: 0.2994 - val_op_conv_loss: 0.3634 - val_avg_loss: 0.2981 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8924\n",
      "Epoch 188/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7207 - op_main_loss: 0.1802 - op_conv_loss: 0.1077 - avg_loss: 0.1358 - op_main_accuracy: 0.9360 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9549\n",
      "Epoch 00188: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7207 - op_main_loss: 0.1802 - op_conv_loss: 0.1077 - avg_loss: 0.1358 - op_main_accuracy: 0.9360 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9549 - val_loss: 1.4795 - val_op_main_loss: 0.3524 - val_op_conv_loss: 0.4652 - val_avg_loss: 0.3645 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8602 - val_avg_accuracy: 0.8602\n",
      "Epoch 189/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7287 - op_main_loss: 0.1858 - op_conv_loss: 0.1082 - avg_loss: 0.1370 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9534\n",
      "Epoch 00189: val_avg_accuracy improved from 0.90274 to 0.90368, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.7287 - op_main_loss: 0.1858 - op_conv_loss: 0.1082 - avg_loss: 0.1370 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9534 - val_loss: 1.2003 - val_op_main_loss: 0.2892 - val_op_conv_loss: 0.3352 - val_avg_loss: 0.2781 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 190/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7089 - op_main_loss: 0.1790 - op_conv_loss: 0.1000 - avg_loss: 0.1320 - op_main_accuracy: 0.9374 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9591\n",
      "Epoch 00190: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7089 - op_main_loss: 0.1790 - op_conv_loss: 0.1000 - avg_loss: 0.1320 - op_main_accuracy: 0.9374 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9591 - val_loss: 1.2427 - val_op_main_loss: 0.2957 - val_op_conv_loss: 0.3593 - val_avg_loss: 0.2900 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8942\n",
      "Epoch 191/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7190 - op_main_loss: 0.1803 - op_conv_loss: 0.1063 - avg_loss: 0.1346 - op_main_accuracy: 0.9343 - op_conv_accuracy: 0.9568 - avg_accuracy: 0.9568\n",
      "Epoch 00191: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7190 - op_main_loss: 0.1803 - op_conv_loss: 0.1063 - avg_loss: 0.1346 - op_main_accuracy: 0.9343 - op_conv_accuracy: 0.9568 - avg_accuracy: 0.9568 - val_loss: 1.4014 - val_op_main_loss: 0.3280 - val_op_conv_loss: 0.4381 - val_avg_loss: 0.3379 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8735\n",
      "Epoch 192/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7516 - op_main_loss: 0.1899 - op_conv_loss: 0.1195 - avg_loss: 0.1447 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9501\n",
      "Epoch 00192: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7516 - op_main_loss: 0.1899 - op_conv_loss: 0.1195 - avg_loss: 0.1447 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9501 - val_loss: 1.2860 - val_op_main_loss: 0.2955 - val_op_conv_loss: 0.3889 - val_avg_loss: 0.3043 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8895\n",
      "Epoch 193/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7033 - op_main_loss: 0.1767 - op_conv_loss: 0.0989 - avg_loss: 0.1304 - op_main_accuracy: 0.9390 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9594\n",
      "Epoch 00193: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7033 - op_main_loss: 0.1767 - op_conv_loss: 0.0989 - avg_loss: 0.1304 - op_main_accuracy: 0.9390 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9594 - val_loss: 1.2261 - val_op_main_loss: 0.2908 - val_op_conv_loss: 0.3509 - val_avg_loss: 0.2870 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 194/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7063 - op_main_loss: 0.1816 - op_conv_loss: 0.0963 - avg_loss: 0.1314 - op_main_accuracy: 0.9319 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9605\n",
      "Epoch 00194: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7063 - op_main_loss: 0.1816 - op_conv_loss: 0.0963 - avg_loss: 0.1314 - op_main_accuracy: 0.9319 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9605 - val_loss: 1.2355 - val_op_main_loss: 0.2862 - val_op_conv_loss: 0.3661 - val_avg_loss: 0.2864 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8924\n",
      "Epoch 195/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7181 - op_main_loss: 0.1823 - op_conv_loss: 0.1044 - avg_loss: 0.1343 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9582\n",
      "Epoch 00195: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7181 - op_main_loss: 0.1823 - op_conv_loss: 0.1044 - avg_loss: 0.1343 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9582 - val_loss: 1.1723 - val_op_main_loss: 0.2777 - val_op_conv_loss: 0.3284 - val_avg_loss: 0.2697 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8971\n",
      "Epoch 196/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7089 - op_main_loss: 0.1787 - op_conv_loss: 0.1022 - avg_loss: 0.1317 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9622\n",
      "Epoch 00196: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7089 - op_main_loss: 0.1787 - op_conv_loss: 0.1022 - avg_loss: 0.1317 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9622 - val_loss: 1.2134 - val_op_main_loss: 0.2903 - val_op_conv_loss: 0.3443 - val_avg_loss: 0.2813 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 197/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.7080 - op_main_loss: 0.1755 - op_conv_loss: 0.1046 - avg_loss: 0.1306 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9591 - avg_accuracy: 0.9589\n",
      "Epoch 00197: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7080 - op_main_loss: 0.1755 - op_conv_loss: 0.1046 - avg_loss: 0.1306 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9591 - avg_accuracy: 0.9589 - val_loss: 1.3659 - val_op_main_loss: 0.3203 - val_op_conv_loss: 0.4206 - val_avg_loss: 0.3276 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8763\n",
      "Epoch 198/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7088 - op_main_loss: 0.1811 - op_conv_loss: 0.0993 - avg_loss: 0.1316 - op_main_accuracy: 0.9346 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9567\n",
      "Epoch 00198: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7100 - op_main_loss: 0.1817 - op_conv_loss: 0.0996 - avg_loss: 0.1319 - op_main_accuracy: 0.9338 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9565 - val_loss: 1.2144 - val_op_main_loss: 0.2862 - val_op_conv_loss: 0.3499 - val_avg_loss: 0.2818 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8980\n",
      "Epoch 199/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6936 - op_main_loss: 0.1714 - op_conv_loss: 0.0983 - avg_loss: 0.1270 - op_main_accuracy: 0.9392 - op_conv_accuracy: 0.9609 - avg_accuracy: 0.9588\n",
      "Epoch 00199: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6958 - op_main_loss: 0.1720 - op_conv_loss: 0.0992 - avg_loss: 0.1277 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9584 - val_loss: 1.2253 - val_op_main_loss: 0.2900 - val_op_conv_loss: 0.3544 - val_avg_loss: 0.2842 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8952\n",
      "Epoch 200/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7027 - op_main_loss: 0.1729 - op_conv_loss: 0.1026 - avg_loss: 0.1302 - op_main_accuracy: 0.9408 - op_conv_accuracy: 0.9587 - avg_accuracy: 0.9614\n",
      "Epoch 00200: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7038 - op_main_loss: 0.1733 - op_conv_loss: 0.1030 - avg_loss: 0.1306 - op_main_accuracy: 0.9407 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9610 - val_loss: 1.5440 - val_op_main_loss: 0.3645 - val_op_conv_loss: 0.5015 - val_avg_loss: 0.3810 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8631 - val_avg_accuracy: 0.8659\n",
      "Epoch 201/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7239 - op_main_loss: 0.1849 - op_conv_loss: 0.1062 - avg_loss: 0.1366 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9575\n",
      "Epoch 00201: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7239 - op_main_loss: 0.1849 - op_conv_loss: 0.1062 - avg_loss: 0.1366 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9575 - val_loss: 1.5114 - val_op_main_loss: 0.3345 - val_op_conv_loss: 0.5185 - val_avg_loss: 0.3617 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8725\n",
      "Epoch 202/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6871 - op_main_loss: 0.1702 - op_conv_loss: 0.0951 - avg_loss: 0.1247 - op_main_accuracy: 0.9387 - op_conv_accuracy: 0.9637 - avg_accuracy: 0.9632\n",
      "Epoch 00202: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6887 - op_main_loss: 0.1709 - op_conv_loss: 0.0954 - avg_loss: 0.1252 - op_main_accuracy: 0.9390 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9629 - val_loss: 1.2132 - val_op_main_loss: 0.2846 - val_op_conv_loss: 0.3533 - val_avg_loss: 0.2782 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8961\n",
      "Epoch 203/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6993 - op_main_loss: 0.1741 - op_conv_loss: 0.0996 - avg_loss: 0.1286 - op_main_accuracy: 0.9365 - op_conv_accuracy: 0.9620 - avg_accuracy: 0.9611\n",
      "Epoch 00203: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6976 - op_main_loss: 0.1735 - op_conv_loss: 0.0990 - avg_loss: 0.1281 - op_main_accuracy: 0.9374 - op_conv_accuracy: 0.9622 - avg_accuracy: 0.9615 - val_loss: 1.3176 - val_op_main_loss: 0.3160 - val_op_conv_loss: 0.3928 - val_avg_loss: 0.3123 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8905\n",
      "Epoch 204/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7068 - op_main_loss: 0.1750 - op_conv_loss: 0.1041 - avg_loss: 0.1315 - op_main_accuracy: 0.9364 - op_conv_accuracy: 0.9572 - avg_accuracy: 0.9563\n",
      "Epoch 00204: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7068 - op_main_loss: 0.1750 - op_conv_loss: 0.1041 - avg_loss: 0.1315 - op_main_accuracy: 0.9364 - op_conv_accuracy: 0.9572 - avg_accuracy: 0.9563 - val_loss: 1.4343 - val_op_main_loss: 0.3462 - val_op_conv_loss: 0.4416 - val_avg_loss: 0.3508 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8782\n",
      "Epoch 205/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7100 - op_main_loss: 0.1771 - op_conv_loss: 0.1055 - avg_loss: 0.1318 - op_main_accuracy: 0.9370 - op_conv_accuracy: 0.9588 - avg_accuracy: 0.9542\n",
      "Epoch 00205: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7111 - op_main_loss: 0.1774 - op_conv_loss: 0.1059 - avg_loss: 0.1322 - op_main_accuracy: 0.9364 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9542 - val_loss: 1.2243 - val_op_main_loss: 0.2880 - val_op_conv_loss: 0.3575 - val_avg_loss: 0.2834 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8914\n",
      "Epoch 206/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6804 - op_main_loss: 0.1683 - op_conv_loss: 0.0928 - avg_loss: 0.1235 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9613 - avg_accuracy: 0.9625\n",
      "Epoch 00206: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6795 - op_main_loss: 0.1676 - op_conv_loss: 0.0929 - avg_loss: 0.1233 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9622 - val_loss: 1.3402 - val_op_main_loss: 0.3148 - val_op_conv_loss: 0.4098 - val_avg_loss: 0.3198 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8839\n",
      "Epoch 207/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6941 - op_main_loss: 0.1715 - op_conv_loss: 0.0992 - avg_loss: 0.1275 - op_main_accuracy: 0.9418 - op_conv_accuracy: 0.9628 - avg_accuracy: 0.9609\n",
      "Epoch 00207: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6945 - op_main_loss: 0.1718 - op_conv_loss: 0.0992 - avg_loss: 0.1277 - op_main_accuracy: 0.9414 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9610 - val_loss: 1.2249 - val_op_main_loss: 0.2912 - val_op_conv_loss: 0.3520 - val_avg_loss: 0.2860 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8980\n",
      "Epoch 208/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6810 - op_main_loss: 0.1672 - op_conv_loss: 0.0945 - avg_loss: 0.1239 - op_main_accuracy: 0.9418 - op_conv_accuracy: 0.9600 - avg_accuracy: 0.9593\n",
      "Epoch 00208: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6815 - op_main_loss: 0.1673 - op_conv_loss: 0.0947 - avg_loss: 0.1241 - op_main_accuracy: 0.9414 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9591 - val_loss: 1.2733 - val_op_main_loss: 0.3001 - val_op_conv_loss: 0.3796 - val_avg_loss: 0.2983 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8961\n",
      "Epoch 209/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6955 - op_main_loss: 0.1751 - op_conv_loss: 0.0966 - avg_loss: 0.1281 - op_main_accuracy: 0.9339 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9593\n",
      "Epoch 00209: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6944 - op_main_loss: 0.1749 - op_conv_loss: 0.0960 - avg_loss: 0.1278 - op_main_accuracy: 0.9343 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9596 - val_loss: 1.2935 - val_op_main_loss: 0.3073 - val_op_conv_loss: 0.3839 - val_avg_loss: 0.3065 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8867\n",
      "Epoch 210/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7032 - op_main_loss: 0.1848 - op_conv_loss: 0.0928 - avg_loss: 0.1298 - op_main_accuracy: 0.9295 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9603\n",
      "Epoch 00210: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7000 - op_main_loss: 0.1835 - op_conv_loss: 0.0920 - avg_loss: 0.1288 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9605 - val_loss: 1.2536 - val_op_main_loss: 0.2955 - val_op_conv_loss: 0.3699 - val_avg_loss: 0.2924 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8933\n",
      "Epoch 211/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7041 - op_main_loss: 0.1758 - op_conv_loss: 0.1023 - avg_loss: 0.1298 - op_main_accuracy: 0.9345 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9565\n",
      "Epoch 00211: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.7041 - op_main_loss: 0.1758 - op_conv_loss: 0.1023 - avg_loss: 0.1298 - op_main_accuracy: 0.9345 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9565 - val_loss: 1.2908 - val_op_main_loss: 0.2977 - val_op_conv_loss: 0.3931 - val_avg_loss: 0.3038 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8924\n",
      "Epoch 212/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7036 - op_main_loss: 0.1750 - op_conv_loss: 0.1031 - avg_loss: 0.1301 - op_main_accuracy: 0.9374 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9572\n",
      "Epoch 00212: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7036 - op_main_loss: 0.1750 - op_conv_loss: 0.1031 - avg_loss: 0.1301 - op_main_accuracy: 0.9374 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9572 - val_loss: 1.2332 - val_op_main_loss: 0.2919 - val_op_conv_loss: 0.3579 - val_avg_loss: 0.2883 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8895\n",
      "Epoch 213/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6767 - op_main_loss: 0.1646 - op_conv_loss: 0.0947 - avg_loss: 0.1218 - op_main_accuracy: 0.9389 - op_conv_accuracy: 0.9600 - avg_accuracy: 0.9583\n",
      "Epoch 00213: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6788 - op_main_loss: 0.1653 - op_conv_loss: 0.0954 - avg_loss: 0.1225 - op_main_accuracy: 0.9386 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9579 - val_loss: 1.5846 - val_op_main_loss: 0.3418 - val_op_conv_loss: 0.5650 - val_avg_loss: 0.3814 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8480 - val_avg_accuracy: 0.8593\n",
      "Epoch 214/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6951 - op_main_loss: 0.1699 - op_conv_loss: 0.1021 - avg_loss: 0.1270 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9575\n",
      "Epoch 00214: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6951 - op_main_loss: 0.1699 - op_conv_loss: 0.1021 - avg_loss: 0.1270 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9575 - val_loss: 1.1777 - val_op_main_loss: 0.2762 - val_op_conv_loss: 0.3366 - val_avg_loss: 0.2694 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8999\n",
      "Epoch 215/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6950 - op_main_loss: 0.1755 - op_conv_loss: 0.0969 - avg_loss: 0.1274 - op_main_accuracy: 0.9351 - op_conv_accuracy: 0.9621 - avg_accuracy: 0.9628\n",
      "Epoch 00215: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6980 - op_main_loss: 0.1768 - op_conv_loss: 0.0977 - avg_loss: 0.1284 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9620 - val_loss: 1.2709 - val_op_main_loss: 0.2890 - val_op_conv_loss: 0.3921 - val_avg_loss: 0.2953 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8914\n",
      "Epoch 216/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6933 - op_main_loss: 0.1717 - op_conv_loss: 0.1000 - avg_loss: 0.1268 - op_main_accuracy: 0.9358 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9605\n",
      "Epoch 00216: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6915 - op_main_loss: 0.1712 - op_conv_loss: 0.0992 - avg_loss: 0.1263 - op_main_accuracy: 0.9364 - op_conv_accuracy: 0.9620 - avg_accuracy: 0.9612 - val_loss: 1.3259 - val_op_main_loss: 0.3070 - val_op_conv_loss: 0.4104 - val_avg_loss: 0.3136 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8848\n",
      "Epoch 217/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6724 - op_main_loss: 0.1648 - op_conv_loss: 0.0914 - avg_loss: 0.1211 - op_main_accuracy: 0.9432 - op_conv_accuracy: 0.9630 - avg_accuracy: 0.9630\n",
      "Epoch 00217: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6729 - op_main_loss: 0.1652 - op_conv_loss: 0.0913 - avg_loss: 0.1213 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9629 - val_loss: 1.5671 - val_op_main_loss: 0.3261 - val_op_conv_loss: 0.5751 - val_avg_loss: 0.3707 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8432 - val_avg_accuracy: 0.8593\n",
      "Epoch 218/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6850 - op_main_loss: 0.1698 - op_conv_loss: 0.0961 - avg_loss: 0.1241 - op_main_accuracy: 0.9432 - op_conv_accuracy: 0.9614 - avg_accuracy: 0.9609\n",
      "Epoch 00218: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6846 - op_main_loss: 0.1695 - op_conv_loss: 0.0961 - avg_loss: 0.1240 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9608 - val_loss: 1.3361 - val_op_main_loss: 0.2978 - val_op_conv_loss: 0.4294 - val_avg_loss: 0.3137 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8848\n",
      "Epoch 219/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6993 - op_main_loss: 0.1756 - op_conv_loss: 0.1000 - avg_loss: 0.1289 - op_main_accuracy: 0.9337 - op_conv_accuracy: 0.9609 - avg_accuracy: 0.9571\n",
      "Epoch 00219: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6991 - op_main_loss: 0.1755 - op_conv_loss: 0.0998 - avg_loss: 0.1288 - op_main_accuracy: 0.9338 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9572 - val_loss: 1.3345 - val_op_main_loss: 0.3281 - val_op_conv_loss: 0.3907 - val_avg_loss: 0.3208 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8886\n",
      "Epoch 220/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6714 - op_main_loss: 0.1645 - op_conv_loss: 0.0918 - avg_loss: 0.1202 - op_main_accuracy: 0.9432 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9633\n",
      "Epoch 00220: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6702 - op_main_loss: 0.1641 - op_conv_loss: 0.0914 - avg_loss: 0.1199 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9631 - val_loss: 1.2610 - val_op_main_loss: 0.2963 - val_op_conv_loss: 0.3764 - val_avg_loss: 0.2930 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8971\n",
      "Epoch 221/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6931 - op_main_loss: 0.1709 - op_conv_loss: 0.1004 - avg_loss: 0.1273 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9587 - avg_accuracy: 0.9583\n",
      "Epoch 00221: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6927 - op_main_loss: 0.1711 - op_conv_loss: 0.1000 - avg_loss: 0.1271 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9584 - val_loss: 1.2545 - val_op_main_loss: 0.2847 - val_op_conv_loss: 0.3853 - val_avg_loss: 0.2901 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8971\n",
      "Epoch 222/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6798 - op_main_loss: 0.1733 - op_conv_loss: 0.0889 - avg_loss: 0.1230 - op_main_accuracy: 0.9370 - op_conv_accuracy: 0.9625 - avg_accuracy: 0.9611\n",
      "Epoch 00222: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6799 - op_main_loss: 0.1733 - op_conv_loss: 0.0890 - avg_loss: 0.1230 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9624 - avg_accuracy: 0.9608 - val_loss: 1.2411 - val_op_main_loss: 0.2859 - val_op_conv_loss: 0.3780 - val_avg_loss: 0.2824 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8980\n",
      "Epoch 223/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6620 - op_main_loss: 0.1611 - op_conv_loss: 0.0888 - avg_loss: 0.1173 - op_main_accuracy: 0.9478 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9660\n",
      "Epoch 00223: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6620 - op_main_loss: 0.1611 - op_conv_loss: 0.0888 - avg_loss: 0.1173 - op_main_accuracy: 0.9478 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9660 - val_loss: 1.2316 - val_op_main_loss: 0.2907 - val_op_conv_loss: 0.3605 - val_avg_loss: 0.2850 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9018\n",
      "Epoch 224/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7026 - op_main_loss: 0.1779 - op_conv_loss: 0.1007 - avg_loss: 0.1296 - op_main_accuracy: 0.9345 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9570\n",
      "Epoch 00224: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7026 - op_main_loss: 0.1779 - op_conv_loss: 0.1007 - avg_loss: 0.1296 - op_main_accuracy: 0.9345 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9570 - val_loss: 1.3422 - val_op_main_loss: 0.2991 - val_op_conv_loss: 0.4341 - val_avg_loss: 0.3152 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8848\n",
      "Epoch 225/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6688 - op_main_loss: 0.1637 - op_conv_loss: 0.0914 - avg_loss: 0.1199 - op_main_accuracy: 0.9432 - op_conv_accuracy: 0.9645 - avg_accuracy: 0.9623\n",
      "Epoch 00225: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6681 - op_main_loss: 0.1635 - op_conv_loss: 0.0911 - avg_loss: 0.1197 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9624 - val_loss: 1.3547 - val_op_main_loss: 0.3133 - val_op_conv_loss: 0.4259 - val_avg_loss: 0.3215 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8857\n",
      "Epoch 226/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6840 - op_main_loss: 0.1710 - op_conv_loss: 0.0946 - avg_loss: 0.1244 - op_main_accuracy: 0.9339 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9590\n",
      "Epoch 00226: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6836 - op_main_loss: 0.1709 - op_conv_loss: 0.0944 - avg_loss: 0.1243 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9591 - val_loss: 1.3336 - val_op_main_loss: 0.3148 - val_op_conv_loss: 0.4070 - val_avg_loss: 0.3179 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 227/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6805 - op_main_loss: 0.1671 - op_conv_loss: 0.0963 - avg_loss: 0.1234 - op_main_accuracy: 0.9397 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9620\n",
      "Epoch 00227: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6805 - op_main_loss: 0.1671 - op_conv_loss: 0.0963 - avg_loss: 0.1234 - op_main_accuracy: 0.9397 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9620 - val_loss: 1.2519 - val_op_main_loss: 0.2876 - val_op_conv_loss: 0.3809 - val_avg_loss: 0.2896 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8990\n",
      "Epoch 228/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6889 - op_main_loss: 0.1758 - op_conv_loss: 0.0939 - avg_loss: 0.1255 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9610\n",
      "Epoch 00228: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6889 - op_main_loss: 0.1758 - op_conv_loss: 0.0939 - avg_loss: 0.1255 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9610 - val_loss: 1.2297 - val_op_main_loss: 0.2875 - val_op_conv_loss: 0.3651 - val_avg_loss: 0.2838 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8980\n",
      "Epoch 229/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6559 - op_main_loss: 0.1613 - op_conv_loss: 0.0851 - avg_loss: 0.1159 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9622\n",
      "Epoch 00229: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6559 - op_main_loss: 0.1613 - op_conv_loss: 0.0851 - avg_loss: 0.1159 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9622 - val_loss: 1.3793 - val_op_main_loss: 0.3191 - val_op_conv_loss: 0.4383 - val_avg_loss: 0.3281 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8867\n",
      "Epoch 230/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7082 - op_main_loss: 0.1808 - op_conv_loss: 0.1020 - avg_loss: 0.1315 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9591\n",
      "Epoch 00230: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7082 - op_main_loss: 0.1808 - op_conv_loss: 0.1020 - avg_loss: 0.1315 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9591 - val_loss: 2.1157 - val_op_main_loss: 0.4672 - val_op_conv_loss: 0.8170 - val_avg_loss: 0.5384 - val_op_main_accuracy: 0.8225 - val_op_conv_accuracy: 0.7998 - val_avg_accuracy: 0.8064\n",
      "Epoch 231/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6789 - op_main_loss: 0.1691 - op_conv_loss: 0.0938 - avg_loss: 0.1228 - op_main_accuracy: 0.9400 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9617\n",
      "Epoch 00231: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6789 - op_main_loss: 0.1691 - op_conv_loss: 0.0938 - avg_loss: 0.1228 - op_main_accuracy: 0.9400 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9617 - val_loss: 1.2372 - val_op_main_loss: 0.3005 - val_op_conv_loss: 0.3556 - val_avg_loss: 0.2874 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8933\n",
      "Epoch 232/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6867 - op_main_loss: 0.1715 - op_conv_loss: 0.0963 - avg_loss: 0.1255 - op_main_accuracy: 0.9367 - op_conv_accuracy: 0.9579 - avg_accuracy: 0.9553\n",
      "Epoch 00232: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6867 - op_main_loss: 0.1715 - op_conv_loss: 0.0963 - avg_loss: 0.1255 - op_main_accuracy: 0.9367 - op_conv_accuracy: 0.9579 - avg_accuracy: 0.9553 - val_loss: 1.3467 - val_op_main_loss: 0.2975 - val_op_conv_loss: 0.4434 - val_avg_loss: 0.3126 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8839\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6567 - op_main_loss: 0.1598 - op_conv_loss: 0.0877 - avg_loss: 0.1159 - op_main_accuracy: 0.9445 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9643\n",
      "Epoch 00233: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6567 - op_main_loss: 0.1598 - op_conv_loss: 0.0877 - avg_loss: 0.1159 - op_main_accuracy: 0.9445 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9643 - val_loss: 1.2204 - val_op_main_loss: 0.2838 - val_op_conv_loss: 0.3625 - val_avg_loss: 0.2808 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8980\n",
      "Epoch 234/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6562 - op_main_loss: 0.1585 - op_conv_loss: 0.0882 - avg_loss: 0.1163 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9648\n",
      "Epoch 00234: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6562 - op_main_loss: 0.1585 - op_conv_loss: 0.0882 - avg_loss: 0.1163 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9648 - val_loss: 1.2916 - val_op_main_loss: 0.3033 - val_op_conv_loss: 0.3930 - val_avg_loss: 0.3024 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8886\n",
      "Epoch 235/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6690 - op_main_loss: 0.1681 - op_conv_loss: 0.0885 - avg_loss: 0.1192 - op_main_accuracy: 0.9344 - op_conv_accuracy: 0.9619 - avg_accuracy: 0.9595\n",
      "Epoch 00235: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6701 - op_main_loss: 0.1681 - op_conv_loss: 0.0893 - avg_loss: 0.1195 - op_main_accuracy: 0.9343 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9594 - val_loss: 1.4141 - val_op_main_loss: 0.3331 - val_op_conv_loss: 0.4457 - val_avg_loss: 0.3424 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8735\n",
      "Epoch 236/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6877 - op_main_loss: 0.1708 - op_conv_loss: 0.0979 - avg_loss: 0.1253 - op_main_accuracy: 0.9336 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9575\n",
      "Epoch 00236: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6877 - op_main_loss: 0.1708 - op_conv_loss: 0.0979 - avg_loss: 0.1253 - op_main_accuracy: 0.9336 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9575 - val_loss: 1.2267 - val_op_main_loss: 0.2854 - val_op_conv_loss: 0.3647 - val_avg_loss: 0.2828 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8990\n",
      "Epoch 237/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6467 - op_main_loss: 0.1599 - op_conv_loss: 0.0805 - avg_loss: 0.1130 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9657\n",
      "Epoch 00237: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6467 - op_main_loss: 0.1599 - op_conv_loss: 0.0805 - avg_loss: 0.1130 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9657 - val_loss: 1.3286 - val_op_main_loss: 0.3137 - val_op_conv_loss: 0.4058 - val_avg_loss: 0.3159 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8895\n",
      "Epoch 238/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6665 - op_main_loss: 0.1646 - op_conv_loss: 0.0900 - avg_loss: 0.1189 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9634\n",
      "Epoch 00238: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6665 - op_main_loss: 0.1646 - op_conv_loss: 0.0900 - avg_loss: 0.1189 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9634 - val_loss: 1.2324 - val_op_main_loss: 0.2833 - val_op_conv_loss: 0.3751 - val_avg_loss: 0.2811 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 239/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6805 - op_main_loss: 0.1672 - op_conv_loss: 0.0965 - avg_loss: 0.1237 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9615\n",
      "Epoch 00239: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6805 - op_main_loss: 0.1672 - op_conv_loss: 0.0965 - avg_loss: 0.1237 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9615 - val_loss: 1.4807 - val_op_main_loss: 0.3289 - val_op_conv_loss: 0.5024 - val_avg_loss: 0.3567 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8735\n",
      "Epoch 240/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6565 - op_main_loss: 0.1590 - op_conv_loss: 0.0885 - avg_loss: 0.1161 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9629\n",
      "Epoch 00240: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6565 - op_main_loss: 0.1590 - op_conv_loss: 0.0885 - avg_loss: 0.1161 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9629 - val_loss: 1.2419 - val_op_main_loss: 0.2837 - val_op_conv_loss: 0.3799 - val_avg_loss: 0.2853 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8961\n",
      "Epoch 241/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6623 - op_main_loss: 0.1595 - op_conv_loss: 0.0919 - avg_loss: 0.1176 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9638\n",
      "Epoch 00241: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6623 - op_main_loss: 0.1595 - op_conv_loss: 0.0919 - avg_loss: 0.1176 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9638 - val_loss: 1.4933 - val_op_main_loss: 0.3599 - val_op_conv_loss: 0.4732 - val_avg_loss: 0.3670 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8735\n",
      "Epoch 242/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6533 - op_main_loss: 0.1625 - op_conv_loss: 0.0834 - avg_loss: 0.1143 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9641\n",
      "Epoch 00242: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6533 - op_main_loss: 0.1625 - op_conv_loss: 0.0834 - avg_loss: 0.1143 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9641 - val_loss: 1.3282 - val_op_main_loss: 0.3064 - val_op_conv_loss: 0.4198 - val_avg_loss: 0.3092 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8924\n",
      "Epoch 243/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6534 - op_main_loss: 0.1588 - op_conv_loss: 0.0863 - avg_loss: 0.1152 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9631\n",
      "Epoch 00243: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6534 - op_main_loss: 0.1588 - op_conv_loss: 0.0863 - avg_loss: 0.1152 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9631 - val_loss: 1.3723 - val_op_main_loss: 0.3142 - val_op_conv_loss: 0.4402 - val_avg_loss: 0.3251 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8876\n",
      "Epoch 244/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6569 - op_main_loss: 0.1616 - op_conv_loss: 0.0862 - avg_loss: 0.1162 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9638\n",
      "Epoch 00244: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6569 - op_main_loss: 0.1616 - op_conv_loss: 0.0862 - avg_loss: 0.1162 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9638 - val_loss: 1.2157 - val_op_main_loss: 0.2855 - val_op_conv_loss: 0.3592 - val_avg_loss: 0.2781 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9008\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6677 - op_main_loss: 0.1600 - op_conv_loss: 0.0955 - avg_loss: 0.1197 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9608 - avg_accuracy: 0.9596\n",
      "Epoch 00245: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6677 - op_main_loss: 0.1600 - op_conv_loss: 0.0955 - avg_loss: 0.1197 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9608 - avg_accuracy: 0.9596 - val_loss: 1.4062 - val_op_main_loss: 0.3371 - val_op_conv_loss: 0.4373 - val_avg_loss: 0.3404 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8839\n",
      "Epoch 246/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6703 - op_main_loss: 0.1648 - op_conv_loss: 0.0936 - avg_loss: 0.1201 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9634\n",
      "Epoch 00246: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6703 - op_main_loss: 0.1648 - op_conv_loss: 0.0936 - avg_loss: 0.1201 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9634 - val_loss: 1.5512 - val_op_main_loss: 0.3912 - val_op_conv_loss: 0.4791 - val_avg_loss: 0.3887 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8744\n",
      "Epoch 247/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6498 - op_main_loss: 0.1596 - op_conv_loss: 0.0833 - avg_loss: 0.1144 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9653\n",
      "Epoch 00247: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6498 - op_main_loss: 0.1596 - op_conv_loss: 0.0833 - avg_loss: 0.1144 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9653 - val_loss: 1.2451 - val_op_main_loss: 0.2962 - val_op_conv_loss: 0.3675 - val_avg_loss: 0.2888 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8990\n",
      "Epoch 248/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6624 - op_main_loss: 0.1610 - op_conv_loss: 0.0914 - avg_loss: 0.1180 - op_main_accuracy: 0.9425 - op_conv_accuracy: 0.9652 - avg_accuracy: 0.9645\n",
      "Epoch 00248: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6620 - op_main_loss: 0.1608 - op_conv_loss: 0.0913 - avg_loss: 0.1178 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9646 - val_loss: 1.2488 - val_op_main_loss: 0.2946 - val_op_conv_loss: 0.3722 - val_avg_loss: 0.2903 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 249/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6392 - op_main_loss: 0.1572 - op_conv_loss: 0.0792 - avg_loss: 0.1109 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9674\n",
      "Epoch 00249: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6392 - op_main_loss: 0.1572 - op_conv_loss: 0.0792 - avg_loss: 0.1109 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9674 - val_loss: 1.4055 - val_op_main_loss: 0.3328 - val_op_conv_loss: 0.4415 - val_avg_loss: 0.3397 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8754\n",
      "Epoch 250/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6789 - op_main_loss: 0.1691 - op_conv_loss: 0.0948 - avg_loss: 0.1237 - op_main_accuracy: 0.9353 - op_conv_accuracy: 0.9620 - avg_accuracy: 0.9605\n",
      "Epoch 00250: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6789 - op_main_loss: 0.1691 - op_conv_loss: 0.0948 - avg_loss: 0.1237 - op_main_accuracy: 0.9353 - op_conv_accuracy: 0.9620 - avg_accuracy: 0.9605 - val_loss: 1.2089 - val_op_main_loss: 0.2785 - val_op_conv_loss: 0.3629 - val_avg_loss: 0.2766 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 251/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6513 - op_main_loss: 0.1599 - op_conv_loss: 0.0856 - avg_loss: 0.1147 - op_main_accuracy: 0.9453 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9669\n",
      "Epoch 00251: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6512 - op_main_loss: 0.1596 - op_conv_loss: 0.0858 - avg_loss: 0.1146 - op_main_accuracy: 0.9454 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9669 - val_loss: 1.5024 - val_op_main_loss: 0.3706 - val_op_conv_loss: 0.4681 - val_avg_loss: 0.3726 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8744\n",
      "Epoch 252/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6512 - op_main_loss: 0.1589 - op_conv_loss: 0.0859 - avg_loss: 0.1156 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9646\n",
      "Epoch 00252: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6512 - op_main_loss: 0.1589 - op_conv_loss: 0.0859 - avg_loss: 0.1156 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9646 - val_loss: 1.2465 - val_op_main_loss: 0.2915 - val_op_conv_loss: 0.3735 - val_avg_loss: 0.2907 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8952\n",
      "Epoch 253/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7051 - op_main_loss: 0.1775 - op_conv_loss: 0.1051 - avg_loss: 0.1313 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9584\n",
      "Epoch 00253: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7051 - op_main_loss: 0.1775 - op_conv_loss: 0.1051 - avg_loss: 0.1313 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9584 - val_loss: 1.2209 - val_op_main_loss: 0.2810 - val_op_conv_loss: 0.3699 - val_avg_loss: 0.2781 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9018\n",
      "Epoch 254/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6667 - op_main_loss: 0.1654 - op_conv_loss: 0.0901 - avg_loss: 0.1191 - op_main_accuracy: 0.9396 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9628\n",
      "Epoch 00254: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6670 - op_main_loss: 0.1655 - op_conv_loss: 0.0903 - avg_loss: 0.1193 - op_main_accuracy: 0.9395 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9627 - val_loss: 1.2421 - val_op_main_loss: 0.2808 - val_op_conv_loss: 0.3889 - val_avg_loss: 0.2803 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9018\n",
      "Epoch 255/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6457 - op_main_loss: 0.1561 - op_conv_loss: 0.0845 - avg_loss: 0.1130 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9664\n",
      "Epoch 00255: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6457 - op_main_loss: 0.1561 - op_conv_loss: 0.0845 - avg_loss: 0.1130 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9664 - val_loss: 1.2818 - val_op_main_loss: 0.3026 - val_op_conv_loss: 0.3884 - val_avg_loss: 0.2991 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8914\n",
      "Epoch 256/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6384 - op_main_loss: 0.1551 - op_conv_loss: 0.0806 - avg_loss: 0.1110 - op_main_accuracy: 0.9479 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9671\n",
      "Epoch 00256: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6388 - op_main_loss: 0.1553 - op_conv_loss: 0.0808 - avg_loss: 0.1112 - op_main_accuracy: 0.9478 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9672 - val_loss: 1.2326 - val_op_main_loss: 0.2784 - val_op_conv_loss: 0.3835 - val_avg_loss: 0.2793 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.9018\n",
      "Epoch 257/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6406 - op_main_loss: 0.1531 - op_conv_loss: 0.0848 - avg_loss: 0.1114 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9664\n",
      "Epoch 00257: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6406 - op_main_loss: 0.1531 - op_conv_loss: 0.0848 - avg_loss: 0.1114 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9664 - val_loss: 1.2347 - val_op_main_loss: 0.2862 - val_op_conv_loss: 0.3727 - val_avg_loss: 0.2851 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8971\n",
      "Epoch 258/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6524 - op_main_loss: 0.1604 - op_conv_loss: 0.0859 - avg_loss: 0.1155 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9650\n",
      "Epoch 00258: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6524 - op_main_loss: 0.1604 - op_conv_loss: 0.0859 - avg_loss: 0.1155 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9650 - val_loss: 1.2445 - val_op_main_loss: 0.2818 - val_op_conv_loss: 0.3837 - val_avg_loss: 0.2884 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 259/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6420 - op_main_loss: 0.1568 - op_conv_loss: 0.0824 - avg_loss: 0.1127 - op_main_accuracy: 0.9432 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9650\n",
      "Epoch 00259: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6414 - op_main_loss: 0.1565 - op_conv_loss: 0.0823 - avg_loss: 0.1125 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9650 - val_loss: 1.3496 - val_op_main_loss: 0.3201 - val_op_conv_loss: 0.4181 - val_avg_loss: 0.3209 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8924\n",
      "Epoch 260/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6461 - op_main_loss: 0.1559 - op_conv_loss: 0.0863 - avg_loss: 0.1135 - op_main_accuracy: 0.9428 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9655\n",
      "Epoch 00260: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6461 - op_main_loss: 0.1559 - op_conv_loss: 0.0863 - avg_loss: 0.1135 - op_main_accuracy: 0.9428 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9655 - val_loss: 1.3357 - val_op_main_loss: 0.3101 - val_op_conv_loss: 0.4191 - val_avg_loss: 0.3156 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8867\n",
      "Epoch 261/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6269 - op_main_loss: 0.1520 - op_conv_loss: 0.0770 - avg_loss: 0.1073 - op_main_accuracy: 0.9484 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9709\n",
      "Epoch 00261: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6265 - op_main_loss: 0.1519 - op_conv_loss: 0.0769 - avg_loss: 0.1072 - op_main_accuracy: 0.9485 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9709 - val_loss: 1.3682 - val_op_main_loss: 0.3186 - val_op_conv_loss: 0.4336 - val_avg_loss: 0.3258 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8857\n",
      "Epoch 262/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6338 - op_main_loss: 0.1539 - op_conv_loss: 0.0803 - avg_loss: 0.1100 - op_main_accuracy: 0.9455 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9664\n",
      "Epoch 00262: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6339 - op_main_loss: 0.1539 - op_conv_loss: 0.0804 - avg_loss: 0.1100 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9664 - val_loss: 1.3543 - val_op_main_loss: 0.3125 - val_op_conv_loss: 0.4332 - val_avg_loss: 0.3195 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8848\n",
      "Epoch 263/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6537 - op_main_loss: 0.1591 - op_conv_loss: 0.0892 - avg_loss: 0.1158 - op_main_accuracy: 0.9425 - op_conv_accuracy: 0.9666 - avg_accuracy: 0.9630\n",
      "Epoch 00263: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6542 - op_main_loss: 0.1597 - op_conv_loss: 0.0890 - avg_loss: 0.1159 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9634 - val_loss: 1.6726 - val_op_main_loss: 0.3708 - val_op_conv_loss: 0.6034 - val_avg_loss: 0.4088 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8536 - val_avg_accuracy: 0.8574\n",
      "Epoch 264/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6965 - op_main_loss: 0.1779 - op_conv_loss: 0.0999 - avg_loss: 0.1281 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9570\n",
      "Epoch 00264: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6956 - op_main_loss: 0.1775 - op_conv_loss: 0.0998 - avg_loss: 0.1278 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9572 - val_loss: 1.3677 - val_op_main_loss: 0.3073 - val_op_conv_loss: 0.4477 - val_avg_loss: 0.3215 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8895\n",
      "Epoch 265/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6533 - op_main_loss: 0.1641 - op_conv_loss: 0.0835 - avg_loss: 0.1148 - op_main_accuracy: 0.9420 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9654\n",
      "Epoch 00265: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6528 - op_main_loss: 0.1639 - op_conv_loss: 0.0833 - avg_loss: 0.1146 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9655 - val_loss: 1.2263 - val_op_main_loss: 0.2848 - val_op_conv_loss: 0.3700 - val_avg_loss: 0.2809 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9008\n",
      "Epoch 266/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6701 - op_main_loss: 0.1681 - op_conv_loss: 0.0913 - avg_loss: 0.1190 - op_main_accuracy: 0.9375 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9630\n",
      "Epoch 00266: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6699 - op_main_loss: 0.1681 - op_conv_loss: 0.0910 - avg_loss: 0.1190 - op_main_accuracy: 0.9379 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9631 - val_loss: 1.2566 - val_op_main_loss: 0.2950 - val_op_conv_loss: 0.3776 - val_avg_loss: 0.2917 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8980\n",
      "Epoch 267/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6600 - op_main_loss: 0.1609 - op_conv_loss: 0.0895 - avg_loss: 0.1172 - op_main_accuracy: 0.9434 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9619\n",
      "Epoch 00267: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6598 - op_main_loss: 0.1609 - op_conv_loss: 0.0894 - avg_loss: 0.1171 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9620 - val_loss: 1.2661 - val_op_main_loss: 0.2929 - val_op_conv_loss: 0.3880 - val_avg_loss: 0.2931 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8952\n",
      "Epoch 268/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6401 - op_main_loss: 0.1529 - op_conv_loss: 0.0838 - avg_loss: 0.1116 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9655\n",
      "Epoch 00268: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6401 - op_main_loss: 0.1529 - op_conv_loss: 0.0838 - avg_loss: 0.1116 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9655 - val_loss: 1.3288 - val_op_main_loss: 0.3041 - val_op_conv_loss: 0.4222 - val_avg_loss: 0.3111 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8914\n",
      "Epoch 269/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.6169 - op_main_loss: 0.1457 - op_conv_loss: 0.0756 - avg_loss: 0.1042 - op_main_accuracy: 0.9493 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9680\n",
      "Epoch 00269: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6167 - op_main_loss: 0.1457 - op_conv_loss: 0.0755 - avg_loss: 0.1042 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9681 - val_loss: 1.3507 - val_op_main_loss: 0.3114 - val_op_conv_loss: 0.4282 - val_avg_loss: 0.3199 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8876\n",
      "Epoch 270/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6255 - op_main_loss: 0.1513 - op_conv_loss: 0.0764 - avg_loss: 0.1068 - op_main_accuracy: 0.9462 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9683\n",
      "Epoch 00270: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6318 - op_main_loss: 0.1529 - op_conv_loss: 0.0792 - avg_loss: 0.1087 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9672 - val_loss: 1.4313 - val_op_main_loss: 0.3147 - val_op_conv_loss: 0.4879 - val_avg_loss: 0.3382 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8810\n",
      "Epoch 271/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6504 - op_main_loss: 0.1603 - op_conv_loss: 0.0856 - avg_loss: 0.1140 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9650\n",
      "Epoch 00271: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6504 - op_main_loss: 0.1603 - op_conv_loss: 0.0856 - avg_loss: 0.1140 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9650 - val_loss: 1.4591 - val_op_main_loss: 0.3232 - val_op_conv_loss: 0.4988 - val_avg_loss: 0.3470 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8801\n",
      "Epoch 272/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6719 - op_main_loss: 0.1655 - op_conv_loss: 0.0960 - avg_loss: 0.1210 - op_main_accuracy: 0.9406 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9605\n",
      "Epoch 00272: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6718 - op_main_loss: 0.1656 - op_conv_loss: 0.0959 - avg_loss: 0.1210 - op_main_accuracy: 0.9405 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9605 - val_loss: 1.2366 - val_op_main_loss: 0.2856 - val_op_conv_loss: 0.3753 - val_avg_loss: 0.2862 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8952\n",
      "Epoch 273/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6309 - op_main_loss: 0.1544 - op_conv_loss: 0.0780 - avg_loss: 0.1084 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9655\n",
      "Epoch 00273: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6309 - op_main_loss: 0.1544 - op_conv_loss: 0.0780 - avg_loss: 0.1084 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9655 - val_loss: 1.3465 - val_op_main_loss: 0.3159 - val_op_conv_loss: 0.4211 - val_avg_loss: 0.3194 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8933\n",
      "Epoch 274/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6229 - op_main_loss: 0.1481 - op_conv_loss: 0.0782 - avg_loss: 0.1064 - op_main_accuracy: 0.9475 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9681\n",
      "Epoch 00274: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6229 - op_main_loss: 0.1481 - op_conv_loss: 0.0782 - avg_loss: 0.1064 - op_main_accuracy: 0.9475 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9681 - val_loss: 1.2187 - val_op_main_loss: 0.2885 - val_op_conv_loss: 0.3596 - val_avg_loss: 0.2804 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 275/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6331 - op_main_loss: 0.1550 - op_conv_loss: 0.0790 - avg_loss: 0.1091 - op_main_accuracy: 0.9437 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9683\n",
      "Epoch 00275: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6340 - op_main_loss: 0.1555 - op_conv_loss: 0.0791 - avg_loss: 0.1094 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9681 - val_loss: 1.2659 - val_op_main_loss: 0.2839 - val_op_conv_loss: 0.4086 - val_avg_loss: 0.2843 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.9037\n",
      "Epoch 276/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6394 - op_main_loss: 0.1527 - op_conv_loss: 0.0859 - avg_loss: 0.1114 - op_main_accuracy: 0.9448 - op_conv_accuracy: 0.9678 - avg_accuracy: 0.9645\n",
      "Epoch 00276: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6388 - op_main_loss: 0.1525 - op_conv_loss: 0.0857 - avg_loss: 0.1112 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9646 - val_loss: 1.2244 - val_op_main_loss: 0.2868 - val_op_conv_loss: 0.3682 - val_avg_loss: 0.2802 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8999\n",
      "Epoch 277/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6353 - op_main_loss: 0.1501 - op_conv_loss: 0.0858 - avg_loss: 0.1101 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9654 - avg_accuracy: 0.9652\n",
      "Epoch 00277: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6384 - op_main_loss: 0.1512 - op_conv_loss: 0.0869 - avg_loss: 0.1110 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9648 - avg_accuracy: 0.9643 - val_loss: 1.2099 - val_op_main_loss: 0.2772 - val_op_conv_loss: 0.3663 - val_avg_loss: 0.2775 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9018\n",
      "Epoch 278/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6399 - op_main_loss: 0.1526 - op_conv_loss: 0.0866 - avg_loss: 0.1121 - op_main_accuracy: 0.9474 - op_conv_accuracy: 0.9642 - avg_accuracy: 0.9630\n",
      "Epoch 00278: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6400 - op_main_loss: 0.1529 - op_conv_loss: 0.0864 - avg_loss: 0.1121 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9631 - val_loss: 1.3996 - val_op_main_loss: 0.3187 - val_op_conv_loss: 0.4566 - val_avg_loss: 0.3363 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8725\n",
      "Epoch 279/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6155 - op_main_loss: 0.1475 - op_conv_loss: 0.0752 - avg_loss: 0.1047 - op_main_accuracy: 0.9470 - op_conv_accuracy: 0.9711 - avg_accuracy: 0.9676\n",
      "Epoch 00279: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6154 - op_main_loss: 0.1474 - op_conv_loss: 0.0752 - avg_loss: 0.1046 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9676 - val_loss: 1.3702 - val_op_main_loss: 0.3255 - val_op_conv_loss: 0.4267 - val_avg_loss: 0.3299 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8905\n",
      "Epoch 280/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6143 - op_main_loss: 0.1480 - op_conv_loss: 0.0737 - avg_loss: 0.1043 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9695\n",
      "Epoch 00280: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6143 - op_main_loss: 0.1480 - op_conv_loss: 0.0737 - avg_loss: 0.1043 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9695 - val_loss: 1.4255 - val_op_main_loss: 0.3227 - val_op_conv_loss: 0.4741 - val_avg_loss: 0.3403 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8791\n",
      "Epoch 281/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6281 - op_main_loss: 0.1545 - op_conv_loss: 0.0776 - avg_loss: 0.1081 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9676\n",
      "Epoch 00281: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6281 - op_main_loss: 0.1545 - op_conv_loss: 0.0776 - avg_loss: 0.1081 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9676 - val_loss: 1.2268 - val_op_main_loss: 0.2807 - val_op_conv_loss: 0.3758 - val_avg_loss: 0.2828 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8990\n",
      "Epoch 282/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6223 - op_main_loss: 0.1491 - op_conv_loss: 0.0791 - avg_loss: 0.1069 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9695\n",
      "Epoch 00282: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6219 - op_main_loss: 0.1489 - op_conv_loss: 0.0791 - avg_loss: 0.1067 - op_main_accuracy: 0.9485 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9695 - val_loss: 1.4278 - val_op_main_loss: 0.3224 - val_op_conv_loss: 0.4791 - val_avg_loss: 0.3392 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8820\n",
      "Epoch 283/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6006 - op_main_loss: 0.1422 - op_conv_loss: 0.0709 - avg_loss: 0.1001 - op_main_accuracy: 0.9524 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9692\n",
      "Epoch 00283: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6003 - op_main_loss: 0.1421 - op_conv_loss: 0.0708 - avg_loss: 0.1000 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9693 - val_loss: 1.3862 - val_op_main_loss: 0.3065 - val_op_conv_loss: 0.4680 - val_avg_loss: 0.3240 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8867\n",
      "Epoch 284/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6303 - op_main_loss: 0.1538 - op_conv_loss: 0.0806 - avg_loss: 0.1089 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9660\n",
      "Epoch 00284: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6303 - op_main_loss: 0.1538 - op_conv_loss: 0.0806 - avg_loss: 0.1089 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9660 - val_loss: 1.4250 - val_op_main_loss: 0.3254 - val_op_conv_loss: 0.4728 - val_avg_loss: 0.3399 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8801\n",
      "Epoch 285/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6386 - op_main_loss: 0.1560 - op_conv_loss: 0.0841 - avg_loss: 0.1114 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9646\n",
      "Epoch 00285: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6386 - op_main_loss: 0.1560 - op_conv_loss: 0.0841 - avg_loss: 0.1114 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9646 - val_loss: 1.6321 - val_op_main_loss: 0.3889 - val_op_conv_loss: 0.5541 - val_avg_loss: 0.4019 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8706\n",
      "Epoch 286/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6173 - op_main_loss: 0.1491 - op_conv_loss: 0.0767 - avg_loss: 0.1046 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9673\n",
      "Epoch 00286: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6167 - op_main_loss: 0.1489 - op_conv_loss: 0.0766 - avg_loss: 0.1044 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9674 - val_loss: 1.3418 - val_op_main_loss: 0.3229 - val_op_conv_loss: 0.4133 - val_avg_loss: 0.3189 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8905\n",
      "Epoch 287/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6383 - op_main_loss: 0.1575 - op_conv_loss: 0.0835 - avg_loss: 0.1114 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9646\n",
      "Epoch 00287: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6383 - op_main_loss: 0.1575 - op_conv_loss: 0.0835 - avg_loss: 0.1114 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9646 - val_loss: 1.4574 - val_op_main_loss: 0.3349 - val_op_conv_loss: 0.4848 - val_avg_loss: 0.3528 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8744\n",
      "Epoch 288/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6105 - op_main_loss: 0.1463 - op_conv_loss: 0.0754 - avg_loss: 0.1035 - op_main_accuracy: 0.9571 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9697\n",
      "Epoch 00288: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6111 - op_main_loss: 0.1465 - op_conv_loss: 0.0756 - avg_loss: 0.1037 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9695 - val_loss: 1.3752 - val_op_main_loss: 0.3169 - val_op_conv_loss: 0.4454 - val_avg_loss: 0.3278 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8867\n",
      "Epoch 289/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6522 - op_main_loss: 0.1587 - op_conv_loss: 0.0920 - avg_loss: 0.1159 - op_main_accuracy: 0.9425 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9661\n",
      "Epoch 00289: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6522 - op_main_loss: 0.1587 - op_conv_loss: 0.0920 - avg_loss: 0.1159 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9662 - val_loss: 1.2927 - val_op_main_loss: 0.2881 - val_op_conv_loss: 0.4268 - val_avg_loss: 0.2924 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8942\n",
      "Epoch 290/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6098 - op_main_loss: 0.1444 - op_conv_loss: 0.0767 - avg_loss: 0.1031 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9712\n",
      "Epoch 00290: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6098 - op_main_loss: 0.1444 - op_conv_loss: 0.0767 - avg_loss: 0.1031 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9712 - val_loss: 1.5630 - val_op_main_loss: 0.3616 - val_op_conv_loss: 0.5338 - val_avg_loss: 0.3816 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8687\n",
      "Epoch 291/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6172 - op_main_loss: 0.1496 - op_conv_loss: 0.0766 - avg_loss: 0.1050 - op_main_accuracy: 0.9498 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9702\n",
      "Epoch 00291: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6193 - op_main_loss: 0.1505 - op_conv_loss: 0.0771 - avg_loss: 0.1057 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9695 - val_loss: 1.6235 - val_op_main_loss: 0.3844 - val_op_conv_loss: 0.5492 - val_avg_loss: 0.4042 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8612\n",
      "Epoch 292/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6193 - op_main_loss: 0.1495 - op_conv_loss: 0.0782 - avg_loss: 0.1060 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9674\n",
      "Epoch 00292: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6193 - op_main_loss: 0.1495 - op_conv_loss: 0.0782 - avg_loss: 0.1060 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9674 - val_loss: 1.3997 - val_op_main_loss: 0.3218 - val_op_conv_loss: 0.4606 - val_avg_loss: 0.3318 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8848\n",
      "Epoch 293/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6380 - op_main_loss: 0.1534 - op_conv_loss: 0.0869 - avg_loss: 0.1119 - op_main_accuracy: 0.9454 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9629\n",
      "Epoch 00293: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6380 - op_main_loss: 0.1534 - op_conv_loss: 0.0869 - avg_loss: 0.1119 - op_main_accuracy: 0.9454 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9629 - val_loss: 1.2184 - val_op_main_loss: 0.2795 - val_op_conv_loss: 0.3733 - val_avg_loss: 0.2799 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 294/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6189 - op_main_loss: 0.1475 - op_conv_loss: 0.0797 - avg_loss: 0.1060 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9669\n",
      "Epoch 00294: val_avg_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6189 - op_main_loss: 0.1475 - op_conv_loss: 0.0797 - avg_loss: 0.1060 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9669 - val_loss: 1.2446 - val_op_main_loss: 0.2891 - val_op_conv_loss: 0.3861 - val_avg_loss: 0.2839 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8999\n",
      "Epoch 295/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6101 - op_main_loss: 0.1432 - op_conv_loss: 0.0788 - avg_loss: 0.1027 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9672\n",
      "Epoch 00295: val_avg_accuracy improved from 0.90368 to 0.90463, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6101 - op_main_loss: 0.1432 - op_conv_loss: 0.0788 - avg_loss: 0.1027 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9672 - val_loss: 1.2103 - val_op_main_loss: 0.2800 - val_op_conv_loss: 0.3686 - val_avg_loss: 0.2762 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9046\n",
      "Epoch 296/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6183 - op_main_loss: 0.1504 - op_conv_loss: 0.0774 - avg_loss: 0.1058 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9695\n",
      "Epoch 00296: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6183 - op_main_loss: 0.1504 - op_conv_loss: 0.0774 - avg_loss: 0.1058 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9695 - val_loss: 1.2569 - val_op_main_loss: 0.2858 - val_op_conv_loss: 0.3953 - val_avg_loss: 0.2913 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8971\n",
      "Epoch 297/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6490 - op_main_loss: 0.1551 - op_conv_loss: 0.0939 - avg_loss: 0.1149 - op_main_accuracy: 0.9451 - op_conv_accuracy: 0.9624 - avg_accuracy: 0.9621\n",
      "Epoch 00297: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6499 - op_main_loss: 0.1551 - op_conv_loss: 0.0945 - avg_loss: 0.1152 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9620 - avg_accuracy: 0.9617 - val_loss: 1.2146 - val_op_main_loss: 0.2736 - val_op_conv_loss: 0.3809 - val_avg_loss: 0.2742 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8990\n",
      "Epoch 298/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6405 - op_main_loss: 0.1581 - op_conv_loss: 0.0846 - avg_loss: 0.1119 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9649 - avg_accuracy: 0.9625\n",
      "Epoch 00298: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6434 - op_main_loss: 0.1582 - op_conv_loss: 0.0866 - avg_loss: 0.1127 - op_main_accuracy: 0.9445 - op_conv_accuracy: 0.9648 - avg_accuracy: 0.9624 - val_loss: 1.2933 - val_op_main_loss: 0.2837 - val_op_conv_loss: 0.4280 - val_avg_loss: 0.2967 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8886\n",
      "Epoch 299/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6254 - op_main_loss: 0.1501 - op_conv_loss: 0.0818 - avg_loss: 0.1080 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9660\n",
      "Epoch 00299: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6254 - op_main_loss: 0.1501 - op_conv_loss: 0.0818 - avg_loss: 0.1080 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9660 - val_loss: 1.2329 - val_op_main_loss: 0.2742 - val_op_conv_loss: 0.3920 - val_avg_loss: 0.2814 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8971\n",
      "Epoch 300/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6014 - op_main_loss: 0.1421 - op_conv_loss: 0.0732 - avg_loss: 0.1008 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9695\n",
      "Epoch 00300: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6001 - op_main_loss: 0.1417 - op_conv_loss: 0.0728 - avg_loss: 0.1004 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9698 - val_loss: 1.2491 - val_op_main_loss: 0.2846 - val_op_conv_loss: 0.3908 - val_avg_loss: 0.2892 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8942\n",
      "Epoch 301/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6266 - op_main_loss: 0.1536 - op_conv_loss: 0.0793 - avg_loss: 0.1095 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9653\n",
      "Epoch 00301: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6266 - op_main_loss: 0.1536 - op_conv_loss: 0.0793 - avg_loss: 0.1095 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9653 - val_loss: 1.2849 - val_op_main_loss: 0.2858 - val_op_conv_loss: 0.4188 - val_avg_loss: 0.2962 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8971\n",
      "Epoch 302/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6269 - op_main_loss: 0.1497 - op_conv_loss: 0.0848 - avg_loss: 0.1083 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9655 - avg_accuracy: 0.9653\n",
      "Epoch 00302: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6269 - op_main_loss: 0.1497 - op_conv_loss: 0.0848 - avg_loss: 0.1083 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9655 - avg_accuracy: 0.9653 - val_loss: 1.4774 - val_op_main_loss: 0.3415 - val_op_conv_loss: 0.4911 - val_avg_loss: 0.3602 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8782\n",
      "Epoch 303/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6079 - op_main_loss: 0.1440 - op_conv_loss: 0.0767 - avg_loss: 0.1032 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9700\n",
      "Epoch 00303: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6079 - op_main_loss: 0.1440 - op_conv_loss: 0.0767 - avg_loss: 0.1032 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9700 - val_loss: 1.1931 - val_op_main_loss: 0.2763 - val_op_conv_loss: 0.3584 - val_avg_loss: 0.2745 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8952\n",
      "Epoch 304/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6160 - op_main_loss: 0.1497 - op_conv_loss: 0.0762 - avg_loss: 0.1060 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9679\n",
      "Epoch 00304: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6160 - op_main_loss: 0.1497 - op_conv_loss: 0.0762 - avg_loss: 0.1060 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9679 - val_loss: 1.2948 - val_op_main_loss: 0.2924 - val_op_conv_loss: 0.4180 - val_avg_loss: 0.3003 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8961\n",
      "Epoch 305/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6100 - op_main_loss: 0.1473 - op_conv_loss: 0.0743 - avg_loss: 0.1040 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9700\n",
      "Epoch 00305: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6100 - op_main_loss: 0.1473 - op_conv_loss: 0.0743 - avg_loss: 0.1040 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9700 - val_loss: 1.3797 - val_op_main_loss: 0.3070 - val_op_conv_loss: 0.4624 - val_avg_loss: 0.3256 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8810\n",
      "Epoch 306/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6305 - op_main_loss: 0.1524 - op_conv_loss: 0.0836 - avg_loss: 0.1098 - op_main_accuracy: 0.9478 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9655\n",
      "Epoch 00306: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6305 - op_main_loss: 0.1524 - op_conv_loss: 0.0836 - avg_loss: 0.1098 - op_main_accuracy: 0.9478 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9655 - val_loss: 1.2770 - val_op_main_loss: 0.2853 - val_op_conv_loss: 0.4089 - val_avg_loss: 0.2988 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8942\n",
      "Epoch 307/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5905 - op_main_loss: 0.1440 - op_conv_loss: 0.0649 - avg_loss: 0.0976 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9731\n",
      "Epoch 00307: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5905 - op_main_loss: 0.1440 - op_conv_loss: 0.0649 - avg_loss: 0.0976 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9731 - val_loss: 1.1851 - val_op_main_loss: 0.2719 - val_op_conv_loss: 0.3573 - val_avg_loss: 0.2721 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8999\n",
      "Epoch 308/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6206 - op_main_loss: 0.1492 - op_conv_loss: 0.0806 - avg_loss: 0.1071 - op_main_accuracy: 0.9497 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9667\n",
      "Epoch 00308: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6206 - op_main_loss: 0.1492 - op_conv_loss: 0.0806 - avg_loss: 0.1071 - op_main_accuracy: 0.9497 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9667 - val_loss: 1.2446 - val_op_main_loss: 0.2826 - val_op_conv_loss: 0.3890 - val_avg_loss: 0.2895 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8942\n",
      "Epoch 309/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6037 - op_main_loss: 0.1443 - op_conv_loss: 0.0734 - avg_loss: 0.1026 - op_main_accuracy: 0.9510 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9664\n",
      "Epoch 00309: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6034 - op_main_loss: 0.1442 - op_conv_loss: 0.0733 - avg_loss: 0.1025 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9664 - val_loss: 1.2890 - val_op_main_loss: 0.2978 - val_op_conv_loss: 0.4061 - val_avg_loss: 0.3018 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8924\n",
      "Epoch 310/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6189 - op_main_loss: 0.1477 - op_conv_loss: 0.0816 - avg_loss: 0.1065 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9664\n",
      "Epoch 00310: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6189 - op_main_loss: 0.1477 - op_conv_loss: 0.0816 - avg_loss: 0.1065 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9664 - val_loss: 1.2662 - val_op_main_loss: 0.2870 - val_op_conv_loss: 0.4028 - val_avg_loss: 0.2935 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8952\n",
      "Epoch 311/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6058 - op_main_loss: 0.1473 - op_conv_loss: 0.0734 - avg_loss: 0.1029 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9681\n",
      "Epoch 00311: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6058 - op_main_loss: 0.1473 - op_conv_loss: 0.0734 - avg_loss: 0.1029 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9681 - val_loss: 1.3210 - val_op_main_loss: 0.3070 - val_op_conv_loss: 0.4185 - val_avg_loss: 0.3132 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8933\n",
      "Epoch 312/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6211 - op_main_loss: 0.1459 - op_conv_loss: 0.0855 - avg_loss: 0.1075 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9664\n",
      "Epoch 00312: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6211 - op_main_loss: 0.1459 - op_conv_loss: 0.0855 - avg_loss: 0.1075 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9664 - val_loss: 1.2213 - val_op_main_loss: 0.2868 - val_op_conv_loss: 0.3692 - val_avg_loss: 0.2837 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8961\n",
      "Epoch 313/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6253 - op_main_loss: 0.1489 - op_conv_loss: 0.0854 - avg_loss: 0.1089 - op_main_accuracy: 0.9468 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9667\n",
      "Epoch 00313: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6253 - op_main_loss: 0.1489 - op_conv_loss: 0.0854 - avg_loss: 0.1089 - op_main_accuracy: 0.9468 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9667 - val_loss: 1.4434 - val_op_main_loss: 0.3366 - val_op_conv_loss: 0.4743 - val_avg_loss: 0.3501 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8820\n",
      "Epoch 314/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6432 - op_main_loss: 0.1630 - op_conv_loss: 0.0846 - avg_loss: 0.1135 - op_main_accuracy: 0.9407 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9672\n",
      "Epoch 00314: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6432 - op_main_loss: 0.1630 - op_conv_loss: 0.0846 - avg_loss: 0.1135 - op_main_accuracy: 0.9407 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9672 - val_loss: 1.1948 - val_op_main_loss: 0.2737 - val_op_conv_loss: 0.3632 - val_avg_loss: 0.2753 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9046\n",
      "Epoch 315/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6091 - op_main_loss: 0.1487 - op_conv_loss: 0.0736 - avg_loss: 0.1039 - op_main_accuracy: 0.9497 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9681\n",
      "Epoch 00315: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6091 - op_main_loss: 0.1487 - op_conv_loss: 0.0736 - avg_loss: 0.1039 - op_main_accuracy: 0.9497 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9681 - val_loss: 1.3788 - val_op_main_loss: 0.3082 - val_op_conv_loss: 0.4612 - val_avg_loss: 0.3266 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8895\n",
      "Epoch 316/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6034 - op_main_loss: 0.1435 - op_conv_loss: 0.0749 - avg_loss: 0.1022 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9681\n",
      "Epoch 00316: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6034 - op_main_loss: 0.1435 - op_conv_loss: 0.0749 - avg_loss: 0.1022 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9681 - val_loss: 1.2479 - val_op_main_loss: 0.2888 - val_op_conv_loss: 0.3845 - val_avg_loss: 0.2916 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8895\n",
      "Epoch 317/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5975 - op_main_loss: 0.1442 - op_conv_loss: 0.0703 - avg_loss: 0.0999 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9719\n",
      "Epoch 00317: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5975 - op_main_loss: 0.1442 - op_conv_loss: 0.0703 - avg_loss: 0.0999 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9719 - val_loss: 1.4727 - val_op_main_loss: 0.3490 - val_op_conv_loss: 0.4799 - val_avg_loss: 0.3610 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8754\n",
      "Epoch 318/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5992 - op_main_loss: 0.1410 - op_conv_loss: 0.0751 - avg_loss: 0.1009 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9695\n",
      "Epoch 00318: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5992 - op_main_loss: 0.1410 - op_conv_loss: 0.0751 - avg_loss: 0.1009 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9695 - val_loss: 1.3436 - val_op_main_loss: 0.3055 - val_op_conv_loss: 0.4396 - val_avg_loss: 0.3169 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8924\n",
      "Epoch 319/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5900 - op_main_loss: 0.1379 - op_conv_loss: 0.0719 - avg_loss: 0.0982 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9702\n",
      "Epoch 00319: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5900 - op_main_loss: 0.1379 - op_conv_loss: 0.0719 - avg_loss: 0.0982 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9702 - val_loss: 1.2958 - val_op_main_loss: 0.2910 - val_op_conv_loss: 0.4220 - val_avg_loss: 0.3006 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8952\n",
      "Epoch 320/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6261 - op_main_loss: 0.1518 - op_conv_loss: 0.0841 - avg_loss: 0.1082 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9638\n",
      "Epoch 00320: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6261 - op_main_loss: 0.1518 - op_conv_loss: 0.0841 - avg_loss: 0.1082 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9638 - val_loss: 1.2206 - val_op_main_loss: 0.2840 - val_op_conv_loss: 0.3729 - val_avg_loss: 0.2816 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.9027\n",
      "Epoch 321/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5834 - op_main_loss: 0.1415 - op_conv_loss: 0.0639 - avg_loss: 0.0958 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9761\n",
      "Epoch 00321: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5834 - op_main_loss: 0.1415 - op_conv_loss: 0.0639 - avg_loss: 0.0958 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9761 - val_loss: 1.6095 - val_op_main_loss: 0.3771 - val_op_conv_loss: 0.5531 - val_avg_loss: 0.3973 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8754\n",
      "Epoch 322/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6206 - op_main_loss: 0.1523 - op_conv_loss: 0.0787 - avg_loss: 0.1072 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9672\n",
      "Epoch 00322: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6206 - op_main_loss: 0.1523 - op_conv_loss: 0.0787 - avg_loss: 0.1072 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9672 - val_loss: 1.2597 - val_op_main_loss: 0.2933 - val_op_conv_loss: 0.3909 - val_avg_loss: 0.2932 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8942\n",
      "Epoch 323/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6136 - op_main_loss: 0.1445 - op_conv_loss: 0.0810 - avg_loss: 0.1058 - op_main_accuracy: 0.9493 - op_conv_accuracy: 0.9666 - avg_accuracy: 0.9678\n",
      "Epoch 00323: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6134 - op_main_loss: 0.1445 - op_conv_loss: 0.0809 - avg_loss: 0.1057 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9679 - val_loss: 1.4493 - val_op_main_loss: 0.3220 - val_op_conv_loss: 0.4995 - val_avg_loss: 0.3454 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8801\n",
      "Epoch 324/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6145 - op_main_loss: 0.1474 - op_conv_loss: 0.0790 - avg_loss: 0.1056 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9667\n",
      "Epoch 00324: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6145 - op_main_loss: 0.1474 - op_conv_loss: 0.0790 - avg_loss: 0.1056 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9667 - val_loss: 1.2102 - val_op_main_loss: 0.2668 - val_op_conv_loss: 0.3863 - val_avg_loss: 0.2749 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8971\n",
      "Epoch 325/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5770 - op_main_loss: 0.1362 - op_conv_loss: 0.0646 - avg_loss: 0.0938 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9726\n",
      "Epoch 00325: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5770 - op_main_loss: 0.1362 - op_conv_loss: 0.0646 - avg_loss: 0.0938 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9726 - val_loss: 1.3296 - val_op_main_loss: 0.3009 - val_op_conv_loss: 0.4346 - val_avg_loss: 0.3120 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8886\n",
      "Epoch 326/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6081 - op_main_loss: 0.1434 - op_conv_loss: 0.0792 - avg_loss: 0.1038 - op_main_accuracy: 0.9510 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9695\n",
      "Epoch 00326: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6078 - op_main_loss: 0.1433 - op_conv_loss: 0.0790 - avg_loss: 0.1037 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9695 - val_loss: 1.2453 - val_op_main_loss: 0.2857 - val_op_conv_loss: 0.3901 - val_avg_loss: 0.2878 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8952\n",
      "Epoch 327/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6174 - op_main_loss: 0.1479 - op_conv_loss: 0.0814 - avg_loss: 0.1065 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9676\n",
      "Epoch 00327: val_avg_accuracy did not improve from 0.90463\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6174 - op_main_loss: 0.1479 - op_conv_loss: 0.0814 - avg_loss: 0.1065 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9676 - val_loss: 1.2847 - val_op_main_loss: 0.2969 - val_op_conv_loss: 0.4103 - val_avg_loss: 0.2960 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8876\n",
      "Epoch 328/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6002 - op_main_loss: 0.1404 - op_conv_loss: 0.0775 - avg_loss: 0.1008 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9702\n",
      "Epoch 00328: val_avg_accuracy improved from 0.90463 to 0.90557, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6002 - op_main_loss: 0.1404 - op_conv_loss: 0.0775 - avg_loss: 0.1008 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9702 - val_loss: 1.2184 - val_op_main_loss: 0.2736 - val_op_conv_loss: 0.3853 - val_avg_loss: 0.2780 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9056\n",
      "Epoch 329/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5804 - op_main_loss: 0.1391 - op_conv_loss: 0.0649 - avg_loss: 0.0948 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9735\n",
      "Epoch 00329: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5804 - op_main_loss: 0.1391 - op_conv_loss: 0.0649 - avg_loss: 0.0948 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9735 - val_loss: 1.4660 - val_op_main_loss: 0.3199 - val_op_conv_loss: 0.5180 - val_avg_loss: 0.3467 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8801\n",
      "Epoch 330/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6102 - op_main_loss: 0.1473 - op_conv_loss: 0.0776 - avg_loss: 0.1045 - op_main_accuracy: 0.9461 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9683\n",
      "Epoch 00330: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6102 - op_main_loss: 0.1473 - op_conv_loss: 0.0776 - avg_loss: 0.1045 - op_main_accuracy: 0.9461 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9683 - val_loss: 1.2172 - val_op_main_loss: 0.2783 - val_op_conv_loss: 0.3784 - val_avg_loss: 0.2796 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8971\n",
      "Epoch 331/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6040 - op_main_loss: 0.1408 - op_conv_loss: 0.0793 - avg_loss: 0.1026 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9672\n",
      "Epoch 00331: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6040 - op_main_loss: 0.1408 - op_conv_loss: 0.0793 - avg_loss: 0.1026 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9672 - val_loss: 1.2230 - val_op_main_loss: 0.2773 - val_op_conv_loss: 0.3850 - val_avg_loss: 0.2792 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9018\n",
      "Epoch 332/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5979 - op_main_loss: 0.1417 - op_conv_loss: 0.0744 - avg_loss: 0.1004 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9709\n",
      "Epoch 00332: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5979 - op_main_loss: 0.1417 - op_conv_loss: 0.0744 - avg_loss: 0.1004 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9709 - val_loss: 1.2520 - val_op_main_loss: 0.2850 - val_op_conv_loss: 0.3946 - val_avg_loss: 0.2914 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8999\n",
      "Epoch 333/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6101 - op_main_loss: 0.1470 - op_conv_loss: 0.0778 - avg_loss: 0.1045 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9688\n",
      "Epoch 00333: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6101 - op_main_loss: 0.1470 - op_conv_loss: 0.0778 - avg_loss: 0.1045 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9688 - val_loss: 1.3453 - val_op_main_loss: 0.3034 - val_op_conv_loss: 0.4425 - val_avg_loss: 0.3189 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8867\n",
      "Epoch 334/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6091 - op_main_loss: 0.1446 - op_conv_loss: 0.0796 - avg_loss: 0.1042 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9667\n",
      "Epoch 00334: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6091 - op_main_loss: 0.1446 - op_conv_loss: 0.0796 - avg_loss: 0.1042 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9667 - val_loss: 1.4627 - val_op_main_loss: 0.3587 - val_op_conv_loss: 0.4619 - val_avg_loss: 0.3618 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8744\n",
      "Epoch 335/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5883 - op_main_loss: 0.1391 - op_conv_loss: 0.0708 - avg_loss: 0.0979 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9721\n",
      "Epoch 00335: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5883 - op_main_loss: 0.1391 - op_conv_loss: 0.0708 - avg_loss: 0.0979 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9721 - val_loss: 1.2320 - val_op_main_loss: 0.2808 - val_op_conv_loss: 0.3890 - val_avg_loss: 0.2812 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9056\n",
      "Epoch 336/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5804 - op_main_loss: 0.1368 - op_conv_loss: 0.0674 - avg_loss: 0.0953 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9714\n",
      "Epoch 00336: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5804 - op_main_loss: 0.1368 - op_conv_loss: 0.0674 - avg_loss: 0.0953 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9714 - val_loss: 1.2217 - val_op_main_loss: 0.2798 - val_op_conv_loss: 0.3783 - val_avg_loss: 0.2827 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9027\n",
      "Epoch 337/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5991 - op_main_loss: 0.1470 - op_conv_loss: 0.0712 - avg_loss: 0.1006 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9698\n",
      "Epoch 00337: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5991 - op_main_loss: 0.1470 - op_conv_loss: 0.0712 - avg_loss: 0.1006 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9698 - val_loss: 1.5950 - val_op_main_loss: 0.3163 - val_op_conv_loss: 0.6340 - val_avg_loss: 0.3651 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8565 - val_avg_accuracy: 0.8697\n",
      "Epoch 338/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6354 - op_main_loss: 0.1556 - op_conv_loss: 0.0875 - avg_loss: 0.1122 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9629\n",
      "Epoch 00338: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6354 - op_main_loss: 0.1556 - op_conv_loss: 0.0875 - avg_loss: 0.1122 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9629 - val_loss: 1.2823 - val_op_main_loss: 0.2815 - val_op_conv_loss: 0.4254 - val_avg_loss: 0.2947 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 339/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6495 - op_main_loss: 0.1649 - op_conv_loss: 0.0874 - avg_loss: 0.1158 - op_main_accuracy: 0.9387 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9633\n",
      "Epoch 00339: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6494 - op_main_loss: 0.1648 - op_conv_loss: 0.0874 - avg_loss: 0.1158 - op_main_accuracy: 0.9390 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9634 - val_loss: 1.3013 - val_op_main_loss: 0.2965 - val_op_conv_loss: 0.4216 - val_avg_loss: 0.3006 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8980\n",
      "Epoch 340/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6132 - op_main_loss: 0.1421 - op_conv_loss: 0.0842 - avg_loss: 0.1042 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9674\n",
      "Epoch 00340: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.6132 - op_main_loss: 0.1421 - op_conv_loss: 0.0842 - avg_loss: 0.1042 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9674 - val_loss: 1.5817 - val_op_main_loss: 0.3869 - val_op_conv_loss: 0.5166 - val_avg_loss: 0.3955 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8754\n",
      "Epoch 341/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5953 - op_main_loss: 0.1418 - op_conv_loss: 0.0715 - avg_loss: 0.0993 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9695\n",
      "Epoch 00341: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5953 - op_main_loss: 0.1418 - op_conv_loss: 0.0715 - avg_loss: 0.0993 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9695 - val_loss: 1.2515 - val_op_main_loss: 0.2903 - val_op_conv_loss: 0.3942 - val_avg_loss: 0.2849 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8999\n",
      "Epoch 342/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5917 - op_main_loss: 0.1394 - op_conv_loss: 0.0714 - avg_loss: 0.0991 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9733\n",
      "Epoch 00342: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5917 - op_main_loss: 0.1394 - op_conv_loss: 0.0714 - avg_loss: 0.0991 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9733 - val_loss: 1.2034 - val_op_main_loss: 0.2806 - val_op_conv_loss: 0.3637 - val_avg_loss: 0.2773 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8999\n",
      "Epoch 343/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5809 - op_main_loss: 0.1356 - op_conv_loss: 0.0687 - avg_loss: 0.0951 - op_main_accuracy: 0.9545 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9695\n",
      "Epoch 00343: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5814 - op_main_loss: 0.1359 - op_conv_loss: 0.0687 - avg_loss: 0.0952 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9693 - val_loss: 1.4946 - val_op_main_loss: 0.3395 - val_op_conv_loss: 0.5127 - val_avg_loss: 0.3611 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8763\n",
      "Epoch 344/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5949 - op_main_loss: 0.1406 - op_conv_loss: 0.0736 - avg_loss: 0.0996 - op_main_accuracy: 0.9529 - op_conv_accuracy: 0.9725 - avg_accuracy: 0.9706\n",
      "Epoch 00344: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5947 - op_main_loss: 0.1405 - op_conv_loss: 0.0735 - avg_loss: 0.0995 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9707 - val_loss: 1.2831 - val_op_main_loss: 0.2820 - val_op_conv_loss: 0.4239 - val_avg_loss: 0.2961 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8961\n",
      "Epoch 345/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5869 - op_main_loss: 0.1381 - op_conv_loss: 0.0707 - avg_loss: 0.0972 - op_main_accuracy: 0.9550 - op_conv_accuracy: 0.9737 - avg_accuracy: 0.9718\n",
      "Epoch 00345: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5875 - op_main_loss: 0.1383 - op_conv_loss: 0.0708 - avg_loss: 0.0973 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9716 - val_loss: 1.5872 - val_op_main_loss: 0.3872 - val_op_conv_loss: 0.5271 - val_avg_loss: 0.3920 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8782\n",
      "Epoch 346/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6146 - op_main_loss: 0.1482 - op_conv_loss: 0.0799 - avg_loss: 0.1059 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9679\n",
      "Epoch 00346: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6146 - op_main_loss: 0.1482 - op_conv_loss: 0.0799 - avg_loss: 0.1059 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9679 - val_loss: 1.2493 - val_op_main_loss: 0.2746 - val_op_conv_loss: 0.4090 - val_avg_loss: 0.2852 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 347/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5786 - op_main_loss: 0.1337 - op_conv_loss: 0.0698 - avg_loss: 0.0943 - op_main_accuracy: 0.9567 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9716\n",
      "Epoch 00347: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5789 - op_main_loss: 0.1338 - op_conv_loss: 0.0698 - avg_loss: 0.0945 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9716 - val_loss: 1.3308 - val_op_main_loss: 0.2988 - val_op_conv_loss: 0.4400 - val_avg_loss: 0.3115 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8942\n",
      "Epoch 348/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5588 - op_main_loss: 0.1286 - op_conv_loss: 0.0609 - avg_loss: 0.0890 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9789 - avg_accuracy: 0.9761\n",
      "Epoch 00348: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5587 - op_main_loss: 0.1285 - op_conv_loss: 0.0608 - avg_loss: 0.0889 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9790 - avg_accuracy: 0.9761 - val_loss: 1.2416 - val_op_main_loss: 0.2806 - val_op_conv_loss: 0.4001 - val_avg_loss: 0.2803 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 349/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6195 - op_main_loss: 0.1534 - op_conv_loss: 0.0790 - avg_loss: 0.1070 - op_main_accuracy: 0.9428 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9660\n",
      "Epoch 00349: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6195 - op_main_loss: 0.1534 - op_conv_loss: 0.0790 - avg_loss: 0.1070 - op_main_accuracy: 0.9428 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9660 - val_loss: 1.2197 - val_op_main_loss: 0.2704 - val_op_conv_loss: 0.3958 - val_avg_loss: 0.2732 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8961\n",
      "Epoch 350/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5802 - op_main_loss: 0.1364 - op_conv_loss: 0.0685 - avg_loss: 0.0946 - op_main_accuracy: 0.9548 - op_conv_accuracy: 0.9732 - avg_accuracy: 0.9730\n",
      "Epoch 00350: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5802 - op_main_loss: 0.1363 - op_conv_loss: 0.0686 - avg_loss: 0.0946 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9731 - val_loss: 1.2315 - val_op_main_loss: 0.2700 - val_op_conv_loss: 0.4063 - val_avg_loss: 0.2748 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8980\n",
      "Epoch 351/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5931 - op_main_loss: 0.1401 - op_conv_loss: 0.0732 - avg_loss: 0.0989 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9676\n",
      "Epoch 00351: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5931 - op_main_loss: 0.1401 - op_conv_loss: 0.0732 - avg_loss: 0.0989 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9676 - val_loss: 1.2322 - val_op_main_loss: 0.2854 - val_op_conv_loss: 0.3808 - val_avg_loss: 0.2849 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8990\n",
      "Epoch 352/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5951 - op_main_loss: 0.1401 - op_conv_loss: 0.0745 - avg_loss: 0.0994 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9690\n",
      "Epoch 00352: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5951 - op_main_loss: 0.1401 - op_conv_loss: 0.0745 - avg_loss: 0.0994 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9690 - val_loss: 1.4043 - val_op_main_loss: 0.3284 - val_op_conv_loss: 0.4601 - val_avg_loss: 0.3349 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8924\n",
      "Epoch 353/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5942 - op_main_loss: 0.1385 - op_conv_loss: 0.0755 - avg_loss: 0.0999 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9690\n",
      "Epoch 00353: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5942 - op_main_loss: 0.1385 - op_conv_loss: 0.0755 - avg_loss: 0.0999 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9690 - val_loss: 1.2969 - val_op_main_loss: 0.2952 - val_op_conv_loss: 0.4211 - val_avg_loss: 0.3004 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8980\n",
      "Epoch 354/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5831 - op_main_loss: 0.1353 - op_conv_loss: 0.0715 - avg_loss: 0.0963 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9726\n",
      "Epoch 00354: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5831 - op_main_loss: 0.1353 - op_conv_loss: 0.0715 - avg_loss: 0.0963 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9726 - val_loss: 1.4816 - val_op_main_loss: 0.3407 - val_op_conv_loss: 0.5050 - val_avg_loss: 0.3563 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8848\n",
      "Epoch 355/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5827 - op_main_loss: 0.1390 - op_conv_loss: 0.0677 - avg_loss: 0.0968 - op_main_accuracy: 0.9573 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9735\n",
      "Epoch 00355: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5834 - op_main_loss: 0.1391 - op_conv_loss: 0.0680 - avg_loss: 0.0970 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9735 - val_loss: 1.2530 - val_op_main_loss: 0.2790 - val_op_conv_loss: 0.4066 - val_avg_loss: 0.2884 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8961\n",
      "Epoch 356/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5775 - op_main_loss: 0.1341 - op_conv_loss: 0.0693 - avg_loss: 0.0954 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9714\n",
      "Epoch 00356: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5775 - op_main_loss: 0.1341 - op_conv_loss: 0.0693 - avg_loss: 0.0954 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9714 - val_loss: 1.2229 - val_op_main_loss: 0.2788 - val_op_conv_loss: 0.3874 - val_avg_loss: 0.2783 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9037\n",
      "Epoch 357/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5916 - op_main_loss: 0.1412 - op_conv_loss: 0.0732 - avg_loss: 0.0986 - op_main_accuracy: 0.9548 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9721\n",
      "Epoch 00357: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5912 - op_main_loss: 0.1411 - op_conv_loss: 0.0731 - avg_loss: 0.0985 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9721 - val_loss: 1.2315 - val_op_main_loss: 0.2767 - val_op_conv_loss: 0.3957 - val_avg_loss: 0.2806 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 358/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5839 - op_main_loss: 0.1369 - op_conv_loss: 0.0713 - avg_loss: 0.0974 - op_main_accuracy: 0.9529 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9712\n",
      "Epoch 00358: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5825 - op_main_loss: 0.1365 - op_conv_loss: 0.0708 - avg_loss: 0.0970 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9714 - val_loss: 1.2917 - val_op_main_loss: 0.2905 - val_op_conv_loss: 0.4252 - val_avg_loss: 0.2981 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8990\n",
      "Epoch 359/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5878 - op_main_loss: 0.1373 - op_conv_loss: 0.0737 - avg_loss: 0.0989 - op_main_accuracy: 0.9561 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9711\n",
      "Epoch 00359: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5874 - op_main_loss: 0.1374 - op_conv_loss: 0.0734 - avg_loss: 0.0987 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9712 - val_loss: 1.5513 - val_op_main_loss: 0.3391 - val_op_conv_loss: 0.5626 - val_avg_loss: 0.3715 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8697\n",
      "Epoch 360/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5941 - op_main_loss: 0.1418 - op_conv_loss: 0.0742 - avg_loss: 0.0998 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9695\n",
      "Epoch 00360: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5941 - op_main_loss: 0.1418 - op_conv_loss: 0.0742 - avg_loss: 0.0998 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9695 - val_loss: 1.2104 - val_op_main_loss: 0.2733 - val_op_conv_loss: 0.3811 - val_avg_loss: 0.2783 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8999\n",
      "Epoch 361/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5618 - op_main_loss: 0.1297 - op_conv_loss: 0.0643 - avg_loss: 0.0902 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9773 - avg_accuracy: 0.9745\n",
      "Epoch 00361: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5618 - op_main_loss: 0.1297 - op_conv_loss: 0.0643 - avg_loss: 0.0902 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9773 - avg_accuracy: 0.9745 - val_loss: 1.3983 - val_op_main_loss: 0.3157 - val_op_conv_loss: 0.4720 - val_avg_loss: 0.3336 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8867\n",
      "Epoch 362/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5602 - op_main_loss: 0.1309 - op_conv_loss: 0.0617 - avg_loss: 0.0905 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9738\n",
      "Epoch 00362: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5602 - op_main_loss: 0.1309 - op_conv_loss: 0.0617 - avg_loss: 0.0905 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9738 - val_loss: 1.2211 - val_op_main_loss: 0.2817 - val_op_conv_loss: 0.3804 - val_avg_loss: 0.2822 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8961\n",
      "Epoch 363/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6023 - op_main_loss: 0.1461 - op_conv_loss: 0.0770 - avg_loss: 0.1032 - op_main_accuracy: 0.9497 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9688\n",
      "Epoch 00363: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6016 - op_main_loss: 0.1458 - op_conv_loss: 0.0768 - avg_loss: 0.1030 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9688 - val_loss: 1.3110 - val_op_main_loss: 0.2848 - val_op_conv_loss: 0.4463 - val_avg_loss: 0.3045 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8886\n",
      "Epoch 364/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5620 - op_main_loss: 0.1289 - op_conv_loss: 0.0659 - avg_loss: 0.0915 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9721\n",
      "Epoch 00364: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5620 - op_main_loss: 0.1289 - op_conv_loss: 0.0659 - avg_loss: 0.0915 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9721 - val_loss: 1.2781 - val_op_main_loss: 0.2833 - val_op_conv_loss: 0.4273 - val_avg_loss: 0.2917 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8961\n",
      "Epoch 365/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6251 - op_main_loss: 0.1504 - op_conv_loss: 0.0890 - avg_loss: 0.1096 - op_main_accuracy: 0.9458 - op_conv_accuracy: 0.9671 - avg_accuracy: 0.9637\n",
      "Epoch 00365: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6268 - op_main_loss: 0.1512 - op_conv_loss: 0.0893 - avg_loss: 0.1102 - op_main_accuracy: 0.9454 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9634 - val_loss: 1.3051 - val_op_main_loss: 0.2940 - val_op_conv_loss: 0.4290 - val_avg_loss: 0.3059 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8914\n",
      "Epoch 366/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5761 - op_main_loss: 0.1332 - op_conv_loss: 0.0713 - avg_loss: 0.0956 - op_main_accuracy: 0.9606 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9695\n",
      "Epoch 00366: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5764 - op_main_loss: 0.1332 - op_conv_loss: 0.0714 - avg_loss: 0.0958 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9693 - val_loss: 1.4926 - val_op_main_loss: 0.3218 - val_op_conv_loss: 0.5420 - val_avg_loss: 0.3528 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8782\n",
      "Epoch 367/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5729 - op_main_loss: 0.1358 - op_conv_loss: 0.0663 - avg_loss: 0.0944 - op_main_accuracy: 0.9552 - op_conv_accuracy: 0.9723 - avg_accuracy: 0.9723\n",
      "Epoch 00367: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5713 - op_main_loss: 0.1353 - op_conv_loss: 0.0658 - avg_loss: 0.0940 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9726 - val_loss: 1.2370 - val_op_main_loss: 0.2767 - val_op_conv_loss: 0.3983 - val_avg_loss: 0.2858 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8999\n",
      "Epoch 368/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5785 - op_main_loss: 0.1335 - op_conv_loss: 0.0722 - avg_loss: 0.0964 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9695\n",
      "Epoch 00368: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5772 - op_main_loss: 0.1331 - op_conv_loss: 0.0717 - avg_loss: 0.0959 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9695 - val_loss: 1.2978 - val_op_main_loss: 0.2956 - val_op_conv_loss: 0.4204 - val_avg_loss: 0.3043 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8980\n",
      "Epoch 369/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5757 - op_main_loss: 0.1323 - op_conv_loss: 0.0717 - avg_loss: 0.0948 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9695\n",
      "Epoch 00369: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5757 - op_main_loss: 0.1323 - op_conv_loss: 0.0717 - avg_loss: 0.0948 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9695 - val_loss: 1.7122 - val_op_main_loss: 0.3860 - val_op_conv_loss: 0.6280 - val_avg_loss: 0.4219 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8527 - val_avg_accuracy: 0.8584\n",
      "Epoch 370/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5742 - op_main_loss: 0.1335 - op_conv_loss: 0.0697 - avg_loss: 0.0949 - op_main_accuracy: 0.9576 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9735\n",
      "Epoch 00370: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5743 - op_main_loss: 0.1335 - op_conv_loss: 0.0698 - avg_loss: 0.0949 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9735 - val_loss: 1.2510 - val_op_main_loss: 0.2809 - val_op_conv_loss: 0.4066 - val_avg_loss: 0.2878 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9037\n",
      "Epoch 371/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5828 - op_main_loss: 0.1364 - op_conv_loss: 0.0729 - avg_loss: 0.0977 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9725\n",
      "Epoch 00371: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5824 - op_main_loss: 0.1363 - op_conv_loss: 0.0727 - avg_loss: 0.0976 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9726 - val_loss: 1.3714 - val_op_main_loss: 0.3205 - val_op_conv_loss: 0.4449 - val_avg_loss: 0.3297 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8895\n",
      "Epoch 372/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5608 - op_main_loss: 0.1304 - op_conv_loss: 0.0641 - avg_loss: 0.0901 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9784 - avg_accuracy: 0.9760\n",
      "Epoch 00372: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5583 - op_main_loss: 0.1294 - op_conv_loss: 0.0634 - avg_loss: 0.0893 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9764 - val_loss: 1.2416 - val_op_main_loss: 0.2803 - val_op_conv_loss: 0.4030 - val_avg_loss: 0.2824 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9027\n",
      "Epoch 373/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5706 - op_main_loss: 0.1343 - op_conv_loss: 0.0670 - avg_loss: 0.0935 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9719\n",
      "Epoch 00373: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5706 - op_main_loss: 0.1343 - op_conv_loss: 0.0670 - avg_loss: 0.0935 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9719 - val_loss: 1.2676 - val_op_main_loss: 0.2845 - val_op_conv_loss: 0.4122 - val_avg_loss: 0.2953 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8971\n",
      "Epoch 374/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5915 - op_main_loss: 0.1402 - op_conv_loss: 0.0761 - avg_loss: 0.1001 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9695\n",
      "Epoch 00374: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5915 - op_main_loss: 0.1402 - op_conv_loss: 0.0761 - avg_loss: 0.1001 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9695 - val_loss: 1.2451 - val_op_main_loss: 0.2784 - val_op_conv_loss: 0.4041 - val_avg_loss: 0.2878 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 375/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6074 - op_main_loss: 0.1418 - op_conv_loss: 0.0854 - avg_loss: 0.1053 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9693\n",
      "Epoch 00375: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6074 - op_main_loss: 0.1418 - op_conv_loss: 0.0854 - avg_loss: 0.1053 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9693 - val_loss: 1.2535 - val_op_main_loss: 0.2872 - val_op_conv_loss: 0.3992 - val_avg_loss: 0.2921 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8961\n",
      "Epoch 376/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5699 - op_main_loss: 0.1305 - op_conv_loss: 0.0700 - avg_loss: 0.0943 - op_main_accuracy: 0.9557 - op_conv_accuracy: 0.9711 - avg_accuracy: 0.9721\n",
      "Epoch 00376: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5696 - op_main_loss: 0.1304 - op_conv_loss: 0.0699 - avg_loss: 0.0942 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9721 - val_loss: 1.4572 - val_op_main_loss: 0.3691 - val_op_conv_loss: 0.4518 - val_avg_loss: 0.3613 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8820\n",
      "Epoch 377/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5688 - op_main_loss: 0.1325 - op_conv_loss: 0.0676 - avg_loss: 0.0936 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9738\n",
      "Epoch 00377: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5688 - op_main_loss: 0.1325 - op_conv_loss: 0.0676 - avg_loss: 0.0936 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9738 - val_loss: 1.2485 - val_op_main_loss: 0.2917 - val_op_conv_loss: 0.3951 - val_avg_loss: 0.2865 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8980\n",
      "Epoch 378/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5933 - op_main_loss: 0.1384 - op_conv_loss: 0.0790 - avg_loss: 0.1010 - op_main_accuracy: 0.9524 - op_conv_accuracy: 0.9671 - avg_accuracy: 0.9673\n",
      "Epoch 00378: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5934 - op_main_loss: 0.1384 - op_conv_loss: 0.0790 - avg_loss: 0.1010 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9674 - val_loss: 1.2136 - val_op_main_loss: 0.2824 - val_op_conv_loss: 0.3756 - val_avg_loss: 0.2808 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 379/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5841 - op_main_loss: 0.1364 - op_conv_loss: 0.0740 - avg_loss: 0.0985 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9698\n",
      "Epoch 00379: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5841 - op_main_loss: 0.1364 - op_conv_loss: 0.0740 - avg_loss: 0.0985 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9698 - val_loss: 1.2662 - val_op_main_loss: 0.2922 - val_op_conv_loss: 0.4065 - val_avg_loss: 0.2927 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8924\n",
      "Epoch 380/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5632 - op_main_loss: 0.1327 - op_conv_loss: 0.0639 - avg_loss: 0.0913 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9776\n",
      "Epoch 00380: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5632 - op_main_loss: 0.1327 - op_conv_loss: 0.0639 - avg_loss: 0.0913 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9776 - val_loss: 1.2695 - val_op_main_loss: 0.2903 - val_op_conv_loss: 0.4095 - val_avg_loss: 0.2943 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8971\n",
      "Epoch 381/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5773 - op_main_loss: 0.1327 - op_conv_loss: 0.0731 - avg_loss: 0.0963 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9728\n",
      "Epoch 00381: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5773 - op_main_loss: 0.1327 - op_conv_loss: 0.0731 - avg_loss: 0.0963 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9728 - val_loss: 1.3116 - val_op_main_loss: 0.3043 - val_op_conv_loss: 0.4217 - val_avg_loss: 0.3105 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8961\n",
      "Epoch 382/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5741 - op_main_loss: 0.1345 - op_conv_loss: 0.0699 - avg_loss: 0.0947 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9726\n",
      "Epoch 00382: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5741 - op_main_loss: 0.1345 - op_conv_loss: 0.0699 - avg_loss: 0.0947 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9726 - val_loss: 1.5869 - val_op_main_loss: 0.3538 - val_op_conv_loss: 0.5702 - val_avg_loss: 0.3880 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8631\n",
      "Epoch 383/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5828 - op_main_loss: 0.1375 - op_conv_loss: 0.0731 - avg_loss: 0.0974 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9716\n",
      "Epoch 00383: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5828 - op_main_loss: 0.1375 - op_conv_loss: 0.0731 - avg_loss: 0.0974 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9716 - val_loss: 1.2243 - val_op_main_loss: 0.2784 - val_op_conv_loss: 0.3929 - val_avg_loss: 0.2786 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8980\n",
      "Epoch 384/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5593 - op_main_loss: 0.1293 - op_conv_loss: 0.0643 - avg_loss: 0.0912 - op_main_accuracy: 0.9576 - op_conv_accuracy: 0.9756 - avg_accuracy: 0.9742\n",
      "Epoch 00384: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5599 - op_main_loss: 0.1296 - op_conv_loss: 0.0644 - avg_loss: 0.0915 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9740 - val_loss: 1.2316 - val_op_main_loss: 0.2895 - val_op_conv_loss: 0.3820 - val_avg_loss: 0.2856 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9018\n",
      "Epoch 385/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5621 - op_main_loss: 0.1307 - op_conv_loss: 0.0652 - avg_loss: 0.0917 - op_main_accuracy: 0.9548 - op_conv_accuracy: 0.9730 - avg_accuracy: 0.9725\n",
      "Epoch 00385: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5626 - op_main_loss: 0.1308 - op_conv_loss: 0.0653 - avg_loss: 0.0918 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9724 - val_loss: 1.2481 - val_op_main_loss: 0.2868 - val_op_conv_loss: 0.3966 - val_avg_loss: 0.2902 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 386/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5611 - op_main_loss: 0.1340 - op_conv_loss: 0.0619 - avg_loss: 0.0909 - op_main_accuracy: 0.9541 - op_conv_accuracy: 0.9755 - avg_accuracy: 0.9736\n",
      "Epoch 00386: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5620 - op_main_loss: 0.1342 - op_conv_loss: 0.0622 - avg_loss: 0.0912 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9731 - val_loss: 1.2448 - val_op_main_loss: 0.2818 - val_op_conv_loss: 0.4016 - val_avg_loss: 0.2871 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8990\n",
      "Epoch 387/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5978 - op_main_loss: 0.1428 - op_conv_loss: 0.0785 - avg_loss: 0.1024 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9679\n",
      "Epoch 00387: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5978 - op_main_loss: 0.1428 - op_conv_loss: 0.0785 - avg_loss: 0.1024 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9679 - val_loss: 1.3760 - val_op_main_loss: 0.2798 - val_op_conv_loss: 0.5101 - val_avg_loss: 0.3113 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8848\n",
      "Epoch 388/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5528 - op_main_loss: 0.1259 - op_conv_loss: 0.0632 - avg_loss: 0.0887 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9721\n",
      "Epoch 00388: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5528 - op_main_loss: 0.1259 - op_conv_loss: 0.0632 - avg_loss: 0.0887 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9721 - val_loss: 1.6103 - val_op_main_loss: 0.4239 - val_op_conv_loss: 0.4999 - val_avg_loss: 0.4116 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8735\n",
      "Epoch 389/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5656 - op_main_loss: 0.1309 - op_conv_loss: 0.0679 - avg_loss: 0.0922 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9724\n",
      "Epoch 00389: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5656 - op_main_loss: 0.1309 - op_conv_loss: 0.0679 - avg_loss: 0.0922 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9724 - val_loss: 1.1909 - val_op_main_loss: 0.2792 - val_op_conv_loss: 0.3637 - val_avg_loss: 0.2735 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9056\n",
      "Epoch 390/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5593 - op_main_loss: 0.1316 - op_conv_loss: 0.0629 - avg_loss: 0.0906 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9792 - avg_accuracy: 0.9759\n",
      "Epoch 00390: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5593 - op_main_loss: 0.1316 - op_conv_loss: 0.0629 - avg_loss: 0.0906 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9792 - avg_accuracy: 0.9759 - val_loss: 1.2832 - val_op_main_loss: 0.2931 - val_op_conv_loss: 0.4180 - val_avg_loss: 0.2981 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8999\n",
      "Epoch 391/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5688 - op_main_loss: 0.1302 - op_conv_loss: 0.0711 - avg_loss: 0.0936 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9731\n",
      "Epoch 00391: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5688 - op_main_loss: 0.1302 - op_conv_loss: 0.0711 - avg_loss: 0.0936 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9731 - val_loss: 1.2879 - val_op_main_loss: 0.2935 - val_op_conv_loss: 0.4226 - val_avg_loss: 0.2980 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8980\n",
      "Epoch 392/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6654 - op_main_loss: 0.1678 - op_conv_loss: 0.1014 - avg_loss: 0.1227 - op_main_accuracy: 0.9390 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9579\n",
      "Epoch 00392: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6654 - op_main_loss: 0.1678 - op_conv_loss: 0.1014 - avg_loss: 0.1227 - op_main_accuracy: 0.9390 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9579 - val_loss: 1.6978 - val_op_main_loss: 0.3413 - val_op_conv_loss: 0.6863 - val_avg_loss: 0.3967 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8432 - val_avg_accuracy: 0.8470\n",
      "Epoch 393/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5933 - op_main_loss: 0.1439 - op_conv_loss: 0.0742 - avg_loss: 0.1009 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9719\n",
      "Epoch 00393: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5933 - op_main_loss: 0.1439 - op_conv_loss: 0.0742 - avg_loss: 0.1009 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9719 - val_loss: 1.2140 - val_op_main_loss: 0.2792 - val_op_conv_loss: 0.3806 - val_avg_loss: 0.2796 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9008\n",
      "Epoch 394/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5526 - op_main_loss: 0.1263 - op_conv_loss: 0.0633 - avg_loss: 0.0886 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9768\n",
      "Epoch 00394: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5526 - op_main_loss: 0.1263 - op_conv_loss: 0.0633 - avg_loss: 0.0886 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9768 - val_loss: 1.2045 - val_op_main_loss: 0.2730 - val_op_conv_loss: 0.3811 - val_avg_loss: 0.2760 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9046\n",
      "Epoch 395/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5804 - op_main_loss: 0.1347 - op_conv_loss: 0.0744 - avg_loss: 0.0970 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9716\n",
      "Epoch 00395: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5804 - op_main_loss: 0.1347 - op_conv_loss: 0.0744 - avg_loss: 0.0970 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9716 - val_loss: 1.2345 - val_op_main_loss: 0.2808 - val_op_conv_loss: 0.3949 - val_avg_loss: 0.2842 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 396/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5851 - op_main_loss: 0.1380 - op_conv_loss: 0.0742 - avg_loss: 0.0981 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9721\n",
      "Epoch 00396: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5851 - op_main_loss: 0.1380 - op_conv_loss: 0.0742 - avg_loss: 0.0981 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9721 - val_loss: 1.3408 - val_op_main_loss: 0.3239 - val_op_conv_loss: 0.4193 - val_avg_loss: 0.3230 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8905\n",
      "Epoch 397/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5452 - op_main_loss: 0.1243 - op_conv_loss: 0.0600 - avg_loss: 0.0864 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9738\n",
      "Epoch 00397: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5452 - op_main_loss: 0.1243 - op_conv_loss: 0.0600 - avg_loss: 0.0864 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9738 - val_loss: 1.2863 - val_op_main_loss: 0.3160 - val_op_conv_loss: 0.3900 - val_avg_loss: 0.3060 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8952\n",
      "Epoch 398/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5472 - op_main_loss: 0.1252 - op_conv_loss: 0.0607 - avg_loss: 0.0873 - op_main_accuracy: 0.9585 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9761\n",
      "Epoch 00398: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5471 - op_main_loss: 0.1253 - op_conv_loss: 0.0605 - avg_loss: 0.0873 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9764 - val_loss: 1.6770 - val_op_main_loss: 0.3565 - val_op_conv_loss: 0.6442 - val_avg_loss: 0.4026 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8489 - val_avg_accuracy: 0.8602\n",
      "Epoch 399/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5767 - op_main_loss: 0.1338 - op_conv_loss: 0.0724 - avg_loss: 0.0966 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9705\n",
      "Epoch 00399: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5767 - op_main_loss: 0.1338 - op_conv_loss: 0.0724 - avg_loss: 0.0966 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9705 - val_loss: 1.2624 - val_op_main_loss: 0.2898 - val_op_conv_loss: 0.4039 - val_avg_loss: 0.2948 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8961\n",
      "Epoch 400/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5761 - op_main_loss: 0.1351 - op_conv_loss: 0.0727 - avg_loss: 0.0944 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9714\n",
      "Epoch 00400: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5751 - op_main_loss: 0.1348 - op_conv_loss: 0.0723 - avg_loss: 0.0941 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9716 - val_loss: 1.2320 - val_op_main_loss: 0.2883 - val_op_conv_loss: 0.3864 - val_avg_loss: 0.2838 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8980\n",
      "Epoch 401/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.5533 - op_main_loss: 0.1312 - op_conv_loss: 0.0592 - avg_loss: 0.0893 - op_main_accuracy: 0.9561 - op_conv_accuracy: 0.9769 - avg_accuracy: 0.9757\n",
      "Epoch 00401: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5544 - op_main_loss: 0.1316 - op_conv_loss: 0.0595 - avg_loss: 0.0896 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9752 - val_loss: 1.2238 - val_op_main_loss: 0.2715 - val_op_conv_loss: 0.3981 - val_avg_loss: 0.2807 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 402/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5661 - op_main_loss: 0.1301 - op_conv_loss: 0.0699 - avg_loss: 0.0927 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9731\n",
      "Epoch 00402: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5661 - op_main_loss: 0.1301 - op_conv_loss: 0.0699 - avg_loss: 0.0927 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9731 - val_loss: 1.2348 - val_op_main_loss: 0.2737 - val_op_conv_loss: 0.4029 - val_avg_loss: 0.2852 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8990\n",
      "Epoch 403/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5877 - op_main_loss: 0.1364 - op_conv_loss: 0.0788 - avg_loss: 0.0995 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9690\n",
      "Epoch 00403: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5877 - op_main_loss: 0.1364 - op_conv_loss: 0.0788 - avg_loss: 0.0995 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9690 - val_loss: 1.4389 - val_op_main_loss: 0.3217 - val_op_conv_loss: 0.4974 - val_avg_loss: 0.3472 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8744\n",
      "Epoch 404/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5644 - op_main_loss: 0.1303 - op_conv_loss: 0.0688 - avg_loss: 0.0926 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9735\n",
      "Epoch 00404: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5644 - op_main_loss: 0.1303 - op_conv_loss: 0.0688 - avg_loss: 0.0926 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9735 - val_loss: 1.2569 - val_op_main_loss: 0.2914 - val_op_conv_loss: 0.3979 - val_avg_loss: 0.2947 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9008\n",
      "Epoch 405/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5967 - op_main_loss: 0.1403 - op_conv_loss: 0.0811 - avg_loss: 0.1018 - op_main_accuracy: 0.9498 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9702\n",
      "Epoch 00405: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5975 - op_main_loss: 0.1406 - op_conv_loss: 0.0814 - avg_loss: 0.1020 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9700 - val_loss: 1.7314 - val_op_main_loss: 0.4069 - val_op_conv_loss: 0.6226 - val_avg_loss: 0.4284 - val_op_main_accuracy: 0.8508 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8584\n",
      "Epoch 406/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5806 - op_main_loss: 0.1345 - op_conv_loss: 0.0758 - avg_loss: 0.0968 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9700\n",
      "Epoch 00406: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5806 - op_main_loss: 0.1345 - op_conv_loss: 0.0758 - avg_loss: 0.0968 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9700 - val_loss: 1.1959 - val_op_main_loss: 0.2712 - val_op_conv_loss: 0.3756 - val_avg_loss: 0.2755 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9046\n",
      "Epoch 407/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5486 - op_main_loss: 0.1264 - op_conv_loss: 0.0611 - avg_loss: 0.0873 - op_main_accuracy: 0.9602 - op_conv_accuracy: 0.9782 - avg_accuracy: 0.9751\n",
      "Epoch 00407: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5490 - op_main_loss: 0.1267 - op_conv_loss: 0.0611 - avg_loss: 0.0874 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9783 - avg_accuracy: 0.9752 - val_loss: 1.2386 - val_op_main_loss: 0.2921 - val_op_conv_loss: 0.3832 - val_avg_loss: 0.2890 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9018\n",
      "Epoch 408/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5563 - op_main_loss: 0.1275 - op_conv_loss: 0.0646 - avg_loss: 0.0901 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9721\n",
      "Epoch 00408: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5559 - op_main_loss: 0.1274 - op_conv_loss: 0.0645 - avg_loss: 0.0900 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9724 - val_loss: 1.3250 - val_op_main_loss: 0.3202 - val_op_conv_loss: 0.4113 - val_avg_loss: 0.3199 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8961\n",
      "Epoch 409/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5676 - op_main_loss: 0.1298 - op_conv_loss: 0.0717 - avg_loss: 0.0928 - op_main_accuracy: 0.9566 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9730\n",
      "Epoch 00409: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5670 - op_main_loss: 0.1297 - op_conv_loss: 0.0714 - avg_loss: 0.0926 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9733 - val_loss: 1.2363 - val_op_main_loss: 0.2926 - val_op_conv_loss: 0.3811 - val_avg_loss: 0.2888 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9018\n",
      "Epoch 410/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5471 - op_main_loss: 0.1240 - op_conv_loss: 0.0624 - avg_loss: 0.0871 - op_main_accuracy: 0.9599 - op_conv_accuracy: 0.9779 - avg_accuracy: 0.9743\n",
      "Epoch 00410: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5462 - op_main_loss: 0.1239 - op_conv_loss: 0.0619 - avg_loss: 0.0868 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9783 - avg_accuracy: 0.9747 - val_loss: 1.2166 - val_op_main_loss: 0.2805 - val_op_conv_loss: 0.3809 - val_avg_loss: 0.2817 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 411/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5715 - op_main_loss: 0.1319 - op_conv_loss: 0.0717 - avg_loss: 0.0956 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9690\n",
      "Epoch 00411: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5715 - op_main_loss: 0.1319 - op_conv_loss: 0.0717 - avg_loss: 0.0956 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9690 - val_loss: 1.2024 - val_op_main_loss: 0.2754 - val_op_conv_loss: 0.3743 - val_avg_loss: 0.2806 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8999\n",
      "Epoch 412/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5657 - op_main_loss: 0.1335 - op_conv_loss: 0.0673 - avg_loss: 0.0926 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9725\n",
      "Epoch 00412: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5653 - op_main_loss: 0.1334 - op_conv_loss: 0.0671 - avg_loss: 0.0925 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9726 - val_loss: 1.2123 - val_op_main_loss: 0.2843 - val_op_conv_loss: 0.3716 - val_avg_loss: 0.2838 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 413/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.5612 - op_main_loss: 0.1294 - op_conv_loss: 0.0682 - avg_loss: 0.0915 - op_main_accuracy: 0.9555 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9718\n",
      "Epoch 00413: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5632 - op_main_loss: 0.1298 - op_conv_loss: 0.0691 - avg_loss: 0.0921 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9714 - val_loss: 1.5146 - val_op_main_loss: 0.3503 - val_op_conv_loss: 0.5186 - val_avg_loss: 0.3736 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8725\n",
      "Epoch 414/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5810 - op_main_loss: 0.1329 - op_conv_loss: 0.0779 - avg_loss: 0.0973 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9675 - avg_accuracy: 0.9673\n",
      "Epoch 00414: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5825 - op_main_loss: 0.1339 - op_conv_loss: 0.0780 - avg_loss: 0.0978 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9672 - val_loss: 1.4836 - val_op_main_loss: 0.3419 - val_op_conv_loss: 0.5071 - val_avg_loss: 0.3621 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8782\n",
      "Epoch 415/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5353 - op_main_loss: 0.1221 - op_conv_loss: 0.0573 - avg_loss: 0.0834 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9797 - avg_accuracy: 0.9794\n",
      "Epoch 00415: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5353 - op_main_loss: 0.1221 - op_conv_loss: 0.0573 - avg_loss: 0.0834 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9797 - avg_accuracy: 0.9794 - val_loss: 1.2522 - val_op_main_loss: 0.2812 - val_op_conv_loss: 0.4061 - val_avg_loss: 0.2927 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8980\n",
      "Epoch 416/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5479 - op_main_loss: 0.1255 - op_conv_loss: 0.0624 - avg_loss: 0.0877 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9752\n",
      "Epoch 00416: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5479 - op_main_loss: 0.1255 - op_conv_loss: 0.0624 - avg_loss: 0.0877 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9752 - val_loss: 1.2294 - val_op_main_loss: 0.2750 - val_op_conv_loss: 0.3988 - val_avg_loss: 0.2836 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9008\n",
      "Epoch 417/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5564 - op_main_loss: 0.1273 - op_conv_loss: 0.0669 - avg_loss: 0.0901 - op_main_accuracy: 0.9573 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9742\n",
      "Epoch 00417: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5566 - op_main_loss: 0.1275 - op_conv_loss: 0.0667 - avg_loss: 0.0902 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9742 - val_loss: 1.2373 - val_op_main_loss: 0.2881 - val_op_conv_loss: 0.3873 - val_avg_loss: 0.2898 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9037\n",
      "Epoch 418/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5576 - op_main_loss: 0.1315 - op_conv_loss: 0.0636 - avg_loss: 0.0902 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9738\n",
      "Epoch 00418: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5576 - op_main_loss: 0.1315 - op_conv_loss: 0.0636 - avg_loss: 0.0902 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9738 - val_loss: 1.7439 - val_op_main_loss: 0.3819 - val_op_conv_loss: 0.6633 - val_avg_loss: 0.4264 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8527 - val_avg_accuracy: 0.8565\n",
      "Epoch 419/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5678 - op_main_loss: 0.1355 - op_conv_loss: 0.0667 - avg_loss: 0.0937 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9721\n",
      "Epoch 00419: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5678 - op_main_loss: 0.1355 - op_conv_loss: 0.0667 - avg_loss: 0.0937 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9721 - val_loss: 1.2699 - val_op_main_loss: 0.2806 - val_op_conv_loss: 0.4238 - val_avg_loss: 0.2936 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8980\n",
      "Epoch 420/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5458 - op_main_loss: 0.1215 - op_conv_loss: 0.0653 - avg_loss: 0.0871 - op_main_accuracy: 0.9616 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9744\n",
      "Epoch 00420: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5455 - op_main_loss: 0.1214 - op_conv_loss: 0.0652 - avg_loss: 0.0870 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9745 - val_loss: 1.3304 - val_op_main_loss: 0.3131 - val_op_conv_loss: 0.4302 - val_avg_loss: 0.3154 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8924\n",
      "Epoch 421/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5462 - op_main_loss: 0.1260 - op_conv_loss: 0.0611 - avg_loss: 0.0877 - op_main_accuracy: 0.9585 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9752\n",
      "Epoch 00421: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5459 - op_main_loss: 0.1260 - op_conv_loss: 0.0609 - avg_loss: 0.0876 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9754 - val_loss: 1.5356 - val_op_main_loss: 0.3743 - val_op_conv_loss: 0.5082 - val_avg_loss: 0.3819 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8782\n",
      "Epoch 422/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5423 - op_main_loss: 0.1227 - op_conv_loss: 0.0622 - avg_loss: 0.0861 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9747\n",
      "Epoch 00422: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5423 - op_main_loss: 0.1227 - op_conv_loss: 0.0622 - avg_loss: 0.0861 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9747 - val_loss: 1.7946 - val_op_main_loss: 0.4342 - val_op_conv_loss: 0.6324 - val_avg_loss: 0.4565 - val_op_main_accuracy: 0.8480 - val_op_conv_accuracy: 0.8508 - val_avg_accuracy: 0.8536\n",
      "Epoch 423/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5539 - op_main_loss: 0.1293 - op_conv_loss: 0.0635 - avg_loss: 0.0898 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9742\n",
      "Epoch 00423: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5539 - op_main_loss: 0.1293 - op_conv_loss: 0.0635 - avg_loss: 0.0898 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9742 - val_loss: 1.3555 - val_op_main_loss: 0.3118 - val_op_conv_loss: 0.4516 - val_avg_loss: 0.3211 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 424/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5632 - op_main_loss: 0.1285 - op_conv_loss: 0.0709 - avg_loss: 0.0925 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9719\n",
      "Epoch 00424: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5632 - op_main_loss: 0.1285 - op_conv_loss: 0.0709 - avg_loss: 0.0925 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9719 - val_loss: 1.3340 - val_op_main_loss: 0.3158 - val_op_conv_loss: 0.4268 - val_avg_loss: 0.3201 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8895\n",
      "Epoch 425/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5669 - op_main_loss: 0.1268 - op_conv_loss: 0.0747 - avg_loss: 0.0938 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9690\n",
      "Epoch 00425: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5669 - op_main_loss: 0.1268 - op_conv_loss: 0.0747 - avg_loss: 0.0938 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9690 - val_loss: 1.3092 - val_op_main_loss: 0.3019 - val_op_conv_loss: 0.4245 - val_avg_loss: 0.3112 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8924\n",
      "Epoch 426/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5743 - op_main_loss: 0.1337 - op_conv_loss: 0.0734 - avg_loss: 0.0963 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9707\n",
      "Epoch 00426: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5743 - op_main_loss: 0.1337 - op_conv_loss: 0.0734 - avg_loss: 0.0963 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9707 - val_loss: 1.2812 - val_op_main_loss: 0.2841 - val_op_conv_loss: 0.4252 - val_avg_loss: 0.3014 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8905\n",
      "Epoch 427/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5411 - op_main_loss: 0.1206 - op_conv_loss: 0.0635 - avg_loss: 0.0858 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9747\n",
      "Epoch 00427: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5411 - op_main_loss: 0.1206 - op_conv_loss: 0.0635 - avg_loss: 0.0858 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9747 - val_loss: 1.2172 - val_op_main_loss: 0.2807 - val_op_conv_loss: 0.3829 - val_avg_loss: 0.2817 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9046\n",
      "Epoch 428/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5589 - op_main_loss: 0.1284 - op_conv_loss: 0.0678 - avg_loss: 0.0913 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9712\n",
      "Epoch 00428: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5589 - op_main_loss: 0.1284 - op_conv_loss: 0.0678 - avg_loss: 0.0913 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9712 - val_loss: 1.8901 - val_op_main_loss: 0.4917 - val_op_conv_loss: 0.6329 - val_avg_loss: 0.4940 - val_op_main_accuracy: 0.8404 - val_op_conv_accuracy: 0.8584 - val_avg_accuracy: 0.8517\n",
      "Epoch 429/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5606 - op_main_loss: 0.1295 - op_conv_loss: 0.0677 - avg_loss: 0.0924 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9724\n",
      "Epoch 00429: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5606 - op_main_loss: 0.1295 - op_conv_loss: 0.0677 - avg_loss: 0.0924 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9724 - val_loss: 1.2258 - val_op_main_loss: 0.2849 - val_op_conv_loss: 0.3846 - val_avg_loss: 0.2855 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9037\n",
      "Epoch 430/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5566 - op_main_loss: 0.1266 - op_conv_loss: 0.0673 - avg_loss: 0.0917 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9714\n",
      "Epoch 00430: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5566 - op_main_loss: 0.1266 - op_conv_loss: 0.0673 - avg_loss: 0.0917 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9714 - val_loss: 1.3604 - val_op_main_loss: 0.3037 - val_op_conv_loss: 0.4614 - val_avg_loss: 0.3244 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8886\n",
      "Epoch 431/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5480 - op_main_loss: 0.1237 - op_conv_loss: 0.0647 - avg_loss: 0.0886 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9709\n",
      "Epoch 00431: val_avg_accuracy did not improve from 0.90557\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5480 - op_main_loss: 0.1237 - op_conv_loss: 0.0647 - avg_loss: 0.0886 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9709 - val_loss: 1.5368 - val_op_main_loss: 0.3508 - val_op_conv_loss: 0.5403 - val_avg_loss: 0.3748 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8744\n",
      "Epoch 432/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5382 - op_main_loss: 0.1231 - op_conv_loss: 0.0595 - avg_loss: 0.0851 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9761\n",
      "Epoch 00432: val_avg_accuracy improved from 0.90557 to 0.90746, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5382 - op_main_loss: 0.1231 - op_conv_loss: 0.0595 - avg_loss: 0.0851 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9761 - val_loss: 1.2301 - val_op_main_loss: 0.2809 - val_op_conv_loss: 0.3948 - val_avg_loss: 0.2841 - val_op_main_accuracy: 0.9112 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9075\n",
      "Epoch 433/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5423 - op_main_loss: 0.1206 - op_conv_loss: 0.0647 - avg_loss: 0.0868 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9745\n",
      "Epoch 00433: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5423 - op_main_loss: 0.1206 - op_conv_loss: 0.0647 - avg_loss: 0.0868 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9745 - val_loss: 1.3855 - val_op_main_loss: 0.3291 - val_op_conv_loss: 0.4517 - val_avg_loss: 0.3345 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 434/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5645 - op_main_loss: 0.1266 - op_conv_loss: 0.0748 - avg_loss: 0.0930 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9721\n",
      "Epoch 00434: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5645 - op_main_loss: 0.1266 - op_conv_loss: 0.0748 - avg_loss: 0.0930 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9721 - val_loss: 1.2384 - val_op_main_loss: 0.2870 - val_op_conv_loss: 0.3881 - val_avg_loss: 0.2931 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8971\n",
      "Epoch 435/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5452 - op_main_loss: 0.1233 - op_conv_loss: 0.0641 - avg_loss: 0.0876 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9733\n",
      "Epoch 00435: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5452 - op_main_loss: 0.1233 - op_conv_loss: 0.0641 - avg_loss: 0.0876 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9733 - val_loss: 1.4944 - val_op_main_loss: 0.3532 - val_op_conv_loss: 0.5048 - val_avg_loss: 0.3660 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8791\n",
      "Epoch 436/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5954 - op_main_loss: 0.1403 - op_conv_loss: 0.0825 - avg_loss: 0.1025 - op_main_accuracy: 0.9509 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9657\n",
      "Epoch 00436: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5954 - op_main_loss: 0.1403 - op_conv_loss: 0.0825 - avg_loss: 0.1025 - op_main_accuracy: 0.9509 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9657 - val_loss: 1.1993 - val_op_main_loss: 0.2687 - val_op_conv_loss: 0.3833 - val_avg_loss: 0.2777 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9037\n",
      "Epoch 437/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5686 - op_main_loss: 0.1296 - op_conv_loss: 0.0747 - avg_loss: 0.0944 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9724\n",
      "Epoch 00437: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5686 - op_main_loss: 0.1296 - op_conv_loss: 0.0747 - avg_loss: 0.0944 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9724 - val_loss: 1.7291 - val_op_main_loss: 0.4069 - val_op_conv_loss: 0.6172 - val_avg_loss: 0.4346 - val_op_main_accuracy: 0.8555 - val_op_conv_accuracy: 0.8536 - val_avg_accuracy: 0.8584\n",
      "Epoch 438/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5430 - op_main_loss: 0.1238 - op_conv_loss: 0.0622 - avg_loss: 0.0869 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9742\n",
      "Epoch 00438: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5430 - op_main_loss: 0.1238 - op_conv_loss: 0.0622 - avg_loss: 0.0869 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9742 - val_loss: 1.2432 - val_op_main_loss: 0.2933 - val_op_conv_loss: 0.3891 - val_avg_loss: 0.2910 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9056\n",
      "Epoch 439/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5476 - op_main_loss: 0.1245 - op_conv_loss: 0.0649 - avg_loss: 0.0887 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9731\n",
      "Epoch 00439: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5476 - op_main_loss: 0.1245 - op_conv_loss: 0.0649 - avg_loss: 0.0887 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9731 - val_loss: 1.2964 - val_op_main_loss: 0.2947 - val_op_conv_loss: 0.4261 - val_avg_loss: 0.3061 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8876\n",
      "Epoch 440/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5502 - op_main_loss: 0.1290 - op_conv_loss: 0.0623 - avg_loss: 0.0893 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9745\n",
      "Epoch 00440: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5502 - op_main_loss: 0.1290 - op_conv_loss: 0.0623 - avg_loss: 0.0893 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9745 - val_loss: 1.2168 - val_op_main_loss: 0.2943 - val_op_conv_loss: 0.3726 - val_avg_loss: 0.2804 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9027\n",
      "Epoch 441/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5543 - op_main_loss: 0.1287 - op_conv_loss: 0.0656 - avg_loss: 0.0904 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9766\n",
      "Epoch 00441: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5543 - op_main_loss: 0.1287 - op_conv_loss: 0.0656 - avg_loss: 0.0904 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9766 - val_loss: 1.4000 - val_op_main_loss: 0.3287 - val_op_conv_loss: 0.4606 - val_avg_loss: 0.3411 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8848\n",
      "Epoch 442/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5679 - op_main_loss: 0.1320 - op_conv_loss: 0.0717 - avg_loss: 0.0948 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9712\n",
      "Epoch 00442: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5679 - op_main_loss: 0.1320 - op_conv_loss: 0.0717 - avg_loss: 0.0948 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9712 - val_loss: 1.2454 - val_op_main_loss: 0.2904 - val_op_conv_loss: 0.3973 - val_avg_loss: 0.2882 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 443/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5450 - op_main_loss: 0.1250 - op_conv_loss: 0.0626 - avg_loss: 0.0881 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9780\n",
      "Epoch 00443: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5450 - op_main_loss: 0.1250 - op_conv_loss: 0.0626 - avg_loss: 0.0881 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9780 - val_loss: 1.3293 - val_op_main_loss: 0.3138 - val_op_conv_loss: 0.4289 - val_avg_loss: 0.3173 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8924\n",
      "Epoch 444/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5375 - op_main_loss: 0.1208 - op_conv_loss: 0.0620 - avg_loss: 0.0852 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9768 - avg_accuracy: 0.9773\n",
      "Epoch 00444: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5375 - op_main_loss: 0.1208 - op_conv_loss: 0.0620 - avg_loss: 0.0852 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9768 - avg_accuracy: 0.9773 - val_loss: 1.2556 - val_op_main_loss: 0.2868 - val_op_conv_loss: 0.4095 - val_avg_loss: 0.2896 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9027\n",
      "Epoch 445/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5322 - op_main_loss: 0.1190 - op_conv_loss: 0.0604 - avg_loss: 0.0833 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9778\n",
      "Epoch 00445: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5322 - op_main_loss: 0.1190 - op_conv_loss: 0.0604 - avg_loss: 0.0833 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9778 - val_loss: 1.4021 - val_op_main_loss: 0.3126 - val_op_conv_loss: 0.4872 - val_avg_loss: 0.3331 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8924\n",
      "Epoch 446/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5394 - op_main_loss: 0.1240 - op_conv_loss: 0.0615 - avg_loss: 0.0853 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9768\n",
      "Epoch 00446: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5394 - op_main_loss: 0.1240 - op_conv_loss: 0.0615 - avg_loss: 0.0853 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9768 - val_loss: 1.2691 - val_op_main_loss: 0.2907 - val_op_conv_loss: 0.4117 - val_avg_loss: 0.2985 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8952\n",
      "Epoch 447/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5274 - op_main_loss: 0.1189 - op_conv_loss: 0.0577 - avg_loss: 0.0829 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9778\n",
      "Epoch 00447: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5274 - op_main_loss: 0.1189 - op_conv_loss: 0.0577 - avg_loss: 0.0829 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9778 - val_loss: 1.6241 - val_op_main_loss: 0.3784 - val_op_conv_loss: 0.5729 - val_avg_loss: 0.4048 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8706\n",
      "Epoch 448/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5508 - op_main_loss: 0.1253 - op_conv_loss: 0.0683 - avg_loss: 0.0896 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9738\n",
      "Epoch 00448: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5508 - op_main_loss: 0.1253 - op_conv_loss: 0.0683 - avg_loss: 0.0896 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9738 - val_loss: 1.2903 - val_op_main_loss: 0.2844 - val_op_conv_loss: 0.4384 - val_avg_loss: 0.2996 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8971\n",
      "Epoch 449/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5393 - op_main_loss: 0.1257 - op_conv_loss: 0.0595 - avg_loss: 0.0864 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9787 - avg_accuracy: 0.9768\n",
      "Epoch 00449: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5393 - op_main_loss: 0.1257 - op_conv_loss: 0.0595 - avg_loss: 0.0864 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9787 - avg_accuracy: 0.9768 - val_loss: 1.2567 - val_op_main_loss: 0.2730 - val_op_conv_loss: 0.4325 - val_avg_loss: 0.2837 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9027\n",
      "Epoch 450/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5492 - op_main_loss: 0.1245 - op_conv_loss: 0.0677 - avg_loss: 0.0892 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9728\n",
      "Epoch 00450: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5492 - op_main_loss: 0.1245 - op_conv_loss: 0.0677 - avg_loss: 0.0892 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9728 - val_loss: 1.4736 - val_op_main_loss: 0.3490 - val_op_conv_loss: 0.4935 - val_avg_loss: 0.3625 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8772\n",
      "Epoch 451/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5715 - op_main_loss: 0.1340 - op_conv_loss: 0.0735 - avg_loss: 0.0953 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9714\n",
      "Epoch 00451: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5715 - op_main_loss: 0.1340 - op_conv_loss: 0.0735 - avg_loss: 0.0953 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9714 - val_loss: 1.3803 - val_op_main_loss: 0.3198 - val_op_conv_loss: 0.4610 - val_avg_loss: 0.3307 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 452/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5460 - op_main_loss: 0.1230 - op_conv_loss: 0.0661 - avg_loss: 0.0880 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9740\n",
      "Epoch 00452: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5460 - op_main_loss: 0.1230 - op_conv_loss: 0.0661 - avg_loss: 0.0880 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9740 - val_loss: 1.3590 - val_op_main_loss: 0.3437 - val_op_conv_loss: 0.4129 - val_avg_loss: 0.3333 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8933\n",
      "Epoch 453/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5326 - op_main_loss: 0.1214 - op_conv_loss: 0.0584 - avg_loss: 0.0842 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9759\n",
      "Epoch 00453: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5326 - op_main_loss: 0.1214 - op_conv_loss: 0.0584 - avg_loss: 0.0842 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9759 - val_loss: 1.3667 - val_op_main_loss: 0.3114 - val_op_conv_loss: 0.4635 - val_avg_loss: 0.3236 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8905\n",
      "Epoch 454/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5342 - op_main_loss: 0.1191 - op_conv_loss: 0.0621 - avg_loss: 0.0848 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9738\n",
      "Epoch 00454: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5342 - op_main_loss: 0.1191 - op_conv_loss: 0.0621 - avg_loss: 0.0848 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9738 - val_loss: 1.3622 - val_op_main_loss: 0.3135 - val_op_conv_loss: 0.4569 - val_avg_loss: 0.3237 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8886\n",
      "Epoch 455/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5493 - op_main_loss: 0.1227 - op_conv_loss: 0.0688 - avg_loss: 0.0896 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9721\n",
      "Epoch 00455: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5493 - op_main_loss: 0.1227 - op_conv_loss: 0.0688 - avg_loss: 0.0896 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9721 - val_loss: 1.4153 - val_op_main_loss: 0.3358 - val_op_conv_loss: 0.4668 - val_avg_loss: 0.3446 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8829\n",
      "Epoch 456/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5847 - op_main_loss: 0.1410 - op_conv_loss: 0.0756 - avg_loss: 0.1004 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9667\n",
      "Epoch 00456: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5847 - op_main_loss: 0.1410 - op_conv_loss: 0.0756 - avg_loss: 0.1004 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9667 - val_loss: 1.3070 - val_op_main_loss: 0.2870 - val_op_conv_loss: 0.4510 - val_avg_loss: 0.3014 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8924\n",
      "Epoch 457/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5726 - op_main_loss: 0.1332 - op_conv_loss: 0.0744 - avg_loss: 0.0964 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9695\n",
      "Epoch 00457: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5726 - op_main_loss: 0.1332 - op_conv_loss: 0.0744 - avg_loss: 0.0964 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9695 - val_loss: 1.2346 - val_op_main_loss: 0.2949 - val_op_conv_loss: 0.3803 - val_avg_loss: 0.2905 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9046\n",
      "Epoch 458/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5348 - op_main_loss: 0.1173 - op_conv_loss: 0.0638 - avg_loss: 0.0850 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9761\n",
      "Epoch 00458: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5348 - op_main_loss: 0.1173 - op_conv_loss: 0.0638 - avg_loss: 0.0850 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9761 - val_loss: 1.2836 - val_op_main_loss: 0.2984 - val_op_conv_loss: 0.4146 - val_avg_loss: 0.3022 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8980\n",
      "Epoch 459/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5495 - op_main_loss: 0.1264 - op_conv_loss: 0.0653 - avg_loss: 0.0896 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9731\n",
      "Epoch 00459: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5495 - op_main_loss: 0.1264 - op_conv_loss: 0.0653 - avg_loss: 0.0896 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9731 - val_loss: 1.3199 - val_op_main_loss: 0.3035 - val_op_conv_loss: 0.4355 - val_avg_loss: 0.3128 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8933\n",
      "Epoch 460/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5226 - op_main_loss: 0.1150 - op_conv_loss: 0.0584 - avg_loss: 0.0811 - op_main_accuracy: 0.9646 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9757\n",
      "Epoch 00460: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5226 - op_main_loss: 0.1150 - op_conv_loss: 0.0584 - avg_loss: 0.0811 - op_main_accuracy: 0.9646 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9757 - val_loss: 1.2803 - val_op_main_loss: 0.3174 - val_op_conv_loss: 0.3862 - val_avg_loss: 0.3087 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8952\n",
      "Epoch 461/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5413 - op_main_loss: 0.1231 - op_conv_loss: 0.0637 - avg_loss: 0.0867 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9742\n",
      "Epoch 00461: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5413 - op_main_loss: 0.1231 - op_conv_loss: 0.0637 - avg_loss: 0.0867 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9742 - val_loss: 1.3641 - val_op_main_loss: 0.2960 - val_op_conv_loss: 0.4832 - val_avg_loss: 0.3175 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8933\n",
      "Epoch 462/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5406 - op_main_loss: 0.1208 - op_conv_loss: 0.0652 - avg_loss: 0.0871 - op_main_accuracy: 0.9653 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9747\n",
      "Epoch 00462: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5406 - op_main_loss: 0.1208 - op_conv_loss: 0.0652 - avg_loss: 0.0871 - op_main_accuracy: 0.9653 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9747 - val_loss: 1.2620 - val_op_main_loss: 0.2786 - val_op_conv_loss: 0.4257 - val_avg_loss: 0.2902 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9027\n",
      "Epoch 463/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5238 - op_main_loss: 0.1177 - op_conv_loss: 0.0575 - avg_loss: 0.0812 - op_main_accuracy: 0.9629 - op_conv_accuracy: 0.9773 - avg_accuracy: 0.9759\n",
      "Epoch 00463: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5238 - op_main_loss: 0.1177 - op_conv_loss: 0.0575 - avg_loss: 0.0812 - op_main_accuracy: 0.9629 - op_conv_accuracy: 0.9773 - avg_accuracy: 0.9759 - val_loss: 1.2185 - val_op_main_loss: 0.2702 - val_op_conv_loss: 0.4033 - val_avg_loss: 0.2778 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.9027\n",
      "Epoch 464/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5383 - op_main_loss: 0.1218 - op_conv_loss: 0.0634 - avg_loss: 0.0859 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9761\n",
      "Epoch 00464: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5383 - op_main_loss: 0.1218 - op_conv_loss: 0.0634 - avg_loss: 0.0859 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9761 - val_loss: 1.2682 - val_op_main_loss: 0.2796 - val_op_conv_loss: 0.4291 - val_avg_loss: 0.2925 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8999\n",
      "Epoch 465/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5271 - op_main_loss: 0.1154 - op_conv_loss: 0.0625 - avg_loss: 0.0823 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9783\n",
      "Epoch 00465: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5271 - op_main_loss: 0.1154 - op_conv_loss: 0.0625 - avg_loss: 0.0823 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9783 - val_loss: 1.2282 - val_op_main_loss: 0.2834 - val_op_conv_loss: 0.3954 - val_avg_loss: 0.2829 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9056\n",
      "Epoch 466/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5250 - op_main_loss: 0.1177 - op_conv_loss: 0.0580 - avg_loss: 0.0826 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9768 - avg_accuracy: 0.9750\n",
      "Epoch 00466: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5250 - op_main_loss: 0.1177 - op_conv_loss: 0.0580 - avg_loss: 0.0826 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9768 - avg_accuracy: 0.9750 - val_loss: 1.2426 - val_op_main_loss: 0.2768 - val_op_conv_loss: 0.4139 - val_avg_loss: 0.2852 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9046\n",
      "Epoch 467/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5388 - op_main_loss: 0.1220 - op_conv_loss: 0.0643 - avg_loss: 0.0864 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9716\n",
      "Epoch 00467: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5388 - op_main_loss: 0.1220 - op_conv_loss: 0.0643 - avg_loss: 0.0864 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9716 - val_loss: 1.3829 - val_op_main_loss: 0.2979 - val_op_conv_loss: 0.4953 - val_avg_loss: 0.3240 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8886\n",
      "Epoch 468/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5336 - op_main_loss: 0.1205 - op_conv_loss: 0.0625 - avg_loss: 0.0850 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9783\n",
      "Epoch 00468: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5336 - op_main_loss: 0.1205 - op_conv_loss: 0.0625 - avg_loss: 0.0850 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9783 - val_loss: 1.4822 - val_op_main_loss: 0.3732 - val_op_conv_loss: 0.4741 - val_avg_loss: 0.3693 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8810\n",
      "Epoch 469/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5584 - op_main_loss: 0.1305 - op_conv_loss: 0.0695 - avg_loss: 0.0926 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9698\n",
      "Epoch 00469: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5584 - op_main_loss: 0.1305 - op_conv_loss: 0.0695 - avg_loss: 0.0926 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9698 - val_loss: 1.2681 - val_op_main_loss: 0.2948 - val_op_conv_loss: 0.4089 - val_avg_loss: 0.2984 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 470/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5350 - op_main_loss: 0.1190 - op_conv_loss: 0.0645 - avg_loss: 0.0852 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9754\n",
      "Epoch 00470: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5350 - op_main_loss: 0.1190 - op_conv_loss: 0.0645 - avg_loss: 0.0852 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9754 - val_loss: 1.2230 - val_op_main_loss: 0.2824 - val_op_conv_loss: 0.3900 - val_avg_loss: 0.2843 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9046\n",
      "Epoch 471/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5382 - op_main_loss: 0.1227 - op_conv_loss: 0.0629 - avg_loss: 0.0866 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9716\n",
      "Epoch 00471: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5382 - op_main_loss: 0.1227 - op_conv_loss: 0.0629 - avg_loss: 0.0866 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9716 - val_loss: 1.1809 - val_op_main_loss: 0.2704 - val_op_conv_loss: 0.3731 - val_avg_loss: 0.2710 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9046\n",
      "Epoch 472/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5434 - op_main_loss: 0.1218 - op_conv_loss: 0.0674 - avg_loss: 0.0875 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9709\n",
      "Epoch 00472: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5434 - op_main_loss: 0.1218 - op_conv_loss: 0.0674 - avg_loss: 0.0875 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9709 - val_loss: 1.3199 - val_op_main_loss: 0.3120 - val_op_conv_loss: 0.4274 - val_avg_loss: 0.3137 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8971\n",
      "Epoch 473/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5174 - op_main_loss: 0.1137 - op_conv_loss: 0.0579 - avg_loss: 0.0794 - op_main_accuracy: 0.9667 - op_conv_accuracy: 0.9783 - avg_accuracy: 0.9768\n",
      "Epoch 00473: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5174 - op_main_loss: 0.1137 - op_conv_loss: 0.0579 - avg_loss: 0.0794 - op_main_accuracy: 0.9667 - op_conv_accuracy: 0.9783 - avg_accuracy: 0.9768 - val_loss: 1.4849 - val_op_main_loss: 0.3530 - val_op_conv_loss: 0.5002 - val_avg_loss: 0.3653 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8801\n",
      "Epoch 474/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5418 - op_main_loss: 0.1230 - op_conv_loss: 0.0661 - avg_loss: 0.0872 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9731\n",
      "Epoch 00474: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5418 - op_main_loss: 0.1230 - op_conv_loss: 0.0661 - avg_loss: 0.0872 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9731 - val_loss: 1.4362 - val_op_main_loss: 0.2965 - val_op_conv_loss: 0.5400 - val_avg_loss: 0.3344 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8810\n",
      "Epoch 475/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5281 - op_main_loss: 0.1164 - op_conv_loss: 0.0624 - avg_loss: 0.0838 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9747\n",
      "Epoch 00475: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5281 - op_main_loss: 0.1164 - op_conv_loss: 0.0624 - avg_loss: 0.0838 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9747 - val_loss: 1.5218 - val_op_main_loss: 0.3593 - val_op_conv_loss: 0.5238 - val_avg_loss: 0.3735 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8829\n",
      "Epoch 476/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5289 - op_main_loss: 0.1230 - op_conv_loss: 0.0570 - avg_loss: 0.0839 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9776\n",
      "Epoch 00476: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5289 - op_main_loss: 0.1230 - op_conv_loss: 0.0570 - avg_loss: 0.0839 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9776 - val_loss: 1.2541 - val_op_main_loss: 0.3019 - val_op_conv_loss: 0.3930 - val_avg_loss: 0.2942 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9065\n",
      "Epoch 477/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5252 - op_main_loss: 0.1202 - op_conv_loss: 0.0580 - avg_loss: 0.0820 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9787 - avg_accuracy: 0.9752\n",
      "Epoch 00477: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5252 - op_main_loss: 0.1202 - op_conv_loss: 0.0580 - avg_loss: 0.0820 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9787 - avg_accuracy: 0.9752 - val_loss: 1.2411 - val_op_main_loss: 0.2812 - val_op_conv_loss: 0.4075 - val_avg_loss: 0.2874 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9027\n",
      "Epoch 478/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5521 - op_main_loss: 0.1260 - op_conv_loss: 0.0703 - avg_loss: 0.0911 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9721\n",
      "Epoch 00478: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5521 - op_main_loss: 0.1260 - op_conv_loss: 0.0703 - avg_loss: 0.0911 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9721 - val_loss: 2.0368 - val_op_main_loss: 0.4498 - val_op_conv_loss: 0.8214 - val_avg_loss: 0.5008 - val_op_main_accuracy: 0.8461 - val_op_conv_accuracy: 0.8451 - val_avg_accuracy: 0.8432\n",
      "Epoch 479/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5618 - op_main_loss: 0.1278 - op_conv_loss: 0.0745 - avg_loss: 0.0945 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9735\n",
      "Epoch 00479: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5618 - op_main_loss: 0.1278 - op_conv_loss: 0.0745 - avg_loss: 0.0945 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9735 - val_loss: 1.2262 - val_op_main_loss: 0.2840 - val_op_conv_loss: 0.3894 - val_avg_loss: 0.2873 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9037\n",
      "Epoch 480/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5398 - op_main_loss: 0.1207 - op_conv_loss: 0.0663 - avg_loss: 0.0878 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9752\n",
      "Epoch 00480: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5398 - op_main_loss: 0.1207 - op_conv_loss: 0.0663 - avg_loss: 0.0878 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9752 - val_loss: 1.5965 - val_op_main_loss: 0.3525 - val_op_conv_loss: 0.5906 - val_avg_loss: 0.3888 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8697\n",
      "Epoch 481/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5459 - op_main_loss: 0.1273 - op_conv_loss: 0.0651 - avg_loss: 0.0893 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9733\n",
      "Epoch 00481: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5459 - op_main_loss: 0.1273 - op_conv_loss: 0.0651 - avg_loss: 0.0893 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9733 - val_loss: 1.2641 - val_op_main_loss: 0.2873 - val_op_conv_loss: 0.4135 - val_avg_loss: 0.2992 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 482/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5461 - op_main_loss: 0.1250 - op_conv_loss: 0.0673 - avg_loss: 0.0890 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9735\n",
      "Epoch 00482: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5461 - op_main_loss: 0.1250 - op_conv_loss: 0.0673 - avg_loss: 0.0890 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9735 - val_loss: 1.2282 - val_op_main_loss: 0.2858 - val_op_conv_loss: 0.3919 - val_avg_loss: 0.2855 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9018\n",
      "Epoch 483/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5340 - op_main_loss: 0.1208 - op_conv_loss: 0.0623 - avg_loss: 0.0861 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9705\n",
      "Epoch 00483: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5340 - op_main_loss: 0.1208 - op_conv_loss: 0.0623 - avg_loss: 0.0861 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9705 - val_loss: 1.4859 - val_op_main_loss: 0.3491 - val_op_conv_loss: 0.5094 - val_avg_loss: 0.3628 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8791\n",
      "Epoch 484/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5153 - op_main_loss: 0.1141 - op_conv_loss: 0.0563 - avg_loss: 0.0801 - op_main_accuracy: 0.9650 - op_conv_accuracy: 0.9776 - avg_accuracy: 0.9773\n",
      "Epoch 00484: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5153 - op_main_loss: 0.1141 - op_conv_loss: 0.0563 - avg_loss: 0.0801 - op_main_accuracy: 0.9650 - op_conv_accuracy: 0.9776 - avg_accuracy: 0.9773 - val_loss: 1.4431 - val_op_main_loss: 0.3506 - val_op_conv_loss: 0.4706 - val_avg_loss: 0.3574 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8810\n",
      "Epoch 485/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5028 - op_main_loss: 0.1119 - op_conv_loss: 0.0504 - avg_loss: 0.0762 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9790 - avg_accuracy: 0.9797\n",
      "Epoch 00485: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5028 - op_main_loss: 0.1119 - op_conv_loss: 0.0504 - avg_loss: 0.0762 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9790 - avg_accuracy: 0.9797 - val_loss: 1.3860 - val_op_main_loss: 0.3105 - val_op_conv_loss: 0.4820 - val_avg_loss: 0.3294 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8895\n",
      "Epoch 486/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5544 - op_main_loss: 0.1290 - op_conv_loss: 0.0686 - avg_loss: 0.0914 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9707\n",
      "Epoch 00486: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5544 - op_main_loss: 0.1290 - op_conv_loss: 0.0686 - avg_loss: 0.0914 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9707 - val_loss: 1.3773 - val_op_main_loss: 0.3175 - val_op_conv_loss: 0.4644 - val_avg_loss: 0.3296 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8952\n",
      "Epoch 487/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5624 - op_main_loss: 0.1278 - op_conv_loss: 0.0757 - avg_loss: 0.0931 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9714\n",
      "Epoch 00487: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5624 - op_main_loss: 0.1278 - op_conv_loss: 0.0757 - avg_loss: 0.0931 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9714 - val_loss: 1.2584 - val_op_main_loss: 0.2866 - val_op_conv_loss: 0.4133 - val_avg_loss: 0.2928 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.9008\n",
      "Epoch 488/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5218 - op_main_loss: 0.1155 - op_conv_loss: 0.0588 - avg_loss: 0.0817 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9764\n",
      "Epoch 00488: val_avg_accuracy did not improve from 0.90746\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5218 - op_main_loss: 0.1155 - op_conv_loss: 0.0588 - avg_loss: 0.0817 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9764 - val_loss: 1.5041 - val_op_main_loss: 0.3551 - val_op_conv_loss: 0.5149 - val_avg_loss: 0.3683 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8791\n",
      "Epoch 489/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5108 - op_main_loss: 0.1138 - op_conv_loss: 0.0526 - avg_loss: 0.0787 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9768\n",
      "Epoch 00489: val_avg_accuracy improved from 0.90746 to 0.91407, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.5108 - op_main_loss: 0.1138 - op_conv_loss: 0.0526 - avg_loss: 0.0787 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9768 - val_loss: 1.2343 - val_op_main_loss: 0.2846 - val_op_conv_loss: 0.3966 - val_avg_loss: 0.2875 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9141 - val_avg_accuracy: 0.9141\n",
      "Epoch 490/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5491 - op_main_loss: 0.1232 - op_conv_loss: 0.0705 - avg_loss: 0.0900 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9738\n",
      "Epoch 00490: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5491 - op_main_loss: 0.1232 - op_conv_loss: 0.0705 - avg_loss: 0.0900 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9738 - val_loss: 1.2928 - val_op_main_loss: 0.2858 - val_op_conv_loss: 0.4411 - val_avg_loss: 0.3008 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8933\n",
      "Epoch 491/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5041 - op_main_loss: 0.1115 - op_conv_loss: 0.0508 - avg_loss: 0.0765 - op_main_accuracy: 0.9681 - op_conv_accuracy: 0.9804 - avg_accuracy: 0.9811\n",
      "Epoch 00491: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5041 - op_main_loss: 0.1115 - op_conv_loss: 0.0508 - avg_loss: 0.0765 - op_main_accuracy: 0.9681 - op_conv_accuracy: 0.9804 - avg_accuracy: 0.9811 - val_loss: 1.3381 - val_op_main_loss: 0.3208 - val_op_conv_loss: 0.4301 - val_avg_loss: 0.3222 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8933\n",
      "Epoch 492/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5457 - op_main_loss: 0.1222 - op_conv_loss: 0.0703 - avg_loss: 0.0885 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9709\n",
      "Epoch 00492: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5457 - op_main_loss: 0.1222 - op_conv_loss: 0.0703 - avg_loss: 0.0885 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9709 - val_loss: 1.3926 - val_op_main_loss: 0.3098 - val_op_conv_loss: 0.4843 - val_avg_loss: 0.3339 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8895\n",
      "Epoch 493/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5152 - op_main_loss: 0.1155 - op_conv_loss: 0.0550 - avg_loss: 0.0802 - op_main_accuracy: 0.9669 - op_conv_accuracy: 0.9792 - avg_accuracy: 0.9790\n",
      "Epoch 00493: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5152 - op_main_loss: 0.1155 - op_conv_loss: 0.0550 - avg_loss: 0.0802 - op_main_accuracy: 0.9669 - op_conv_accuracy: 0.9792 - avg_accuracy: 0.9790 - val_loss: 1.3611 - val_op_main_loss: 0.3402 - val_op_conv_loss: 0.4218 - val_avg_loss: 0.3348 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8971\n",
      "Epoch 494/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5048 - op_main_loss: 0.1113 - op_conv_loss: 0.0525 - avg_loss: 0.0768 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9809 - avg_accuracy: 0.9785\n",
      "Epoch 00494: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5048 - op_main_loss: 0.1113 - op_conv_loss: 0.0525 - avg_loss: 0.0768 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9809 - avg_accuracy: 0.9785 - val_loss: 1.2654 - val_op_main_loss: 0.2905 - val_op_conv_loss: 0.4124 - val_avg_loss: 0.2983 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9018\n",
      "Epoch 495/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4901 - op_main_loss: 0.1074 - op_conv_loss: 0.0464 - avg_loss: 0.0723 - op_main_accuracy: 0.9693 - op_conv_accuracy: 0.9818 - avg_accuracy: 0.9818\n",
      "Epoch 00495: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4901 - op_main_loss: 0.1074 - op_conv_loss: 0.0464 - avg_loss: 0.0723 - op_main_accuracy: 0.9693 - op_conv_accuracy: 0.9818 - avg_accuracy: 0.9818 - val_loss: 1.2909 - val_op_main_loss: 0.2990 - val_op_conv_loss: 0.4240 - val_avg_loss: 0.3042 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9008\n",
      "Epoch 496/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5201 - op_main_loss: 0.1165 - op_conv_loss: 0.0586 - avg_loss: 0.0813 - op_main_accuracy: 0.9679 - op_conv_accuracy: 0.9794 - avg_accuracy: 0.9806\n",
      "Epoch 00496: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5201 - op_main_loss: 0.1165 - op_conv_loss: 0.0586 - avg_loss: 0.0813 - op_main_accuracy: 0.9679 - op_conv_accuracy: 0.9794 - avg_accuracy: 0.9806 - val_loss: 1.3567 - val_op_main_loss: 0.3152 - val_op_conv_loss: 0.4532 - val_avg_loss: 0.3248 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8924\n",
      "Epoch 497/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5292 - op_main_loss: 0.1193 - op_conv_loss: 0.0624 - avg_loss: 0.0840 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9759\n",
      "Epoch 00497: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5292 - op_main_loss: 0.1193 - op_conv_loss: 0.0624 - avg_loss: 0.0840 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9759 - val_loss: 1.4511 - val_op_main_loss: 0.3357 - val_op_conv_loss: 0.4998 - val_avg_loss: 0.3522 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8857\n",
      "Epoch 498/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5206 - op_main_loss: 0.1162 - op_conv_loss: 0.0598 - avg_loss: 0.0815 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9771\n",
      "Epoch 00498: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5206 - op_main_loss: 0.1162 - op_conv_loss: 0.0598 - avg_loss: 0.0815 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9771 - val_loss: 1.2413 - val_op_main_loss: 0.2856 - val_op_conv_loss: 0.4035 - val_avg_loss: 0.2892 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8999\n",
      "Epoch 499/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5812 - op_main_loss: 0.1353 - op_conv_loss: 0.0828 - avg_loss: 0.1006 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9646\n",
      "Epoch 00499: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5812 - op_main_loss: 0.1353 - op_conv_loss: 0.0828 - avg_loss: 0.1006 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9646 - val_loss: 1.4479 - val_op_main_loss: 0.3337 - val_op_conv_loss: 0.4996 - val_avg_loss: 0.3521 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8791\n",
      "Epoch 500/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5252 - op_main_loss: 0.1187 - op_conv_loss: 0.0602 - avg_loss: 0.0835 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9757\n",
      "Epoch 00500: val_avg_accuracy did not improve from 0.91407\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5252 - op_main_loss: 0.1187 - op_conv_loss: 0.0602 - avg_loss: 0.0835 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9757 - val_loss: 1.2513 - val_op_main_loss: 0.2855 - val_op_conv_loss: 0.4086 - val_avg_loss: 0.2945 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8971\n"
     ]
    }
   ],
   "source": [
    "def nlp_lstm2(w2v):\n",
    "    inputs = Input(shape=(X_train[0].shape[-1],))\n",
    "\n",
    "    embedding_layer = gensim_to_keras_embedding(w2v)\n",
    "    \n",
    "    embedding = embedding_layer(inputs)\n",
    "\n",
    "    lstm1 = LSTM(lstm_units,return_sequences=True, return_state=True, kernel_regularizer=l2(w_decay),recurrent_regularizer=l2(w_decay), dropout=dropout_rate)(embedding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='op_main')(lstm1[1])\n",
    "    \n",
    "\n",
    "    output_td_gap = GlobalAveragePooling1D(data_format='channels_first')(lstm1[0])\n",
    "    \n",
    "    output_td = TimeDistributed(Dense(units=1, activation='sigmoid'))(lstm1[0])\n",
    "    output_td = Flatten()(output_td)\n",
    "    \n",
    "    output_td = Multiply()([output_td_gap, output_td])\n",
    "    \n",
    "    output_td = Activation('relu', name='before_split')(output_td)\n",
    "    \n",
    "    output_td_splits = tf.split(output_td, 10, axis=-1)\n",
    "    \n",
    "    features = concatenate([output_td_splits[0], output_td_splits[1], output_td_splits[-2], output_td_splits[-1]])\n",
    "    \n",
    "    print(features.shape)\n",
    "    \n",
    "    output_td = Reshape((8, 10, 1))(features)\n",
    "    \n",
    "    output_td = Conv2D(2, 8, padding='same', strides=1, activation='relu', kernel_regularizer=l2(w_decay))(output_td)\n",
    "    output_td = BatchNormalization()(output_td)\n",
    "    output_td = Flatten()(output_td)\n",
    "   \n",
    "\n",
    "    output_td = Dense(units=1, activation='sigmoid', name='op_conv')(output_td)\n",
    "    \n",
    "    \n",
    "    \n",
    "    avg = tf.keras.layers.Average(name='avg')([output, output_td])\n",
    "    \n",
    "\n",
    "    model = Model(inputs, [output, output_td, avg])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = nlp_lstm2(w2v_model)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./weight_cp/weight_lstm2.hdf5', save_freq=\"epoch\",  verbose=1, monitor='val_avg_accuracy', save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(0.0001)\n",
    "model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=epochs_to_run, validation_data=(X_val, y_val), callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7013a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['avg_accuracy'])\n",
    "plt.plot(history.history['val_avg_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11c0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./weight_cp/weight_lstm2.hdf5')\n",
    "predictionss = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2963b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF Multi-Supervised LSTM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91       661\n",
      "           1       0.91      0.90      0.90       662\n",
      "\n",
      "    accuracy                           0.91      1323\n",
      "   macro avg       0.91      0.91      0.91      1323\n",
      "weighted avg       0.91      0.91      0.91      1323\n",
      "\n",
      "0.9055177626606198\n"
     ]
    }
   ],
   "source": [
    "predictions = np.where(predictionss[-1] > 0.5, 1, 0)\n",
    "y_pred = []\n",
    "for p in predictions:\n",
    "    y_pred.append(p[0])\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"CLASSIFICATION REPORT OF Multi-Supervised LSTM\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8777138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2 0\n",
      "embedding_1 1\n",
      "lstm_1 2\n",
      "time_distributed 3\n",
      "global_average_pooling1d 4\n",
      "flatten 5\n",
      "multiply 6\n",
      "before_split 7\n",
      "tf_op_layer_split 8\n",
      "concatenate 9\n",
      "reshape 10\n",
      "conv2d 11\n",
      "batch_normalization 12\n",
      "flatten_1 13\n",
      "op_main 14\n",
      "op_conv 15\n",
      "avg 16\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(layer.name, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c7d38a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAGdCAYAAADKYTXmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeHklEQVR4nO3dfVST9/3/8VcASWy/JNYBCanxrq1ivcGW1gyqU49MzPw6cZ21HDfRWruvB9d6qF1LTyu07izbenPaDg7tdkTccZ0357S40zo6pBNngVqgnGnXeYQhN0cCxVMSwAosuX5/+DNdZhJNvQL45vU45zrH5Ppcl2+iT3MDiRpFURQQkRgRIz0AEamLURMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSRM1EgPoAaPx4Pz588jJiYGGo1mpMchUp2iKOjt7YXZbEZERPD7YhFRnz9/HhaLZaTHIAq7trY2TJo0KegaEVHHxMQAABbie4jCuBs6V9f/WdUYCQDgmjeoynli412qnAcAbtNdVOU84yI9qpwHADyKOo+u/u1W79nkgFudNAbdkaqcx31xAA0/LvL+XQ9GRNRXHnJHYRyiNDcWdaRWp8ZIAICI8er8JYu8ZUCV8wBA1Hi3KucZF6nOeQD1ooZKAQGAW6Wo3SrOBOC6nl7yhTIiYRg1kTBhi7qwsBBTp06FTqeD1WrFyZMng64/dOgQEhMTodPpMHfuXBw5ciRcoxGJFpaoDxw4gJycHOTl5aG+vh5JSUlIT09HV1eX3/VVVVXIzMzE5s2b8emnnyIjIwMZGRk4ffp0OMYjEi0sUb/66qvYsmULNm3ahLvvvhtvvvkmbrnlFhQXF/td//rrr2PFihV46qmnMGvWLOzatQv33nsvCgoKwjEekWiqRz04OIi6ujqkpaV9/ZtERCAtLQ3V1dV+j6murvZZDwDp6ekB1w8MDMDlcvlsRHSZ6lF3d3fD7XbDaDT6XG80GuFwOPwe43A4Qlpvt9thMBi8G3/whOhrN+Wr37m5uXA6nd6tra1tpEciGjVU/+GT2NhYREZGorOz0+f6zs5OmEwmv8eYTKaQ1mu1Wmi1WnUGJhJG9Xvq6OhoJCcno6Kiwnudx+NBRUUFUlJS/B6TkpLisx4AysvLA64nosDC8mOiOTk5yMrKwn333YcFCxbgtddeQ39/PzZt2gQA2LBhA26//XbY7XYAwBNPPIHFixfjlVdewcqVK7F//37U1tbit7/9bTjGIxItLFGvW7cOX3zxBXbu3AmHw4H58+ejrKzM+2JYa2urz9vHUlNT8fbbb+O5557Ds88+i7vuugulpaWYM2dOOMYjEk0j4cP8XS4XDAYDlmD1Db+ho/PxVJWmApzz1XmXVpzRqcp5AGDieHXepRU9Ct/QMaTimyfUepfWgFrv0uofQN2Dr8HpdEKv1wdde1O++k1EgYl46+UVFw7eichbbuxVcY/7S5WmAW5R6f29rn713g7a0ztelfN4PCreH6h0T62o9xZvKB6V3g6q0tfm+erSda/lPTWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwjJpIGFEfZ/Tlhf9BxMUb/OgflT5+RlWj8aMhR+VMo/DPTiXK0PXf//KemkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMKoHrXdbsf999+PmJgYxMfHIyMjA2fOnAl6TElJCTQajc+m093gWyiJxijVo66srER2djZqampQXl6OoaEhLF++HP39/UGP0+v16Ojo8G4tLS1qj0Y0Jqj+IQllZWU+l0tKShAfH4+6ujp85zvfCXicRqOByWRSexyiMSfsn3zidDoBABMnTgy6rq+vD1OmTIHH48G9996LX/ziF5g9e7bftQMDAxgYGPBedrlcl3/x74jLmzSj8VNGaHiF8HcgrAV4PB5s374dDzzwAObMmRNw3cyZM1FcXIzDhw9j37598Hg8SE1NRXt7u9/1drsdBoPBu1kslnB9CUQ3HY2iKGG7H9i6dSv+/Oc/48SJE5g0adJ1Hzc0NIRZs2YhMzMTu3btumq/v3tqi8WCSb95ARHjBb7AxnvqMc/z1SW0P54Hp9MJvV4fdG3YHn5v27YN7733Ho4fPx5S0AAwbtw43HPPPWhsbPS7X6vVQqvVqjEmkTiqP/xWFAXbtm3Du+++iw8//BDTpk0L+RxutxunTp1CQkKC2uMRiaf6PXV2djbefvttHD58GDExMXA4HAAAg8GA8ePHAwA2bNiA22+/HXa7HQDw4osv4tvf/jbuvPNO9PT04KWXXkJLSwseffRRtccjEk/1qIuKigAAS5Ys8bl+z5492LhxIwCgtbUVERFfP0j48ssvsWXLFjgcDtx2221ITk5GVVUV7r77brXHIxIvrC+UDReXywWDwcAXykisUF4oE/hNXaKxjVETCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImECfvHGQ0rBfw5aRrzeE9NJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwqkedn58PjUbjsyUmJgY95tChQ0hMTIROp8PcuXNx5MgRtcciGjPCck89e/ZsdHR0eLcTJ04EXFtVVYXMzExs3rwZn376KTIyMpCRkYHTp0+HYzQi8cISdVRUFEwmk3eLjY0NuPb111/HihUr8NRTT2HWrFnYtWsX7r33XhQUFIRjNCLxwhL12bNnYTabMX36dKxfvx6tra0B11ZXVyMtLc3nuvT0dFRXVwc8ZmBgAC6Xy2cjostUj9pqtaKkpARlZWUoKipCc3MzFi1ahN7eXr/rHQ4HjEajz3VGoxEOhyPg72G322EwGLybxWJR9WsgupmpHrXNZsPatWsxb948pKen48iRI+jp6cHBgwdV+z1yc3PhdDq9W1tbm2rnJrrZhf1/vZwwYQJmzJiBxsZGv/tNJhM6Ozt9ruvs7ITJZAp4Tq1WC61Wq+qcRFKE/fvUfX19aGpqQkJCgt/9KSkpqKio8LmuvLwcKSkp4R6NSCTVo96xYwcqKytx7tw5VFVVYc2aNYiMjERmZiYAYMOGDcjNzfWuf+KJJ1BWVoZXXnkF//znP5Gfn4/a2lps27ZN7dGIxgTVH363t7cjMzMTFy5cQFxcHBYuXIiamhrExcUBAFpbWxER8fW/JampqXj77bfx3HPP4dlnn8Vdd92F0tJSzJkzR+3RiMYEjaIoykgPcaNcLhcMBgMmvfECIsbrRnocItV5vrqE9sfz4HQ6odfrg67lz34TCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwqge9dSpU6HRaK7asrOz/a4vKSm5aq1Op1N7LKIxI0rtE37yySdwu93ey6dPn8Z3v/tdrF27NuAxer0eZ86c8V7WaDRqj0U0ZqgedVxcnM/lX/7yl7jjjjuwePHigMdoNBqYTCa1RyEak8L6nHpwcBD79u3DI488EvTet6+vD1OmTIHFYsHq1avx2WefhXMsItHCGnVpaSl6enqwcePGgGtmzpyJ4uJiHD58GPv27YPH40Fqaira29sDHjMwMACXy+WzEdFlYY169+7dsNlsMJvNAdekpKRgw4YNmD9/PhYvXox33nkHcXFxeOuttwIeY7fbYTAYvJvFYgnH+EQ3pbBF3dLSgqNHj+LRRx8N6bhx48bhnnvuQWNjY8A1ubm5cDqd3q2tre1GxyUSI2xR79mzB/Hx8Vi5cmVIx7ndbpw6dQoJCQkB12i1Wuj1ep+NiC4LS9Qejwd79uxBVlYWoqJ8X2DfsGEDcnNzvZdffPFF/OUvf8G//vUv1NfX40c/+hFaWlpCvocnostU/5YWABw9ehStra145JFHrtrX2tqKiIiv/y358ssvsWXLFjgcDtx2221ITk5GVVUV7r777nCMRiSeRlEUZaSHuFEulwsGgwGT3ngBEeP502gkj+erS2h/PA9Op/OaTzf5s99EwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSJiQoz5+/DhWrVoFs9kMjUaD0tJSn/2KomDnzp1ISEjA+PHjkZaWhrNnz17zvIWFhZg6dSp0Oh2sVitOnjwZ6mhEhG8QdX9/P5KSklBYWOh3/69//Wu88cYbePPNN/Hxxx/j1ltvRXp6Oi5duhTwnAcOHEBOTg7y8vJQX1+PpKQkpKeno6urK9TxiMY8jaIoyjc+WKPBu+++i4yMDACX76XNZjOefPJJ7NixAwDgdDphNBpRUlKChx9+2O95rFYr7r//fhQUFAAAPB4PLBYLfvrTn+KZZ5655hwulwsGgwGT3ngBEeN13/TLIRq1PF9dQvvjeXA6ndDr9UHXqvqcurm5GQ6HA2lpad7rDAYDrFYrqqur/R4zODiIuro6n2MiIiKQlpYW8JiBgQG4XC6fjYguUzVqh8MBADAajT7XG41G777/1t3dDbfbHdIxdrsdBoPBu1ksFhWmJ5Lhpnz1Ozc3F06n07u1tbWN9EhEo4aqUZtMJgBAZ2enz/WdnZ3eff8tNjYWkZGRIR2j1Wqh1+t9NiK6TNWop02bBpPJhIqKCu91LpcLH3/8MVJSUvweEx0djeTkZJ9jPB4PKioqAh5DRIFFhXpAX18fGhsbvZebm5vR0NCAiRMnYvLkydi+fTt+/vOf46677sK0adPw/PPPw2w2e18hB4Bly5ZhzZo12LZtGwAgJycHWVlZuO+++7BgwQK89tpr6O/vx6ZNm278KyQaY0KOura2FkuXLvVezsnJAQBkZWWhpKQEP/vZz9Df34/HHnsMPT09WLhwIcrKyqDTff2tpqamJnR3d3svr1u3Dl988QV27twJh8OB+fPno6ys7KoXz4jo2m7o+9SjBb9PTdKN2PepiWjkMWoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwIUd9/PhxrFq1CmazGRqNBqWlpd59Q0NDePrppzF37lzceuutMJvN2LBhA86fPx/0nPn5+dBoND5bYmJiyF8MEX2DqPv7+5GUlITCwsKr9l28eBH19fV4/vnnUV9fj3feeQdnzpzB97///Wued/bs2ejo6PBuJ06cCHU0IgIQFeoBNpsNNpvN7z6DwYDy8nKf6woKCrBgwQK0trZi8uTJgQeJioLJZAp1HCL6L2F/Tu10OqHRaDBhwoSg686ePQuz2Yzp06dj/fr1aG1tDbh2YGAALpfLZyOiy8Ia9aVLl/D0008jMzMTer0+4Dqr1YqSkhKUlZWhqKgIzc3NWLRoEXp7e/2ut9vtMBgM3s1isYTrSyC66YQt6qGhITz00ENQFAVFRUVB19psNqxduxbz5s1Deno6jhw5gp6eHhw8eNDv+tzcXDidTu/W1tYWji+B6KYU8nPq63El6JaWFnz44YdB76X9mTBhAmbMmIHGxka/+7VaLbRarRqjEomj+j31laDPnj2Lo0eP4lvf+lbI5+jr60NTUxMSEhLUHo9IvJCj7uvrQ0NDAxoaGgAAzc3NaGhoQGtrK4aGhvDDH/4QtbW1+MMf/gC32w2HwwGHw4HBwUHvOZYtW4aCggLv5R07dqCyshLnzp1DVVUV1qxZg8jISGRmZt74V0g0xoT88Lu2thZLly71Xs7JyQEAZGVlIT8/H3/6058AAPPnz/c57q9//SuWLFkCAGhqakJ3d7d3X3t7OzIzM3HhwgXExcVh4cKFqKmpQVxcXKjjEY15IUe9ZMkSKIoScH+wfVecO3fO5/L+/ftDHYOIAuDPfhMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETChBz18ePHsWrVKpjNZmg0GpSWlvrs37hxIzQajc+2YsWKa563sLAQU6dOhU6ng9VqxcmTJ0MdjYjwDaLu7+9HUlISCgsLA65ZsWIFOjo6vNsf//jHoOc8cOAAcnJykJeXh/r6eiQlJSE9PR1dXV2hjkc05kWFeoDNZoPNZgu6RqvVwmQyXfc5X331VWzZsgWbNm0CALz55pt4//33UVxcjGeeeSbUEYnGtLA8pz527Bji4+Mxc+ZMbN26FRcuXAi4dnBwEHV1dUhLS/t6qIgIpKWlobq62u8xAwMDcLlcPhsRXaZ61CtWrMDvf/97VFRU4Fe/+hUqKyths9ngdrv9ru/u7obb7YbRaPS53mg0wuFw+D3GbrfDYDB4N4vFovaXQXTTCvnh97U8/PDD3l/PnTsX8+bNwx133IFjx45h2bJlqvweubm5yMnJ8V52uVwMm+j/C/u3tKZPn47Y2Fg0Njb63R8bG4vIyEh0dnb6XN/Z2RnweblWq4Ver/fZiOiysEfd3t6OCxcuICEhwe/+6OhoJCcno6Kiwnudx+NBRUUFUlJSwj0ekTghR93X14eGhgY0NDQAAJqbm9HQ0IDW1lb09fXhqaeeQk1NDc6dO4eKigqsXr0ad955J9LT073nWLZsGQoKCryXc3Jy8Lvf/Q579+7F559/jq1bt6K/v9/7ajgRXb+Qn1PX1tZi6dKl3stXnttmZWWhqKgIf//737F371709PTAbDZj+fLl2LVrF7RarfeYpqYmdHd3ey+vW7cOX3zxBXbu3AmHw4H58+ejrKzsqhfPiOjaNIqiKCM9xI1yuVwwGAyY9MYLiBivG+lxiFTn+eoS2h/Pg9PpvOZrSPzZbyJhGDWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETCMGoiYRg1kTCMmkgYRk0kDKMmEoZREwnDqImEYdREwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSQMoyYShlETCcOoiYRh1ETCMGoiYRg1kTCMmkiYkKM+fvw4Vq1aBbPZDI1Gg9LSUp/9Go3G7/bSSy8FPGd+fv5V6xMTE0P+YojoG0Td39+PpKQkFBYW+t3f0dHhsxUXF0Oj0eDBBx8Met7Zs2f7HHfixIlQRyMiAFGhHmCz2WCz2QLuN5lMPpcPHz6MpUuXYvr06cEHiYq66lgiCl1Yn1N3dnbi/fffx+bNm6+59uzZszCbzZg+fTrWr1+P1tbWgGsHBgbgcrl8NiK6LKxR7927FzExMfjBD34QdJ3VakVJSQnKyspQVFSE5uZmLFq0CL29vX7X2+12GAwG72axWMIxPtFNKaxRFxcXY/369dDpdEHX2Ww2rF27FvPmzUN6ejqOHDmCnp4eHDx40O/63NxcOJ1O79bW1haO8YluSiE/p75ef/vb33DmzBkcOHAg5GMnTJiAGTNmoLGx0e9+rVYLrVZ7oyMSiRS2e+rdu3cjOTkZSUlJIR/b19eHpqYmJCQkhGEyItlCjrqvrw8NDQ1oaGgAADQ3N6OhocHnhS2Xy4VDhw7h0Ucf9XuOZcuWoaCgwHt5x44dqKysxLlz51BVVYU1a9YgMjISmZmZoY5HNOaF/PC7trYWS5cu9V7OyckBAGRlZaGkpAQAsH//fiiKEjDKpqYmdHd3ey+3t7cjMzMTFy5cQFxcHBYuXIiamhrExcWFOh7RmKdRFEUZ6SFulMvlgsFgwKQ3XkDE+OAvyhHdjDxfXUL743lwOp3Q6/VB1/Jnv4mEYdREwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhwvZ+alJP8//+TrVzTfuz/3fOhWrWKyp+hJTbo8ppOl9W7z4qLn+cKuf514PBf077enkuXX+qvKcmEoZREwnDqImEYdREwjBqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSBhGTSQMoyYSRsQnn1z5jzs9X10a4UnCw9WrzieDAOrdRv92D6hyHgCqffKJ+6J691H/VmkmzyV1bm/PwOXzXM9/Uiviv7Jtb2+HxWIZ6TGIwq6trQ2TJk0KukZE1B6PB+fPn0dMTAw0Gk3AdS6XCxaLBW1tbdf8P35HE849vEbj3IqioLe3F2azGRERwR+RiHj4HRERcc1/vf6TXq8fNX9YoeDcw2u0zW0wGK5rHV8oIxKGURMJM6ai1mq1yMvLg1arHelRQsK5h9fNOvcVIl4oI6Kvjal7aqKxgFETCcOoiYRh1ETCiIu6sLAQU6dOhU6ng9VqxcmTJ4OuP3ToEBITE6HT6TB37lwcOXJkmCa9zG634/7770dMTAzi4+ORkZGBM2fOBD2mpKQEGo3GZ9PpdMM08WX5+flXzZCYmBj0mJG+rQFg6tSpV82t0WiQnZ3td/1ouK1DJSrqAwcOICcnB3l5eaivr0dSUhLS09PR1dXld31VVRUyMzOxefNmfPrpp8jIyEBGRgZOnz49bDNXVlYiOzsbNTU1KC8vx9DQEJYvX47+/v6gx+n1enR0dHi3lpaWYZr4a7Nnz/aZ4cSJEwHXjobbGgA++eQTn5nLy8sBAGvXrg14zGi4rUOiCLJgwQIlOzvbe9ntditms1mx2+1+1z/00EPKypUrfa6zWq3KT37yk7DOGUxXV5cCQKmsrAy4Zs+ePYrBYBi+ofzIy8tTkpKSrnv9aLytFUVRnnjiCeWOO+5QPB6P3/2j4bYOlZh76sHBQdTV1SEtLc17XUREBNLS0lBdXe33mOrqap/1AJCenh5w/XBwOp0AgIkTJwZd19fXhylTpsBisWD16tX47LPPhmM8H2fPnoXZbMb06dOxfv16tLa2Blw7Gm/rwcFB7Nu3D4888kjQNwKNhts6FGKi7u7uhtvthtFo9LneaDTC4XD4PcbhcIS0Ptw8Hg+2b9+OBx54AHPmzAm4bubMmSguLsbhw4exb98+eDwepKamor29fdhmtVqtKCkpQVlZGYqKitDc3IxFixaht7fX7/rRdlsDQGlpKXp6erBx48aAa0bDbR0qEe/SkiI7OxunT58O+twUAFJSUpCSkuK9nJqailmzZuGtt97Crl27wj0mAMBms3l/PW/ePFitVkyZMgUHDx7E5s2bh2WGG7V7927YbDaYzeaAa0bDbR0qMVHHxsYiMjISnZ2dPtd3dnbCZDL5PcZkMoW0Ppy2bduG9957D8ePHw/pbaQAMG7cONxzzz1obGwM03TXNmHCBMyYMSPgDKPptgaAlpYWHD16FO+8805Ix42G2/paxDz8jo6ORnJyMioqKrzXeTweVFRU+PxL+59SUlJ81gNAeXl5wPXhoCgKtm3bhnfffRcffvghpk2bFvI53G43Tp06hYSEhDBMeH36+vrQ1NQUcIbRcFv/pz179iA+Ph4rV64M6bjRcFtf00i/Uqem/fv3K1qtVikpKVH+8Y9/KI899pgyYcIExeFwKIqiKD/+8Y+VZ555xrv+o48+UqKiopSXX35Z+fzzz5W8vDxl3LhxyqlTp4Zt5q1btyoGg0E5duyY0tHR4d0uXrzoXfPfc7/wwgvKBx98oDQ1NSl1dXXKww8/rOh0OuWzzz4btrmffPJJ5dixY0pzc7Py0UcfKWlpaUpsbKzS1dXld+bRcFtf4Xa7lcmTJytPP/30VftG420dKlFRK4qi/OY3v1EmT56sREdHKwsWLFBqamq8+xYvXqxkZWX5rD948KAyY8YMJTo6Wpk9e7by/vvvD+u8APxue/bsCTj39u3bvV+j0WhUvve97yn19fXDOve6deuUhIQEJTo6Wrn99tuVdevWKY2NjQFnVpSRv62v+OCDDxQAypkzZ67aNxpv61DxrZdEwoh5Tk1ElzFqImEYNZEwjJpIGEZNJAyjJhKGURMJw6iJhGHURMIwaiJhGDWRMIyaSJj/B7rjfuuftizUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prr = 71\n",
    "f = Model(model.input, model.layers[7].output)\n",
    "predictions = f.predict(X_test)\n",
    "plt.imshow(predictions[prr].reshape((20, 10)), cmap='viridis', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecf8872e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = Model(model.input, model.layers[10].output)\n",
    "predictions = f.predict(X_test)\n",
    "plt.imshow(predictions[prr], cmap='viridis', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b2d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
