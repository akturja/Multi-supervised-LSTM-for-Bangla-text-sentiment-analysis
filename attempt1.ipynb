{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f933f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset (Bangla ( Bengali ) sentiment analysis classification benchmark dataset corpus) : https://data.mendeley.com/datasets/p6zc7krs37/4\n",
    "\"\"\"\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import *\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc3c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 50\n",
    "w_decay = 0.05\n",
    "dropout_rate = 0.2\n",
    "epochs_to_run = 500\n",
    "sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8966bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 positive sentences\n",
      "3307 negative sentences\n",
      "3307 positive sentences\n",
      "3307 negative sentences\n"
     ]
    }
   ],
   "source": [
    "# Loading Bangla ( Bengali ) sentiment analysis classification benchmark dataset\n",
    "positive_sentences = []\n",
    "f = open('../datasets/all_positive_8500.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    positive_sentences.append(line.strip())\n",
    "\n",
    "negative_sentences = []\n",
    "f = open('../datasets/all_negative_3307.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    negative_sentences.append(line.strip())\n",
    "    \n",
    "print(len(positive_sentences), 'positive sentences')\n",
    "print(len(negative_sentences), 'negative sentences')\n",
    "\n",
    "import random\n",
    "random.shuffle(positive_sentences)\n",
    "\n",
    "for i in range(len(positive_sentences)-len(negative_sentences)):\n",
    "    positive_sentences.pop(0)\n",
    "\n",
    "print(len(positive_sentences), 'positive sentences')\n",
    "print(len(negative_sentences), 'negative sentences')\n",
    "\n",
    "\n",
    "y_pos = [1 for i in range(len(positive_sentences))]\n",
    "y_neg = [0 for i in range(len(negative_sentences))]\n",
    "\n",
    "X = positive_sentences + negative_sentences\n",
    "y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c24841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141148\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4113e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(X)\n",
    "y = np.array(y)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "X_train_val = X_train\n",
    "y_train_val = y_train\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_val, test_size=0.2, random_state=1, stratify=y_train_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747457da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856b4b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 100)          14114800  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 200, 50), (None,  30200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 14,145,051\n",
      "Trainable params: 30,251\n",
      "Non-trainable params: 14,114,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 8.7276 - accuracy: 0.5165\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55430, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 8.7276 - accuracy: 0.5165 - val_loss: 7.6852 - val_accuracy: 0.5543\n",
      "Epoch 2/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 6.8302 - accuracy: 0.5761\n",
      "Epoch 00002: val_accuracy improved from 0.55430 to 0.58735, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 6.8226 - accuracy: 0.5766 - val_loss: 6.0185 - val_accuracy: 0.5873\n",
      "Epoch 3/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 5.3574 - accuracy: 0.6150\n",
      "Epoch 00003: val_accuracy improved from 0.58735 to 0.63645, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 5.3522 - accuracy: 0.6127 - val_loss: 4.7239 - val_accuracy: 0.6364\n",
      "Epoch 4/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 4.2134 - accuracy: 0.6400\n",
      "Epoch 00004: val_accuracy improved from 0.63645 to 0.66195, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 4.2089 - accuracy: 0.6406 - val_loss: 3.7223 - val_accuracy: 0.6619\n",
      "Epoch 5/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 3.3284 - accuracy: 0.6687\n",
      "Epoch 00005: val_accuracy improved from 0.66195 to 0.68744, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 3.3250 - accuracy: 0.6685 - val_loss: 2.9490 - val_accuracy: 0.6874\n",
      "Epoch 6/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.6492 - accuracy: 0.6930\n",
      "Epoch 00006: val_accuracy improved from 0.68744 to 0.69877, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 2.6460 - accuracy: 0.6938 - val_loss: 2.3550 - val_accuracy: 0.6988\n",
      "Epoch 7/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.1225 - accuracy: 0.7159\n",
      "Epoch 00007: val_accuracy improved from 0.69877 to 0.71860, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 2.1211 - accuracy: 0.7143 - val_loss: 1.9014 - val_accuracy: 0.7186\n",
      "Epoch 8/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.7232 - accuracy: 0.7266\n",
      "Epoch 00008: val_accuracy improved from 0.71860 to 0.73843, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.7220 - accuracy: 0.7264 - val_loss: 1.5577 - val_accuracy: 0.7384\n",
      "Epoch 9/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.4248 - accuracy: 0.7383\n",
      "Epoch 00009: val_accuracy improved from 0.73843 to 0.74127, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.4241 - accuracy: 0.7379 - val_loss: 1.2983 - val_accuracy: 0.7413\n",
      "Epoch 10/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1988 - accuracy: 0.7426\n",
      "Epoch 00010: val_accuracy improved from 0.74127 to 0.75071, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.1981 - accuracy: 0.7417 - val_loss: 1.1032 - val_accuracy: 0.7507\n",
      "Epoch 11/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0328 - accuracy: 0.7538\n",
      "Epoch 00011: val_accuracy improved from 0.75071 to 0.76393, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.0316 - accuracy: 0.7540 - val_loss: 0.9560 - val_accuracy: 0.7639\n",
      "Epoch 12/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9102 - accuracy: 0.7483\n",
      "Epoch 00012: val_accuracy improved from 0.76393 to 0.76582, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.9096 - accuracy: 0.7488 - val_loss: 0.8502 - val_accuracy: 0.7658\n",
      "Epoch 13/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8199 - accuracy: 0.7576\n",
      "Epoch 00013: val_accuracy improved from 0.76582 to 0.77526, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.8194 - accuracy: 0.7580 - val_loss: 0.7737 - val_accuracy: 0.7753\n",
      "Epoch 14/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7581 - accuracy: 0.7619\n",
      "Epoch 00014: val_accuracy did not improve from 0.77526\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7577 - accuracy: 0.7616 - val_loss: 0.7200 - val_accuracy: 0.7677\n",
      "Epoch 15/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7060 - accuracy: 0.7646\n",
      "Epoch 00015: val_accuracy improved from 0.77526 to 0.77904, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.7057 - accuracy: 0.7649 - val_loss: 0.6745 - val_accuracy: 0.7790\n",
      "Epoch 16/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6740 - accuracy: 0.7641\n",
      "Epoch 00016: val_accuracy improved from 0.77904 to 0.78093, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6739 - accuracy: 0.7639 - val_loss: 0.6450 - val_accuracy: 0.7809\n",
      "Epoch 17/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6468 - accuracy: 0.7703\n",
      "Epoch 00017: val_accuracy did not improve from 0.78093\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6459 - accuracy: 0.7710 - val_loss: 0.6266 - val_accuracy: 0.7809\n",
      "Epoch 18/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6281 - accuracy: 0.7748\n",
      "Epoch 00018: val_accuracy improved from 0.78093 to 0.78281, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6280 - accuracy: 0.7750 - val_loss: 0.6075 - val_accuracy: 0.7828\n",
      "Epoch 19/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6136 - accuracy: 0.7767\n",
      "Epoch 00019: val_accuracy improved from 0.78281 to 0.79981, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6144 - accuracy: 0.7762 - val_loss: 0.5906 - val_accuracy: 0.7998\n",
      "Epoch 20/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5958 - accuracy: 0.7755\n",
      "Epoch 00020: val_accuracy did not improve from 0.79981\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5958 - accuracy: 0.7758 - val_loss: 0.5791 - val_accuracy: 0.7856\n",
      "Epoch 21/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5924 - accuracy: 0.7750\n",
      "Epoch 00021: val_accuracy did not improve from 0.79981\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5915 - accuracy: 0.7758 - val_loss: 0.5848 - val_accuracy: 0.7828\n",
      "Epoch 22/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5861 - accuracy: 0.7793\n",
      "Epoch 00022: val_accuracy improved from 0.79981 to 0.80453, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5861 - accuracy: 0.7786 - val_loss: 0.5627 - val_accuracy: 0.8045\n",
      "Epoch 23/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5777 - accuracy: 0.7824\n",
      "Epoch 00023: val_accuracy did not improve from 0.80453\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5763 - accuracy: 0.7838 - val_loss: 0.5562 - val_accuracy: 0.8026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5697 - accuracy: 0.7889\n",
      "Epoch 00024: val_accuracy improved from 0.80453 to 0.80925, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5702 - accuracy: 0.7880 - val_loss: 0.5468 - val_accuracy: 0.8093\n",
      "Epoch 25/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7863\n",
      "Epoch 00025: val_accuracy improved from 0.80925 to 0.81114, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5674 - accuracy: 0.7866 - val_loss: 0.5467 - val_accuracy: 0.8111\n",
      "Epoch 26/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7913\n",
      "Epoch 00026: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5673 - accuracy: 0.7916 - val_loss: 0.5527 - val_accuracy: 0.7932\n",
      "Epoch 27/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7870\n",
      "Epoch 00027: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5618 - accuracy: 0.7854 - val_loss: 0.5399 - val_accuracy: 0.8083\n",
      "Epoch 28/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5544 - accuracy: 0.7877\n",
      "Epoch 00028: val_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5547 - accuracy: 0.7873 - val_loss: 0.5394 - val_accuracy: 0.8064\n",
      "Epoch 29/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5518 - accuracy: 0.7925\n",
      "Epoch 00029: val_accuracy improved from 0.81114 to 0.81398, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5510 - accuracy: 0.7932 - val_loss: 0.5315 - val_accuracy: 0.8140\n",
      "Epoch 30/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5497 - accuracy: 0.7886\n",
      "Epoch 00030: val_accuracy did not improve from 0.81398\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5498 - accuracy: 0.7890 - val_loss: 0.5630 - val_accuracy: 0.7753\n",
      "Epoch 31/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5493 - accuracy: 0.7882\n",
      "Epoch 00031: val_accuracy improved from 0.81398 to 0.81775, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5495 - accuracy: 0.7883 - val_loss: 0.5224 - val_accuracy: 0.8178\n",
      "Epoch 32/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5383 - accuracy: 0.7970\n",
      "Epoch 00032: val_accuracy did not improve from 0.81775\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5393 - accuracy: 0.7961 - val_loss: 0.5241 - val_accuracy: 0.8121\n",
      "Epoch 33/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5427 - accuracy: 0.7963\n",
      "Epoch 00033: val_accuracy did not improve from 0.81775\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5423 - accuracy: 0.7966 - val_loss: 0.5258 - val_accuracy: 0.8074\n",
      "Epoch 34/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5303 - accuracy: 0.8032\n",
      "Epoch 00034: val_accuracy improved from 0.81775 to 0.82153, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5302 - accuracy: 0.8034 - val_loss: 0.5179 - val_accuracy: 0.8215\n",
      "Epoch 35/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5316 - accuracy: 0.7989\n",
      "Epoch 00035: val_accuracy improved from 0.82153 to 0.82342, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5327 - accuracy: 0.7991 - val_loss: 0.5120 - val_accuracy: 0.8234\n",
      "Epoch 36/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5358 - accuracy: 0.7927\n",
      "Epoch 00036: val_accuracy improved from 0.82342 to 0.83192, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5351 - accuracy: 0.7932 - val_loss: 0.5115 - val_accuracy: 0.8319\n",
      "Epoch 37/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5235 - accuracy: 0.8099\n",
      "Epoch 00037: val_accuracy did not improve from 0.83192\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5243 - accuracy: 0.8091 - val_loss: 0.5066 - val_accuracy: 0.8310\n",
      "Epoch 38/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5250 - accuracy: 0.8075\n",
      "Epoch 00038: val_accuracy did not improve from 0.83192\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5246 - accuracy: 0.8077 - val_loss: 0.5159 - val_accuracy: 0.8253\n",
      "Epoch 39/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5214 - accuracy: 0.8154\n",
      "Epoch 00039: val_accuracy did not improve from 0.83192\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5207 - accuracy: 0.8155 - val_loss: 0.5202 - val_accuracy: 0.8064\n",
      "Epoch 40/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5262 - accuracy: 0.8042\n",
      "Epoch 00040: val_accuracy did not improve from 0.83192\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5257 - accuracy: 0.8046 - val_loss: 0.5024 - val_accuracy: 0.8263\n",
      "Epoch 41/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5199 - accuracy: 0.8104\n",
      "Epoch 00041: val_accuracy improved from 0.83192 to 0.83475, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5197 - accuracy: 0.8107 - val_loss: 0.4974 - val_accuracy: 0.8347\n",
      "Epoch 42/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.8101\n",
      "Epoch 00042: val_accuracy did not improve from 0.83475\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5124 - accuracy: 0.8095 - val_loss: 0.4978 - val_accuracy: 0.8225\n",
      "Epoch 43/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5105 - accuracy: 0.8175\n",
      "Epoch 00043: val_accuracy did not improve from 0.83475\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5107 - accuracy: 0.8166 - val_loss: 0.4987 - val_accuracy: 0.8234\n",
      "Epoch 44/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5102 - accuracy: 0.8144\n",
      "Epoch 00044: val_accuracy did not improve from 0.83475\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5109 - accuracy: 0.8138 - val_loss: 0.4911 - val_accuracy: 0.8338\n",
      "Epoch 45/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5022 - accuracy: 0.8206\n",
      "Epoch 00045: val_accuracy did not improve from 0.83475\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5023 - accuracy: 0.8199 - val_loss: 0.4951 - val_accuracy: 0.8206\n",
      "Epoch 46/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5046 - accuracy: 0.8180\n",
      "Epoch 00046: val_accuracy did not improve from 0.83475\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5053 - accuracy: 0.8176 - val_loss: 0.4898 - val_accuracy: 0.8310\n",
      "Epoch 47/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5055 - accuracy: 0.8173\n",
      "Epoch 00047: val_accuracy improved from 0.83475 to 0.83569, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5058 - accuracy: 0.8171 - val_loss: 0.4905 - val_accuracy: 0.8357\n",
      "Epoch 48/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5058 - accuracy: 0.8177\n",
      "Epoch 00048: val_accuracy did not improve from 0.83569\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5055 - accuracy: 0.8178 - val_loss: 0.5017 - val_accuracy: 0.8130\n",
      "Epoch 49/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.8259\n",
      "Epoch 00049: val_accuracy did not improve from 0.83569\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4973 - accuracy: 0.8263 - val_loss: 0.4852 - val_accuracy: 0.8338\n",
      "Epoch 50/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4923 - accuracy: 0.8266\n",
      "Epoch 00050: val_accuracy did not improve from 0.83569\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4923 - accuracy: 0.8270 - val_loss: 0.4828 - val_accuracy: 0.8338\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.4939 - accuracy: 0.8235\n",
      "Epoch 00051: val_accuracy did not improve from 0.83569\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4937 - accuracy: 0.8237 - val_loss: 0.4864 - val_accuracy: 0.8347\n",
      "Epoch 52/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4972 - accuracy: 0.8309\n",
      "Epoch 00052: val_accuracy did not improve from 0.83569\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4966 - accuracy: 0.8310 - val_loss: 0.4866 - val_accuracy: 0.8310\n",
      "Epoch 53/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4950 - accuracy: 0.8213\n",
      "Epoch 00053: val_accuracy improved from 0.83569 to 0.84136, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4951 - accuracy: 0.8211 - val_loss: 0.4880 - val_accuracy: 0.8414\n",
      "Epoch 54/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4848 - accuracy: 0.8333\n",
      "Epoch 00054: val_accuracy did not improve from 0.84136\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4855 - accuracy: 0.8327 - val_loss: 0.5012 - val_accuracy: 0.8111\n",
      "Epoch 55/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4867 - accuracy: 0.8335\n",
      "Epoch 00055: val_accuracy did not improve from 0.84136\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4871 - accuracy: 0.8327 - val_loss: 0.5063 - val_accuracy: 0.8178\n",
      "Epoch 56/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.8299\n",
      "Epoch 00056: val_accuracy did not improve from 0.84136\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4842 - accuracy: 0.8301 - val_loss: 0.4771 - val_accuracy: 0.8347\n",
      "Epoch 57/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4874 - accuracy: 0.8268\n",
      "Epoch 00057: val_accuracy did not improve from 0.84136\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4874 - accuracy: 0.8266 - val_loss: 0.4819 - val_accuracy: 0.8272\n",
      "Epoch 58/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4785 - accuracy: 0.8399\n",
      "Epoch 00058: val_accuracy improved from 0.84136 to 0.84230, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4778 - accuracy: 0.8403 - val_loss: 0.4751 - val_accuracy: 0.8423\n",
      "Epoch 59/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4733 - accuracy: 0.8445\n",
      "Epoch 00059: val_accuracy improved from 0.84230 to 0.84608, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4736 - accuracy: 0.8443 - val_loss: 0.4738 - val_accuracy: 0.8461\n",
      "Epoch 60/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4784 - accuracy: 0.8385\n",
      "Epoch 00060: val_accuracy did not improve from 0.84608\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4776 - accuracy: 0.8388 - val_loss: 0.4682 - val_accuracy: 0.8423\n",
      "Epoch 61/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4777 - accuracy: 0.8340\n",
      "Epoch 00061: val_accuracy did not improve from 0.84608\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4790 - accuracy: 0.8327 - val_loss: 0.4732 - val_accuracy: 0.8432\n",
      "Epoch 62/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4708 - accuracy: 0.8533\n",
      "Epoch 00062: val_accuracy did not improve from 0.84608\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4710 - accuracy: 0.8530 - val_loss: 0.4699 - val_accuracy: 0.8451\n",
      "Epoch 63/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4708 - accuracy: 0.8411\n",
      "Epoch 00063: val_accuracy did not improve from 0.84608\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4720 - accuracy: 0.8407 - val_loss: 0.4687 - val_accuracy: 0.8442\n",
      "Epoch 64/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4742 - accuracy: 0.8371\n",
      "Epoch 00064: val_accuracy did not improve from 0.84608\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4740 - accuracy: 0.8374 - val_loss: 0.4689 - val_accuracy: 0.8357\n",
      "Epoch 65/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4712 - accuracy: 0.8409\n",
      "Epoch 00065: val_accuracy improved from 0.84608 to 0.84891, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4702 - accuracy: 0.8414 - val_loss: 0.4622 - val_accuracy: 0.8489\n",
      "Epoch 66/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4684 - accuracy: 0.8457\n",
      "Epoch 00066: val_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4695 - accuracy: 0.8448 - val_loss: 0.4638 - val_accuracy: 0.8461\n",
      "Epoch 67/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4665 - accuracy: 0.8423\n",
      "Epoch 00067: val_accuracy improved from 0.84891 to 0.84986, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4671 - accuracy: 0.8419 - val_loss: 0.4615 - val_accuracy: 0.8499\n",
      "Epoch 68/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4662 - accuracy: 0.8457\n",
      "Epoch 00068: val_accuracy did not improve from 0.84986\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4662 - accuracy: 0.8455 - val_loss: 0.4669 - val_accuracy: 0.8395\n",
      "Epoch 69/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4614 - accuracy: 0.8466\n",
      "Epoch 00069: val_accuracy did not improve from 0.84986\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4607 - accuracy: 0.8469 - val_loss: 0.4657 - val_accuracy: 0.8470\n",
      "Epoch 70/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.8457\n",
      "Epoch 00070: val_accuracy did not improve from 0.84986\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4608 - accuracy: 0.8459 - val_loss: 0.5285 - val_accuracy: 0.8074\n",
      "Epoch 71/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.8502\n",
      "Epoch 00071: val_accuracy did not improve from 0.84986\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4605 - accuracy: 0.8507 - val_loss: 0.4593 - val_accuracy: 0.8480\n",
      "Epoch 72/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.4623 - accuracy: 0.8433\n",
      "Epoch 00072: val_accuracy did not improve from 0.84986\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4616 - accuracy: 0.8436 - val_loss: 0.4575 - val_accuracy: 0.8470\n",
      "Epoch 73/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4548 - accuracy: 0.8538\n",
      "Epoch 00073: val_accuracy did not improve from 0.84986\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4546 - accuracy: 0.8540 - val_loss: 0.4896 - val_accuracy: 0.8225\n",
      "Epoch 74/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4563 - accuracy: 0.8476\n",
      "Epoch 00074: val_accuracy did not improve from 0.84986\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4566 - accuracy: 0.8474 - val_loss: 0.4606 - val_accuracy: 0.8480\n",
      "Epoch 75/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4597 - accuracy: 0.8495\n",
      "Epoch 00075: val_accuracy improved from 0.84986 to 0.85080, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4595 - accuracy: 0.8497 - val_loss: 0.4572 - val_accuracy: 0.8508\n",
      "Epoch 76/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4511 - accuracy: 0.8509\n",
      "Epoch 00076: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4508 - accuracy: 0.8509 - val_loss: 0.4848 - val_accuracy: 0.8272\n",
      "Epoch 77/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4473 - accuracy: 0.8607\n",
      "Epoch 00077: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4475 - accuracy: 0.8608 - val_loss: 0.4631 - val_accuracy: 0.8414\n",
      "Epoch 78/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4473 - accuracy: 0.8569\n",
      "Epoch 00078: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4474 - accuracy: 0.8568 - val_loss: 0.4592 - val_accuracy: 0.8451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.8552\n",
      "Epoch 00079: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4482 - accuracy: 0.8549 - val_loss: 0.4747 - val_accuracy: 0.8300\n",
      "Epoch 80/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4489 - accuracy: 0.8597\n",
      "Epoch 00080: val_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4497 - accuracy: 0.8594 - val_loss: 0.4607 - val_accuracy: 0.8442\n",
      "Epoch 81/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4450 - accuracy: 0.8585\n",
      "Epoch 00081: val_accuracy improved from 0.85080 to 0.85364, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4446 - accuracy: 0.8589 - val_loss: 0.4543 - val_accuracy: 0.8536\n",
      "Epoch 82/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.8562\n",
      "Epoch 00082: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4476 - accuracy: 0.8573 - val_loss: 0.4489 - val_accuracy: 0.8499\n",
      "Epoch 83/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4415 - accuracy: 0.8566\n",
      "Epoch 00083: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4411 - accuracy: 0.8570 - val_loss: 0.4576 - val_accuracy: 0.8527\n",
      "Epoch 84/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4439 - accuracy: 0.8609\n",
      "Epoch 00084: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4442 - accuracy: 0.8603 - val_loss: 0.4664 - val_accuracy: 0.8385\n",
      "Epoch 85/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4353 - accuracy: 0.8616\n",
      "Epoch 00085: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4345 - accuracy: 0.8627 - val_loss: 0.4450 - val_accuracy: 0.8517\n",
      "Epoch 86/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.8674\n",
      "Epoch 00086: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4340 - accuracy: 0.8679 - val_loss: 0.4557 - val_accuracy: 0.8404\n",
      "Epoch 87/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.8588\n",
      "Epoch 00087: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4418 - accuracy: 0.8587 - val_loss: 0.4658 - val_accuracy: 0.8432\n",
      "Epoch 88/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4393 - accuracy: 0.8583\n",
      "Epoch 00088: val_accuracy improved from 0.85364 to 0.85458, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4387 - accuracy: 0.8580 - val_loss: 0.4426 - val_accuracy: 0.8546\n",
      "Epoch 89/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4441 - accuracy: 0.8557\n",
      "Epoch 00089: val_accuracy did not improve from 0.85458\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4433 - accuracy: 0.8563 - val_loss: 0.4491 - val_accuracy: 0.8470\n",
      "Epoch 90/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4304 - accuracy: 0.8657\n",
      "Epoch 00090: val_accuracy did not improve from 0.85458\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4295 - accuracy: 0.8660 - val_loss: 0.4461 - val_accuracy: 0.8470\n",
      "Epoch 91/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4370 - accuracy: 0.8640\n",
      "Epoch 00091: val_accuracy did not improve from 0.85458\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4380 - accuracy: 0.8634 - val_loss: 0.4445 - val_accuracy: 0.8508\n",
      "Epoch 92/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.8705\n",
      "Epoch 00092: val_accuracy did not improve from 0.85458\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4292 - accuracy: 0.8710 - val_loss: 0.4415 - val_accuracy: 0.8499\n",
      "Epoch 93/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4374 - accuracy: 0.8638\n",
      "Epoch 00093: val_accuracy improved from 0.85458 to 0.85552, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4368 - accuracy: 0.8641 - val_loss: 0.4438 - val_accuracy: 0.8555\n",
      "Epoch 94/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.8755\n",
      "Epoch 00094: val_accuracy did not improve from 0.85552\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4229 - accuracy: 0.8752 - val_loss: 0.4369 - val_accuracy: 0.8536\n",
      "Epoch 95/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4277 - accuracy: 0.8674\n",
      "Epoch 00095: val_accuracy improved from 0.85552 to 0.85836, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4271 - accuracy: 0.8674 - val_loss: 0.4354 - val_accuracy: 0.8584\n",
      "Epoch 96/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4246 - accuracy: 0.8736\n",
      "Epoch 00096: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4249 - accuracy: 0.8733 - val_loss: 0.4337 - val_accuracy: 0.8565\n",
      "Epoch 97/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4251 - accuracy: 0.8702\n",
      "Epoch 00097: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4252 - accuracy: 0.8705 - val_loss: 0.4697 - val_accuracy: 0.8385\n",
      "Epoch 98/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4293 - accuracy: 0.8662\n",
      "Epoch 00098: val_accuracy did not improve from 0.85836\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4301 - accuracy: 0.8660 - val_loss: 0.4413 - val_accuracy: 0.8499\n",
      "Epoch 99/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4304 - accuracy: 0.8695\n",
      "Epoch 00099: val_accuracy improved from 0.85836 to 0.86213, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4306 - accuracy: 0.8691 - val_loss: 0.4365 - val_accuracy: 0.8621\n",
      "Epoch 100/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4293 - accuracy: 0.8671\n",
      "Epoch 00100: val_accuracy did not improve from 0.86213\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4298 - accuracy: 0.8670 - val_loss: 0.4664 - val_accuracy: 0.8404\n",
      "Epoch 101/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4152 - accuracy: 0.8795\n",
      "Epoch 00101: val_accuracy did not improve from 0.86213\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4151 - accuracy: 0.8795 - val_loss: 0.4326 - val_accuracy: 0.8527\n",
      "Epoch 102/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8783\n",
      "Epoch 00102: val_accuracy did not improve from 0.86213\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4207 - accuracy: 0.8783 - val_loss: 0.4388 - val_accuracy: 0.8536\n",
      "Epoch 103/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4203 - accuracy: 0.8724\n",
      "Epoch 00103: val_accuracy did not improve from 0.86213\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4206 - accuracy: 0.8726 - val_loss: 0.4323 - val_accuracy: 0.8602\n",
      "Epoch 104/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4222 - accuracy: 0.8748\n",
      "Epoch 00104: val_accuracy did not improve from 0.86213\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4217 - accuracy: 0.8748 - val_loss: 0.4302 - val_accuracy: 0.8574\n",
      "Epoch 105/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4228 - accuracy: 0.8717\n",
      "Epoch 00105: val_accuracy did not improve from 0.86213\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4224 - accuracy: 0.8722 - val_loss: 0.4303 - val_accuracy: 0.8565\n",
      "Epoch 106/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8862\n",
      "Epoch 00106: val_accuracy did not improve from 0.86213\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4081 - accuracy: 0.8866 - val_loss: 0.4396 - val_accuracy: 0.8565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8702\n",
      "Epoch 00107: val_accuracy improved from 0.86213 to 0.86308, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4185 - accuracy: 0.8707 - val_loss: 0.4299 - val_accuracy: 0.8631\n",
      "Epoch 108/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4140 - accuracy: 0.8757\n",
      "Epoch 00108: val_accuracy did not improve from 0.86308\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4146 - accuracy: 0.8755 - val_loss: 0.4609 - val_accuracy: 0.8499\n",
      "Epoch 109/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4175 - accuracy: 0.8724\n",
      "Epoch 00109: val_accuracy improved from 0.86308 to 0.86591, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4177 - accuracy: 0.8722 - val_loss: 0.4365 - val_accuracy: 0.8659\n",
      "Epoch 110/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4060 - accuracy: 0.8841\n",
      "Epoch 00110: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4068 - accuracy: 0.8835 - val_loss: 0.4330 - val_accuracy: 0.8612\n",
      "Epoch 111/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4105 - accuracy: 0.8774\n",
      "Epoch 00111: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4112 - accuracy: 0.8774 - val_loss: 0.4287 - val_accuracy: 0.8593\n",
      "Epoch 112/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.8726\n",
      "Epoch 00112: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4179 - accuracy: 0.8733 - val_loss: 0.4355 - val_accuracy: 0.8640\n",
      "Epoch 113/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4066 - accuracy: 0.8781\n",
      "Epoch 00113: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4068 - accuracy: 0.8783 - val_loss: 0.4271 - val_accuracy: 0.8650\n",
      "Epoch 114/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8791\n",
      "Epoch 00114: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4065 - accuracy: 0.8788 - val_loss: 0.4258 - val_accuracy: 0.8602\n",
      "Epoch 115/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4078 - accuracy: 0.8788\n",
      "Epoch 00115: val_accuracy improved from 0.86591 to 0.86686, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4073 - accuracy: 0.8793 - val_loss: 0.4369 - val_accuracy: 0.8669\n",
      "Epoch 116/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4102 - accuracy: 0.8743\n",
      "Epoch 00116: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4094 - accuracy: 0.8745 - val_loss: 0.4213 - val_accuracy: 0.8593\n",
      "Epoch 117/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8805\n",
      "Epoch 00117: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4105 - accuracy: 0.8811 - val_loss: 0.4253 - val_accuracy: 0.8640\n",
      "Epoch 118/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4063 - accuracy: 0.8824\n",
      "Epoch 00118: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4055 - accuracy: 0.8828 - val_loss: 0.4813 - val_accuracy: 0.8338\n",
      "Epoch 119/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4035 - accuracy: 0.8810\n",
      "Epoch 00119: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4028 - accuracy: 0.8819 - val_loss: 0.4305 - val_accuracy: 0.8621\n",
      "Epoch 120/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8856\n",
      "Epoch 00120: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3997 - accuracy: 0.8856 - val_loss: 0.4292 - val_accuracy: 0.8584\n",
      "Epoch 121/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4024 - accuracy: 0.8812\n",
      "Epoch 00121: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4034 - accuracy: 0.8807 - val_loss: 0.4221 - val_accuracy: 0.8584\n",
      "Epoch 122/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4018 - accuracy: 0.8893\n",
      "Epoch 00122: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4020 - accuracy: 0.8882 - val_loss: 0.4434 - val_accuracy: 0.8432\n",
      "Epoch 123/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3967 - accuracy: 0.8860\n",
      "Epoch 00123: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3968 - accuracy: 0.8861 - val_loss: 0.4441 - val_accuracy: 0.8621\n",
      "Epoch 124/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8862\n",
      "Epoch 00124: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3971 - accuracy: 0.8866 - val_loss: 0.4240 - val_accuracy: 0.8659\n",
      "Epoch 125/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8824\n",
      "Epoch 00125: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3993 - accuracy: 0.8828 - val_loss: 0.4235 - val_accuracy: 0.8640\n",
      "Epoch 126/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3967 - accuracy: 0.8867\n",
      "Epoch 00126: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3957 - accuracy: 0.8878 - val_loss: 0.4256 - val_accuracy: 0.8593\n",
      "Epoch 127/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8829\n",
      "Epoch 00127: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3947 - accuracy: 0.8828 - val_loss: 0.4227 - val_accuracy: 0.8621\n",
      "Epoch 128/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.8786\n",
      "Epoch 00128: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4052 - accuracy: 0.8785 - val_loss: 0.4211 - val_accuracy: 0.8546\n",
      "Epoch 129/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3962 - accuracy: 0.8836\n",
      "Epoch 00129: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3960 - accuracy: 0.8840 - val_loss: 0.4256 - val_accuracy: 0.8631\n",
      "Epoch 130/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.8826\n",
      "Epoch 00130: val_accuracy did not improve from 0.86686\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3952 - accuracy: 0.8828 - val_loss: 0.4294 - val_accuracy: 0.8584\n",
      "Epoch 131/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8855\n",
      "Epoch 00131: val_accuracy improved from 0.86686 to 0.86874, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3976 - accuracy: 0.8845 - val_loss: 0.4257 - val_accuracy: 0.8687\n",
      "Epoch 132/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3918 - accuracy: 0.8891\n",
      "Epoch 00132: val_accuracy did not improve from 0.86874\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3917 - accuracy: 0.8889 - val_loss: 0.4291 - val_accuracy: 0.8621\n",
      "Epoch 133/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3945 - accuracy: 0.8850\n",
      "Epoch 00133: val_accuracy did not improve from 0.86874\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3941 - accuracy: 0.8854 - val_loss: 0.4291 - val_accuracy: 0.8650\n",
      "Epoch 134/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8860\n",
      "Epoch 00134: val_accuracy did not improve from 0.86874\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3890 - accuracy: 0.8854 - val_loss: 0.4167 - val_accuracy: 0.8687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8898\n",
      "Epoch 00135: val_accuracy did not improve from 0.86874\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3900 - accuracy: 0.8892 - val_loss: 0.4262 - val_accuracy: 0.8621\n",
      "Epoch 136/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3940 - accuracy: 0.8810\n",
      "Epoch 00136: val_accuracy did not improve from 0.86874\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3943 - accuracy: 0.8811 - val_loss: 0.4207 - val_accuracy: 0.8574\n",
      "Epoch 137/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8817\n",
      "Epoch 00137: val_accuracy did not improve from 0.86874\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3907 - accuracy: 0.8826 - val_loss: 0.4184 - val_accuracy: 0.8669\n",
      "Epoch 138/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3862 - accuracy: 0.8903\n",
      "Epoch 00138: val_accuracy improved from 0.86874 to 0.87063, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3865 - accuracy: 0.8899 - val_loss: 0.4309 - val_accuracy: 0.8706\n",
      "Epoch 139/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8838\n",
      "Epoch 00139: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3941 - accuracy: 0.8833 - val_loss: 0.4177 - val_accuracy: 0.8612\n",
      "Epoch 140/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3915 - accuracy: 0.8845\n",
      "Epoch 00140: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3924 - accuracy: 0.8840 - val_loss: 0.4149 - val_accuracy: 0.8650\n",
      "Epoch 141/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8922\n",
      "Epoch 00141: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3853 - accuracy: 0.8918 - val_loss: 0.4559 - val_accuracy: 0.8499\n",
      "Epoch 142/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3905 - accuracy: 0.8824\n",
      "Epoch 00142: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3905 - accuracy: 0.8826 - val_loss: 0.4138 - val_accuracy: 0.8678\n",
      "Epoch 143/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.8984\n",
      "Epoch 00143: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3767 - accuracy: 0.8970 - val_loss: 0.4281 - val_accuracy: 0.8678\n",
      "Epoch 144/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3870 - accuracy: 0.8896\n",
      "Epoch 00144: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3871 - accuracy: 0.8894 - val_loss: 0.4157 - val_accuracy: 0.8631\n",
      "Epoch 145/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3803 - accuracy: 0.8943\n",
      "Epoch 00145: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3799 - accuracy: 0.8944 - val_loss: 0.4235 - val_accuracy: 0.8602\n",
      "Epoch 146/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3819 - accuracy: 0.8872\n",
      "Epoch 00146: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3823 - accuracy: 0.8866 - val_loss: 0.4145 - val_accuracy: 0.8602\n",
      "Epoch 147/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8900\n",
      "Epoch 00147: val_accuracy improved from 0.87063 to 0.87158, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3818 - accuracy: 0.8901 - val_loss: 0.4202 - val_accuracy: 0.8716\n",
      "Epoch 148/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3827 - accuracy: 0.8922\n",
      "Epoch 00148: val_accuracy did not improve from 0.87158\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3835 - accuracy: 0.8918 - val_loss: 0.4162 - val_accuracy: 0.8640\n",
      "Epoch 149/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.8848\n",
      "Epoch 00149: val_accuracy did not improve from 0.87158\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3945 - accuracy: 0.8852 - val_loss: 0.4159 - val_accuracy: 0.8650\n",
      "Epoch 150/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3845 - accuracy: 0.8884\n",
      "Epoch 00150: val_accuracy did not improve from 0.87158\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3838 - accuracy: 0.8892 - val_loss: 0.4149 - val_accuracy: 0.8697\n",
      "Epoch 151/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3777 - accuracy: 0.8953\n",
      "Epoch 00151: val_accuracy did not improve from 0.87158\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3773 - accuracy: 0.8956 - val_loss: 0.4199 - val_accuracy: 0.8640\n",
      "Epoch 152/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8943\n",
      "Epoch 00152: val_accuracy improved from 0.87158 to 0.87441, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3785 - accuracy: 0.8941 - val_loss: 0.4140 - val_accuracy: 0.8744\n",
      "Epoch 153/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8907\n",
      "Epoch 00153: val_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3801 - accuracy: 0.8908 - val_loss: 0.4215 - val_accuracy: 0.8716\n",
      "Epoch 154/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.9029\n",
      "Epoch 00154: val_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3733 - accuracy: 0.9026 - val_loss: 0.4104 - val_accuracy: 0.8725\n",
      "Epoch 155/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8943\n",
      "Epoch 00155: val_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3793 - accuracy: 0.8939 - val_loss: 0.4100 - val_accuracy: 0.8706\n",
      "Epoch 156/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3772 - accuracy: 0.8962\n",
      "Epoch 00156: val_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3781 - accuracy: 0.8956 - val_loss: 0.4118 - val_accuracy: 0.8735\n",
      "Epoch 157/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8929\n",
      "Epoch 00157: val_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3766 - accuracy: 0.8937 - val_loss: 0.4064 - val_accuracy: 0.8697\n",
      "Epoch 158/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.8962\n",
      "Epoch 00158: val_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3759 - accuracy: 0.8963 - val_loss: 0.4088 - val_accuracy: 0.8706\n",
      "Epoch 159/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8960\n",
      "Epoch 00159: val_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3747 - accuracy: 0.8953 - val_loss: 0.4161 - val_accuracy: 0.8669\n",
      "Epoch 160/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8903\n",
      "Epoch 00160: val_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3851 - accuracy: 0.8906 - val_loss: 0.4124 - val_accuracy: 0.8621\n",
      "Epoch 161/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3799 - accuracy: 0.8907\n",
      "Epoch 00161: val_accuracy improved from 0.87441 to 0.87630, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3796 - accuracy: 0.8906 - val_loss: 0.4133 - val_accuracy: 0.8763\n",
      "Epoch 162/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.8993\n",
      "Epoch 00162: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3683 - accuracy: 0.8991 - val_loss: 0.4150 - val_accuracy: 0.8602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.8931\n",
      "Epoch 00163: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3757 - accuracy: 0.8939 - val_loss: 0.4188 - val_accuracy: 0.8754\n",
      "Epoch 164/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8938\n",
      "Epoch 00164: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3718 - accuracy: 0.8948 - val_loss: 0.4148 - val_accuracy: 0.8678\n",
      "Epoch 165/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3714 - accuracy: 0.8979\n",
      "Epoch 00165: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3703 - accuracy: 0.8984 - val_loss: 0.4020 - val_accuracy: 0.8687\n",
      "Epoch 166/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.9012\n",
      "Epoch 00166: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3625 - accuracy: 0.9015 - val_loss: 0.4160 - val_accuracy: 0.8716\n",
      "Epoch 167/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3701 - accuracy: 0.8969\n",
      "Epoch 00167: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3695 - accuracy: 0.8972 - val_loss: 0.4264 - val_accuracy: 0.8536\n",
      "Epoch 168/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3741 - accuracy: 0.8936\n",
      "Epoch 00168: val_accuracy improved from 0.87630 to 0.88196, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3745 - accuracy: 0.8934 - val_loss: 0.4016 - val_accuracy: 0.8820\n",
      "Epoch 169/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.8965\n",
      "Epoch 00169: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3699 - accuracy: 0.8965 - val_loss: 0.4356 - val_accuracy: 0.8678\n",
      "Epoch 170/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3850 - accuracy: 0.8884\n",
      "Epoch 00170: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3852 - accuracy: 0.8882 - val_loss: 0.4043 - val_accuracy: 0.8716\n",
      "Epoch 171/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3704 - accuracy: 0.8938\n",
      "Epoch 00171: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3707 - accuracy: 0.8939 - val_loss: 0.4158 - val_accuracy: 0.8772\n",
      "Epoch 172/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3715 - accuracy: 0.9024\n",
      "Epoch 00172: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3713 - accuracy: 0.9024 - val_loss: 0.4053 - val_accuracy: 0.8706\n",
      "Epoch 173/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3669 - accuracy: 0.8972\n",
      "Epoch 00173: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3667 - accuracy: 0.8972 - val_loss: 0.4065 - val_accuracy: 0.8659\n",
      "Epoch 174/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3703 - accuracy: 0.8974\n",
      "Epoch 00174: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3698 - accuracy: 0.8977 - val_loss: 0.4333 - val_accuracy: 0.8725\n",
      "Epoch 175/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8948\n",
      "Epoch 00175: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3650 - accuracy: 0.8953 - val_loss: 0.4397 - val_accuracy: 0.8678\n",
      "Epoch 176/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.9015\n",
      "Epoch 00176: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3695 - accuracy: 0.9015 - val_loss: 0.4155 - val_accuracy: 0.8716\n",
      "Epoch 177/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.9051\n",
      "Epoch 00177: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3646 - accuracy: 0.9052 - val_loss: 0.4351 - val_accuracy: 0.8706\n",
      "Epoch 178/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3613 - accuracy: 0.9034\n",
      "Epoch 00178: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3610 - accuracy: 0.9034 - val_loss: 0.3993 - val_accuracy: 0.8687\n",
      "Epoch 179/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3630 - accuracy: 0.9031\n",
      "Epoch 00179: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3622 - accuracy: 0.9036 - val_loss: 0.3991 - val_accuracy: 0.8754\n",
      "Epoch 180/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3571 - accuracy: 0.9060\n",
      "Epoch 00180: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3577 - accuracy: 0.9055 - val_loss: 0.4010 - val_accuracy: 0.8791\n",
      "Epoch 181/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8903\n",
      "Epoch 00181: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3706 - accuracy: 0.8897 - val_loss: 0.4030 - val_accuracy: 0.8678\n",
      "Epoch 182/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8962\n",
      "Epoch 00182: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3658 - accuracy: 0.8963 - val_loss: 0.4096 - val_accuracy: 0.8782\n",
      "Epoch 183/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3629 - accuracy: 0.8981\n",
      "Epoch 00183: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3636 - accuracy: 0.8979 - val_loss: 0.4058 - val_accuracy: 0.8602\n",
      "Epoch 184/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3734 - accuracy: 0.8936\n",
      "Epoch 00184: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3729 - accuracy: 0.8937 - val_loss: 0.4278 - val_accuracy: 0.8725\n",
      "Epoch 185/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8991\n",
      "Epoch 00185: val_accuracy improved from 0.88196 to 0.88574, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3699 - accuracy: 0.8993 - val_loss: 0.4008 - val_accuracy: 0.8857\n",
      "Epoch 186/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3616 - accuracy: 0.9034\n",
      "Epoch 00186: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3618 - accuracy: 0.9034 - val_loss: 0.4090 - val_accuracy: 0.8706\n",
      "Epoch 187/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.9060\n",
      "Epoch 00187: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3578 - accuracy: 0.9057 - val_loss: 0.4134 - val_accuracy: 0.8801\n",
      "Epoch 188/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.9000\n",
      "Epoch 00188: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3593 - accuracy: 0.9000 - val_loss: 0.4059 - val_accuracy: 0.8772\n",
      "Epoch 189/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8996\n",
      "Epoch 00189: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3586 - accuracy: 0.8998 - val_loss: 0.4025 - val_accuracy: 0.8829\n",
      "Epoch 190/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3600 - accuracy: 0.9055\n",
      "Epoch 00190: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3595 - accuracy: 0.9060 - val_loss: 0.4047 - val_accuracy: 0.8744\n",
      "Epoch 191/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.3605 - accuracy: 0.9010\n",
      "Epoch 00191: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3607 - accuracy: 0.9003 - val_loss: 0.4066 - val_accuracy: 0.8791\n",
      "Epoch 192/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3615 - accuracy: 0.8991\n",
      "Epoch 00192: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3607 - accuracy: 0.8998 - val_loss: 0.4106 - val_accuracy: 0.8857\n",
      "Epoch 193/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3605 - accuracy: 0.9034\n",
      "Epoch 00193: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3609 - accuracy: 0.9036 - val_loss: 0.4078 - val_accuracy: 0.8801\n",
      "Epoch 194/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.9067\n",
      "Epoch 00194: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3553 - accuracy: 0.9067 - val_loss: 0.3962 - val_accuracy: 0.8791\n",
      "Epoch 195/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3517 - accuracy: 0.9003\n",
      "Epoch 00195: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3524 - accuracy: 0.9003 - val_loss: 0.4279 - val_accuracy: 0.8763\n",
      "Epoch 196/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3544 - accuracy: 0.9020\n",
      "Epoch 00196: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3543 - accuracy: 0.9024 - val_loss: 0.3937 - val_accuracy: 0.8772\n",
      "Epoch 197/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3561 - accuracy: 0.9036\n",
      "Epoch 00197: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3564 - accuracy: 0.9036 - val_loss: 0.3951 - val_accuracy: 0.8659\n",
      "Epoch 198/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.9022\n",
      "Epoch 00198: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3584 - accuracy: 0.9019 - val_loss: 0.3966 - val_accuracy: 0.8839\n",
      "Epoch 199/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.9039\n",
      "Epoch 00199: val_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3551 - accuracy: 0.9041 - val_loss: 0.4296 - val_accuracy: 0.8536\n",
      "Epoch 200/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3555 - accuracy: 0.9048\n",
      "Epoch 00200: val_accuracy improved from 0.88574 to 0.89046, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3564 - accuracy: 0.9043 - val_loss: 0.3978 - val_accuracy: 0.8905\n",
      "Epoch 201/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3492 - accuracy: 0.9051\n",
      "Epoch 00201: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3495 - accuracy: 0.9052 - val_loss: 0.4053 - val_accuracy: 0.8857\n",
      "Epoch 202/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3599 - accuracy: 0.8989\n",
      "Epoch 00202: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3611 - accuracy: 0.8984 - val_loss: 0.3952 - val_accuracy: 0.8678\n",
      "Epoch 203/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.9034\n",
      "Epoch 00203: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3546 - accuracy: 0.9038 - val_loss: 0.3980 - val_accuracy: 0.8848\n",
      "Epoch 204/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.9041\n",
      "Epoch 00204: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3505 - accuracy: 0.9043 - val_loss: 0.4180 - val_accuracy: 0.8744\n",
      "Epoch 205/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3517 - accuracy: 0.9053\n",
      "Epoch 00205: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3509 - accuracy: 0.9057 - val_loss: 0.3934 - val_accuracy: 0.8810\n",
      "Epoch 206/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.9070\n",
      "Epoch 00206: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3490 - accuracy: 0.9064 - val_loss: 0.3938 - val_accuracy: 0.8801\n",
      "Epoch 207/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.9105\n",
      "Epoch 00207: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3466 - accuracy: 0.9107 - val_loss: 0.4062 - val_accuracy: 0.8782\n",
      "Epoch 208/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3481 - accuracy: 0.9067\n",
      "Epoch 00208: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3478 - accuracy: 0.9067 - val_loss: 0.3924 - val_accuracy: 0.8857\n",
      "Epoch 209/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3547 - accuracy: 0.9041\n",
      "Epoch 00209: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3529 - accuracy: 0.9050 - val_loss: 0.4164 - val_accuracy: 0.8744\n",
      "Epoch 210/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.9029\n",
      "Epoch 00210: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3539 - accuracy: 0.9029 - val_loss: 0.3966 - val_accuracy: 0.8801\n",
      "Epoch 211/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.9060\n",
      "Epoch 00211: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3514 - accuracy: 0.9045 - val_loss: 0.4165 - val_accuracy: 0.8706\n",
      "Epoch 212/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8986\n",
      "Epoch 00212: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3569 - accuracy: 0.8993 - val_loss: 0.3906 - val_accuracy: 0.8810\n",
      "Epoch 213/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.9017\n",
      "Epoch 00213: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3522 - accuracy: 0.9019 - val_loss: 0.4306 - val_accuracy: 0.8678\n",
      "Epoch 214/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3532 - accuracy: 0.8998\n",
      "Epoch 00214: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3530 - accuracy: 0.9000 - val_loss: 0.4088 - val_accuracy: 0.8574\n",
      "Epoch 215/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.9098\n",
      "Epoch 00215: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3448 - accuracy: 0.9095 - val_loss: 0.4118 - val_accuracy: 0.8725\n",
      "Epoch 216/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3474 - accuracy: 0.9108\n",
      "Epoch 00216: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3470 - accuracy: 0.9109 - val_loss: 0.3996 - val_accuracy: 0.8669\n",
      "Epoch 217/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9074\n",
      "Epoch 00217: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3434 - accuracy: 0.9076 - val_loss: 0.3977 - val_accuracy: 0.8791\n",
      "Epoch 218/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9067\n",
      "Epoch 00218: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3443 - accuracy: 0.9057 - val_loss: 0.3953 - val_accuracy: 0.8706\n",
      "Epoch 219/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.9020\n",
      "Epoch 00219: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3520 - accuracy: 0.9017 - val_loss: 0.4309 - val_accuracy: 0.8735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.9077\n",
      "Epoch 00220: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3443 - accuracy: 0.9074 - val_loss: 0.4096 - val_accuracy: 0.8829\n",
      "Epoch 221/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.9067\n",
      "Epoch 00221: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3454 - accuracy: 0.9074 - val_loss: 0.3977 - val_accuracy: 0.8829\n",
      "Epoch 222/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.9065\n",
      "Epoch 00222: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3436 - accuracy: 0.9064 - val_loss: 0.3978 - val_accuracy: 0.8810\n",
      "Epoch 223/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3470 - accuracy: 0.9098\n",
      "Epoch 00223: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3472 - accuracy: 0.9097 - val_loss: 0.4099 - val_accuracy: 0.8820\n",
      "Epoch 224/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.9082\n",
      "Epoch 00224: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3420 - accuracy: 0.9086 - val_loss: 0.4088 - val_accuracy: 0.8772\n",
      "Epoch 225/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.9079\n",
      "Epoch 00225: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3484 - accuracy: 0.9083 - val_loss: 0.4423 - val_accuracy: 0.8678\n",
      "Epoch 226/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.9031\n",
      "Epoch 00226: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3488 - accuracy: 0.9036 - val_loss: 0.4037 - val_accuracy: 0.8621\n",
      "Epoch 227/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3460 - accuracy: 0.9082\n",
      "Epoch 00227: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3476 - accuracy: 0.9076 - val_loss: 0.4003 - val_accuracy: 0.8621\n",
      "Epoch 228/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.9117\n",
      "Epoch 00228: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3391 - accuracy: 0.9119 - val_loss: 0.3930 - val_accuracy: 0.8791\n",
      "Epoch 229/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3429 - accuracy: 0.9129\n",
      "Epoch 00229: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3428 - accuracy: 0.9130 - val_loss: 0.3947 - val_accuracy: 0.8772\n",
      "Epoch 230/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.9015\n",
      "Epoch 00230: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3544 - accuracy: 0.9012 - val_loss: 0.4155 - val_accuracy: 0.8801\n",
      "Epoch 231/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3428 - accuracy: 0.9094\n",
      "Epoch 00231: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3432 - accuracy: 0.9086 - val_loss: 0.3892 - val_accuracy: 0.8801\n",
      "Epoch 232/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.9182\n",
      "Epoch 00232: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3358 - accuracy: 0.9178 - val_loss: 0.4061 - val_accuracy: 0.8848\n",
      "Epoch 233/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.9103\n",
      "Epoch 00233: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3376 - accuracy: 0.9104 - val_loss: 0.3925 - val_accuracy: 0.8754\n",
      "Epoch 234/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.9108\n",
      "Epoch 00234: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3382 - accuracy: 0.9100 - val_loss: 0.4061 - val_accuracy: 0.8678\n",
      "Epoch 235/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3361 - accuracy: 0.9136\n",
      "Epoch 00235: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3355 - accuracy: 0.9142 - val_loss: 0.3943 - val_accuracy: 0.8763\n",
      "Epoch 236/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.9094\n",
      "Epoch 00236: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3349 - accuracy: 0.9090 - val_loss: 0.4078 - val_accuracy: 0.8612\n",
      "Epoch 237/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.9136\n",
      "Epoch 00237: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3323 - accuracy: 0.9128 - val_loss: 0.4177 - val_accuracy: 0.8754\n",
      "Epoch 238/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3383 - accuracy: 0.9115\n",
      "Epoch 00238: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3380 - accuracy: 0.9119 - val_loss: 0.4003 - val_accuracy: 0.8829\n",
      "Epoch 239/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.9101\n",
      "Epoch 00239: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3369 - accuracy: 0.9097 - val_loss: 0.3971 - val_accuracy: 0.8839\n",
      "Epoch 240/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.9184\n",
      "Epoch 00240: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3313 - accuracy: 0.9180 - val_loss: 0.4103 - val_accuracy: 0.8602\n",
      "Epoch 241/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.9058\n",
      "Epoch 00241: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3371 - accuracy: 0.9057 - val_loss: 0.4076 - val_accuracy: 0.8857\n",
      "Epoch 242/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.9203\n",
      "Epoch 00242: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3279 - accuracy: 0.9206 - val_loss: 0.3861 - val_accuracy: 0.8848\n",
      "Epoch 243/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.9110\n",
      "Epoch 00243: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3362 - accuracy: 0.9109 - val_loss: 0.3869 - val_accuracy: 0.8716\n",
      "Epoch 244/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3429 - accuracy: 0.9108\n",
      "Epoch 00244: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3437 - accuracy: 0.9104 - val_loss: 0.3842 - val_accuracy: 0.8839\n",
      "Epoch 245/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3308 - accuracy: 0.9158\n",
      "Epoch 00245: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3303 - accuracy: 0.9159 - val_loss: 0.3896 - val_accuracy: 0.8659\n",
      "Epoch 246/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3401 - accuracy: 0.9101\n",
      "Epoch 00246: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3403 - accuracy: 0.9097 - val_loss: 0.3907 - val_accuracy: 0.8820\n",
      "Epoch 247/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.9151\n",
      "Epoch 00247: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3312 - accuracy: 0.9154 - val_loss: 0.3903 - val_accuracy: 0.8820\n",
      "Epoch 248/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.9132\n",
      "Epoch 00248: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3355 - accuracy: 0.9133 - val_loss: 0.3892 - val_accuracy: 0.8839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3403 - accuracy: 0.9084\n",
      "Epoch 00249: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3399 - accuracy: 0.9088 - val_loss: 0.3858 - val_accuracy: 0.8810\n",
      "Epoch 250/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3256 - accuracy: 0.9163\n",
      "Epoch 00250: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3280 - accuracy: 0.9147 - val_loss: 0.3927 - val_accuracy: 0.8876\n",
      "Epoch 251/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.9158\n",
      "Epoch 00251: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3315 - accuracy: 0.9152 - val_loss: 0.3885 - val_accuracy: 0.8895\n",
      "Epoch 252/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.9165\n",
      "Epoch 00252: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3297 - accuracy: 0.9168 - val_loss: 0.3934 - val_accuracy: 0.8621\n",
      "Epoch 253/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3328 - accuracy: 0.9110\n",
      "Epoch 00253: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3326 - accuracy: 0.9114 - val_loss: 0.3846 - val_accuracy: 0.8782\n",
      "Epoch 254/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3253 - accuracy: 0.9160\n",
      "Epoch 00254: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3258 - accuracy: 0.9154 - val_loss: 0.3863 - val_accuracy: 0.8820\n",
      "Epoch 255/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.9134\n",
      "Epoch 00255: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3248 - accuracy: 0.9128 - val_loss: 0.4314 - val_accuracy: 0.8687\n",
      "Epoch 256/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.9175\n",
      "Epoch 00256: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3291 - accuracy: 0.9171 - val_loss: 0.3827 - val_accuracy: 0.8820\n",
      "Epoch 257/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3328 - accuracy: 0.9113\n",
      "Epoch 00257: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3332 - accuracy: 0.9104 - val_loss: 0.3881 - val_accuracy: 0.8810\n",
      "Epoch 258/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3350 - accuracy: 0.9101\n",
      "Epoch 00258: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3347 - accuracy: 0.9100 - val_loss: 0.4162 - val_accuracy: 0.8754\n",
      "Epoch 259/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.9127\n",
      "Epoch 00259: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3317 - accuracy: 0.9133 - val_loss: 0.4375 - val_accuracy: 0.8706\n",
      "Epoch 260/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3338 - accuracy: 0.9079\n",
      "Epoch 00260: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3336 - accuracy: 0.9086 - val_loss: 0.3900 - val_accuracy: 0.8782\n",
      "Epoch 261/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.9172\n",
      "Epoch 00261: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3291 - accuracy: 0.9171 - val_loss: 0.3957 - val_accuracy: 0.8659\n",
      "Epoch 262/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.9165\n",
      "Epoch 00262: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3245 - accuracy: 0.9166 - val_loss: 0.3877 - val_accuracy: 0.8801\n",
      "Epoch 263/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.9165\n",
      "Epoch 00263: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3282 - accuracy: 0.9164 - val_loss: 0.4119 - val_accuracy: 0.8763\n",
      "Epoch 264/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.9115\n",
      "Epoch 00264: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3349 - accuracy: 0.9109 - val_loss: 0.3834 - val_accuracy: 0.8857\n",
      "Epoch 265/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.9122\n",
      "Epoch 00265: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3301 - accuracy: 0.9121 - val_loss: 0.3839 - val_accuracy: 0.8820\n",
      "Epoch 266/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.9187\n",
      "Epoch 00266: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3272 - accuracy: 0.9187 - val_loss: 0.3905 - val_accuracy: 0.8829\n",
      "Epoch 267/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.9120\n",
      "Epoch 00267: val_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3330 - accuracy: 0.9119 - val_loss: 0.3975 - val_accuracy: 0.8678\n",
      "Epoch 268/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3303 - accuracy: 0.9191\n",
      "Epoch 00268: val_accuracy improved from 0.89046 to 0.89330, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3305 - accuracy: 0.9194 - val_loss: 0.4065 - val_accuracy: 0.8933\n",
      "Epoch 269/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.9187\n",
      "Epoch 00269: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3236 - accuracy: 0.9185 - val_loss: 0.3881 - val_accuracy: 0.8810\n",
      "Epoch 270/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3283 - accuracy: 0.9144\n",
      "Epoch 00270: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3284 - accuracy: 0.9140 - val_loss: 0.3860 - val_accuracy: 0.8839\n",
      "Epoch 271/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.9129\n",
      "Epoch 00271: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3258 - accuracy: 0.9135 - val_loss: 0.3793 - val_accuracy: 0.8820\n",
      "Epoch 272/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3254 - accuracy: 0.9187\n",
      "Epoch 00272: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3257 - accuracy: 0.9187 - val_loss: 0.4228 - val_accuracy: 0.8772\n",
      "Epoch 273/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.9156\n",
      "Epoch 00273: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3278 - accuracy: 0.9156 - val_loss: 0.3906 - val_accuracy: 0.8895\n",
      "Epoch 274/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.9218\n",
      "Epoch 00274: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3242 - accuracy: 0.9211 - val_loss: 0.3839 - val_accuracy: 0.8857\n",
      "Epoch 275/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.9187\n",
      "Epoch 00275: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3191 - accuracy: 0.9190 - val_loss: 0.3839 - val_accuracy: 0.8905\n",
      "Epoch 276/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.9170\n",
      "Epoch 00276: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3204 - accuracy: 0.9171 - val_loss: 0.3905 - val_accuracy: 0.8848\n",
      "Epoch 277/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.9187\n",
      "Epoch 00277: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3189 - accuracy: 0.9185 - val_loss: 0.3777 - val_accuracy: 0.8876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3273 - accuracy: 0.9145\n",
      "Epoch 00278: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3285 - accuracy: 0.9140 - val_loss: 0.3805 - val_accuracy: 0.8914\n",
      "Epoch 279/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.9206\n",
      "Epoch 00279: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3200 - accuracy: 0.9208 - val_loss: 0.3894 - val_accuracy: 0.8857\n",
      "Epoch 280/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.9117\n",
      "Epoch 00280: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3290 - accuracy: 0.9116 - val_loss: 0.4179 - val_accuracy: 0.8744\n",
      "Epoch 281/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.9132\n",
      "Epoch 00281: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3287 - accuracy: 0.9133 - val_loss: 0.4172 - val_accuracy: 0.8725\n",
      "Epoch 282/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.9082\n",
      "Epoch 00282: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3310 - accuracy: 0.9088 - val_loss: 0.3841 - val_accuracy: 0.8876\n",
      "Epoch 283/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.9213\n",
      "Epoch 00283: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3228 - accuracy: 0.9213 - val_loss: 0.3957 - val_accuracy: 0.8602\n",
      "Epoch 284/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.9156\n",
      "Epoch 00284: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3249 - accuracy: 0.9154 - val_loss: 0.3886 - val_accuracy: 0.8763\n",
      "Epoch 285/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.9151\n",
      "Epoch 00285: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3230 - accuracy: 0.9152 - val_loss: 0.3792 - val_accuracy: 0.8810\n",
      "Epoch 286/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.9146\n",
      "Epoch 00286: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3293 - accuracy: 0.9145 - val_loss: 0.3870 - val_accuracy: 0.8886\n",
      "Epoch 287/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.9198\n",
      "Epoch 00287: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3208 - accuracy: 0.9190 - val_loss: 0.3885 - val_accuracy: 0.8905\n",
      "Epoch 288/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.9187\n",
      "Epoch 00288: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3192 - accuracy: 0.9192 - val_loss: 0.3838 - val_accuracy: 0.8905\n",
      "Epoch 289/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.9189\n",
      "Epoch 00289: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3184 - accuracy: 0.9185 - val_loss: 0.4254 - val_accuracy: 0.8810\n",
      "Epoch 290/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.9203\n",
      "Epoch 00290: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3168 - accuracy: 0.9208 - val_loss: 0.3969 - val_accuracy: 0.8876\n",
      "Epoch 291/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.9156\n",
      "Epoch 00291: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3256 - accuracy: 0.9156 - val_loss: 0.4437 - val_accuracy: 0.8602\n",
      "Epoch 292/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3166 - accuracy: 0.9215\n",
      "Epoch 00292: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3170 - accuracy: 0.9211 - val_loss: 0.3854 - val_accuracy: 0.8876\n",
      "Epoch 293/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.9156\n",
      "Epoch 00293: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3182 - accuracy: 0.9164 - val_loss: 0.5094 - val_accuracy: 0.8414\n",
      "Epoch 294/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.9170\n",
      "Epoch 00294: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3193 - accuracy: 0.9166 - val_loss: 0.3780 - val_accuracy: 0.8924\n",
      "Epoch 295/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.9156\n",
      "Epoch 00295: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3213 - accuracy: 0.9159 - val_loss: 0.3829 - val_accuracy: 0.8772\n",
      "Epoch 296/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.9237\n",
      "Epoch 00296: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3186 - accuracy: 0.9232 - val_loss: 0.3787 - val_accuracy: 0.8820\n",
      "Epoch 297/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.9160\n",
      "Epoch 00297: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3232 - accuracy: 0.9152 - val_loss: 0.3760 - val_accuracy: 0.8857\n",
      "Epoch 298/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.9117\n",
      "Epoch 00298: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3233 - accuracy: 0.9123 - val_loss: 0.3785 - val_accuracy: 0.8839\n",
      "Epoch 299/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3190 - accuracy: 0.9151\n",
      "Epoch 00299: val_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3203 - accuracy: 0.9145 - val_loss: 0.4142 - val_accuracy: 0.8612\n",
      "Epoch 300/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.9084\n",
      "Epoch 00300: val_accuracy improved from 0.89330 to 0.90085, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3272 - accuracy: 0.9083 - val_loss: 0.3957 - val_accuracy: 0.9008\n",
      "Epoch 301/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3128 - accuracy: 0.9225\n",
      "Epoch 00301: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3122 - accuracy: 0.9227 - val_loss: 0.4208 - val_accuracy: 0.8782\n",
      "Epoch 302/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.9215\n",
      "Epoch 00302: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3201 - accuracy: 0.9211 - val_loss: 0.3937 - val_accuracy: 0.8772\n",
      "Epoch 303/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.9196\n",
      "Epoch 00303: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3167 - accuracy: 0.9199 - val_loss: 0.3844 - val_accuracy: 0.8754\n",
      "Epoch 304/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.9172\n",
      "Epoch 00304: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3223 - accuracy: 0.9166 - val_loss: 0.3849 - val_accuracy: 0.8678\n",
      "Epoch 305/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.9175\n",
      "Epoch 00305: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3192 - accuracy: 0.9178 - val_loss: 0.3842 - val_accuracy: 0.8810\n",
      "Epoch 306/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.9175\n",
      "Epoch 00306: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3186 - accuracy: 0.9178 - val_loss: 0.3753 - val_accuracy: 0.8895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.9191\n",
      "Epoch 00307: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3143 - accuracy: 0.9192 - val_loss: 0.3969 - val_accuracy: 0.8895\n",
      "Epoch 308/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.9208\n",
      "Epoch 00308: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3139 - accuracy: 0.9206 - val_loss: 0.3950 - val_accuracy: 0.8725\n",
      "Epoch 309/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.9103\n",
      "Epoch 00309: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3241 - accuracy: 0.9102 - val_loss: 0.3821 - val_accuracy: 0.8839\n",
      "Epoch 310/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.9232\n",
      "Epoch 00310: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3114 - accuracy: 0.9230 - val_loss: 0.3801 - val_accuracy: 0.8857\n",
      "Epoch 311/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.9148\n",
      "Epoch 00311: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3213 - accuracy: 0.9156 - val_loss: 0.3814 - val_accuracy: 0.8829\n",
      "Epoch 312/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.9239\n",
      "Epoch 00312: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3133 - accuracy: 0.9237 - val_loss: 0.3779 - val_accuracy: 0.8886\n",
      "Epoch 313/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.9160\n",
      "Epoch 00313: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3205 - accuracy: 0.9161 - val_loss: 0.3858 - val_accuracy: 0.8886\n",
      "Epoch 314/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.9229\n",
      "Epoch 00314: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3135 - accuracy: 0.9225 - val_loss: 0.3783 - val_accuracy: 0.8942\n",
      "Epoch 315/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.9219\n",
      "Epoch 00315: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3116 - accuracy: 0.9225 - val_loss: 0.3814 - val_accuracy: 0.8905\n",
      "Epoch 316/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.9215\n",
      "Epoch 00316: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3148 - accuracy: 0.9208 - val_loss: 0.4139 - val_accuracy: 0.8791\n",
      "Epoch 317/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3103 - accuracy: 0.9244\n",
      "Epoch 00317: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3107 - accuracy: 0.9244 - val_loss: 0.4114 - val_accuracy: 0.8810\n",
      "Epoch 318/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.9198\n",
      "Epoch 00318: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3155 - accuracy: 0.9201 - val_loss: 0.3797 - val_accuracy: 0.8876\n",
      "Epoch 319/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.9208\n",
      "Epoch 00319: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3107 - accuracy: 0.9206 - val_loss: 0.4399 - val_accuracy: 0.8640\n",
      "Epoch 320/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3262 - accuracy: 0.9125\n",
      "Epoch 00320: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3259 - accuracy: 0.9128 - val_loss: 0.3949 - val_accuracy: 0.8810\n",
      "Epoch 321/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.9222\n",
      "Epoch 00321: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3134 - accuracy: 0.9223 - val_loss: 0.4419 - val_accuracy: 0.8650\n",
      "Epoch 322/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3172 - accuracy: 0.9225\n",
      "Epoch 00322: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3172 - accuracy: 0.9225 - val_loss: 0.4091 - val_accuracy: 0.8782\n",
      "Epoch 323/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.9256\n",
      "Epoch 00323: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3126 - accuracy: 0.9251 - val_loss: 0.3742 - val_accuracy: 0.8961\n",
      "Epoch 324/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.9163\n",
      "Epoch 00324: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3154 - accuracy: 0.9166 - val_loss: 0.3798 - val_accuracy: 0.8820\n",
      "Epoch 325/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.9237\n",
      "Epoch 00325: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3057 - accuracy: 0.9239 - val_loss: 0.3763 - val_accuracy: 0.8839\n",
      "Epoch 326/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.9225\n",
      "Epoch 00326: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3085 - accuracy: 0.9223 - val_loss: 0.3821 - val_accuracy: 0.8810\n",
      "Epoch 327/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.9218\n",
      "Epoch 00327: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3087 - accuracy: 0.9220 - val_loss: 0.3805 - val_accuracy: 0.8961\n",
      "Epoch 328/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.9256\n",
      "Epoch 00328: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3090 - accuracy: 0.9253 - val_loss: 0.3907 - val_accuracy: 0.8867\n",
      "Epoch 329/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.9288\n",
      "Epoch 00329: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3060 - accuracy: 0.9282 - val_loss: 0.3856 - val_accuracy: 0.8914\n",
      "Epoch 330/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.9263\n",
      "Epoch 00330: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3025 - accuracy: 0.9263 - val_loss: 0.3838 - val_accuracy: 0.8754\n",
      "Epoch 331/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.9189\n",
      "Epoch 00331: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3137 - accuracy: 0.9187 - val_loss: 0.3969 - val_accuracy: 0.8952\n",
      "Epoch 332/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3092 - accuracy: 0.9271\n",
      "Epoch 00332: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3092 - accuracy: 0.9270 - val_loss: 0.3913 - val_accuracy: 0.8848\n",
      "Epoch 333/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9246\n",
      "Epoch 00333: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3043 - accuracy: 0.9241 - val_loss: 0.3767 - val_accuracy: 0.8829\n",
      "Epoch 334/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.9190\n",
      "Epoch 00334: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3162 - accuracy: 0.9187 - val_loss: 0.3842 - val_accuracy: 0.8782\n",
      "Epoch 335/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.9231\n",
      "Epoch 00335: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3097 - accuracy: 0.9232 - val_loss: 0.3813 - val_accuracy: 0.8924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 336/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.9240\n",
      "Epoch 00336: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3087 - accuracy: 0.9237 - val_loss: 0.3774 - val_accuracy: 0.8820\n",
      "Epoch 337/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.9221\n",
      "Epoch 00337: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3086 - accuracy: 0.9223 - val_loss: 0.3850 - val_accuracy: 0.8905\n",
      "Epoch 338/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.9240\n",
      "Epoch 00338: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3056 - accuracy: 0.9239 - val_loss: 0.3782 - val_accuracy: 0.8914\n",
      "Epoch 339/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.9195\n",
      "Epoch 00339: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3157 - accuracy: 0.9194 - val_loss: 0.3924 - val_accuracy: 0.8839\n",
      "Epoch 340/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.9205\n",
      "Epoch 00340: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3107 - accuracy: 0.9206 - val_loss: 0.3851 - val_accuracy: 0.8905\n",
      "Epoch 341/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.9240\n",
      "Epoch 00341: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3007 - accuracy: 0.9239 - val_loss: 0.3714 - val_accuracy: 0.8839\n",
      "Epoch 342/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.9195\n",
      "Epoch 00342: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3127 - accuracy: 0.9197 - val_loss: 0.3823 - val_accuracy: 0.8942\n",
      "Epoch 343/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3076 - accuracy: 0.9202\n",
      "Epoch 00343: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3074 - accuracy: 0.9204 - val_loss: 0.3751 - val_accuracy: 0.8952\n",
      "Epoch 344/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3084 - accuracy: 0.9207\n",
      "Epoch 00344: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3083 - accuracy: 0.9204 - val_loss: 0.3705 - val_accuracy: 0.8839\n",
      "Epoch 345/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.9256\n",
      "Epoch 00345: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3053 - accuracy: 0.9253 - val_loss: 0.4016 - val_accuracy: 0.8650\n",
      "Epoch 346/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.9222\n",
      "Epoch 00346: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3074 - accuracy: 0.9225 - val_loss: 0.3788 - val_accuracy: 0.8801\n",
      "Epoch 347/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3122 - accuracy: 0.9197\n",
      "Epoch 00347: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3125 - accuracy: 0.9194 - val_loss: 0.3858 - val_accuracy: 0.8857\n",
      "Epoch 348/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.9190\n",
      "Epoch 00348: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3153 - accuracy: 0.9190 - val_loss: 0.3849 - val_accuracy: 0.8669\n",
      "Epoch 349/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.9250\n",
      "Epoch 00349: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3029 - accuracy: 0.9251 - val_loss: 0.4275 - val_accuracy: 0.8716\n",
      "Epoch 350/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.9263\n",
      "Epoch 00350: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3032 - accuracy: 0.9258 - val_loss: 0.4091 - val_accuracy: 0.8867\n",
      "Epoch 351/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.9184\n",
      "Epoch 00351: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3139 - accuracy: 0.9185 - val_loss: 0.4087 - val_accuracy: 0.8829\n",
      "Epoch 352/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3114 - accuracy: 0.9182\n",
      "Epoch 00352: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3121 - accuracy: 0.9180 - val_loss: 0.3892 - val_accuracy: 0.8763\n",
      "Epoch 353/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9263\n",
      "Epoch 00353: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3027 - accuracy: 0.9263 - val_loss: 0.3709 - val_accuracy: 0.8782\n",
      "Epoch 354/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2977 - accuracy: 0.9287\n",
      "Epoch 00354: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2968 - accuracy: 0.9293 - val_loss: 0.4422 - val_accuracy: 0.8735\n",
      "Epoch 355/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3100 - accuracy: 0.9163\n",
      "Epoch 00355: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3097 - accuracy: 0.9171 - val_loss: 0.3719 - val_accuracy: 0.8839\n",
      "Epoch 356/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.9204\n",
      "Epoch 00356: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3125 - accuracy: 0.9204 - val_loss: 0.3730 - val_accuracy: 0.8876\n",
      "Epoch 357/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3076 - accuracy: 0.9240\n",
      "Epoch 00357: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3077 - accuracy: 0.9239 - val_loss: 0.3790 - val_accuracy: 0.8924\n",
      "Epoch 358/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.9261\n",
      "Epoch 00358: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3025 - accuracy: 0.9263 - val_loss: 0.3737 - val_accuracy: 0.8744\n",
      "Epoch 359/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3045 - accuracy: 0.9241\n",
      "Epoch 00359: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3047 - accuracy: 0.9239 - val_loss: 0.3724 - val_accuracy: 0.8857\n",
      "Epoch 360/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.9245\n",
      "Epoch 00360: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3015 - accuracy: 0.9244 - val_loss: 0.3723 - val_accuracy: 0.8829\n",
      "Epoch 361/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9289\n",
      "Epoch 00361: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2999 - accuracy: 0.9286 - val_loss: 0.3832 - val_accuracy: 0.8716\n",
      "Epoch 362/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.9231\n",
      "Epoch 00362: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3064 - accuracy: 0.9227 - val_loss: 0.4382 - val_accuracy: 0.8650\n",
      "Epoch 363/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.9245\n",
      "Epoch 00363: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3065 - accuracy: 0.9244 - val_loss: 0.3856 - val_accuracy: 0.8895\n",
      "Epoch 364/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.9208\n",
      "Epoch 00364: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3083 - accuracy: 0.9204 - val_loss: 0.3861 - val_accuracy: 0.8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.9268\n",
      "Epoch 00365: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3034 - accuracy: 0.9267 - val_loss: 0.4234 - val_accuracy: 0.8791\n",
      "Epoch 366/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2948 - accuracy: 0.9272\n",
      "Epoch 00366: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2956 - accuracy: 0.9267 - val_loss: 0.3806 - val_accuracy: 0.8839\n",
      "Epoch 367/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.9193\n",
      "Epoch 00367: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3129 - accuracy: 0.9194 - val_loss: 0.4610 - val_accuracy: 0.8621\n",
      "Epoch 368/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9277\n",
      "Epoch 00368: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3031 - accuracy: 0.9277 - val_loss: 0.3748 - val_accuracy: 0.8895\n",
      "Epoch 369/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.9239\n",
      "Epoch 00369: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3085 - accuracy: 0.9239 - val_loss: 0.3722 - val_accuracy: 0.8886\n",
      "Epoch 370/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2984 - accuracy: 0.9270\n",
      "Epoch 00370: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2984 - accuracy: 0.9275 - val_loss: 0.3888 - val_accuracy: 0.8867\n",
      "Epoch 371/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3092 - accuracy: 0.9186\n",
      "Epoch 00371: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3090 - accuracy: 0.9187 - val_loss: 0.3948 - val_accuracy: 0.8772\n",
      "Epoch 372/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.9280\n",
      "Epoch 00372: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2957 - accuracy: 0.9279 - val_loss: 0.3679 - val_accuracy: 0.8924\n",
      "Epoch 373/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.9223\n",
      "Epoch 00373: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3014 - accuracy: 0.9225 - val_loss: 0.3850 - val_accuracy: 0.8848\n",
      "Epoch 374/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9267\n",
      "Epoch 00374: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2961 - accuracy: 0.9258 - val_loss: 0.3662 - val_accuracy: 0.8886\n",
      "Epoch 375/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.9191\n",
      "Epoch 00375: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3051 - accuracy: 0.9190 - val_loss: 0.3763 - val_accuracy: 0.8839\n",
      "Epoch 376/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3044 - accuracy: 0.9233\n",
      "Epoch 00376: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3052 - accuracy: 0.9232 - val_loss: 0.3879 - val_accuracy: 0.8725\n",
      "Epoch 377/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.9270\n",
      "Epoch 00377: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3036 - accuracy: 0.9267 - val_loss: 0.3708 - val_accuracy: 0.8924\n",
      "Epoch 378/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.9294\n",
      "Epoch 00378: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2976 - accuracy: 0.9291 - val_loss: 0.3664 - val_accuracy: 0.8961\n",
      "Epoch 379/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9260\n",
      "Epoch 00379: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2991 - accuracy: 0.9265 - val_loss: 0.3866 - val_accuracy: 0.8725\n",
      "Epoch 380/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.9233\n",
      "Epoch 00380: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3025 - accuracy: 0.9232 - val_loss: 0.3704 - val_accuracy: 0.8857\n",
      "Epoch 381/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.9264\n",
      "Epoch 00381: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2981 - accuracy: 0.9265 - val_loss: 0.3815 - val_accuracy: 0.8640\n",
      "Epoch 382/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.9256\n",
      "Epoch 00382: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3008 - accuracy: 0.9256 - val_loss: 0.3849 - val_accuracy: 0.8886\n",
      "Epoch 383/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.9227\n",
      "Epoch 00383: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3027 - accuracy: 0.9227 - val_loss: 0.3990 - val_accuracy: 0.8801\n",
      "Epoch 384/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3029 - accuracy: 0.9204\n",
      "Epoch 00384: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3031 - accuracy: 0.9208 - val_loss: 0.3723 - val_accuracy: 0.8820\n",
      "Epoch 385/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.9296\n",
      "Epoch 00385: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2976 - accuracy: 0.9291 - val_loss: 0.3819 - val_accuracy: 0.8876\n",
      "Epoch 386/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.9280\n",
      "Epoch 00386: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2929 - accuracy: 0.9277 - val_loss: 0.3880 - val_accuracy: 0.8848\n",
      "Epoch 387/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.9278\n",
      "Epoch 00387: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3019 - accuracy: 0.9277 - val_loss: 0.3764 - val_accuracy: 0.8905\n",
      "Epoch 388/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.9270\n",
      "Epoch 00388: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3015 - accuracy: 0.9267 - val_loss: 0.3756 - val_accuracy: 0.8829\n",
      "Epoch 389/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.9265\n",
      "Epoch 00389: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2931 - accuracy: 0.9258 - val_loss: 0.3779 - val_accuracy: 0.8942\n",
      "Epoch 390/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.9268\n",
      "Epoch 00390: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3048 - accuracy: 0.9256 - val_loss: 0.3700 - val_accuracy: 0.8801\n",
      "Epoch 391/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.9318\n",
      "Epoch 00391: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2926 - accuracy: 0.9319 - val_loss: 0.3677 - val_accuracy: 0.8914\n",
      "Epoch 392/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9309\n",
      "Epoch 00392: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2911 - accuracy: 0.9308 - val_loss: 0.3800 - val_accuracy: 0.8961\n",
      "Epoch 393/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.9228\n",
      "Epoch 00393: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2997 - accuracy: 0.9230 - val_loss: 0.3792 - val_accuracy: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.9299\n",
      "Epoch 00394: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2938 - accuracy: 0.9301 - val_loss: 0.3819 - val_accuracy: 0.8905\n",
      "Epoch 395/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.9249\n",
      "Epoch 00395: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3059 - accuracy: 0.9251 - val_loss: 0.3944 - val_accuracy: 0.8687\n",
      "Epoch 396/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2977 - accuracy: 0.9272\n",
      "Epoch 00396: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2997 - accuracy: 0.9263 - val_loss: 0.4312 - val_accuracy: 0.8754\n",
      "Epoch 397/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.9195\n",
      "Epoch 00397: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3069 - accuracy: 0.9197 - val_loss: 0.3876 - val_accuracy: 0.8905\n",
      "Epoch 398/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.9292\n",
      "Epoch 00398: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2959 - accuracy: 0.9293 - val_loss: 0.3810 - val_accuracy: 0.8801\n",
      "Epoch 399/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2955 - accuracy: 0.9270\n",
      "Epoch 00399: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2963 - accuracy: 0.9270 - val_loss: 0.4007 - val_accuracy: 0.8942\n",
      "Epoch 400/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9270\n",
      "Epoch 00400: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2996 - accuracy: 0.9272 - val_loss: 0.3709 - val_accuracy: 0.8942\n",
      "Epoch 401/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.9283\n",
      "Epoch 00401: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2969 - accuracy: 0.9284 - val_loss: 0.3835 - val_accuracy: 0.8933\n",
      "Epoch 402/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.9266\n",
      "Epoch 00402: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2936 - accuracy: 0.9263 - val_loss: 0.3834 - val_accuracy: 0.8914\n",
      "Epoch 403/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9242\n",
      "Epoch 00403: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3032 - accuracy: 0.9241 - val_loss: 0.3779 - val_accuracy: 0.8924\n",
      "Epoch 404/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.9321\n",
      "Epoch 00404: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2907 - accuracy: 0.9319 - val_loss: 0.3859 - val_accuracy: 0.8716\n",
      "Epoch 405/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.9258\n",
      "Epoch 00405: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2925 - accuracy: 0.9258 - val_loss: 0.3742 - val_accuracy: 0.8961\n",
      "Epoch 406/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9315\n",
      "Epoch 00406: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2897 - accuracy: 0.9317 - val_loss: 0.3688 - val_accuracy: 0.8867\n",
      "Epoch 407/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9295\n",
      "Epoch 00407: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2951 - accuracy: 0.9296 - val_loss: 0.3779 - val_accuracy: 0.8914\n",
      "Epoch 408/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.9332\n",
      "Epoch 00408: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2939 - accuracy: 0.9336 - val_loss: 0.3903 - val_accuracy: 0.8980\n",
      "Epoch 409/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9273\n",
      "Epoch 00409: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2923 - accuracy: 0.9272 - val_loss: 0.3698 - val_accuracy: 0.8895\n",
      "Epoch 410/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.9314\n",
      "Epoch 00410: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2931 - accuracy: 0.9324 - val_loss: 0.3764 - val_accuracy: 0.8820\n",
      "Epoch 411/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2945 - accuracy: 0.9255\n",
      "Epoch 00411: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2951 - accuracy: 0.9251 - val_loss: 0.3749 - val_accuracy: 0.8914\n",
      "Epoch 412/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9228\n",
      "Epoch 00412: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2950 - accuracy: 0.9230 - val_loss: 0.4002 - val_accuracy: 0.8895\n",
      "Epoch 413/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.9200\n",
      "Epoch 00413: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3083 - accuracy: 0.9201 - val_loss: 0.3923 - val_accuracy: 0.8857\n",
      "Epoch 414/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.9245\n",
      "Epoch 00414: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2963 - accuracy: 0.9244 - val_loss: 0.3869 - val_accuracy: 0.8763\n",
      "Epoch 415/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.9299\n",
      "Epoch 00415: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2930 - accuracy: 0.9298 - val_loss: 0.3787 - val_accuracy: 0.8905\n",
      "Epoch 416/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.9254\n",
      "Epoch 00416: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2926 - accuracy: 0.9249 - val_loss: 0.3824 - val_accuracy: 0.8782\n",
      "Epoch 417/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9330\n",
      "Epoch 00417: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2887 - accuracy: 0.9331 - val_loss: 0.3833 - val_accuracy: 0.8952\n",
      "Epoch 418/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2933 - accuracy: 0.9265\n",
      "Epoch 00418: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2956 - accuracy: 0.9253 - val_loss: 0.3937 - val_accuracy: 0.8810\n",
      "Epoch 419/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.9249\n",
      "Epoch 00419: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3006 - accuracy: 0.9246 - val_loss: 0.3761 - val_accuracy: 0.8839\n",
      "Epoch 420/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2944 - accuracy: 0.9280\n",
      "Epoch 00420: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2942 - accuracy: 0.9279 - val_loss: 0.3755 - val_accuracy: 0.8895\n",
      "Epoch 421/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.9291\n",
      "Epoch 00421: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2921 - accuracy: 0.9291 - val_loss: 0.3719 - val_accuracy: 0.8876\n",
      "Epoch 422/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.9238\n",
      "Epoch 00422: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3031 - accuracy: 0.9239 - val_loss: 0.4113 - val_accuracy: 0.8782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2924 - accuracy: 0.9272\n",
      "Epoch 00423: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2932 - accuracy: 0.9272 - val_loss: 0.4118 - val_accuracy: 0.8782\n",
      "Epoch 424/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9299\n",
      "Epoch 00424: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2894 - accuracy: 0.9296 - val_loss: 0.3912 - val_accuracy: 0.8669\n",
      "Epoch 425/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.9257\n",
      "Epoch 00425: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2937 - accuracy: 0.9256 - val_loss: 0.3846 - val_accuracy: 0.8754\n",
      "Epoch 426/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9233\n",
      "Epoch 00426: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2954 - accuracy: 0.9234 - val_loss: 0.3726 - val_accuracy: 0.8933\n",
      "Epoch 427/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9271\n",
      "Epoch 00427: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2916 - accuracy: 0.9272 - val_loss: 0.3820 - val_accuracy: 0.8971\n",
      "Epoch 428/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.9295\n",
      "Epoch 00428: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2938 - accuracy: 0.9296 - val_loss: 0.3774 - val_accuracy: 0.8942\n",
      "Epoch 429/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9299\n",
      "Epoch 00429: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2877 - accuracy: 0.9305 - val_loss: 0.3682 - val_accuracy: 0.8820\n",
      "Epoch 430/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9311\n",
      "Epoch 00430: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2877 - accuracy: 0.9312 - val_loss: 0.3659 - val_accuracy: 0.8886\n",
      "Epoch 431/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9316\n",
      "Epoch 00431: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2864 - accuracy: 0.9315 - val_loss: 0.3804 - val_accuracy: 0.8867\n",
      "Epoch 432/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9313\n",
      "Epoch 00432: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2869 - accuracy: 0.9305 - val_loss: 0.3866 - val_accuracy: 0.8687\n",
      "Epoch 433/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.9337\n",
      "Epoch 00433: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2869 - accuracy: 0.9341 - val_loss: 0.3767 - val_accuracy: 0.8924\n",
      "Epoch 434/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.3036 - accuracy: 0.9214\n",
      "Epoch 00434: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3046 - accuracy: 0.9213 - val_loss: 0.3875 - val_accuracy: 0.8763\n",
      "Epoch 435/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9299\n",
      "Epoch 00435: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2923 - accuracy: 0.9296 - val_loss: 0.3734 - val_accuracy: 0.8942\n",
      "Epoch 436/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9283\n",
      "Epoch 00436: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2904 - accuracy: 0.9279 - val_loss: 0.3662 - val_accuracy: 0.8933\n",
      "Epoch 437/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2846 - accuracy: 0.9336\n",
      "Epoch 00437: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2850 - accuracy: 0.9338 - val_loss: 0.3761 - val_accuracy: 0.8848\n",
      "Epoch 438/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.9254\n",
      "Epoch 00438: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2939 - accuracy: 0.9256 - val_loss: 0.3800 - val_accuracy: 0.8905\n",
      "Epoch 439/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.9285\n",
      "Epoch 00439: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2906 - accuracy: 0.9286 - val_loss: 0.3739 - val_accuracy: 0.8961\n",
      "Epoch 440/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9277\n",
      "Epoch 00440: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2864 - accuracy: 0.9279 - val_loss: 0.3858 - val_accuracy: 0.8905\n",
      "Epoch 441/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9294\n",
      "Epoch 00441: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2875 - accuracy: 0.9291 - val_loss: 0.3737 - val_accuracy: 0.8895\n",
      "Epoch 442/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9332\n",
      "Epoch 00442: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2890 - accuracy: 0.9334 - val_loss: 0.3702 - val_accuracy: 0.8829\n",
      "Epoch 443/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9302\n",
      "Epoch 00443: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2894 - accuracy: 0.9301 - val_loss: 0.3800 - val_accuracy: 0.8848\n",
      "Epoch 444/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.9311\n",
      "Epoch 00444: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2853 - accuracy: 0.9308 - val_loss: 0.3807 - val_accuracy: 0.8725\n",
      "Epoch 445/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9301\n",
      "Epoch 00445: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2883 - accuracy: 0.9301 - val_loss: 0.3898 - val_accuracy: 0.8857\n",
      "Epoch 446/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.9233\n",
      "Epoch 00446: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2929 - accuracy: 0.9234 - val_loss: 0.4471 - val_accuracy: 0.8631\n",
      "Epoch 447/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2831 - accuracy: 0.9321\n",
      "Epoch 00447: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2820 - accuracy: 0.9324 - val_loss: 0.3932 - val_accuracy: 0.8933\n",
      "Epoch 448/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.9272\n",
      "Epoch 00448: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2941 - accuracy: 0.9272 - val_loss: 0.3714 - val_accuracy: 0.8971\n",
      "Epoch 449/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9327\n",
      "Epoch 00449: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2849 - accuracy: 0.9329 - val_loss: 0.4155 - val_accuracy: 0.8791\n",
      "Epoch 450/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9347\n",
      "Epoch 00450: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2873 - accuracy: 0.9348 - val_loss: 0.3610 - val_accuracy: 0.8886\n",
      "Epoch 451/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2798 - accuracy: 0.9351\n",
      "Epoch 00451: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2795 - accuracy: 0.9353 - val_loss: 0.4039 - val_accuracy: 0.8829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9285\n",
      "Epoch 00452: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2886 - accuracy: 0.9286 - val_loss: 0.4466 - val_accuracy: 0.8669\n",
      "Epoch 453/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.9234\n",
      "Epoch 00453: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2937 - accuracy: 0.9234 - val_loss: 0.3682 - val_accuracy: 0.8980\n",
      "Epoch 454/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2864 - accuracy: 0.9312\n",
      "Epoch 00454: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2864 - accuracy: 0.9315 - val_loss: 0.3823 - val_accuracy: 0.8952\n",
      "Epoch 455/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9318\n",
      "Epoch 00455: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2885 - accuracy: 0.9319 - val_loss: 0.3686 - val_accuracy: 0.8886\n",
      "Epoch 456/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9270\n",
      "Epoch 00456: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2918 - accuracy: 0.9270 - val_loss: 0.4008 - val_accuracy: 0.8659\n",
      "Epoch 457/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2902 - accuracy: 0.9263\n",
      "Epoch 00457: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2904 - accuracy: 0.9263 - val_loss: 0.3877 - val_accuracy: 0.8754\n",
      "Epoch 458/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9339\n",
      "Epoch 00458: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2862 - accuracy: 0.9338 - val_loss: 0.3780 - val_accuracy: 0.8914\n",
      "Epoch 459/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2771 - accuracy: 0.9299\n",
      "Epoch 00459: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2769 - accuracy: 0.9301 - val_loss: 0.3761 - val_accuracy: 0.8980\n",
      "Epoch 460/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9278\n",
      "Epoch 00460: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2878 - accuracy: 0.9279 - val_loss: 0.3768 - val_accuracy: 0.8876\n",
      "Epoch 461/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2817 - accuracy: 0.9307\n",
      "Epoch 00461: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2816 - accuracy: 0.9308 - val_loss: 0.3848 - val_accuracy: 0.8867\n",
      "Epoch 462/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.9325\n",
      "Epoch 00462: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2834 - accuracy: 0.9322 - val_loss: 0.3867 - val_accuracy: 0.8886\n",
      "Epoch 463/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2872 - accuracy: 0.9309\n",
      "Epoch 00463: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2890 - accuracy: 0.9291 - val_loss: 0.3971 - val_accuracy: 0.8810\n",
      "Epoch 464/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9339\n",
      "Epoch 00464: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2833 - accuracy: 0.9336 - val_loss: 0.3796 - val_accuracy: 0.8933\n",
      "Epoch 465/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.9301\n",
      "Epoch 00465: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2836 - accuracy: 0.9308 - val_loss: 0.3990 - val_accuracy: 0.8791\n",
      "Epoch 466/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9321\n",
      "Epoch 00466: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2854 - accuracy: 0.9319 - val_loss: 0.3686 - val_accuracy: 0.8848\n",
      "Epoch 467/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.9299\n",
      "Epoch 00467: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2836 - accuracy: 0.9296 - val_loss: 0.3641 - val_accuracy: 0.8886\n",
      "Epoch 468/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2774 - accuracy: 0.9320\n",
      "Epoch 00468: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2774 - accuracy: 0.9322 - val_loss: 0.3752 - val_accuracy: 0.8905\n",
      "Epoch 469/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2763 - accuracy: 0.9341\n",
      "Epoch 00469: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2757 - accuracy: 0.9345 - val_loss: 0.3739 - val_accuracy: 0.8952\n",
      "Epoch 470/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9264\n",
      "Epoch 00470: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2893 - accuracy: 0.9265 - val_loss: 0.3753 - val_accuracy: 0.8914\n",
      "Epoch 471/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9305\n",
      "Epoch 00471: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2843 - accuracy: 0.9305 - val_loss: 0.4069 - val_accuracy: 0.8829\n",
      "Epoch 472/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.9327\n",
      "Epoch 00472: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2842 - accuracy: 0.9317 - val_loss: 0.4172 - val_accuracy: 0.8829\n",
      "Epoch 473/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2788 - accuracy: 0.9363\n",
      "Epoch 00473: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2788 - accuracy: 0.9364 - val_loss: 0.3660 - val_accuracy: 0.8857\n",
      "Epoch 474/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9337\n",
      "Epoch 00474: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2888 - accuracy: 0.9336 - val_loss: 0.3725 - val_accuracy: 0.8990\n",
      "Epoch 475/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2755 - accuracy: 0.9299\n",
      "Epoch 00475: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2750 - accuracy: 0.9301 - val_loss: 0.4000 - val_accuracy: 0.8621\n",
      "Epoch 476/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2771 - accuracy: 0.9329\n",
      "Epoch 00476: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2775 - accuracy: 0.9329 - val_loss: 0.3922 - val_accuracy: 0.8867\n",
      "Epoch 477/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2778 - accuracy: 0.9299\n",
      "Epoch 00477: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2768 - accuracy: 0.9305 - val_loss: 0.3685 - val_accuracy: 0.8990\n",
      "Epoch 478/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9316\n",
      "Epoch 00478: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2860 - accuracy: 0.9317 - val_loss: 0.3621 - val_accuracy: 0.8933\n",
      "Epoch 479/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.9318\n",
      "Epoch 00479: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2843 - accuracy: 0.9312 - val_loss: 0.3677 - val_accuracy: 0.8914\n",
      "Epoch 480/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9225\n",
      "Epoch 00480: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2855 - accuracy: 0.9227 - val_loss: 0.3701 - val_accuracy: 0.8810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2841 - accuracy: 0.9314\n",
      "Epoch 00481: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2846 - accuracy: 0.9305 - val_loss: 0.3739 - val_accuracy: 0.8857\n",
      "Epoch 482/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2807 - accuracy: 0.9348\n",
      "Epoch 00482: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2792 - accuracy: 0.9355 - val_loss: 0.3914 - val_accuracy: 0.8857\n",
      "Epoch 483/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2769 - accuracy: 0.9351\n",
      "Epoch 00483: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2772 - accuracy: 0.9348 - val_loss: 0.4125 - val_accuracy: 0.8791\n",
      "Epoch 484/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2799 - accuracy: 0.9321\n",
      "Epoch 00484: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2800 - accuracy: 0.9317 - val_loss: 0.3894 - val_accuracy: 0.8857\n",
      "Epoch 485/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9311\n",
      "Epoch 00485: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2848 - accuracy: 0.9310 - val_loss: 0.3618 - val_accuracy: 0.8942\n",
      "Epoch 486/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2745 - accuracy: 0.9358\n",
      "Epoch 00486: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2744 - accuracy: 0.9360 - val_loss: 0.3584 - val_accuracy: 0.8961\n",
      "Epoch 487/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9268\n",
      "Epoch 00487: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2848 - accuracy: 0.9270 - val_loss: 0.3672 - val_accuracy: 0.8848\n",
      "Epoch 488/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.9280\n",
      "Epoch 00488: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2842 - accuracy: 0.9284 - val_loss: 0.4078 - val_accuracy: 0.8706\n",
      "Epoch 489/500\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 0.2865 - accuracy: 0.9282\n",
      "Epoch 00489: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2857 - accuracy: 0.9286 - val_loss: 0.3626 - val_accuracy: 0.8848\n",
      "Epoch 490/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2721 - accuracy: 0.9392\n",
      "Epoch 00490: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2722 - accuracy: 0.9393 - val_loss: 0.4000 - val_accuracy: 0.8857\n",
      "Epoch 491/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2801 - accuracy: 0.9320\n",
      "Epoch 00491: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2803 - accuracy: 0.9317 - val_loss: 0.3662 - val_accuracy: 0.8886\n",
      "Epoch 492/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9328\n",
      "Epoch 00492: val_accuracy did not improve from 0.90085\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2870 - accuracy: 0.9327 - val_loss: 0.3746 - val_accuracy: 0.8914\n",
      "Epoch 493/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.9349\n",
      "Epoch 00493: val_accuracy improved from 0.90085 to 0.90368, saving model to ./weight_cp\\weight_lstm1.hdf5\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2759 - accuracy: 0.9350 - val_loss: 0.3655 - val_accuracy: 0.9037\n",
      "Epoch 494/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.9313\n",
      "Epoch 00494: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2822 - accuracy: 0.9312 - val_loss: 0.3899 - val_accuracy: 0.8857\n",
      "Epoch 495/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.9342\n",
      "Epoch 00495: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2766 - accuracy: 0.9341 - val_loss: 0.4026 - val_accuracy: 0.8820\n",
      "Epoch 496/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2816 - accuracy: 0.9309\n",
      "Epoch 00496: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2815 - accuracy: 0.9310 - val_loss: 0.3702 - val_accuracy: 0.8895\n",
      "Epoch 497/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9337\n",
      "Epoch 00497: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2838 - accuracy: 0.9341 - val_loss: 0.4463 - val_accuracy: 0.8659\n",
      "Epoch 498/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.9342\n",
      "Epoch 00498: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2812 - accuracy: 0.9341 - val_loss: 0.3651 - val_accuracy: 0.8933\n",
      "Epoch 499/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.9318\n",
      "Epoch 00499: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2807 - accuracy: 0.9315 - val_loss: 0.3666 - val_accuracy: 0.8924\n",
      "Epoch 500/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2695 - accuracy: 0.9396\n",
      "Epoch 00500: val_accuracy did not improve from 0.90368\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2693 - accuracy: 0.9397 - val_loss: 0.3613 - val_accuracy: 0.8924\n"
     ]
    }
   ],
   "source": [
    "def nlp_lstm(w2v):\n",
    "    inputs = Input(shape=(X_train[0].shape[-1],))\n",
    "\n",
    "    embedding_layer = gensim_to_keras_embedding(w2v)\n",
    "    \n",
    "    embedding = embedding_layer(inputs)\n",
    "\n",
    "    lstm1 = LSTM(lstm_units,return_sequences=True, return_state=True, kernel_regularizer=l2(w_decay),recurrent_regularizer=l2(w_decay), dropout=dropout_rate)(embedding)\n",
    "    output = Dense(units=1, activation='sigmoid')(lstm1[1])\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = nlp_lstm(w2v_model)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./weight_cp/weight_lstm1.hdf5', save_freq=\"epoch\",  verbose=1, monitor='val_accuracy', save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(0.0001)\n",
    "model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=epochs_to_run, validation_data=(X_val, y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd2d3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACWwklEQVR4nOzdd3xT1fsH8E+StukedLcUyt5LRhkqKihLlqiAKEMEQVAQXKgIqF/wp34R+aLiQlwMmaIIyJS999600E3p3sn9/XEy7s3owLSB8nm/XnmR3NzcnNyW3ifPec45KkmSJBARERFVEWpnN4CIiIjIkRjcEBERUZXC4IaIiIiqFAY3REREVKUwuCEiIqIqhcENERERVSkMboiIiKhKYXBDREREVQqDGyIiIqpSGNwQkcOoVCpMnz693K+7evUqVCoVFi5c6PA2EdG9h8ENURWzcOFCqFQqqFQq7Ny50+p5SZIQFRUFlUqFxx9/3AktJCKqWAxuiKood3d3LFq0yGr7P//8g+vXr0Or1TqhVUREFY/BDVEV1bNnTyxbtgzFxcWK7YsWLULr1q0RFhbmpJbdO3JycpzdBKJ7EoMboipq8ODBuHnzJjZu3GjaVlhYiOXLl+OZZ56x+ZqcnBxMnjwZUVFR0Gq1aNCgAT799FNIkqTYr6CgAK+++iqCg4Ph4+ODPn364Pr16zaPeePGDTz//PMIDQ2FVqtFkyZNsGDBgtv6TGlpaXjttdfQrFkzeHt7w9fXFz169MCxY8es9s3Pz8f06dNRv359uLu7Izw8HE888QQuXbpk2kev1+Pzzz9Hs2bN4O7ujuDgYHTv3h0HDx4EUHItkGV90fTp06FSqXD69Gk888wzCAgIwP333w8AOH78OIYPH47atWvD3d0dYWFheP7553Hz5k2b52vkyJGIiIiAVqtFrVq1MHbsWBQWFuLy5ctQqVT47LPPrF63e/duqFQqLF68uLynlajKcXF2A4ioYkRHR6NDhw5YvHgxevToAQBYt24dMjIyMGjQIMydO1exvyRJ6NOnD7Zu3YqRI0eiZcuW2LBhA15//XXcuHFDcUF94YUX8Msvv+CZZ55Bx44dsWXLFvTq1cuqDUlJSWjfvj1UKhXGjx+P4OBgrFu3DiNHjkRmZiYmTpxYrs90+fJlrF69Gk899RRq1aqFpKQkfP311+jcuTNOnz6NiIgIAIBOp8Pjjz+OzZs3Y9CgQZgwYQKysrKwceNGnDx5EnXq1AEAjBw5EgsXLkSPHj3wwgsvoLi4GDt27MDevXvRpk2bcrXN6KmnnkK9evUwc+ZMU1C4ceNGXL58GSNGjEBYWBhOnTqFb775BqdOncLevXuhUqkAAPHx8WjXrh3S09MxevRoNGzYEDdu3MDy5cuRm5uL2rVro1OnTvj111/x6quvKt73119/hY+PD/r27Xtb7SaqUiQiqlJ++OEHCYB04MABad68eZKPj4+Um5srSZIkPfXUU9LDDz8sSZIk1axZU+rVq5fpdatXr5YASB9++KHieE8++aSkUqmkixcvSpIkSUePHpUASC+99JJiv2eeeUYCIE2bNs20beTIkVJ4eLiUmpqq2HfQoEGSn5+fqV1XrlyRAEg//PBDiZ8tPz9f0ul0im1XrlyRtFqt9P7775u2LViwQAIgzZ492+oYer1ekiRJ2rJliwRAeuWVV+zuU1K7LD/rtGnTJADS4MGDrfY1fk65xYsXSwCk7du3m7YNHTpUUqvV0oEDB+y26euvv5YASGfOnDE9V1hYKAUFBUnDhg2zeh3RvYjdUkRV2NNPP428vDz8+eefyMrKwp9//mm3S+qvv/6CRqPBK6+8otg+efJkSJKEdevWmfYDYLWfZRZGkiSsWLECvXv3hiRJSE1NNd26deuGjIwMHD58uFyfR6vVQq0Wf7Z0Oh1u3rwJb29vNGjQQHGsFStWICgoCC+//LLVMYxZkhUrVkClUmHatGl297kdY8aMsdrm4eFhup+fn4/U1FS0b98eAEzt1uv1WL16NXr37m0za2Rs09NPPw13d3f8+uuvpuc2bNiA1NRUPPvss7fdbqKqhMENURUWHByMrl27YtGiRVi5ciV0Oh2efPJJm/teu3YNERER8PHxUWxv1KiR6Xnjv2q12tS1Y9SgQQPF45SUFKSnp+Obb75BcHCw4jZixAgAQHJycrk+j16vx2effYZ69epBq9UiKCgIwcHBOH78ODIyMkz7Xbp0CQ0aNICLi/2e90uXLiEiIgLVqlUrVxtKU6tWLattaWlpmDBhAkJDQ+Hh4YHg4GDTfsZ2p6SkIDMzE02bNi3x+P7+/ujdu7diJNyvv/6KyMhIPPLIIw78JER3L9bcEFVxzzzzDEaNGoXExET06NED/v7+lfK+er0eAPDss89i2LBhNvdp3rx5uY45c+ZMTJ06Fc8//zw++OADVKtWDWq1GhMnTjS9nyPZy+DodDq7r5FnaYyefvpp7N69G6+//jpatmwJb29v6PV6dO/e/bbaPXToUCxbtgy7d+9Gs2bNsGbNGrz00kumrBbRvY7BDVEV179/f7z44ovYu3cvli5dane/mjVrYtOmTcjKylJkb86ePWt63vivXq83ZUeMzp07pziecSSVTqdD165dHfJZli9fjocffhjff/+9Ynt6ejqCgoJMj+vUqYN9+/ahqKgIrq6uNo9Vp04dbNiwAWlpaXazNwEBAabjyxmzWGVx69YtbN68GTNmzMB7771n2n7hwgXFfsHBwfD19cXJkydLPWb37t0RHByMX3/9FTExMcjNzcVzzz1X5jYRVXUM84mqOG9vb3z11VeYPn06evfubXe/nj17QqfTYd68eYrtn332GVQqlWnElfFfy9FWc+bMUTzWaDQYMGAAVqxYYfOCnZKSUu7PotForIalL1u2DDdu3FBsGzBgAFJTU60+CwDT6wcMGABJkjBjxgy7+/j6+iIoKAjbt29XPP/ll1+Wq83yYxpZni+1Wo1+/frhjz/+MA1Ft9UmAHBxccHgwYPx22+/YeHChWjWrFm5s2BEVRkzN0T3AHvdQnK9e/fGww8/jHfeeQdXr15FixYt8Pfff+P333/HxIkTTTU2LVu2xODBg/Hll18iIyMDHTt2xObNm3Hx4kWrY3700UfYunUrYmJiMGrUKDRu3BhpaWk4fPgwNm3ahLS0tHJ9jscffxzvv/8+RowYgY4dO+LEiRP49ddfUbt2bcV+Q4cOxU8//YRJkyZh//79eOCBB5CTk4NNmzbhpZdeQt++ffHwww/jueeew9y5c3HhwgVTF9GOHTvw8MMPY/z48QDEsPePPvoIL7zwAtq0aYPt27fj/PnzZW6zr68vHnzwQXz88ccoKipCZGQk/v77b1y5csVq35kzZ+Lvv/9G586dMXr0aDRq1AgJCQlYtmwZdu7cqehSHDp0KObOnYutW7fi//7v/8p1HomqPKeN0yKiCiEfCl4Sy6HgkiRJWVlZ0quvvipFRERIrq6uUr169aRPPvnENAzZKC8vT3rllVekwMBAycvLS+rdu7cUFxdnNTxakiQpKSlJGjdunBQVFSW5urpKYWFhUpcuXaRvvvnGtE95hoJPnjxZCg8Plzw8PKROnTpJe/bskTp37ix17txZsW9ubq70zjvvSLVq1TK975NPPildunTJtE9xcbH0ySefSA0bNpTc3Nyk4OBgqUePHtKhQ4cUxxk5cqTk5+cn+fj4SE8//bSUnJxsdyh4SkqKVbuvX78u9e/fX/L395f8/Pykp556SoqPj7d5vq5duyYNHTpUCg4OlrRarVS7dm1p3LhxUkFBgdVxmzRpIqnVaun69eslnjeie41KkixypUREdFdo1aoVqlWrhs2bNzu7KUR3FNbcEBHdhQ4ePIijR49i6NChzm4K0R2HmRsiorvIyZMncejQIfz3v/9FamoqLl++DHd3d2c3i+iOwswNEdFdZPny5RgxYgSKioqwePFiBjZENjBzQ0RERFUKMzdERERUpTC4ISIioirlnpvET6/XIz4+Hj4+Pv9q5V8iIiKqPJIkISsrCxEREaWuo3bPBTfx8fGIiopydjOIiIjoNsTFxaF69eol7nPPBTfGBQHj4uLg6+vr5NYQERFRWWRmZiIqKkqxsK8991xwY+yK8vX1ZXBDRER0lylLSQkLiomIiKhKYXBDREREVQqDGyIiIqpS7rmam7LS6XQoKipydjPIAVxdXaHRaJzdDCIiqiQMbixIkoTExESkp6c7uynkQP7+/ggLC+PcRkRE9wAGNxaMgU1ISAg8PT15MbzLSZKE3NxcJCcnAwDCw8Od3CIiIqpoDG5kdDqdKbAJDAx0dnPIQTw8PAAAycnJCAkJYRcVEVEVx4JiGWONjaenp5NbQo5m/JmyjoqIqOpjcGMDu6KqHv5MiYjuHQxuiIiIqEphcEN2RUdHY86cOc5uBhERUbkwuKkCVCpVibfp06ff1nEPHDiA0aNHO7axREREFYyjpaqAhIQE0/2lS5fivffew7lz50zbvL29TfclSYJOp4OLS+k/+uDgYMc2lIiIqrTcwmLcuJWHqGqecHd13shUZm6qgLCwMNPNz88PKpXK9Pjs2bPw8fHBunXr0Lp1a2i1WuzcuROXLl1C3759ERoaCm9vb7Rt2xabNm1SHNeyW0qlUuG7775D//794enpiXr16mHNmjWV/GmJiMgZMvKK8OrSo/jnfIrdfY7EpuPRz7aj19wdldgyawxuSiFJEnILi51ykyTJYZ/jrbfewkcffYQzZ86gefPmyM7ORs+ePbF582YcOXIE3bt3R+/evREbG1vicWbMmIGnn34ax48fR8+ePTFkyBCkpaU5rJ1ERHRn+mrbJaw6cgPDFuy3u8/VmzkAgJqBXpXVLJvYLVWKvCIdGr+3wSnvffr9bvB0c8yP6P3338ejjz5qelytWjW0aNHC9PiDDz7AqlWrsGbNGowfP97ucYYPH47BgwcDAGbOnIm5c+di//796N69u0PaSUREFSc+PQ+7L91E/1aR0KhLnyJDr5egUonMfVxarmm7JEk2p9iIvSn2qVHNufPFMXNzj2jTpo3icXZ2Nl577TU0atQI/v7+8Pb2xpkzZ0rN3DRv3tx038vLC76+vqalDYiIyLH2Xr6Jb7ZfQnZBsUOON3HJUby27Bjmbblo8/lT8RkY9+thbD2bDEmSMPjbvXjg463IK9QpamjiM/Kx/XwK9HplD8M1Q3BTM9C5wQ0zN6XwcNXg9PvdnPbejuLlpUwRvvbaa9i4cSM+/fRT1K1bFx4eHnjyySdRWFhY4nFcXV0Vj1UqFfR6vcPaSUTkDHFpuQjzc4erxrHf+fdfScPJGxkY0SkaKpUKW84moXaQN6KDSu+2+XH3VUxbcwoAkF+kxytd6ime33khFbWCvRDp72HallNQjMz8IoT7iW1XUnMwcuEBDGwbhefvr4X9V0UZwWebzuOVLnWtsi9f/3MZa08kYO2JBLzftwn2XRH7n07IREae+frw5Fe7kZCRjz4tItA4whfVPN1w4Goa1p9KBABEs1vqzqZSqRzWNXQn2bVrF4YPH47+/fsDEJmcq1evOrdRREROsO5EAsb+ehgvPVQHvZqHo3G4r+Kin5ZTiDG/HMKA+yIxsG2NMh+3WKfH01/vAQA0CveFBAnPLzwId1c1zn7QAwAwb8sFfLvjChaPao/GEb4AgGs3c5BXpMPH68+ajnXyRobi2IeupeHZ7/dB66LGuQ/FsfIKdRjzyyHsu5yG1eM6oXGEL+ZuvoDLqTlYfug6Ymor10x8fflx1A/1xqgHaps+78Gr5hrK934/ZbqflJmPhIx802Pj/TXH4rHmWLzVZ6/BzA05Q7169bBy5Ur07t0bKpUKU6dOZQaGiO5a8/+5hD+Px+O/T7VEgzCfEvfV6yW8+MshqFVAkLcWv+4T3fFfbruEL7ddwueDWqJvy0jT/l//cwn7r6Rh/5W0MgU32QXFWHs8HhtPJ5m2pWQX4GhsOgCRhTHWsnz693kAwNTfT2LF2I64fisXnT/ZZnXM80lZ2HUxFZ9vvgBfd1c0i/QDABQU63HtZg4W7LyCH/dcM+3fc+4ONAr3xZmETADAtbRc7LqYqjjm8kPXAQA/772Gbo3D8HiLCMTLAhi5uLRcJGXafs6W6gEepe9UgRjc3KNmz56N559/Hh07dkRQUBDefPNNZGZmOrtZRES35aN1IsvRbc52LBoVg0Avrd0g5/qtPEXgYenj9edMwc3N7AJcTM42PafXS1CXUIi79/JNTFxyFIkWgUB6biHScgpMjxMz85Ejq6M5dO0WOs7ajFY1AxSvG94xGgt3X8W1tFy8svgIbuaIrqGUbPOxXll8BMeuKzM7AEyBDQAUFuvxyQYx/1nHOoHYfemm6bm4tDx8t/MKvtt5BQDQNNIXPlpX7Lls3udySg5Ss83dUtUDPHD9Vp7i/TrVDcSuizfh4+4CrYvz5rgBGNxUOcOHD8fw4cNNjx966CGbQ8qjo6OxZcsWxbZx48YpHlt2U9k6Tnp6+m23lYjufln5RZj02zE8UC8IQztEK567mV2AnALdbXVRXLuZg6+3X8bYznUQJRt5o9dL+OTvc2gW6YeezcIBALdylLWCz3y7DwBw7sPuiEvLxZSVJzCxa310rBOIvCKdabiyPT7u4tKYkVuERz/bjjTZ8eMz8nD9Vh7+OpGASY/Wx+ifDyEzrwirx3XCqfhMPPvdPhTrJYT5uiO7oNhUCCzv4gGA9ScTFcGNOHY+4o8nKLY91CAYfxyLx82cQlNgAwDH4tLN920ENnKuGhWKdOLv94P1gzHvmVZoPv1vu/u3qO6PiV3r4z9rT2P1UdHltOOCmNvGRa3CzyNj0Ly6H45fz8Cfx+Oh00tIzirAF8/ch79PJ6J2kLfdY1cWBjdERPeAzzaexz/nU/D1c60R6uvusON+uuEcNp5OwsbTSYrgRpIk9PtyFxIz8rHjjUcQ5md+z5SsAvx+9AZ6NAtHUmY+jsSmIyWrAG92b2Cq/Rj90yGcS8rChaQsLBvT0fTa/VfT8NW2SwCA/q0iMfahOriZbXsgxKn4TPy4+yoOXL2FId/tQ6e6gThw9RYebx5e4mfKyCsCAOy7clMR2ADAbwevY+7mCwBEUGEMMvZcvonvdlxGsV5Cl4YhmPfMfdBLEib/dsxUZCv3/p+nTfcfbRyK6EBPfLvjitV+zSL9UD/UR5FFsceYTfnmudZoXTMAg7/diwh/D0gSTBPvje1cB77urvjmuda4mJKN3s0jUKjTo8t//zEdp06wN4J9tJgzqBWebB2FZ7/fZ+quivD3QIc6onanQ51A030jeXeeMzG4ISKq4lKyCvC54YL8xvLjyC/SIchHi3mDW9mcq6QsjN0z8mJSSZKQVVCM7edTUC/EB3Fpotti58VUdGsSiolLjuJmTiFOx2eiUKfHh2vPKI7Zo2kYWkT540jsLZxLygIAHLh6C4v3xyI5swCjHqyFK6nmrMuqIzew/mQiWkb522zjx+vPIivfnB3ZdVEECCsP31Dst2xMB4z5+ZApM5KQkY/sgmKcTrDuqjcGNoAye/LV1kvYfzUNLmoVZvRtAg830S1zf70gm8GNUcc6gfjf4FZwd9XgckoONp9VTq0R6K1Fs+p+doObYR1q4sc91zCobRRe79YAp+Iz8UC9IKhUKvz9amcAwFPzd5v2b23o9nqsSRgekx2nmpebKZCrFWwe6RRVzVw746N1wfiH69r9LHcSBjdERFWIJElYfzIR9UJ9UDdEdA8sOxRnel4+df7znWqZLnb25BYW49MN59GnZQRaRvnjcko2Xv3tGG7cysX3w9riVm6Rad/M/GK8ufw41p9KhK+7+fJyKSUbG0/D6sJtqe8Xu9CnRQS83ZWXpikrTwAA1p1MQMc6QQBE90jDcB+cvJFpuvD7e7oiXdaevZeVs6e7qFUo1lt3rzcK98X3w9ti6uqTOGEYlXQlJUcRvJTGOMT60cahqB5g7kYL8taa7lcP8EDLKH/8aeh6+nxQS3RvGmaqT6kf5qM4Rx/0awoAaCUL3no0DcOha7eQnFUAH3cXTOvdBD2ahaNllD/cXTV4sL71moC9moXjwNVbaBDqAzcX20Pdqwd4mIMb2TDuSH8PNAj1gU6SsHR0ewTKPs+djJP4ERHdJY7E3sKNdHMR55pj8ejx+Q5cTjEXvC47eB1jfz2MZ77di2+2X8KN9DysMIyKsbRoXywy8oqw5WwSziZm4tvtl1FYrMfOC6lYf1JkG347EIcFu67gtWXHIEkSpv5+Esfi0pGaXYglB+IUx9t5IdWUpciUZUy+2nYJk347VqbPuOZYPP4wZIOeal1d8dzZxCws2CW6bqb0bITVL3UyzB8jnh/avqbd4y4aFYML/+mBFzvXVmzv1zIC3loXtIzyxx8v34/2tasBALZfSLFby1JHltmoE+ylmJPs0cahin2DvN0Uz43pXAcqlehS69syUlF4Wz/UXKuyaVJnPGf4PK1qmAPQ6gEe2DDxQbz0UB3M7N8MarUK7WsHlrhI5XMdovH5oJZYPLq93X3cZPP7yEc6uWjUWDfhAayf8MBdE9gAzNwQEVWqZQfj8Onf5/Dt0DZoXt0fAPDrvmsI9NKie9Mwm6+Z/fc5HI5Nx86LqdCoVZj9dAtkFxTjnVUnAQAfrj2DBcPbIjkzH2+sOA4ASM4qwMy/zmLmX2IUkZuLGh3rBGLbOXPmZsXh6/j7VCKyZIWtBcU60/Dkf15/CAeu3QIAXEzOxuHYW6bhzACw1SITM2fT+X9xZsyy8ovh6+6Cdx9vjGV2ArPoQE+4aNSY1rsJXnmkHnIKxWeYa5h5t0fTMKw7ae4OqhPsDZVKhZc618Wyg9eRV6jDrrceQTUvN8Vxn2odhb2X00wjizRqFQbcF4l2tQIRezMHq4/G451ejTD5t2O4lVuE5++vhaupOaZ6mYcbhCiOJ8/c1KzmiaaRftg3pQv8PZXvCwA1qpmDJnmAIa9X8nRzQYCXG97o3rCEM6ikUatKrYXRyQaMuFhMZKhWq6DG7XVfOguDGyKiEhTr9DidkImmEX6mIcDFOj30kggYCop1WH3kBh6oF4wIf+XcHkU6PVzUKlxJzUGgtxZ+Hq54fbkIPt5dfRJrxt+P67dyTUHKsjEd0Da6muIYBcU60wUbAHR6CROWHFXsczE5GwXFOjw2Z7vdz/FIgxDUD/VWBDcAFIENYJ53BQB2XEjFoau3zM9tOI+cQp3pseVw5wuGIdPv922CwmI9HqwfjH5f7EKu7DVyDzUItmqP+bkQ+Hm4YtQDtfDz3mv46tnWeOHHg9AZupXk0/sHeLkhwMsNer0EN40ahTo9Pn6yOfZdSTN1tQQbggw/T1f88fL9yCsstgpsAKBfq0h8vf0SzieJz9KhdiA+ftK8Dt+kxxoAAJa+2AFH49LxVOvqyCooxtnELDSJ8EOAxTGDfMzBTbCPCFJC7BR0t4zyR5eGIQj1c7fKxHzYrynWHIvH851q2Xztv9WrWTiOxKY7fU0oR2FwQ0R3jTMJmZi45ChefbQeujctecSLo/y45xo++PM0Xri/Ft59vDH0egm95+1CbmExNr7aGT/vuYYP155BqK8W+97uanpdVn4R+szbhWs3c6CXgJha1bD0xQ6m540X3aRM83wl4xcdxm8vdkDNQC9IkoT//n0eSw6UvN4bAMSm5aLZtL9RqLM9EaenmwYjOkUrZpiVG3BfdWw4lWi1ftG7q08qHpdlxI6/pysGt6thWsbg55ExmLr6pM3i3Hd6NjIFN9UDPJCcVYDCYvEZHqgnamve7tkIU3o0glqtQv9WkaaJ5+R1LUZqtQo733wYhTo9fNxd4aXVIC3H/JxRpL/9CeY0ahWGdog2ffZOdYNs7lc/1Af1Q8U8Or7urvh5ZIzN/bzczEFKrVKWXNCoVfh+eFubzz3bviaeLaHb7d8a1jEa/p5u6FQ3sPSd7wKsuSGiu8aUlSdwLikLY3457NDjJmTkYf3JRLyy+Ai6fbYdZxIyIUkStpxNwsrD4mL6w+6rOBWfgQNX03AmIRPXbubi2s0cU4GuMUjZfyUNo386iGbT/8aVVBHYAMC+K2lIzjIHF8YVmeVztCRlFuCtFaJ49td9sZi39aJi4jS5++sGIVzWXWEMbOqHeqOjbHjuX688gFMzuiGmdqDi4ir/hv5Wj4Y48t6jds9PTK1qiJZlSixHJ9WW1aA81jhUsT5T65oB+PSpFqa2uWrMQYY8QBn1QG2M7VzH9PiBeqIwVqVSmQKT93o3RuuaAXi6TXW7NSYhvu6m49YJvr35Vvq2jJC1w3ZwU1YqlQrfDW2DD/s1NS2vcCdy1ajxZOvqpjWp7nbM3BDRHSExIx9fbruIoR1qom6IcmbZnIJijPnlEI6WY/SKkXHySfmQZ0mSIEnmb/NPzd+jmG11wFe78cmTLTBukTmI0ukl9Jq7U3HshIx8BMu6HW5mF+Dd1SdMXRqWlsvqR4yjetJyRfBSJ9gLl1JysOfyTfT/cheOyGpbbHmjewM0r+6Pi8lZ6Drb3B0V6e+hCC5qBXmZPrt8iG+v5uGm+WKMn2FwuyisORqPzwe1wp/H45FdUIweTcPRuUEwtpxNxhuGLrUn7ovEsevpMJZptK4RgMspIkVinFhPrnGEL9ZPfADVPN1w7HoGxv5yCB/0a2oaLg2IzI1xdBegrDMx8nV3xYqxHa222zOjTxOM+eUwRj1Qvq4cH3dXLB/TAUmZBWhqWObg3+hqUWRMFY/BDQEQMxm3bNkSc+bMASBmMJ44cSImTpxo9zUqlQqrVq1Cv379/tV7O+o4dHd7Z9UJbD6bjL9OJOLgu10Vz/289xp2XFCui1Os0yOvSAcfd+VK9SeuZ+DNFcfxYufa6NsyElN/P4lVh29g/cQHEVXNE/Hpeej22Xb4eriiS6MQvNi5jtU08rmFOrz/p3JGWVuu38pDnqye5J/zKabA5oF6QVZt/nj9OdP9jLwipOcWmjI3Lar7w9/TDYeu3TIFNm4ualM3jaV6hgCwbogPxnSug/n/iEAlMsADN2SfRx5A+Lq7Ymb/Zigs1mFI+5pwUavwUAPz0OFZTzTH9D5NoHXRWF2Qn24ThcbhvriQnIU+LSJxNTUXC3dfQfUAT7Ss4Y9lh67Dz8PVbjdOwzCRtXi0sTtOvd/NNEro26FtcOJ6Oh5pGAKVSoVvh7ZBvRDHzHBbM9AL6yY8cFuvbWNR+0R3FwY3VUDv3r1RVFSE9evXWz23Y8cOPPjggzh27BiaN29e5mMeOHAAXl6OXbJ++vTpWL16NY4eParYnpCQgICAkufaoKpvr6GeI1W2Zo5RfHqe1bapv5/EsoPX0adlBNw0aszo2wSXU3LQe57IrkxYchTLD103BRhLDsTi9W4NsffyTWQVFCOroBg/7bmmmBROTl4LA4hMiTw4AYAb6bmK9hqHOzcO98XPI2Ow4tB1TF5mfwj0ldQcU+1NgJcbYmpXw6Frt1CjmidefbQeEjLyrd7TyENRy2Hu3on098T9dYOw9VwKWtXwt3rdMzHmhR8nG4pj5UpaE6hppJ8pk/Fe78YY/0hduGpUyC3U4YddVzGobZQia2SP/D0ebRyqGD5tOZSa6HYwuKkCRo4ciQEDBuD69euoXl05L8QPP/yANm3alCuwAYDgYOuJoCpKWJjt4a90b9G6akwjcVYcuo4/jsfj84Gt4OfpalXoCgCL94s5VoyzzUoSsPywctiwPHOSlV8MnV7CtZu5dvexZ+rjjTHy/lro0jAU3WQjki6n5CAlyzoYa19b1Lx0bRyKCD931A/zwYP1gnE+KQuHY2/hSmoOinQS+n9pnjm2mpcbnmodJTIhUf7w0rrgz+PxiuM2DPNByyh/dG2kDABqydbyiQzwQLcmYVg6ur0pW1JRjKONfNxdsWlS5wp9L6LyYEFxFfD4448jODgYCxcuVGzPzs7GsmXL0K9fPwwePBiRkZHw9PREs2bNsHjx4hKPGR0dbeqiAoALFy7gwQcfhLu7Oxo3boyNGzdavebNN99E/fr14enpidq1a2Pq1KkoKhJ1BQsXLsSMGTNw7NgxqFQqqFQqU3tVKhVWr15tOs6JEyfwyCOPwMPDA4GBgRg9ejSys801DMOHD0e/fv3w6aefIjw8HIGBgRg3bpzpvejucf1WLp78ajc+3XDOVGALAJOXHcO2cyno+NFmTF9zyiogsWXpwTjo9JLiOHLH4tLRdfY/pmUI7NVhyCeOC/B0xaWZPTHyfrGvfJI1AFh3MhFXDW177/HGeLRxKLy1LqaCVD8PV+ye0gULR7TD8/fXwkcDmuPvVztjep8mVu8b4OkGtVqFTnWD4KUV3zvlBcNTejTE2lcewEcDmlt1GUXLMjfB3lqoVCrE1A6En6eyy47oXsHMTWkkCSgq/Q9rhXD1BMqw7ouLiwuGDh2KhQsX4p133jEVDy5btgw6nQ7PPvssli1bhjfffBO+vr5Yu3YtnnvuOdSpUwft2rUr9fh6vR5PPPEEQkNDsW/fPmRkZNisxfHx8cHChQsRERGBEydOYNSoUfDx8cEbb7yBgQMH4uTJk1i/fj02bdoEAPDzsy7Uy8nJQbdu3dChQwccOHAAycnJeOGFFzB+/HhF8LZ161aEh4dj69atuHjxIgYOHIiWLVti1KhRpX4eqliZ+UXIL9TZncvDqLBYj66z/0F+kR4Hr92yGZTkFOqwcPfVcr3/sA7Rplls5Sxnm20R5Y8IP3fTgoBG9UK98fPIdli8PxZdGoYq2qVSqfDP6w9h27kUTFujrMkZ0Lo6nr+/lmnNpZL0bRlpmtvGyNacK/KRK6G+7nYDt2DZRHF1HVSvQnQ3Y3BTmqJcYGZE6ftVhLfjAbey1b08//zz+OSTT/DPP//goYceAiC6pAYMGICaNWvitddeM+378ssvY8OGDfjtt9/KFNxs2rQJZ8+exYYNGxARIc7FzJkz0aNHD8V+7777rul+dHQ0XnvtNSxZsgRvvPEGPDw84O3tDRcXlxK7oRYtWoT8/Hz89NNPppqfefPmoXfv3vi///s/hIaKb6wBAQGYN28eNBoNGjZsiF69emHz5s0MbipYYbEeT329B56uGvz6QozVRTw5Kx/95u1CZn4xtkzurAhwCop1OJeYZZoM7+DVNOQXmYtldTbW/LkdTSJ8MfL+Wvh+5xV0rBOIMZ3rYNgP+yFZHL5GNU80ifQzBTf1Q72RkJGPfq0iEeLjbhqKbKlmoBeeuM8Nn244p5gAz7iWUmmBDQB4a13wv8Gt8PLiI6ZttoIb+Uiskr7nqFQq7HjjYWTlFyteQ3SvYnBTRTRs2BAdO3bEggUL8NBDD+HixYvYsWMH3n//feh0OsycORO//fYbbty4gcLCQhQUFMDTs2wzUZ45cwZRUVGmwAYAOnToYLXf0qVLMXfuXFy6dAnZ2dkoLi6Gr2/5+vzPnDmDFi1aKIqZO3XqBL1ej3PnzpmCmyZNmkCjMRclhoeH48SJE+V6Lyq/w7G3TIsJnk7ItBom+86qk6ZgYc2xeLzwgHkdn4/Xn8P3O6+gZ7MwTH28Mf65YHtm2rII8HRVLNhoHEYNAE0ifdGreTgahvmgR7NweGtd0Ly6v9UiiDWqeeLFB2sjI7cIL3epi/vrBqFQpy+xoNbIx90V6199EE98uctUeFze1bV7t4iA1kWN0T8fAgBU87LuQpIX5zYOL/n/UlQVmVmWyBEY3JTG1VNkUJz13uUwcuRIvPzyy/jiiy/www8/oE6dOujcuTP+7//+D59//jnmzJmDZs2awcvLCxMnTkRhoe3JwW7Hnj17MGTIEMyYMQPdunWDn58flixZgv/+978Oew85V1flhUClUkGvtz1klhxnp6z49p/zKVbBzcGr5lWY/zgWj5H318L6k4lIzSnE9ztFV9FfJxLx96kkqG0EA53qBmJITE1cTM7G6iM3cDk1Bytf6ogVh67j133mmXr3vt0FKVkFOHTtFnZdTMXQDtF4/H9ilFSdYG+4atR4qk2Uaf+fRrTDisPXsetiqmnVZT8PV7SJrobfxpgD9bIENkaR/h4Y27kOpv9xGj7a2/tT2kgWsNhaawgAtkzujMSMfNQL9bH5PBFZY3BTGpWqzF1Dzvb0009jwoQJWLRoEX766SeMHTsWKpUKu3btQt++ffHss88CEDU058+fR+PGjct03EaNGiEuLg4JCQkIDxcTdO3du1exz+7du1GzZk288847pm3Xrl1T7OPm5gadzvYaM/L3WrhwIXJyckzZm127dkGtVqNBA+thq1R2xnWOVCoVziVmITrIE1oXDX7ecxXbzqVg7uBWpkJWW/KLdPj92A3T401nkvDSQ3VMGYvcwmJFNuXY9Qz8Z+0ZfLfTuv6lWC8BEP1ENap5IjZN1LXVDvI2TQI3qG0UUrML0TjCF/fVCMC1m7nYeVEEV1oXDaoHeKJ6gKdpQcBvnmsNf083m0OR/Txd8fz9tTC8YzRmbzyPeqHe5c602DKsYzQ8tS64z8aQ67KoHuCBPi0ioNNLCLTRLQUAtYO9Ufs2Z9oluldxtFQV4u3tjYEDB2LKlClISEjA8OHDAQD16tXDxo0bsXv3bpw5cwYvvvgikpKSynzcrl27on79+hg2bBiOHTuGHTt2KIIY43vExsZiyZIluHTpEubOnYtVq1Yp9omOjsaVK1dw9OhRpKamoqDAegjtkCFD4O7ujmHDhuHkyZPYunUrXn75ZTz33HOmLilS0uklfPjnaSzeH4tD126h19wd2HZOuVpzanYBOn+8Fc99vx9/Ho9HtznbMXPtGWQXFGPq76ew+Wwy1hxTZiiPxaVj4pIjOB2fifwiHQZ/uxdxaeb5Zo7EpuP3o/G4lVMISZJME8f5aF0wvGM0AFgFNsM7RuOdno1Mj5tE+KJhmDkjUUc2g26Ir7tiuno/j5JH/jzWJAztapU88ZparcJr3RqUukJyWalUKjzdJspqRuXyvH7u4Fb4Ysh9Dgm2iEhwenDzxRdfIDo6Gu7u7oiJicH+/fvt7ltUVIT3338fderUgbu7O1q0aGFz4rp72ciRI3Hr1i1069bNVCPz7rvv4r777kO3bt3w0EMPISwsrFyzAavVaqxatQp5eXlo164dXnjhBfznP/9R7NOnTx+8+uqrGD9+PFq2bIndu3dj6tSpin0GDBiA7t274+GHH0ZwcLDN4eienp7YsGED0tLS0LZtWzz55JPo0qUL5s2bV/6TcY/YfiEF3+28gikrT2DAV7txKj4Tn208r9hn3paLiM/Ix86LqZi0VEwq9+Oea/jrRIJpn+u3cnEk9hYOXUvDCz8eRN8vdmH10XhM+u0onpy/G0di0+Hn4Ypvh7bBpEfrAwAmLj2KVh9sxFf/XMINw0R7kQEeeLVrfbjJMijv9GyEL565D691a6AIQO6vF4QQX3MBbJ0SRvq82b0hqgd44L3Hy5ZxJKIKlJUEHPoRKHTSaOJSqCTJcgxB5Vm6dCmGDh2K+fPnIyYmBnPmzMGyZctw7tw5hISEWO3/5ptv4pdffsG3336Lhg0bYsOGDZg0aRJ2796NVq1alek9MzMz4efnh4yMDKti1/z8fFy5cgW1atWCu3vJw1jp7nIn/WxXHLqOlOwCjJEtElgeSZn52HclDY83C4darcK7q0/gl73KlaNb1wwwrcGTX6RDmw832ZwIr03NABy8dqvM773ohRh0rBuEIp0eD32yzRTQeGtd8Npj9TH9j9Po0jAE3w9viyHf7cWui2LW4TXjO6F5dX8AYtmEuu+sAwAsGN4GR2PTMXfLRQDArrceKXHFZiK6Q3zZAUg+DXQYD3T7T+n7O0BJ129LTs3czJ49G6NGjcKIESPQuHFjzJ8/H56enliwYIHN/X/++We8/fbb6NmzJ2rXro2xY8eiZ8+eFVa0SuRokiRh8rJj+GjdWZxJyCxxX71ewuyN5/HrPmXt0sgfD+CVxUfw9fbLYuXqM8lWry2WDavec/mmzcAGQImBzfiH6yoefz+sDToa1g1y1ajxXm9zBiW7oBjT/zgNQGRuAChmx20g63py0ajxzXOt8Wb3hni4QQjyZWsnhZcyN06VlJsmbkR3k2Tx/x1n1ji3HXY4LbgpLCzEoUOH0LWreYE8tVqNrl27Ys+ePTZfU1BQYPWt28PDAzt37rS5v/E1mZmZihuRs8gLbhMyrNdLktt5MRVzN1/AO6tOIj3XPLLt5A3xO/zL3muITctFfEY+3FzUOPtBdywzjPzJkO3/z7mSh1wHeZsLWSP9PVA/1BsD20ThVUPXk9GD9ZXzvnRrEoZ/Xn8ILhbzugQZJpQb0SkaPloXdGsSajUK6bEmYRhrKEZuIqurKcscMVWKrhj4uJa4FTtu9CJRpdHcmfMqOS24SU1NhU6nsyoSDQ0NRWJios3XdOvWDbNnz8aFCxeg1+uxceNGrFy5EgkJCTb3B4BZs2bBz8/PdIuKirK7L1FFS84yz4abYDEzLgCsO5GAET/sR1pOIVYfMY9MMo4S0ssyMqnZBTgdLwKdBqE+cHfVIMAw3b48iPrnvAhuRj9onnNG7on7qqNRuC9cNSp8/Vxr/P1qZ/zfk82tZsO1NQqpZqAXAixG+TSNFMFK9QBP7H27C+Y9c5/N9zV6vHkEZvRpgr9eub3Vm+9qhVnm+zm3P+8PUbmc+QP4fRxQlA9smgHs/hc1jS53ZnBzVw0F//zzzzFq1Cg0bNgQKpUKderUwYgRI+x2YwHAlClTMGnSJNPjzMxMBjhUIYp0euQX6eDjbn9UT7JspWnj8Ge5sb8eBgC8vfKEKSgBgG3nUvB48wikyFagLijWm/ZvFC66ffw8RKCRmV+E+f9cwi97r+G6YRTTuIfrom6wN47EpaNfywjEpuXiVHwmxnaugzGd6yA7vxg1ApVzK/l7uiI9t0gxosnSB32b4rVlxzD5sfrw83DFQ/XN9XIlDS030qhVGGYYXXXP0cm6C3XWowepklw/CCx9FujyHtDymds/TsZ1wKMa4OaACRUz48WxXA29Fbs+B478CgxZBgTUFNtSLwI/9QE6jBO3sloqpgWBuz+wxxDY3Pcc4G69JI5N8t/bOzS4cVrmJigoCBqNxmpIclJSkt3p+YODg7F69Wrk5OTg2rVrOHv2LLy9vVG7tu1vpACg1Wrh6+uruJXGiTXWVEEq42c69Pv9aPufTUjNtn2RkiRJsYL09bQ8FBbr8c32SzhkUfuy/lQi8orMcwItP3QdL/16CMsOxtk8tnH2Wn9D5kaSgI/WnTUFNrWCvODn4Yqn20Zh1hPNEFM7EE+1icL0Pk0Q4OWGal5uVoENAPz6Qgy6NQnFt0Pb2P3c3ZuG4eSMbhjRqRaeuK/6vde19G8Uy7J3hTmOOaYkAcd/A5LPiMdZicChhXfsqBa7CnOAI78ABVml72t04zCw7f+A83+Xbf8zfwLXDwGrxgBZCcDqsbfXVgBIOA583sIcOJREVwwcXQRk28nWXdsDzG4E/DnRvG3je0DqOWCJLPha/xaQeQPY8PbttTn1gvl+wnHb+1w/CFzYpNyWa57ME6qyT3xZmZwW3Li5uaF169bYvHmzaZter8fmzZttTu0v5+7ujsjISBQXF2PFihXo27evQ9pknPU2N/cu+yNApTL+TC1nNnakPZdvIr9IrxhebfTDrito9cFGLD1gDk7WnkhA/XfXYeZfZzHgq91YdeS61euGdqgJTzfxx+OvE4n49O/zVvsA5pluXTVqeNvIlljOJFxWTSL88PVzbTi1f0XRyepsCrIdc8wLfwMrRwFfthePFw0E/pgAbJ4hHu+YLQKA0uSmAX9MBA7/5Jh2ldcvA0TXybaPyra/JAGLBwPbZgKLngKyrQvtFVIvAEuHAN89IjIuJSnIBlaNBc5vsL/PoYWAvhi4tBlIOl3y8bZ/LAKpJYNtP/+XYS3AY9bTZSDpJFBkqNfLsP1lR6EwB1jzijlA0csmUpUvCh1/BFZuXgK+6wL8OkAM/TbKlt0vT/BZiZzaLTVp0iQMGzYMbdq0Qbt27TBnzhzk5ORgxIgRAIChQ4ciMjISs2bNAgDs27cPN27cQMuWLXHjxg1Mnz4der0eb7zxhkPao9Fo4O/vj+Rk8Z/C09OTE2vd5SRJQm5uLpKTk+Hv769Yj8qR8grNfzDe+/0U/jyegKWj25t+f/635SLSc4uw/6r9UTHT11j/QezTIgJXb+Zi+3nlN7zB7Wpg6uON8Nhn25FXqEMTWfDi5+FqNTqqQShnuC2T/EzxB9/H/uKuDqXI3JQxuEm9CPzyBNDpFaDtC9bPW16kEo6Kf48uArpONwc5rZ4F/OxMZqjXAV91ArLigZMrgfuGlq1tjiJJQKxhYMmZNSUPNT65AtjwDvDw20C2rF7zxmGgQXdx/+xaYMUo0YXSbSbQcrDIaBkVy4r7dUWAxuJL0J55wLFF4jZdubq8yVXZwJbDPwE9ZEGZJAE/9xOBxhPfAgcNpRTXD9j+7Eknrbf7RZmDmbj9QO3OQL6sLbpiQOMiAp+cVMDfUH6xex5w+Edxm5auHJknzxYaf0/kNr5nvp9+DfAx1MjKA0d5G7KSxDn28Lc+ViVzanAzcOBApKSk4L333kNiYiJatmyJ9evXm4qMY2NjoVabk0v5+fl49913cfnyZXh7e6Nnz574+eef4e/v77A2GbvEjAEOVQ3+/v4lrkb+b93MUXZF7b+Shlu5RQjwdIUkAdn5todiy2XkFSkeD25XA61rBiCmVjWr4CY60BOebi5Y+/ID0EuSIlujdbVOyLaMCijPx7l3zWkq/li/fgnwCnLssQtzgBPLgfrdzReJYtnvTVm/AW/9UFxo1k4G2oy0Xi5cLfuzLl9vrTBbBG9GOcmAZyBwfClQt6sy0Em/JgIbACjIEIWnrmUYpp+XLoKNJv0Bz5Jni1a4tkecd2MwkihbBDeydcmvXf68+HfNy8rtCUdlwc1fQFGOuK0eYzt4MMqIA6pZlDqkl5IhSY8TXUam9z5mvp90SgQ+l7eJx4sHQbFEfW6a8lzdsl6uBIDydyXlLBB9vzJQyU4SP8PvHgWSTgDjDgDB9ZVtMXZVGmXJsszXdit/zpIEXNlhe1955sYY3BRkA/9rDeiLRNAd3FDU8TiJ0wuKx48fj/Hjx9t8btu2bYrHnTt3xunTpaT7/iWVSoXw8HCEhISgqKio9BfQHc/V1bXCMjYAMP+fS/ho3Vmr7Yv3x5pWwS7UKRf1nNi1Hnq3iMDeyzfRNMIPIxYeQFqO6KJoX7saJj3aAG2jA6BSqTCiUzSu38rF4v3mP7BNIkSmxs/TuptNXtfzx/j7cSE5C/fXc/CFuiqSJPMf6rj9QMOetvc78ovo0ugyDVCXo2d/0wxg/9dAUH1gvOEbu7xbqqyZG/kFLekkENZM+bxa9ruen26+L+mBZcPMjzMTRDfLtlmAXw3g0RnAub+A7v8HZJhH6gEQGZGA6NLbtmIkcHGTyEj0n1+2zyNJwA+GIGTCMfE+8iyIvvQvBgru/uJzyzNYuTeV+xiLaG05uADIuwV0nWEOcEv7OWdaLK6cI/ty/FVH5XMpFn8rEo4BdR42P7bsTisuBFzczF1RgAiYbl1VFqFnxovgJskQGJ75HQh6TZmROb8OiJTVz8kDlqwEYPf/gM6vi8d5t0Rga3reTrdUUY7IdmUlmEf/7ZkHVG93bwc3dyqNRlOhF0S6u+UWFmPRvlh0axJmM7ABgE82iG9yxtmDowM9cfWm6ONuGeWPOsHeqGNYELFtdAA2nBJ/MNrUrKZYosDTzQWznmiOjaeTTcXKzUqoocmSZYmaVfdDs+q3UW+TdFrUPDwwCWg3qvyvd6b0OMA7pPyjOOTfjOXp+lvXRLCzabq4YP9uGJVStwtQ60Hr46x/G9g3H6hWCxi1FXA3DGI4L2ZlRqqsbkreLSWvudHrgPijgNZHfPuWS5FlCM6vtw5u5IXDlhd1Y1cPIApRz/0l7mfEAstFOQBOrgQkiwVus8oY3Fw01HWcXKEMbmL3iYxJnUeAXv8VGaW0y0BgHWVQl3xGvI+83fYKrZPPiHoiS+1Gi5qW+KPiXP3cX3xWQHQJbTFkvuzZ/T/xb/wxYPRW4PvHgPjD5ucLssXFPVA2w7gxKHbzERf40up95E6tAmo/ZM7AyYMY47G9g5VdZ8lnrDM8mTcAnWymfpVG1MxkygLV2H2Af03rNkTcJz7jqZXm4ObWVeU+8kAodp/yuYw48V5GbZ4v2+9LBXL62lJEzpRXqFNkOmxZceg63lh+DBm5RRj8zV5MWXkcn2++gA/XnsGQ7/aV+Fq5NtHVMOuJZhjYJgr311VmUga2jYKP1gXVvNzQtbHtBUKzC8yZRFsZG6PahsUna/ybIuA980S3hLGw8W5xapXoWvpjQvlfKy+uNH4DvbAR+Lw5sPIFIPM68OPj5n1sFY1e2w3s/UIEBzcvAjcOmp9zszGcvthO5mb7J6LQ9Yu2yizGni+UdSWW3QyARQ2E7TnDAIiLnq0J2CwDm9KOYyQPzkKbmO8X5gALHhPBzIHvxLbdc4F5rYFfn1JmPQ7+IDIZ8i46Y3ATd0CMbjJaMcp2kGLMgmQnilFE8ou7X5QIOMsi6YQoEpYHNgCwcSrwv/uAXXPN24wZsiDDrN4FmcDe+WUrEj/8owgGjSyDm11zRJZNnsFKPmM9q3XCMWCHfLZ+Cdg83XDfEDilXxP1OJYa9BD/ys+VVXBj+B1IjwMublQ+N7cVsNgQaFarDTz+GdDpNv4POhAzN3RPG/njARyJTcfKlzqiRjVPTPrtKB5uEIJB7WoAECtuT14m+qxPxWfiVHwm9ly+iTDDMgG25qqxp1mkHwa3q4HBhmPLPdIwFCdmdCvx9TWreeFcUul1GV8NaY2vtl3EhK71S93XLm9ZgGVZE3CnKsoHlg0X948tLnu3iJE8uDBmDnZ9bn//hKMiS7J2MtC4j7hA7P1Suc/qcUDr4eKbbfIp62Po7NTcxMkWEE46LeorEk9YD/nNSgTOrQNO/w70/ERkeuTBzc0LsCszvuzZrbIEN9dlbXaTFbBbvrYoz1xTc3GjMnt1YYO4NZdlZIwBwveG2exf3C7mezF2v1gKrGe+b5m58goCvAKBsOZAosXQ5+aDgONLlNtOrrQ+vrEYeONUoOkTgF918zn3ixKBR3E+sP5N4ORy220EgEemip/Bwe9FANvsSbG9yOJvyp55wOV/lNsKs6wD252zlY/j9ovMnkoDDPxZDCG/dVXZpWQUYcj45GeI8631tp+5OfyT6OKMfgBIuyKCfjlXL/ufuRIxc0N3vaTMfPT+304s2R9b+s4Ger2EjNwi7L50E3lFOvT4fAeaTNuADaeS8NbKE6Z5ceTrP52KN99PzLSeXRgQQ7fnPWN7EdfOFssXlNd/n26B1jUDsHhU+xL3axDmgzmDWqFWkOGPTH6G7YuTrtj6D5iRVnZxit1r/XzGDecMAU27Yn+ZAnmXi2/1sh1PrxPHlCRld45x/hF/60DUJP6ICH6OLRJFogCQbOiiNBakZsWLoclHfla+tihPnHu7XWFXzffz00WAeXateduj7xuOnyje+9hikWUAlMFNainBTWmjQY2ZnawEca5SL4hztWkGMP9+IEcWPFySZUSyk81FuJbdStlJyuzQ1R2wIg/u8m6Jn5HR1pmidskWFw8RwGgMs2brLOp1PAPFv1ob850Zgwu52N2238fonKGr0Zi58fAXXaJGtkZDGUW0BGoa6nGMgUpumu3h3aZATiVqigBz1srFTqH3+fXi3zqPAPUeE0FOcb6o17FUrbb5nJxfL7JAaZfF4yjD35vsJHE+jb/LbUbYnvTPERMYOgCDG7rrfbrhHE7cyMBbK+18k7P1mr/PocX79if6MtbG7L180+4+tgR4uqF97UD4uLvg/rpB8HU3J0ejg/7dN5qmkX5YMbYjOtQJLN8LP2sK/LeBdRp752di0rFTq61fI7/onlwOHPrRvC0zHvisMfDNQ+VrR2mKC0Sxrr1FJGP3AXNbAl+0sz0vSbKNbqLSHPheHHP/N6Iw0sj47TavhBXTU88rR93ois11ELUftv0ao9/HAZ+3VHZHGDNHeh2QLgvUb14SKzBvE1Ni4IHJQEND95j8QmgMzuRFxPL6HkuZN4DcUlaEjzQsnZGVCGz9DzCvDXDoB5ElSDwhAjcj44UeECOH5jQVo5Qsg5usxNInFJTXk2TFi5+RkWUWQ847RARsWkMXoOWsz8bAQGvRRXjfMDG6x0jrC6jKcHk0/pyMAaW7H+AVYn9/ufBWQEgjcT9uL3BsqVhjbNN0+69xcTe33Rg8thstbvY06C6GtvsZAn55N6eRRwDgGyHurxgJfP2gOYipESP+zYwH/n5HBLqeQUDD3raDG1cP+22pRAxu6K5nHGVkdCM9DysOXYdOtg6TnE4v4cttl2w+Z/Ttjst4ev4efLjWRk1DCVzUKgR5a7Hv7S74fngbvNdb1B68cH+tch3HYYoLRf8/oBxeC4ghxQCw6kXzNr0O+PNVcxYAEBfgP14x9+dfMASFNy8qhxr/W39OEhd9yyG9eeniuQ1TxONbV0S2YsULIoMkSWJSOnmXTZGdIlRL6wzFk+veUF6ET68G9n5lO4Xf/iXR7SLpzd9uAfFNWl8ssgdRMSW/78kVACRzQS9g7n7JvCGG0xodX6KsswlvaZ6HRz7aysMw3F+euTEW+FZvJ0Z3yaXH2u/akb8XIN7f+PNfLzvPxqHCNy/Z7gJbMtg6uFnYy/w75Hob3/KL8+w/Z7ywGrvFMi0m1DSOepIHNw+8BvT+HPCVDYX3rAYElOH/rDG4zEsX/7r7lZwNC2su/g2IFt1j8i60VSUEKEauHubPZgzwvYJFl+SI9bZfU7+7+T0B2/833LzNwY2laEPRfH66KJQHgEfeFSO4LIvZgTumW4o1N3TXswxhhny7F1dv5uJWbiFeeEA5X8Xeyzcx6BtlN0uv5uFoUd0PM/8yj3patM/8zVnrosamSZ2hUavw4+6r+Hr7ZdiTa1gywdNN/NcacF8kGob5lLg2U5ncugr4hJd/BJB8MUbjxfiHXkCM7A+pvObh9GpzTYGlU6vERGnyuVJyU5VpeEvpsWLejdxUUWPQoIcYCm3rAnD0F/HvWVnRqF4nhtLKCx0BEaglnhDBR0Qr69qYojwReKVfE+ctK0GMXrLkW91cM/Bjb+Vz698y3x+5SXTVXfgbiBkr/r15UdldcvOi+DewbsndWfYYi5jtdRUaRbQE3LxEdqFA9rMwZn7kwY2RZzXruilbQ6xDmykDnhoxwL6vlKN/fELNbUw9J4IXY/dLYF3zeTCyzKjJ3ze4gXnItmeQ6BoyXkBL0/Bx5e8KAKgNhfbGLpZCO12n8uDGzUv8Psp/J3XFQFA0kGbnS1BALRFkG7MnpsyNv3Wdj1z97kCzp4DqhuHYLm7297XF1UO0FzDPQ2QMakMaWu/vF2XO2FSrBVyxkfXSuIl2+NgJbmrEiC4tY1diVHvRJQWI/8+WXYTsliJyDL1sQiy9XjJ1KS0/pOy6SM8txPhF1lOMh/u6o1+rSNOK2pY+6NsUUdU8EeHvgboh5lqUcD93PFg/GB8/2dz0Wsu6GpVKhaaRfnCxsaJ2mcXuE91HP93GMiPy+TbybokZR7PirVPfxm4Cy2+6cqnnRepc/gc/K0F0Ix1cYP72Kndpq/jWry8GNk0TXUrHlljvZ6+O5tZV68BG7sp220W/xfnApvdEd8Z/QsW/m9+33k+eISmJd4joQug0wXAhCDe8jyyLYKxvCaxjnh22PIyZG3nAJOcVIi4sfoZjW86ifGmL6AayFdykx5ovggDQ1EZ9CVTAsDXAyI1ivpmX9pqDNHmA4mLR7ZB0ypxFCG9hfVh5LZSloAbm+21GAG3LOu2ACuj/NfD0z+aABhAz9ALKmjFbFMGNjX2r1TKfZ1seNGT8MmwEN/aCBED8DDq9Yq61AYDOb9nf35Krh/VnM/5c5T9fo4iW5vt1u4quNpUaqPuoebsxWLI1z5JvdXGuPGVd4fLJLWt2st3GOwAzN3TXycwvQlp2oamGRd77lCVbdiA123zBlCQJfxyLN80TE+nvgRvp4sLk5+GKEB937HjzERQW63HfB+Zhjv+8/hBqBprTrA3DzIWIP49sh7oh4o/kww1CcCU1RzE/jcMYMxolXSTskS/M9+dE2xc+QIx0qf1Q6StTW6bOF/YWaW59saiFePpH5fO2iiPPrRXT3xsVZAHfPmJ+7OIuuppUKussQHkY5ysx2vFfMXNuWDPxnn++arvbyRbL7JSt5RmMAUBQPXPwUx7Gi4uxjqNaHXMg6eIOTD6nnEzOK1hZU3N1h7I4N7yleQK3qHbKi19UjHIkT/QDoqvBsxrg2c68PdXG+TdmDIySTpmHc1vO7Aso5z+xJJ/Dx6Oaeebm0tTvLi7yjfsAb8cDHxq+VJgyN6VkSt3N/49NF3cAeG6VWHvr8TlizpeS3h8Qvz9F+cqam8dnixF0vhFi9mfF+9qoUXl4inj9vq9KbjMgAkvLYMxWUGMU0th8v1Fv4E1DEfLZtebh3MZupIhWInOreD9DptgryPxFSR7ouLiJcyVf4JPdUkS3Z/RPB7H3cho+6NcUz7WviRxZQJOSZe5iuZlTgLOJmfh2+xX8cTwehcWiPmTcw3XwereGWHYwDn+dSMDwTtEAIJYwsOj1kQc2gJgU73+DW6FeqLcpsAGAYB8tgn3K2WVUVuWpS4g7ICZje3SGuLjJMzf2AhsA+G0YMGan/QyKPfIZTI1/GI3r6Oh15n78DuPFhGoXN4rRV5Ikusl+7ANcsyhwLM4Xc3F4B5c82kfulaNA3D7xh/fXp2DdWWkQt19cDPZ+BZxYJra5egIthwAHvhWPNW7AkwuUqztbfhv1tnERNo4Wqt5WzBI8aJFyBWdLNe9XfnZj5sb4MwtuaA5ufMKsZ8ktbRXxFzaLbquDC0TBrDyQc/cVWRljIDX8T9vHkAcBRpa/R5e3moNoW3Ub9rp2AGURr0eACErULqXPSixfa8rFTWQQru0C2o4U22xlY1rIfhZaO8FNnUfEDbCduenyHtB0gAgCXTxE5i5ur7mI291PdLUN/xM480fZghsA8C1jMOzqbh24yYObF3eIofRbDPV0lhP2GX+e8u5t4+fvME58oWj4uJjHBzBniexlbgCRcUu9IOZ3Api5Ibpdey+L0TRTV59EtyahuCUrKD6XaE6tShLQfY71MNP6oeKPw1NtovBUG+s/YIPaRmHJgTi0rmn7G1HvFiWknSuC/I+FcUhw86dt198Y5wIxrrdjLGC0p/1Lomsn6aThQmwnKCiPnBTzOjp+hm6NiFbiwjArSjx/85IIAOQX9wa9RE1AYbaYMdc7uOyZG48AoIVhOLarp/2C4rN/AmsnWW+X16M0Hyi+5frXtD+Tra3MTFY8AJW5mLhhLxHo2BsObLloZU6KKNA1TrIWVBcwTkZs6/06GIqvQ5sqJwsExMVd4yI+14OGiRjlxcdaXzG78pFfbLdNvl9pTv9uvu8baf28MVCp95i5kNhIXlArGYrT3bzsB+JewcDIv60zRM8sFRkk47m3DACGr1UuO1BatxRgu2vxgcmy52uImiN5d7E8eLE5TNpOVsNWsNzsKTFpYWAd86g811IyN+HNxe3aHrHMQ+M+tt9PPnzc2CaNq3nivW6zRDfu43PEY3lw42ljKRf552LNDVH5FRQrZ089GpuOtFx5cJNp+RL4e7qaJt0DxDwwJZnepwne7dUIXw6571+2tpziDgDLR1qv6yPP3PzUF1gzHthoMfIFEOu7WLKcqMxS475A5zfN+1qOqLJUvV3pK0TLi5gzDJkBv+oiGDMugnj9gHLIssYNeOIbcxp9/7fAqjHKRf/sUamVF+GSvjle2mK9rShXdIkYGf9QGyeSs5U5s7dqeGhT5YrI9rJuka2tL1L6IpEtMq49JL/w2+p6aP40MOWGubhTztaki/JjqNTAox+IIev97cwZA4hMga1ZjAHbRdP2RtwAYtbi8YeU2+QBgLHwtaRujdAmtru+tD5AjfbmomB58OIZJCZBlC/8aVlQbIs8cxPaVKy5JdfpFevXyH/29rI0ttgqyq/ZEZgSZ57TCBDZIns1N3JDlgMTjtvvnrOVuZHr8BLwbqJ5KgB5tkYe6Ng6BruliOwrKNZh+IIDaFXDH290N6eub2Yru00+XHsG6bnmi/rcLdbf9NtGV0NuYbFp4r3aQSUXG7q7aqxGWTlEUZ74Vm6v2NSYdTm5XAzbfGGz+KMiT9Ebg5WDC4AeHylfX5aaHLWr4XiGDE1EK3Gh84uyXR9jyTMQ6PM/0Z1gOVuusd7BVh2L8UIRVFdMjJZ+TRkgDFkm/mj7R4n6n2OLS2+Lkbu/ssvGzRMo+8TR4tu4/A+28Q/1g6+LoKu+jZmj7QU31S1WsLZ14QhuCAxabE7jA2I48vn14hu6MeiTr11kb6I2jYvtzIOXjQkj5UGfpBMB0NDVto8r5+6n7N40qtFRjJyTB6m2MjdGbl7WQZebJzDsD5F1Ma7TVVJwai/LYkkR3NgI9MoS3MgDtZEbrTMSrZ4Vq7xflk1eWFrmxjivjSVbmRtXT5FNkQcvru7Kc+DqaXu1drUaJeYuFJmbMpxTRbeUjeBGHnDdId1SzNzQHWnbuRTsuXwTX267hK1nk5FhCGCMBcFG9pY/8HQzL3rapmYAWtcw/4Fwc3HSr/0fE4E5zcxdNufWATcO29731lWxjhBgu7bCVpfU9YPW2yw9OgNoJFsfyUUr/oA+b2eODEtB9ey/v0ol+gItFw1Uu5iDAWM3VXqc+aJYo6MoZgbsL7bnbSeYAKy/udr65hjW3Pbkaq1HiCDGU34BMVzEXNzEIoLhNrr27BUMW76HrT/0j0wVhbPytabc/cQwezn5Ba+8F3xbwY1KJQphfSPN57ssbNXdAKJbbfRWseiikSI7ZLHwsJu3dVtdPUVQ036sOetiL9go7TnL9zKy9bMqS3CjcQVeOQKMP2i/q0V+bDdvZdBgnDAQAOr3AMbusR8U2/rdNB7Lw+J3U952y/qXsrLVLVUSeVeUzW4p2fku68+ogjG4oTtSfpG5+2nEwgN45jsxN41lcGNPU9mq2c2r++Olh+vi2fY18MOIto5taHkcXwJAEl1L33cTE9F9+7D9Il7jWi6WC+kBIuA5/DOw4R1RuAvYXkTRkl+USK/X6SK+MZu2VzcXUtpT5xFzzUHzQcrhpICo6Ti0UDkpoPHYasOFzpi1yog1Dx2Xp/ID69p4Y1XJQ6stv5nbCgRctEDNDtbb63cX+yu6pcrwTTaglvXF22ZbbFwUjUsDyL/tumitJ0STX7hKmt/I1sXE3kVv8BIxzLs8FyB7dTce1UQXUcwY8zaVSvxe1XoQGLFOub+bl/W8Lmob57CktpW1uF5+buWLeJreowzBDSA+X1A9+8/Lg5WAWsq5cuTnzScUCJWNXLJkK7tk/KyW3Yny308/G12DZSH/OZQpuJG1r9RuqTuj5obdUnTH2XEhRTGJHiDWdbqamoPULBEIdKwTiEsp2UjKtB3stIryx/CO0bh6Mwfta1eDSqXCh/1szKb5b9y6Kr4Fa2zMj5OZAHz/mChyfeQdkdGQi5NNJHjkJ6Dls7CScV0EQfJ9jSSdqL0BxMJ61duJ4sbS+EeJb9zP2RjmWtrw5edWme9rvYFnlwMzqysnSpMPCTVq0Mt839g9Jc/cyL/hBtq4kHj4l1y/YBnM2PtjXbOTsvgVMAcB8j/eZSmIVKuBwYuBRU9btNUyi2QruHGxbqfGzXo9LK1sRFOTJ+y3xdacLvaWAFCpbP++lsTeuTees+ZPiwJuY/F6rQfFTZIM3aCGbuOyXvTaPC+6WKu3Exmoc7I1tcoalMmzG7a6guTn4N90oyiCG4uRSfKu0tJGf9kK8oztkgdJxfnKc3A78ykB5c/cyPe3Gdzced1SDG7ojlKk0+O57/fbfO63g3Hw0opf2Uh/D3zxzH3QSRJWH7mBRfticTlVdN+8/EhdjH6wNnzcy/lHvDxOrhAjkh54Degy1fr5/d+I7MT2j4GO48UaRvasnSxGN1iytaCgPddtnzMrJS0oaS9lXpLgBtYjdSzJC16NRaOZN8wzuSoyN7I6EyPPIOUf4A7jgX1fmy+almsA2buIVreRtTP+oZb/wS7LmkKAqMUZuwdYNsw834xlcGMrUDJmbuQXBBd366HeKhUw+h8xqZ9lLY/iPWxlbv7dIq12jy+fFdk4zFilEgGJJZUh42ZcoqKs9TLNnhLHDmkkujRP/w6sHlO+Y8gzMyE2MiZ+1YGQJiLIKcuIMHsUwU20/f30OvvPGU06K9bvMq7pZAwS5L8XRfnKYLakiQZLUlpBsSX5e9r6nWa3FJGSce4ZI3s1NADw5bZL+GSDyE4E+WgR4OWGIG8tXnigNhaNao++LSOwYmwHTH6sgeMCm/N/izWM8jPFQpM754jtxqHWOz6180JZpmbps8DmGeK+ZxDwTpIYcSRPKcsnVCtN9ANl3xcAYLj4xIwRw6vtKamuxZ7+882FoJZc3IGHpijT+r4RInjQFZrnsJFnbmyl5z0DlX88Wz0LvCObSdkquLFTfBveEmhn0WVmDGrkx5cvR1Ga0MbKuUTKUv9j6paSXYCN3QSWs/96Vis5sAHKXnNzu+QXZvl940iaksgDi7Je9FQqMeW/u6+4kNaTdX/a+9laks88HdzA+nm1BhizAxi1tfSV0Usiz3aWGNyUkrkBxFw3NdqbH9vKgBTnKQM3vxK+rJREnokpS0Yt+gExv88jNr7IARbdUndG5obBDTnNwatpaDptA+b/Y57k60pK2RY8DPRS9t2H+bnj80Gt0LqmnRmCb10Ts9KW16KnxGRvK0aKJQs2TbO99s+WD4Hp/sD/2ohASD4s+8p2832/SPEHus//RO1D1xnlb5OtFYCb9Bf/2qqb8QwEHv8M6PF/1s/JlTQ7rNpOkjeonqixqGdjRNHgxcBDFlPLa1zN09Mbh52XNMMqILqOLEeIKLpWLC5OkixgNs5t0uZ58Q2458dAm5Hm540BhvwCV96JDOV/zK2CGxt/6I3nUv6ZjMOtbycosRnc3GahqS3yC7N8/qCyXMRuJ7ixVNZsjZy8zsbecGi1xjpbVl6lZW6M3awldSvKlda9U1ygzKLcdreULHOjcbO/n5FaIya2NM6ZZEkxWoqZG7rHTVhyFIU6PT5aJ+b1OHE9A+/9ftJqv0h/6//k/p52/kNmJQIHvjOvlQSIYOTz5sDCx22/BgAublYGIZbkk4/JF5ZUacQie6d/ByCJlZEvb7M/rb983Rm1Grh/ovJiK2frQvfYf5Tr0gBifZ6+XwIvbgcGfA/0/0b5fEkLW8rZSs8//K648A4qZWi2rfewl+43dj0Z14ySd0sB1gGfZzXlN3bLi6Rl5qZYVoc1dDXw/N+iANpIXt8gD2paDBZFssb5bcpK/i24PN1SWotuKUDM9aPRip9zWdm6CDo0c2Mj61Da5JBG8nqX2wlSAOWF2LJ2zZ6AaDFb7+Qy1KH9G/IRbbaGc7+wSSy62qBH2Y6n+J2w8XO1zGLedkGx7HfW3heX8lB0sZZzMdAKwuCGnMa4tpPRiIX7EZ+h7BKY2b8ZVr3UEU+2FulXX3cXPNW6Ono1s1P8umqMqGH563Vxkdv4HvA/Q1o/4ajtie4ybgC/DBCrQifKgiudnVSyvChV0gE/9VGu8RO7RwRZNtn441y3i/W2Oo8Ak86IFbSNhv4uZqWVfyt/4ltg/H5xEQ1vIQKBFgOBN2SLL5b1QmdrUrY2I4C3bwD1Hyv5tZbp8aZPKocIy8kX8wOU3VKACPjG7DI/9gxSBjCWwY1xVlojebeS1kd0cci/oYdbvL9Rv6+A187bnsejJPKZfy0/S0mjpWxdEGp2EOe74/iyv7+tbhVHZm4kWVfUkOWii2Lgz2V7rTxzc7vdFfLPJ+nt72cpvPnt1ZGVh4sWuP9V0VVqOdoNEIF7VNuyd33JJ0yUn69nfhMTLnafpcxEWc5yXVbybI2tYubyUsxQfJtBrIOxoJicwnKm4WKdXrHQpVGflhHw1rpg6uON0bpmAPq1jISHm43/jJIkajiME2od/UWMxjlrsWbOB0FAr9nmNWgA4Pw6mIKODVNEN4teb39SPMtuqWu7rB8X2qkdslXPUcPGEGWVWnS9yEequHqZ/0i+uF2sFt7sKdvv4xEgjiHpy565CawjMkB/vmpeQFPjVrbRNfIJz6q3BZ4soYDaMriwzNwAyouSu59yOn7jt84xu0RGrcM45WtL61ZqMQjITbVe0fh2RhIBykyRxuJPqq1vxaaVq2UXKcXK1g6oF7M1ouV2tXxWZDUj24j6F3kNTGnkBeK26qnKzQHLgzha1+mOO5b890Ue3NTvppxMsv834nlHBIyOyNy4aIGnfxJfHh3yc/73GNyQU5y8oVw7xjjSycjDVYOOdQLFYpYQK3cPbqABjv8kLk6W/6mPLQZWj1VuswxsjNZOEhf9FoPEBeacbD6OK9vFoopZCcCuz2/rsyHxhPIbZqM+oqhx9/+Ah9+x3t/mHwPDHx9FcCP7zOEtxM0elUq8Nu+W/WHBtrQaIha3PGUY9m1vZlxL8uDGo5Q/bqVlbiy3qTXKP8bG+2FNxc1SaSubqzXmNXQcoTwFyIAscyP7tqu3kVEsj+YDRdfq4CUia+OIAMl07KeBarXsz65bEo2rWNRUX/zvRtG0ek78f75v2O0f425gOT2APS3K2XVaEkcEN4BYyuUOwuCGKkxuYTE2nk5CtyZhcHdVZlt+2aucx2b/lTTT/f8+1QIDWtsYBbDlAxHEuGiBlharLa+1U+hmz1+vARc2igX3rlpkXta/Zfs17V8C9n5p/5gqtViDRr6e0xtXzMFL5zftX3TCWyjXUVKVEtyUhTG4KWmElC3yLqCyXiTl0+6XljUIqCX2MQ4Ft9WFIs+AqNSwKhouSVR7IP6IMhtSkYpLCaYsGS9a8sDRVndpefT/WgQQjgxqjFQqIKrd7b++Wi3l4/rdxVIT8i7X0vSdJ4riK+Lz3UnCmokuLp+IfzeKqzxsrdVVBbDmhirMpxvOY8KSo5i45ChuZhfgcOwt6PQSbqTnYc2xeACAj7u4iB24KoKbhmE+tgMbQMz3AYh6Fr0e+PkJ4Kd+ojbG3irQJQ1vvrBBzP5bbKj9sTeiwTsM6PVf4LEPlRmFVhYT72l9lfN9uHkrszIl/WF+5jfgIdn0+8ZvU4rgppwzfxrbWp7MDQBFIFHWP7DyzE1pQZhKJQqf274gLsr20tjGURe1Hyr73DOAmDTxkanASzYmP6wIkq70feSMvwfyc3s7WRG52+1Sc4a+XwKPvKucFLIs7pbP92+oVEDfL8TvcEUbugbo+SkQ3an0fe9CzNxQhflxz1UAwPpTiUjLKcT+q2moH+qNhxqEQKeXEFOrGmoGeuK3g9fx+1ER7ChGRkkScGyJmE8juAGQbSjSLcgSE4Nd2iweX9xovxGdXxcrTBtXWrb0lyHjo1KLUUinbMzcW62WuBADoi3GlaW7TBPFujs/E4/d/UQNzM7ZYlZZW4XC9viEAQ+9Kb597/kC6PKe2C4PaMoyc65c7YeAmxetC25LU55AwshyFtXS1HlY3Eoy4RiQFS+G9XoGiYkRy5L61vrYH7JaEbp/BCzsZV5dXa5+d/EtPCvevE2eUZp0RtQTlbSadlXjFSjW8yLnqt1Z3KooBjdUYWoHeeFCcjYAYL8hM3M+KRvnk8S2fq0iIUnAbwevm14TIQ9uLm42z0z6Vqx5QcbCbCBFto7SJhtzxTz6vii4bT5IBCYF2cDclkBOinK/o7+Kf7W+YijwlX+AM38o95EX/MonbPMKFhcveXCj9QZePiIuWLdTWPfIO0DnN8zfUhXTxJczuHl0hviGXN5vvLeTDpe/pqg8S3KXwDvY3KXmEwq8fvHO/PYeeZ/4/bTVNq03MPGEGIkXu1tsk9dS+EbcW4ENUSVhtxRVmOwC66HUasM10EWtQvcmYRjYNgovP2JeLLGacXK+7GTgxG/mF27+wHzRLMhSLhIpD3SMOk0ABi8yzxuh9RbDWO1x9xX7DPxF+c266wzlN/LWhoLG8Bbigi6vLzF2IWlcxLfT2+0zl18k5RfC27mw385rbidzA5iHwjZ7uuT9btedGNgYldQ2jYvyd+FO/hxEVQQzN+RQkiRBpVKhWKdHUqaye+LpNtUxqF0NrDuRgFY1AhBgCGQmdfBH9YsH8H5sUzxQL0gEL1/EAHnmImOc+8t8vyALSD4t7ldvB6ScAwqUo69ssqwFkQ8xlte2qF3Mo1fun6h8TUQr4KV95iLY8q4mXV7OuBDebnAzfC2Qch6o3sax7akSbqOOiYhuG4MbKpOEjDy4adQI9Nba3SevUIde/9uBJhF+eKtHQ+glkaFpVcMfB67ewgteO1H/XBLu6zFNMXGUasVIDEzegb5tBsM9+kkxHFse2ADm2WwBIC/dXMPQ+U2g1gNAYQ7wscWoDEuWwY1vdXNwo5UFN036A8cWAYF1YVNIQ/N9+Rwt5ZlgrKzKMjW6o93uxdfdT0xYRkTkZOyWolJlFxSjw6wtaP3hJkglTH++/2oaLqfk4I9j8bhqmLcm3N8dPz7fDmvGdUT9fW+LuWPerwasHA3cvCRWuTWsfu1+0jDFf/zRkht0bacoKNZoRfeQi7Zs9S2WNSvyWXXlmZvuM8WyA8+uKP2Y8tk9yztqpiyMa0WVdb4ZR2hsWKfqdqd2J2vM1hBVKmZuqFRxspW6swuKba64fSU1BwcMc9WooEfa7p9QXRWAcL+G8HRzQfMAi/qb40vFzXKF6+uHgI2GlWebDxTDrX/sbbthfeaWbw4XywBBEdzIRvt4BIhRVuWlr4DgJqyZWCOnMotO63UFRm0BqtUpfV8iojsQgxsqlU5vztakZBWI4Cb5LLD9E6DrNBT7VMfDn24z7fOmyxL0vvwnQlwbYmXgt2JjRixsMmRtTBbLFjhsPlAsCmlP7VKGEluy7JaSr8tib5HH8qiIzA0g1sipbJGtK/89iYgchMENlSoz3zx7auaNs8BPz5lrXgqzcfrBr+GLbHigEEkIwBgXsexBjPoswh821K2kx5XtzXIMw701bmKIra2VcQEAKuuZcDuMB/bMA2LG2H6JWiOOa1zo0NdOt1R5RcUAcfuAVkNv/xhUtbFbiqhSMbihUmXnm7uUfE/+qJiQTLq4GftTP8fvbr+iljoJC4q7m57L9whFjUBDnUuGIbip3lZMja+3s+I2IEZA9Z0nuocAUVtjuV6QZ6D1IoVdpwONettfjRpQjgSy1y1VXs+uEJksjhIiIrojsKCY7Dv9O7DmFeTkZJs25RQr14hS6YvwQtps1FInAQCed1lvek7rJhvpY8zc1OxYepdH+zFiRmIjW1kVrY1h1xpXoEZ7wKWEEUbygmh5cPNvuqW0PmKUEL+dk1383SCqTMzckG16HfCb6GapXdsNq922oACu8L5V9u4bVWEOcGwpsGm6ea0kvyjA86L1zm4+wH3PiYxM437K59z9zN1VRuWdrdcWL1kxslpjfz8iIrqrMLgh264fNN1tcflbc47PMC3MXn0jtFfbmBlYrjAbWDVauc2/hu1h27U7A91n2T6OfC4Zo/KukG0iy9zI12oqqZuM6N9iVo+oUrFbigSL+WtyT/5Z4u4/Fz9a+jGNhbty3qHm2X3lShrqbOyWUv+LdZaM7M3TExB9e8cjKhMGN0SVicENAXm3xKKS66eYNmVfOaDYZU7xEyiQzMFFgnQbi0ICotvJw8ZrG/S0/xpjcOMdYt7miG4pQCwZ0OMToFbVXR2XiOhew+CGgP3fAreuAnu/NG0qzBSrZ79eNBo9C2ZiTvGTiJPMNSr1a5ey1IE9XkHm+hu5koILU3ATat4WUNP2vqWyyNxE3w/EjGa3AVWsbjNFQP7Q285uCdE9gTU3JFbgtqAtEsU15/RROC1FAwBSJH/UhRgG3q1dM+DvYCAnpezv4+olamXkazDVfRRoPxZQlxBnu/uLf33CgAHfA8eWAA9Nsb9/SUpYPoKowoQ2Bt6Ks56+gIgqBP+nkeiWsuCjF8HNLZiHXGfDvHxBuwY1gMj1wLxyzGRrnHSvjmFmYa0v8Ozy0l9nXMAypBHQ7ElxI7rbMLAhqjT833avybgBnF8HtHjGPFrIcgXuojy4QxQDh4RGIi5RjCSKDg8BDEkeL3dXwL2ueVbgsvAyBDehTYAxOwGf8LK9rsUgEeCEtyjb/iVi5oaIqKpjzU1VtmkGsP1Tcd/YHfPtw8DayaK+pjBHFBFf2mJ+TdIp6BeKhSqLJA1a14syPaWta7HIJVC+1aqN3UuAWBDS1qgpW9QaoEYM4FqJK2MTEdFdi5mbqirtMrBztrhf7zHg1yeBji8D2WImYWz5QNwsFC8eApf0KwCAdHijfZ1AfLNDPC5o9izgpRdFuEYu2rK36Y6YKI+Fw0REVR0zN1VFdjKw7xsg56Z4nHHD/NzKUSKo+fvdUg9jDGwAIFPlg6YR5hmJPdzdgE6viAUtjTQlLHVg5Q4ILJ75TSzG2W++s1tCREQVhMFNVbHjv8C614H/1geyksTQbqOUc3ZfJjUZYPe5HI0fgn206N0iAg/UC0KEn41ZgcvTLaW6A37d6nUFplwHWg52dkuIiKiCsFuqqkg4Jv7VFwPHFgMFWbIn7RfRnqr+NJqeWmF6nC+5wl1VBABQa1yhUqnwv8Gt7L9vSYtUWtK4lr5PZeCoFSKiKu0O+CpNDpEZb76fcFSZuSnBFU0tdC6YjUJJ1MMYAxsA8Fdl2XuZmaYMNTed3wTcvIFHppapTURERP8Gg5uqQFcEZFw3P44/Cty6Ynd3IwkqzNhwDdekMMRKoVbPexVnlP7eZSkofvht4M1rQEjD0vclIiL6lxjcVAUZ1wFJZ35864oIcEqRJXkgNUdkavJg3b20qdrA0t9bHtzIl1V4YLL4t+Hj4l92BRERUSXhFacqMGZpghoAugLRJSUPduzIgrlAOA/KDMycoBkYMHBk6e8t75Zy8QAKDV1Z908C6vcAwpqWfgwiIiIHYnBTFaQZgptqtcTEfGWst8mWzMFNvqTM3EwcO65sBcDyzI2L1hzcuHkBUW3L1A4iIiJHYrfU3U6vAw7/JO6HNDKvoF0G+WpPNK8u9s+Xd0u5eZd9ZJMiuJENC+cq20RE5CQMbu525/4So6O0fkD7l8RilEaunoBvJBBheyi33s0bv4/rhMmP1ld2S8mXSSiNfBK/Bj0AvxrmOhsiIiInYHBzt7t5SfzboAfgHQK4y4Ibn3BgwnFg1FbcrPcUAKDA1d/0tKubO1QqFZ5uG4VitSy48TDvUyp5tsbDH5hwFBj4S3k/BRERkcMwuLlbZdwQSy3kporHxkUo5d1S7r5ilJJKhekFQ/Bx0UB0zZ5mftpVlFyF+rqjT5s6stf5l70dljU3ag27pIiIyKlYUHw3KsgGPmss7rcwLCPgGSj+lXdLGe4fupaGk2kq/KHrqziMp5v5x+/q7mV+ojyZG3m3VFkm9CMiIqpgDG7uRlkJ5vvXD4p/7WRuTlzPwICv9tg8TIifrEvJRbZuVEB02dsi75Yq1yKaREREFYPdUnebvFvArWvmxzcviH89jcGNOXNzS+eOnRdTFS/f93YX030XtezH7yoLbkIal7098rWl2B1FRER3AGZu7jb/bQgU51tvN3RLFbr4mAZ1pxRpkZCRp9gt1Fc+XFse3Hia74c0Knt72BVFRER3GGZu7iYF2bYDG8DULRWfb86k5Ku9cDE52/T4P/0tZguuLptkr9C8H4IblL1NZVlbioiIqBIxc3OnO7Ua+Os1oO8XgE+Y/f08qwEAYnM0iDZsWnchB7sLbwIAVr3UEa1qBIgnxu4GLm0B2r1ofn2OrPvKTVZcXBp2RRER0R2GmZs7WW4asGwYkJMC7JsPpF6wv6+7PzLyirDkRKZpU4bOnFWpG+Jt3je0CdDxZWW9TNuRYiLA+1915CcgIiKqdMzc3MmOLTHfv3FIZFvsUanw0boz2Hg5HzCU1eghsir1Q73h417KcgqBdYA3LnP1biIiuusxc3MnSzppvp+fUeruf59KQpEsXlVBQuNwX/w8MqZs7/dvAxtJ+nevJyIicgAGN3ey5NMlP+8XJSbq6zAeABDkrSzuzZI88cR9kcoRUhWh5bOAdyjQYmDFvg8REVEZsA/iTqQrAv6YAMQfKXm/4AbAM7+JJQ8ApOcVAgAut52G03s3YJ2+HZ4ILkdx8O3q94VYndzQDiIiImdyeubmiy++QHR0NNzd3RETE4P9+/eXuP+cOXPQoEEDeHh4ICoqCq+++iry8+0Mj75bXdsNHP3V9nOhzcz3NVokZhVh4Nd78PepRNzKLQIAuHYYg3dcJkMHDVpFBVRCg8HAhoiI7hhOzdwsXboUkyZNwvz58xETE4M5c+agW7duOHfuHEJCQqz2X7RoEd566y0sWLAAHTt2xPnz5zF8+HCoVCrMnj3bCZ+gghRk2X/ONxxIOiHuu7jh/9afxb4radh3Jc20S4CXGzZP7oz8Ih0CvLgkAhER3VucmrmZPXs2Ro0ahREjRqBx48aYP38+PD09sWDBApv77969G506dcIzzzyD6OhoPPbYYxg8eHCp2Z67iiQpJ9TrM0/5vIcsE6PRIjW7QPG0q0YFLzcNgry1qB7gCSIionuN04KbwsJCHDp0CF27djU3Rq1G165dsWeP7YUeO3bsiEOHDpmCmcuXL+Ovv/5Cz5497b5PQUEBMjMzFbc71m9Dga86Abli4j006gO0HGJ+Xu2inBHYxQ23cgsVh/DzcIOKE+sREdE9zGndUqmpqdDpdAgNDVVsDw0NxdmzZ22+5plnnkFqairuv/9+SJKE4uJijBkzBm+//bbd95k1axZmzJjh0LZXCEkCTv8u7l/YKP7V+gDyxS1VGsUq3JJGq1heAQD8PUuZz4aIiKiKc3pBcXls27YNM2fOxJdffonDhw9j5cqVWLt2LT744AO7r5kyZQoyMjJMt7i4uEpscTnI62yMmRs3b+U+ag2gMdfQ3MwH8ov0il0CGNwQEdE9zmmZm6CgIGg0GiQlJSm2JyUlISzM9hpKU6dOxXPPPYcXXngBANCsWTPk5ORg9OjReOedd6BWW8dqWq0WWu1dsLhj3i3z/WzDOdH6KPdRqRWZm+VHxX5ebhrkFOoAAN5aju4nIqJ7m9MyN25ubmjdujU2b95s2qbX67F582Z06NDB5mtyc3OtAhiNRgxBlu722XHz0833TcGNjcyNrOYmXxKBzJfPtjZtu5mjrMEhIiK61zj1a/6kSZMwbNgwtGnTBu3atcOcOXOQk5ODESNGAACGDh2KyMhIzJo1CwDQu3dvzJ49G61atUJMTAwuXryIqVOnonfv3qYg566Vl269zSpzo8GB6zloa3hYKLmiZ7MwdK4fbNrlxq28CmsiERHR3cCpwc3AgQORkpKC9957D4mJiWjZsiXWr19vKjKOjY1VZGreffddqFQqvPvuu7hx4waCg4PRu3dv/Oc//3HWR3AceebGyE0EN4WNnoDbmZUo6vgq1q6/gLaGspoCuKBWkJiB+Ln2NfHz3muY+Gj9SmowERHRnUkl3fX9OeWTmZkJPz8/ZGRkwNfX19nNMTv0I/DHK8ptgxYBDXth4JfbkB93DMH12yP44lLMcv0eADC1aDia938NT7WJQpFOjwtJ2WgU7sOh4EREVOWU5/rN6tM7ha3MjaFbal9sDoC6wLlUDFCbR0MVwhW1DWtHuWrUaBxxBwVrRERETnJXDQWv0mzU3GTo3fHCjwcV2wpgDm6KJA2iAythYUwiIqK7CDM3dwr5UHCDFSfTselMkWJboexH5u/jjWpcO4qIiEiBmZs7hY1uqc2XrUc+FcAczLzWqznra4iIiCwwuLlT2OiWOpZcDAB44f5auK+GPwCgS9Mapuc9PbgwJhERkSV2S90pbGRusiUtagd54d3HGyM+PQ8Hr91CL/844LxhBw27pIiIiCwxc3MnyIwHUgwRS+e3ZE+oEFO7GgAgwt8DfVpEQONmXn5BsUI4ERERAWDmxvm2fwpsMSz8Gd4S6PwGEFwf07fcBOKBVlEByv01soCGmRsiIiIrzNw4U8p5YNsswwMV8PhngFqD4kb9sTRF1Na0MtTamLgwuCEiIioJMzfOdGghoC8Goh8Anl0JuIhg5VR8JvKKdPDRuqBOsMXimfLght1SREREVhjcOFNGrPi3UR/AxQ1JmflIyynE8kPXAQAP1g+GWm0x1NtFVnOjvssXCyUiIqoADG6cobhAZG2u7RGPfcJw8kYGnvl2LzLzi027DW5Xw/q17IoiIiIqEYMbZ9jzBbB5hvmxTzgm/3ZMEdg0CvdFxzqB1q+VZ260fhXYSCIiorsTgxtnuLRF8fBGsS/OJSXDRa3ClskPoVCnQ1Q1T+suKQDQuACDlwLF+YCXjeCHiIjoHsfgxhksCoG3xosg5r6aAagRWIZZhxt0r4hWERERVQkcCu4MxQWKh9suZgIAHmoQ7IzWEBERVSkMbpwhPVbx8EyCCG7a1KzmjNYQERFVKQxuKpteB2TeUGy6kS5W/64X4m3rFURERFQODG4qW1aimLjPIDeoGQAg0MsNAV4c5k1ERPRvsaC4st28KP5194Ou0yTMT2oOXM9DHWZtiIiIHIKZm8okScA/H4v79R7D53k9MPeg6JKqy+CGiIjIIcod3ERHR+P9999HbGxs6TuTUspZ4NpOQKOF9Mi7WHowzvRUo3BfJzaMiIio6ih3cDNx4kSsXLkStWvXxqOPPoolS5agoKCg9BcSkBkv/g2qh+PZ/kjKFOfttcfqY8B9kU5sGBERUdVxW8HN0aNHsX//fjRq1Agvv/wywsPDMX78eBw+fLgi2lh15KSKf72C8NeJBADA483DMf6RevB0Y/kTERGRI9x2zc19992HuXPnIj4+HtOmTcN3332Htm3bomXLlliwYAEkSXJkO6uGnBQAgOQVjLWG4KZXs3BntoiIiKjKue10QVFREVatWoUffvgBGzduRPv27TFy5Ehcv34db7/9NjZt2oRFixY5sq13P0Nwkyr54vqtPHi4avBQgxAnN4qIiKhqKXdwc/jwYfzwww9YvHgx1Go1hg4dis8++wwNGzY07dO/f3+0bdvWoQ2tEgzdUrH5XgCATnUD4eGmcWaLiIiIqpxyBzdt27bFo48+iq+++gr9+vWDq6ur1T61atXCoEGDHNLAKiUnGQCQUCyCm5qBXs5sDRERUZVU7uDm8uXLqFmzZon7eHl54YcffrjtRlVVUk4KVABWnS8EAFQP8HBug4iIiKqgchcUJycnY9++fVbb9+3bh4MHDzqkUVVVYUYSAOCm5AcAqB7g6czmEBERVUnlDm7GjRuHuLg4q+03btzAuHHjHNKoKkmS4JJ/EwCQCjFhX6Q/MzdERESOVu7g5vTp07jvvvustrdq1QqnT592SKOqpMJsaHRi0r6bkiG4YbcUERGRw5U7uNFqtUhKSrLanpCQABcXTkRnV6aY1yZL8kAe3AEAfh7WxdhERET075Q7uHnssccwZcoUZGRkmLalp6fj7bffxqOPPurQxlUpGWItrhtSkJMbQkREVLWVO9Xy6aef4sEHH0TNmjXRqlUrAMDRo0cRGhqKn3/+2eENrDLSRZ3SDSkI0YGemPeMddceERER/XvlDm4iIyNx/Phx/Prrrzh27Bg8PDwwYsQIDB482OacN2SQYQ5uXuvWAE0j/ZzcICIioqrptopkvLy8MHr0aEe3pWrLuA4AiJcCUd9b6+TGEBERVV23XQF8+vRpxMbGorCwULG9T58+/7pRVZKsWyrYh8ENERFRRbmtGYr79++PEydOQKVSmVb/VqlUAACdTufYFlYRUnosVGBwQ0REVNHKPVpqwoQJqFWrFpKTk+Hp6YlTp05h+/btaNOmDbZt21YBTawCspKArHgAQKZ7BHy0HDJPRERUUcp9ld2zZw+2bNmCoKAgqNVqqNVq3H///Zg1axZeeeUVHDlypCLaeXc7+gtUkh6H9PXQvEkDU5aLiIiIHK/cmRudTgcfHx8AQFBQEOLjRUaiZs2aOHfunGNbV1Uc/w0AsFj3CLo0CnVyY4iIiKq2cmdumjZtimPHjqFWrVqIiYnBxx9/DDc3N3zzzTeoXbt2RbTxridl3oAKwCF9fbxTJ9DZzSEiIqrSyh3cvPvuu8jJyQEAvP/++3j88cfxwAMPIDAwEEuXLnV4A+96ej1QkA0AKNR4wd+TcwERERFVpHIHN926dTPdr1u3Ls6ePYu0tDQEBASwlsSWwmyoIEaUab39eY6IiIgqWLlqboqKiuDi4oKTJ08qtlerVo0XbXsKMgEAhZIG/oZaJSIiIqo45QpuXF1dUaNGDc5lUx75IrjJgidCfD2c3BgiIqKqr9yjpd555x28/fbbSEtLq4j2VD0FWQCAbMmDk/cRERFVgnLX3MybNw8XL15EREQEatasCS8vL8Xzhw8fdljjqoQCWeaGwQ0REVGFK3dw069fvwpoRhWWnwEAyJI8EeLL4IaIiKiilTu4mTZtWkW0o+oydkuB3VJERESVodw1N1ROhm6pTHggxMfdyY0hIiKq+sqduVGr1SUO++ZIKiV9XibUEN1SzNwQERFVvHIHN6tWrVI8LioqwpEjR/Djjz9ixowZDmtYVVGQfQseALLhiUAvN2c3h4iIqMord3DTt29fq21PPvkkmjRpgqVLl2LkyJEOaVhVUZCTDg8AejcfuGjYC0hERFTRHHa1bd++PTZv3uyow1UZRblitBTcfZ3bECIionuEQ4KbvLw8zJ07F5GRkY44XJUiGYaCu3j4ObklRERE94Zyd0tZLpApSRKysrLg6emJX375xaGNqxIMQ8G1XgxuiIiIKkO5g5vPPvtMEdyo1WoEBwcjJiYGAQEBDm1cVaApFMGNuw/PDRERUWUod3AzfPjwCmhG1eWmywYAePlWc3JLiIiI7g3lrrn54YcfsGzZMqvty5Ytw48//uiQRlUZkgR3XQ4AwMcv0MmNISIiujeUO7iZNWsWgoKCrLaHhIRg5syZDmlUlVFcAFcUAwC8/dgtRUREVBnKHdzExsaiVq1aVttr1qyJ2NhYhzSqyjAsvaCXVPD09nduW4iIiO4R5Q5uQkJCcPz4cavtx44dQ2Agu14U8kVwkw13eHtwdmIiIqLKUO7gZvDgwXjllVewdetW6HQ66HQ6bNmyBRMmTMCgQYMqoo13Lb0puPGAt7bctdtERER0G8p9xf3ggw9w9epVdOnSBS4u4uV6vR5Dhw5lzY2F/Oxb8IRYNLOmO4MbIiKiylDuK66bmxuWLl2KDz/8EEePHoWHhweaNWuGmjVrVkT77moFhuAmG57QunBdKSIiospw2+mEevXqoV69eo5sS5VTmJMOAMhXeyomPiQiIqKKU+50woABA/B///d/Vts//vhjPPXUU7fViC+++ALR0dFwd3dHTEwM9u/fb3ffhx56CCqVyurWq1ev23rvilSYI9aVytd4O7klRERE945yBzfbt29Hz549rbb36NED27dvL3cDli5dikmTJmHatGk4fPgwWrRogW7duiE5Odnm/itXrkRCQoLpdvLkSWg0mtsOrCqSLk8ENwUMboiIiCpNuYOb7OxsuLlZD2t2dXVFZmZmuRswe/ZsjBo1CiNGjEDjxo0xf/58eHp6YsGCBTb3r1atGsLCwky3jRs3wtPT844MbvSG4KbIlcENERFRZSl3cNOsWTMsXbrUavuSJUvQuHHjch2rsLAQhw4dQteuXc0NUqvRtWtX7Nmzp0zH+P777zFo0CB4eXnZfL6goACZmZmKW2WRDCuC6xncEBERVZpyFxRPnToVTzzxBC5duoRHHnkEALB582YsWrQIy5cvL9exUlNTodPpEBoaqtgeGhqKs2fPlvr6/fv34+TJk/j+++/t7jNr1izMmDGjXO1yFJVhhmKdm69T3p+IiOheVO7MTe/evbF69WpcvHgRL730EiZPnowbN25gy5YtqFu3bkW00a7vv/8ezZo1Q7t27ezuM2XKFGRkZJhucXFxldY+l4JbAAC9u1+lvScREdG97raGgvfq1cs0OikzMxOLFy/Ga6+9hkOHDkGn05X5OEFBQdBoNEhKSlJsT0pKQlhYWImvzcnJwZIlS/D++++XuJ9Wq4VWqy1zmxzJoyAVAFDsGVrKnkREROQotz2z3Pbt2zFs2DBERETgv//9Lx555BHs3bu3XMdwc3ND69atsXnzZtM2vV6PzZs3o0OHDiW+dtmyZSgoKMCzzz57W+2vDN6FIriRvBncEBERVZZyZW4SExOxcOFCfP/998jMzMTTTz+NgoICrF69utzFxEaTJk3CsGHD0KZNG7Rr1w5z5sxBTk4ORowYAQAYOnQoIiMjMWvWLMXrvv/+e/Tr1+/OXayzMBce+mwAgMon3MmNISIiuneUObjp3bs3tm/fjl69emHOnDno3r07NBoN5s+f/68aMHDgQKSkpOC9995DYmIiWrZsifXr15uKjGNjY6FWKxNM586dw86dO/H333//q/euUNmJAIBcSQtXT9bcEBERVZYyBzfr1q3DK6+8grFjxzp82YXx48dj/PjxNp/btm2b1bYGDRpAkiSHtsHhskRwkyz5w1Pr6uTGEBER3TvKXHOzc+dOZGVloXXr1oiJicG8efOQmppakW27uxmCmyQEwNNN4+TGEBER3TvKHNy0b98e3377LRISEvDiiy9iyZIliIiIgF6vx8aNG5GVlVWR7bz7yDI3HgxuiIiIKk25R0t5eXnh+eefx86dO3HixAlMnjwZH330EUJCQtCnT5+KaOPdKSsBAJAsMXNDRERUmW57KDggal8+/vhjXL9+HYsXL3ZUm6qG3DQAwE3Jl8ENERFRJfpXwY2RRqNBv379sGbNGkccrmooFMPAs+EOd1cGN0RERJXFIcENWZMKcwAAedDC0+22JoImIiKi28DgpoLoCkTmJkdyZ7cUERFRJWJwU0GkwlwAQJ5KC60LTzMREVFl4VW3ohgyNzqNJ1QqlZMbQ0REdO9gcFNRikTmRu/q6eSGEBER3VsY3FQQVZEoKJYY3BAREVUqBjcVRF2cJ+64eju3IURERPcYBjcVobgQan2RuO/GzA0REVFlYnBTEQxdUgCgcvNyYkOIiIjuPQxuKoJhGHihpIFW6+7kxhAREd1bGNxUBNnsxFwRnIiIqHIxuKkIhm6pHHB2YiIiosrG4KYiGDM3EteVIiIiqmwMbiqCoeYmhyuCExERVToGNxWhSL4iOIMbIiKiysTgpiIYuqW4IjgREVHlY3BTEQzdUrkcLUVERFTpGNxUhEKxIniepIUHa26IiIgqFYObilBkLihmtxQREVHlYnBTERST+HEoOBERUWVicFMRWFBMRETkNAxuKoKhWyoPrLkhIiKqbAxuKkKhefkFjpYiIiKqXAxuKoIhuMmVOIkfERFRZWNwUwH0soJiT1cWFBMREVUmBjcVQCpgtxQREZGzMLipAJJhEr9ClTvcXHiKiYiIKhOvvBVAZRgtpXPxdHJLiIiI7j0MbiqAMbiRXBncEBERVTYGN46m10NdnCfuuzG4ISIiqmwMbhzNkLUBALh5O68dRERE9ygGN45mGAaul1TQMHNDRERU6RjcOFqRYQI/LppJRETkFAxuHE2xIjjnuCEiIqpsDG4crVDU3HBFcCIiIudgcONoRbKlFxjcEBERVToGN45WaK65cXdlcENERFTZGNw4GruliIiInIrBjaMpuqU4WoqIiKiyMbhxtOJCAEABXOHBbikiIqJKx+DG0XQiuCmCC7uliIiInIDBjaMZgptCyYXz3BARETkBgxtH0xUBEJkbdksRERFVPgY3jqbolmJBMRERUWVjcONopuBGw24pIiIiJ2Bw42iybikWFBMREVU+BjeOZszcSKy5ISIicgYGNw4mcSg4ERGRUzG4cTC9YRK/QtbcEBEROQWDGwfTFRUA4FBwIiIiZ2Fw42C6IpG5kVSucNHw9BIREVU2Xn0dzNgtBRc35zaEiIjoHsXgxsGMwY2awQ0REZFTMLhxMONoKRWDGyIiIqdgcONgkjFz46p1ckuIiIjuTQxuHM2QudEwc0NEROQUDG4czbD8AjM3REREzsHgxtGYuSEiInIqBjcOptaLzI2rK4MbIiIiZ2Bw42AqQ3CjdmG3FBERkTMwuHEwY+ZGzcwNERGRUzC4cTC1JIIbDQuKiYiInILBjYNpDJkbFwY3RERETuH04OaLL75AdHQ03N3dERMTg/3795e4f3p6OsaNG4fw8HBotVrUr18ff/31VyW1tnQaqVj8y+CGiIjIKVyc+eZLly7FpEmTMH/+fMTExGDOnDno1q0bzp07h5CQEKv9CwsL8eijjyIkJATLly9HZGQkrl27Bn9//8pvvC16HdTQAwBc3FhzQ0RE5AxODW5mz56NUaNGYcSIEQCA+fPnY+3atViwYAHeeustq/0XLFiAtLQ07N69G66urgCA6OjoymxyyQwT+AGAi6u7ExtCRER073Jat1RhYSEOHTqErl27mhujVqNr167Ys2ePzdesWbMGHTp0wLhx4xAaGoqmTZti5syZ0Ol0ldXskhkm8AMAV3ZLEREROYXTMjepqanQ6XQIDQ1VbA8NDcXZs2dtvuby5cvYsmULhgwZgr/++gsXL17ESy+9hKKiIkybNs3mawoKClBQUGB6nJmZ6bgPYUmWuXHVsluKiIjIGZxeUFweer0eISEh+Oabb9C6dWsMHDgQ77zzDubPn2/3NbNmzYKfn5/pFhUVVXENNGRuiiQNtIZuMyIiIqpcTgtugoKCoNFokJSUpNielJSEsLAwm68JDw9H/fr1odFoTNsaNWqExMREFBYW2nzNlClTkJGRYbrFxcU57kNYMgY3cIHW5a6KG4mIiKoMp12B3dzc0Lp1a2zevNm0Ta/XY/PmzejQoYPN13Tq1AkXL16EXq83bTt//jzCw8PhZmd0klarha+vr+JWYQzdUkXQQOuiKWVnIiIiqghOTS9MmjQJ3377LX788UecOXMGY8eORU5Ojmn01NChQzFlyhTT/mPHjkVaWhomTJiA8+fPY+3atZg5cybGjRvnrI+gJM/cuDJzQ0RE5AxOHQo+cOBApKSk4L333kNiYiJatmyJ9evXm4qMY2NjoVabg4SoqChs2LABr776Kpo3b47IyEhMmDABb775prM+ghK7pYiIiJxOJUmS5OxGVKbMzEz4+fkhIyPD8V1UcQeA77siVh+MvJeOoEGYj2OPT0REdI8qz/Wb6QVHYuaGiIjI6XgFdiRDcFMIF7i7sqCYiIjIGRjcOJCuSEwWyMwNERGR8/AK7EDFhuCmGBqOliIiInISXoEdqKggHwCQL7nBTcNTS0RE5Ay8AjuQrjAXAFCgcoMLgxsiIiKn4BXYgYoLRHBTrOKimURERM7C4MaB9IV5AIAitdbJLSEiIrp3MbhxIJ0xuFExuCEiInIWBjcOpC8SwU0xMzdEREROw+DGgaQiMVpKp3Z3ckuIiIjuXQxuHEgydEvpNMzcEBEROQuDG0cqZnBDRETkbAxuHEhVbOyWYnBDRETkLAxuHEilMwQ3zNwQERE5DYMbB1IXG9aWYuaGiIjIaRjcOJDalLnhaCkiIiJnYXDjQGqdyNzoGdwQERE5DYMbB9IYMjd61twQERE5DYMbBzJmbiRmboiIiJyGwY0DuegNmRsXZm6IiIichcGNA2n0hQAAycXDyS0hIiK6dzG4cSCNsVvKhd1SREREzsLgxlEkCa4SgxsiIiJnY3DjKIalFwAADG6IiIichsGNoxTlme8zuCEiInIaBjeOYsjcFEtqqF3cnNwYIiKiexeDG0cxBDf5cIOrRuXkxhAREd27GNw4SpE5uHHR8LQSERE5C6/CjlIsam7y4QYXNTM3REREzsLgxlF0RSiGCwokV7gyc0NEROQ0vAo7So32eKn2Bjxa+AlcWHNDRETkNAxuHKhYL0EPNbuliIiInIjBjQMV6fQAABc1TysREZGz8CrsQMU6CQDYLUVEROREDG4cqFgvMjcsKCYiInIeXoUdqFhvyNyw5oaIiMhpGNw4kLFbipkbIiIi5+FV2IFMBcWsuSEiInIaBjcOZOyW0rBbioiIyGkY3DhQsY4FxURERM7Gq7ADFelYUExERORsDG4ciEPBiYiInI9XYQfS6TmJHxERkbMxuHEgc7cUTysREZGz8CrsQOaCYmZuiIiInIXBjQMVmbqleFqJiIichVdhByo2rQrOzA0REZGzMLhxEL1egiFxw+CGiIjIiRjcOIhxdmKA3VJERETOxKuwgxjnuAFYUExERORMDG4cxDgMHOBQcCIiImfiVdhBjMXEADM3REREzsTgxkHkK4KrVAxuiIiInIXBjYMUcRg4ERHRHYHBjYMUc0VwIiKiOwKDGwcp5uzEREREdwReiR3EOBScxcRERETOxeDGQYq5IjgREdEdgVdiBzEVFDNzQ0RE5FQMbhxEAuDhqoGHq8bZTSEiIrqnuTi7AVXFfTUCcOaD7s5uBhER0T2PmRsiIiKqUhjcEBERUZXC4IaIiIiqFAY3REREVKUwuCEiIqIqhcENERERVSkMboiIiKhKuSOCmy+++ALR0dFwd3dHTEwM9u/fb3ffhQsXQqVSKW7u7u6V2FoiIiK6kzk9uFm6dCkmTZqEadOm4fDhw2jRogW6deuG5ORku6/x9fVFQkKC6Xbt2rVKbDERERHdyZwe3MyePRujRo3CiBEj0LhxY8yfPx+enp5YsGCB3deoVCqEhYWZbqGhoZXYYiIiIrqTOTW4KSwsxKFDh9C1a1fTNrVaja5du2LPnj12X5ednY2aNWsiKioKffv2xalTp+zuW1BQgMzMTMWNiIiIqi6nBjepqanQ6XRWmZfQ0FAkJibafE2DBg2wYMEC/P777/jll1+g1+vRsWNHXL9+3eb+s2bNgp+fn+kWFRXl8M9BREREdw6nd0uVV4cOHTB06FC0bNkSnTt3xsqVKxEcHIyvv/7a5v5TpkxBRkaG6RYXF1fJLSYiIqLK5NRVwYOCgqDRaJCUlKTYnpSUhLCwsDIdw9XVFa1atcLFixdtPq/VaqHVav91W4mIiOju4NTgxs3NDa1bt8bmzZvRr18/AIBer8fmzZsxfvz4Mh1Dp9PhxIkT6NmzZ5n2lyQJAFh7Q0REdBcxXreN1/ESSU62ZMkSSavVSgsXLpROnz4tjR49WvL395cSExMlSZKk5557TnrrrbdM+8+YMUPasGGDdOnSJenQoUPSoEGDJHd3d+nUqVNler+4uDgJAG+88cYbb7zxdhfe4uLiSr3WOzVzAwADBw5ESkoK3nvvPSQmJqJly5ZYv369qcg4NjYWarW5NOjWrVsYNWoUEhMTERAQgNatW2P37t1o3Lhxmd4vIiICcXFx8PHxgUqlcuhnyczMRFRUFOLi4uDr6+vQY5MZz3Pl4bmuHDzPlYPnufJUxLmWJAlZWVmIiIgodV+VJJUlv0NlkZmZCT8/P2RkZPA/TgXiea48PNeVg+e5cvA8Vx5nn+u7brQUERERUUkY3BAREVGVwuDGgbRaLaZNm8ah5xWM57ny8FxXDp7nysHzXHmcfa5Zc0NERERVCjM3REREVKUwuCEiIqIqhcENERERVSkMboiIiKhKYXDjIF988QWio6Ph7u6OmJgY7N+/39lNuuts374dvXv3RkREBFQqFVavXq14XpIkvPfeewgPD4eHhwe6du2KCxcuKPZJS0vDkCFD4OvrC39/f4wcORLZ2dmV+CnubLNmzULbtm3h4+ODkJAQ9OvXD+fOnVPsk5+fj3HjxiEwMBDe3t4YMGCA1eK2sbGx6NWrFzw9PRESEoLXX38dxcXFlflR7nhfffUVmjdvDl9fX/j6+qJDhw5Yt26d6Xme54rx0UcfQaVSYeLEiaZtPNeOMX36dKhUKsWtYcOGpufvqPNc3rWgyNqSJUskNzc3acGCBdKpU6ekUaNGSf7+/lJSUpKzm3ZX+euvv6R33nlHWrlypQRAWrVqleL5jz76SPLz85NWr14tHTt2TOrTp49Uq1YtKS8vz7RP9+7dpRYtWkh79+6VduzYIdWtW1caPHhwJX+SO1e3bt2kH374QTp58qR09OhRqWfPnlKNGjWk7Oxs0z5jxoyRoqKipM2bN0sHDx6U2rdvL3Xs2NH0fHFxsdS0aVOpa9eu0pEjR6S//vpLCgoKkqZMmeKMj3THWrNmjbR27Vrp/Pnz0rlz56S3335bcnV1lU6ePClJEs9zRdi/f78UHR0tNW/eXJowYYJpO8+1Y0ybNk1q0qSJlJCQYLqlpKSYnr+TzjODGwdo166dNG7cONNjnU4nRURESLNmzXJiq+5ulsGNXq+XwsLCpE8++cS0LT09XdJqtdLixYslSZKk06dPSwCkAwcOmPZZt26dpFKppBs3blRa2+8mycnJEgDpn3/+kSRJnFNXV1dp2bJlpn3OnDkjAZD27NkjSZIIQtVqtWlxW0mSpK+++kry9fWVCgoKKvcD3GUCAgKk7777jue5AmRlZUn16tWTNm7cKHXu3NkU3PBcO860adOkFi1a2HzuTjvP7Jb6lwoLC3Ho0CF07drVtE2tVqNr167Ys2ePE1tWtVy5cgWJiYmK8+zn54eYmBjTed6zZw/8/f3Rpk0b0z5du3aFWq3Gvn37Kr3Nd4OMjAwAQLVq1QAAhw4dQlFRkeI8N2zYEDVq1FCc52bNmpkWtwWAbt26ITMzE6dOnarE1t89dDodlixZgpycHHTo0IHnuQKMGzcOvXr1UpxTgL/TjnbhwgVERESgdu3aGDJkCGJjYwHceefZ6auC3+1SU1Oh0+kUPywACA0NxdmzZ53UqqonMTERAGyeZ+NziYmJCAkJUTzv4uKCatWqmfYhM71ej4kTJ6JTp05o2rQpAHEO3dzc4O/vr9jX8jzb+jkYnyOzEydOoEOHDsjPz4e3tzdWrVqFxo0b4+jRozzPDrRkyRIcPnwYBw4csHqOv9OOExMTg4ULF6JBgwZISEjAjBkz8MADD+DkyZN33HlmcEN0jxo3bhxOnjyJnTt3OrspVVaDBg1w9OhRZGRkYPny5Rg2bBj++ecfZzerSomLi8OECROwceNGuLu7O7s5VVqPHj1M95s3b46YmBjUrFkTv/32Gzw8PJzYMmvslvqXgoKCoNForCrCk5KSEBYW5qRWVT3Gc1nSeQ4LC0NycrLi+eLiYqSlpfFnYWH8+PH4888/sXXrVlSvXt20PSwsDIWFhUhPT1fsb3mebf0cjM+RmZubG+rWrYvWrVtj1qxZaNGiBT7//HOeZwc6dOgQkpOTcd9998HFxQUuLi74559/MHfuXLi4uCA0NJTnuoL4+/ujfv36uHjx4h33O83g5l9yc3ND69atsXnzZtM2vV6PzZs3o0OHDk5sWdVSq1YthIWFKc5zZmYm9u3bZzrPHTp0QHp6Og4dOmTaZ8uWLdDr9YiJian0Nt+JJEnC+PHjsWrVKmzZsgW1atVSPN+6dWu4uroqzvO5c+cQGxurOM8nTpxQBJIbN26Er68vGjduXDkf5C6l1+tRUFDA8+xAXbp0wYkTJ3D06FHTrU2bNhgyZIjpPs91xcjOzsalS5cQHh5+5/1OO7Q8+R61ZMkSSavVSgsXLpROnz4tjR49WvL391dUhFPpsrKypCNHjkhHjhyRAEizZ8+Wjhw5Il27dk2SJDEU3N/fX/r999+l48ePS3379rU5FLxVq1bSvn37pJ07d0r16tXjUHCZsWPHSn5+ftK2bdsUwzlzc3NN+4wZM0aqUaOGtGXLFungwYNShw4dpA4dOpieNw7nfOyxx6SjR49K69evl4KDgzls1sJbb70l/fPPP9KVK1ek48ePS2+99ZakUqmkv//+W5IknueKJB8tJUk8144yefJkadu2bdKVK1ekXbt2SV27dpWCgoKk5ORkSZLurPPM4MZB/ve//0k1atSQ3NzcpHbt2kl79+51dpPuOlu3bpUAWN2GDRsmSZIYDj516lQpNDRU0mq1UpcuXaRz584pjnHz5k1p8ODBkre3t+Tr6yuNGDFCysrKcsKnuTPZOr8ApB9++MG0T15envTSSy9JAQEBkqenp9S/f38pISFBcZyrV69KPXr0kDw8PKSgoCBp8uTJUlFRUSV/mjvb888/L9WsWVNyc3OTgoODpS5dupgCG0niea5IlsENz7VjDBw4UAoPD5fc3NykyMhIaeDAgdLFixdNz99J51klSZLk2FwQERERkfOw5oaIiIiqFAY3REREVKUwuCEiIqIqhcENERERVSkMboiIiKhKYXBDREREVQqDGyIiIqpSGNwQ0T1PpVJh9erVzm4GETkIgxsicqrhw4dDpVJZ3bp37+7sphHRXcrF2Q0gIurevTt++OEHxTatVuuk1hDR3Y6ZGyJyOq1Wi7CwMMUtICAAgOgy+uqrr9CjRw94eHigdu3aWL58ueL1J06cwCOPPAIPDw8EBgZi9OjRyM7OVuyzYMECNGnSBFqtFuHh4Rg/frzi+dTUVPTv3x+enp6oV68e1qxZU7EfmogqDIMbIrrjTZ06FQMGDMCxY8cwZMgQDBo0CGfOnAEA5OTkoFu3bggICMCBAwewbNkybNq0SRG8fPXVVxg3bhxGjx6NEydOYM2aNahbt67iPWbMmIGnn34ax48fR8+ePTFkyBCkpaVV6uckIgdx+FKcRETlMGzYMEmj0UheXl6K23/+8x9JksRK5mPGjFG8JiYmRho7dqwkSZL0zTffSAEBAVJ2drbp+bVr10pqtVpKTEyUJEmSIiIipHfeecduGwBI7777rulxdna2BEBat26dwz4nEVUe1twQkdM9/PDD+OqrrxTbqlWrZrrfoUMHxXMdOnTA0aNHAQBnzpxBixYt4OXlZXq+U6dO0Ov1OHfuHFQqFeLj49GlS5cS29C8eXPTfS8vL/j6+iI5Ofl2PxIRORGDGyJyOi8vL6tuIkfx8PAo036urq6KxyqVCnq9viKaREQVjDU3RHTH27t3r9XjRo0aAQAaNWqEY8eOIScnx/T8rl27oFar0aBBA/j4+CA6OhqbN2+u1DYTkfMwc0NETldQUIDExETFNhcXFwQFBQEAli1bhjZt2uD+++/Hr7/+iv379+P7778HAAwZMgTTpk3DsGHDMH36dKSkpODll1/Gc889h9DQUADA9OnTMWbMGISEhKBHjx7IysrCrl278PLLL1fuByWiSsHghoicbv369QgPD1dsa9CgAc6ePQtAjGRasmQJXnrpJYSHh2Px4sVo3LgxAMDT0xMbNmzAhAkT0LZtW3h6emLAgAGYPXu26VjDhg1Dfn4+Pvvs/9u1YyOIQRiKgqIHCiOgFPq4UiiRq8CBE/vuz24FInsj9Km1VvXea8753AOBR7Vzznl7CIArrbXae9cY4+1RgD/h5gYAiCJuAIAobm6An+bnHLjL5gYAiCJuAIAo4gYAiCJuAIAo4gYAiCJuAIAo4gYAiCJuAIAo4gYAiPIFaAfpMOfChOAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97d4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF LSTM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.88       661\n",
      "           1       0.93      0.82      0.87       662\n",
      "\n",
      "    accuracy                           0.88      1323\n",
      "   macro avg       0.88      0.88      0.88      1323\n",
      "weighted avg       0.88      0.88      0.88      1323\n",
      "\n",
      "0.8783068783068783\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./weight_cp/weight_lstm1.hdf5')\n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "y_pred = []\n",
    "for p in predictions:\n",
    "    y_pred.append(p[0])\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"CLASSIFICATION REPORT OF LSTM\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41679010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 80)\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     14114800    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200, 50), (N 30200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 200, 1)       51          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 200)          0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 200)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "before_split (Activation)       (None, 200)          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 20), (None,  0           before_split[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 80)           0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][1]          \n",
      "                                                                 tf_op_layer_split[0][8]          \n",
      "                                                                 tf_op_layer_split[0][9]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 8, 10, 1)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 8, 10, 2)     130         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 8, 10, 2)     8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 160)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "op_main (Dense)                 (None, 1)            51          lstm_1[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "op_conv (Dense)                 (None, 1)            161         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "avg (Average)                   (None, 1)            0           op_main[0][0]                    \n",
      "                                                                 op_conv[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,145,401\n",
      "Trainable params: 30,597\n",
      "Non-trainable params: 14,114,804\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 10.3313 - op_main_loss: 0.6913 - op_conv_loss: 0.6983 - avg_loss: 0.6898 - op_main_accuracy: 0.5468 - op_conv_accuracy: 0.4809 - avg_accuracy: 0.5390\n",
      "Epoch 00001: val_avg_accuracy improved from -inf to 0.59585, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 3s 20ms/step - loss: 10.3313 - op_main_loss: 0.6913 - op_conv_loss: 0.6983 - avg_loss: 0.6898 - op_main_accuracy: 0.5468 - op_conv_accuracy: 0.4809 - avg_accuracy: 0.5390 - val_loss: 9.3374 - val_op_main_loss: 0.6723 - val_op_conv_loss: 0.6931 - val_avg_loss: 0.6792 - val_op_main_accuracy: 0.5940 - val_op_conv_accuracy: 0.4967 - val_avg_accuracy: 0.5958\n",
      "Epoch 2/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 8.5167 - op_main_loss: 0.6612 - op_conv_loss: 0.6938 - avg_loss: 0.6734 - op_main_accuracy: 0.6153 - op_conv_accuracy: 0.4894 - avg_accuracy: 0.6155\n",
      "Epoch 00002: val_avg_accuracy improved from 0.59585 to 0.64212, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 8.5167 - op_main_loss: 0.6612 - op_conv_loss: 0.6938 - avg_loss: 0.6734 - op_main_accuracy: 0.6153 - op_conv_accuracy: 0.4894 - avg_accuracy: 0.6155 - val_loss: 7.7285 - val_op_main_loss: 0.6465 - val_op_conv_loss: 0.6931 - val_avg_loss: 0.6657 - val_op_main_accuracy: 0.6431 - val_op_conv_accuracy: 0.4995 - val_avg_accuracy: 0.6421\n",
      "Epoch 3/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 7.0740 - op_main_loss: 0.6340 - op_conv_loss: 0.6936 - avg_loss: 0.6590 - op_main_accuracy: 0.6593 - op_conv_accuracy: 0.4981 - avg_accuracy: 0.6541\n",
      "Epoch 00003: val_avg_accuracy improved from 0.64212 to 0.66478, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 7.0740 - op_main_loss: 0.6340 - op_conv_loss: 0.6936 - avg_loss: 0.6590 - op_main_accuracy: 0.6593 - op_conv_accuracy: 0.4981 - avg_accuracy: 0.6541 - val_loss: 6.4547 - val_op_main_loss: 0.6239 - val_op_conv_loss: 0.6930 - val_avg_loss: 0.6532 - val_op_main_accuracy: 0.6657 - val_op_conv_accuracy: 0.4967 - val_avg_accuracy: 0.6648\n",
      "Epoch 4/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 5.9486 - op_main_loss: 0.6153 - op_conv_loss: 0.6931 - avg_loss: 0.6485 - op_main_accuracy: 0.6846 - op_conv_accuracy: 0.5171 - avg_accuracy: 0.6812\n",
      "Epoch 00004: val_avg_accuracy improved from 0.66478 to 0.69216, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 5.9393 - op_main_loss: 0.6145 - op_conv_loss: 0.6931 - avg_loss: 0.6480 - op_main_accuracy: 0.6857 - op_conv_accuracy: 0.5184 - avg_accuracy: 0.6824 - val_loss: 5.4445 - val_op_main_loss: 0.6017 - val_op_conv_loss: 0.6928 - val_avg_loss: 0.6406 - val_op_main_accuracy: 0.6922 - val_op_conv_accuracy: 0.4844 - val_avg_accuracy: 0.6922\n",
      "Epoch 5/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 5.0363 - op_main_loss: 0.5916 - op_conv_loss: 0.6927 - avg_loss: 0.6348 - op_main_accuracy: 0.7070 - op_conv_accuracy: 0.5177 - avg_accuracy: 0.7032\n",
      "Epoch 00005: val_avg_accuracy improved from 0.69216 to 0.71010, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 5.0363 - op_main_loss: 0.5916 - op_conv_loss: 0.6927 - avg_loss: 0.6348 - op_main_accuracy: 0.7070 - op_conv_accuracy: 0.5177 - avg_accuracy: 0.7032 - val_loss: 4.6450 - val_op_main_loss: 0.5790 - val_op_conv_loss: 0.6925 - val_avg_loss: 0.6273 - val_op_main_accuracy: 0.7110 - val_op_conv_accuracy: 0.5515 - val_avg_accuracy: 0.7101\n",
      "Epoch 6/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 4.3334 - op_main_loss: 0.5722 - op_conv_loss: 0.6926 - avg_loss: 0.6236 - op_main_accuracy: 0.7228 - op_conv_accuracy: 0.5197 - avg_accuracy: 0.7214\n",
      "Epoch 00006: val_avg_accuracy improved from 0.71010 to 0.73182, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 4.3280 - op_main_loss: 0.5719 - op_conv_loss: 0.6926 - avg_loss: 0.6233 - op_main_accuracy: 0.7226 - op_conv_accuracy: 0.5194 - avg_accuracy: 0.7212 - val_loss: 4.0149 - val_op_main_loss: 0.5570 - val_op_conv_loss: 0.6918 - val_avg_loss: 0.6138 - val_op_main_accuracy: 0.7337 - val_op_conv_accuracy: 0.5024 - val_avg_accuracy: 0.7318\n",
      "Epoch 7/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 3.7709 - op_main_loss: 0.5493 - op_conv_loss: 0.6909 - avg_loss: 0.6089 - op_main_accuracy: 0.7362 - op_conv_accuracy: 0.5451 - avg_accuracy: 0.7340\n",
      "Epoch 00007: val_avg_accuracy improved from 0.73182 to 0.74315, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 3.7667 - op_main_loss: 0.5501 - op_conv_loss: 0.6909 - avg_loss: 0.6093 - op_main_accuracy: 0.7356 - op_conv_accuracy: 0.5454 - avg_accuracy: 0.7335 - val_loss: 3.5166 - val_op_main_loss: 0.5352 - val_op_conv_loss: 0.6886 - val_avg_loss: 0.5989 - val_op_main_accuracy: 0.7413 - val_op_conv_accuracy: 0.5873 - val_avg_accuracy: 0.7432\n",
      "Epoch 8/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 3.3244 - op_main_loss: 0.5294 - op_conv_loss: 0.6871 - avg_loss: 0.5948 - op_main_accuracy: 0.7467 - op_conv_accuracy: 0.5595 - avg_accuracy: 0.7453\n",
      "Epoch 00008: val_avg_accuracy improved from 0.74315 to 0.75921, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 3.3244 - op_main_loss: 0.5294 - op_conv_loss: 0.6871 - avg_loss: 0.5948 - op_main_accuracy: 0.7467 - op_conv_accuracy: 0.5595 - avg_accuracy: 0.7453 - val_loss: 3.1234 - val_op_main_loss: 0.5154 - val_op_conv_loss: 0.6812 - val_avg_loss: 0.5831 - val_op_main_accuracy: 0.7535 - val_op_conv_accuracy: 0.4948 - val_avg_accuracy: 0.7592\n",
      "Epoch 9/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.9709 - op_main_loss: 0.5167 - op_conv_loss: 0.6682 - avg_loss: 0.5774 - op_main_accuracy: 0.7531 - op_conv_accuracy: 0.6061 - avg_accuracy: 0.7517\n",
      "Epoch 00009: val_avg_accuracy improved from 0.75921 to 0.76298, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.9709 - op_main_loss: 0.5167 - op_conv_loss: 0.6682 - avg_loss: 0.5774 - op_main_accuracy: 0.7531 - op_conv_accuracy: 0.6061 - avg_accuracy: 0.7517 - val_loss: 2.7784 - val_op_main_loss: 0.4984 - val_op_conv_loss: 0.6400 - val_avg_loss: 0.5560 - val_op_main_accuracy: 0.7686 - val_op_conv_accuracy: 0.5836 - val_avg_accuracy: 0.7630\n",
      "Epoch 10/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.6325 - op_main_loss: 0.4916 - op_conv_loss: 0.6127 - avg_loss: 0.5392 - op_main_accuracy: 0.7691 - op_conv_accuracy: 0.6682 - avg_accuracy: 0.7606\n",
      "Epoch 00010: val_avg_accuracy improved from 0.76298 to 0.76676, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.6325 - op_main_loss: 0.4916 - op_conv_loss: 0.6127 - avg_loss: 0.5392 - op_main_accuracy: 0.7691 - op_conv_accuracy: 0.6682 - avg_accuracy: 0.7606 - val_loss: 2.4664 - val_op_main_loss: 0.4772 - val_op_conv_loss: 0.5737 - val_avg_loss: 0.5138 - val_op_main_accuracy: 0.7790 - val_op_conv_accuracy: 0.6676 - val_avg_accuracy: 0.7668\n",
      "Epoch 11/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 2.3842 - op_main_loss: 0.4811 - op_conv_loss: 0.5602 - avg_loss: 0.5088 - op_main_accuracy: 0.7796 - op_conv_accuracy: 0.7168 - avg_accuracy: 0.7651\n",
      "Epoch 00011: val_avg_accuracy did not improve from 0.76676\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 2.3830 - op_main_loss: 0.4815 - op_conv_loss: 0.5597 - avg_loss: 0.5088 - op_main_accuracy: 0.7793 - op_conv_accuracy: 0.7176 - avg_accuracy: 0.7654 - val_loss: 2.2633 - val_op_main_loss: 0.4699 - val_op_conv_loss: 0.5317 - val_avg_loss: 0.4902 - val_op_main_accuracy: 0.7734 - val_op_conv_accuracy: 0.7110 - val_avg_accuracy: 0.7649\n",
      "Epoch 12/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.1884 - op_main_loss: 0.4655 - op_conv_loss: 0.5190 - avg_loss: 0.4814 - op_main_accuracy: 0.7817 - op_conv_accuracy: 0.7514 - avg_accuracy: 0.7791\n",
      "Epoch 00012: val_avg_accuracy improved from 0.76676 to 0.77243, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.1884 - op_main_loss: 0.4655 - op_conv_loss: 0.5190 - avg_loss: 0.4814 - op_main_accuracy: 0.7817 - op_conv_accuracy: 0.7514 - avg_accuracy: 0.7791 - val_loss: 2.1149 - val_op_main_loss: 0.4631 - val_op_conv_loss: 0.5033 - val_avg_loss: 0.4733 - val_op_main_accuracy: 0.7762 - val_op_conv_accuracy: 0.7517 - val_avg_accuracy: 0.7724\n",
      "Epoch 13/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 2.0387 - op_main_loss: 0.4519 - op_conv_loss: 0.4878 - avg_loss: 0.4598 - op_main_accuracy: 0.8028 - op_conv_accuracy: 0.7777 - avg_accuracy: 0.7992\n",
      "Epoch 00013: val_avg_accuracy improved from 0.77243 to 0.81114, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.0391 - op_main_loss: 0.4522 - op_conv_loss: 0.4878 - avg_loss: 0.4600 - op_main_accuracy: 0.8025 - op_conv_accuracy: 0.7776 - avg_accuracy: 0.7991 - val_loss: 1.9414 - val_op_main_loss: 0.4341 - val_op_conv_loss: 0.4628 - val_avg_loss: 0.4382 - val_op_main_accuracy: 0.8064 - val_op_conv_accuracy: 0.7913 - val_avg_accuracy: 0.8111\n",
      "Epoch 14/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.9215 - op_main_loss: 0.4384 - op_conv_loss: 0.4617 - avg_loss: 0.4402 - op_main_accuracy: 0.8099 - op_conv_accuracy: 0.7877 - avg_accuracy: 0.8084\n",
      "Epoch 00014: val_avg_accuracy did not improve from 0.81114\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.9187 - op_main_loss: 0.4380 - op_conv_loss: 0.4605 - avg_loss: 0.4394 - op_main_accuracy: 0.8103 - op_conv_accuracy: 0.7895 - avg_accuracy: 0.8095 - val_loss: 1.8739 - val_op_main_loss: 0.4281 - val_op_conv_loss: 0.4570 - val_avg_loss: 0.4322 - val_op_main_accuracy: 0.7951 - val_op_conv_accuracy: 0.7923 - val_avg_accuracy: 0.7970\n",
      "Epoch 15/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.8531 - op_main_loss: 0.4354 - op_conv_loss: 0.4496 - avg_loss: 0.4328 - op_main_accuracy: 0.8099 - op_conv_accuracy: 0.7937 - avg_accuracy: 0.8075\n",
      "Epoch 00015: val_avg_accuracy improved from 0.81114 to 0.81586, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.8500 - op_main_loss: 0.4345 - op_conv_loss: 0.4485 - avg_loss: 0.4319 - op_main_accuracy: 0.8107 - op_conv_accuracy: 0.7951 - avg_accuracy: 0.8081 - val_loss: 1.7726 - val_op_main_loss: 0.4151 - val_op_conv_loss: 0.4290 - val_avg_loss: 0.4121 - val_op_main_accuracy: 0.8130 - val_op_conv_accuracy: 0.7998 - val_avg_accuracy: 0.8159\n",
      "Epoch 16/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.7727 - op_main_loss: 0.4244 - op_conv_loss: 0.4286 - avg_loss: 0.4176 - op_main_accuracy: 0.8164 - op_conv_accuracy: 0.8103 - avg_accuracy: 0.8229\n",
      "Epoch 00016: val_avg_accuracy improved from 0.81586 to 0.81964, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.7738 - op_main_loss: 0.4248 - op_conv_loss: 0.4291 - avg_loss: 0.4181 - op_main_accuracy: 0.8159 - op_conv_accuracy: 0.8105 - avg_accuracy: 0.8225 - val_loss: 1.7042 - val_op_main_loss: 0.4056 - val_op_conv_loss: 0.4109 - val_avg_loss: 0.3999 - val_op_main_accuracy: 0.8206 - val_op_conv_accuracy: 0.8140 - val_avg_accuracy: 0.8196\n",
      "Epoch 17/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.6959 - op_main_loss: 0.4099 - op_conv_loss: 0.4086 - avg_loss: 0.4005 - op_main_accuracy: 0.8233 - op_conv_accuracy: 0.8216 - avg_accuracy: 0.8296\n",
      "Epoch 00017: val_avg_accuracy improved from 0.81964 to 0.82908, saving model to ./weight_cp\\weight_lstm2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 16ms/step - loss: 1.6937 - op_main_loss: 0.4095 - op_conv_loss: 0.4076 - avg_loss: 0.3999 - op_main_accuracy: 0.8237 - op_conv_accuracy: 0.8228 - avg_accuracy: 0.8306 - val_loss: 1.6656 - val_op_main_loss: 0.3999 - val_op_conv_loss: 0.4049 - val_avg_loss: 0.3934 - val_op_main_accuracy: 0.8206 - val_op_conv_accuracy: 0.8178 - val_avg_accuracy: 0.8291\n",
      "Epoch 18/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.6595 - op_main_loss: 0.4071 - op_conv_loss: 0.3978 - avg_loss: 0.3945 - op_main_accuracy: 0.8304 - op_conv_accuracy: 0.8207 - avg_accuracy: 0.8309\n",
      "Epoch 00018: val_avg_accuracy improved from 0.82908 to 0.83003, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.6586 - op_main_loss: 0.4074 - op_conv_loss: 0.3972 - avg_loss: 0.3943 - op_main_accuracy: 0.8308 - op_conv_accuracy: 0.8209 - avg_accuracy: 0.8315 - val_loss: 1.6315 - val_op_main_loss: 0.3950 - val_op_conv_loss: 0.3978 - val_avg_loss: 0.3873 - val_op_main_accuracy: 0.8244 - val_op_conv_accuracy: 0.8187 - val_avg_accuracy: 0.8300\n",
      "Epoch 19/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.6134 - op_main_loss: 0.3997 - op_conv_loss: 0.3850 - avg_loss: 0.3841 - op_main_accuracy: 0.8357 - op_conv_accuracy: 0.8371 - avg_accuracy: 0.8419\n",
      "Epoch 00019: val_avg_accuracy improved from 0.83003 to 0.83475, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.6135 - op_main_loss: 0.3997 - op_conv_loss: 0.3851 - avg_loss: 0.3842 - op_main_accuracy: 0.8355 - op_conv_accuracy: 0.8374 - avg_accuracy: 0.8419 - val_loss: 1.5727 - val_op_main_loss: 0.3862 - val_op_conv_loss: 0.3755 - val_avg_loss: 0.3730 - val_op_main_accuracy: 0.8329 - val_op_conv_accuracy: 0.8263 - val_avg_accuracy: 0.8347\n",
      "Epoch 20/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.5652 - op_main_loss: 0.3909 - op_conv_loss: 0.3688 - avg_loss: 0.3716 - op_main_accuracy: 0.8421 - op_conv_accuracy: 0.8433 - avg_accuracy: 0.8476\n",
      "Epoch 00020: val_avg_accuracy did not improve from 0.83475\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.5655 - op_main_loss: 0.3907 - op_conv_loss: 0.3692 - avg_loss: 0.3717 - op_main_accuracy: 0.8419 - op_conv_accuracy: 0.8429 - avg_accuracy: 0.8478 - val_loss: 1.5555 - val_op_main_loss: 0.3818 - val_op_conv_loss: 0.3741 - val_avg_loss: 0.3694 - val_op_main_accuracy: 0.8376 - val_op_conv_accuracy: 0.8291 - val_avg_accuracy: 0.8319\n",
      "Epoch 21/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.5255 - op_main_loss: 0.3846 - op_conv_loss: 0.3543 - avg_loss: 0.3614 - op_main_accuracy: 0.8378 - op_conv_accuracy: 0.8471 - avg_accuracy: 0.8471\n",
      "Epoch 00021: val_avg_accuracy improved from 0.83475 to 0.83664, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.5242 - op_main_loss: 0.3841 - op_conv_loss: 0.3540 - avg_loss: 0.3610 - op_main_accuracy: 0.8381 - op_conv_accuracy: 0.8476 - avg_accuracy: 0.8476 - val_loss: 1.5150 - val_op_main_loss: 0.3774 - val_op_conv_loss: 0.3576 - val_avg_loss: 0.3591 - val_op_main_accuracy: 0.8414 - val_op_conv_accuracy: 0.8319 - val_avg_accuracy: 0.8366\n",
      "Epoch 22/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.4842 - op_main_loss: 0.3741 - op_conv_loss: 0.3413 - avg_loss: 0.3500 - op_main_accuracy: 0.8535 - op_conv_accuracy: 0.8546 - avg_accuracy: 0.8625\n",
      "Epoch 00022: val_avg_accuracy improved from 0.83664 to 0.83853, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4842 - op_main_loss: 0.3741 - op_conv_loss: 0.3413 - avg_loss: 0.3500 - op_main_accuracy: 0.8535 - op_conv_accuracy: 0.8547 - avg_accuracy: 0.8625 - val_loss: 1.5191 - val_op_main_loss: 0.3757 - val_op_conv_loss: 0.3655 - val_avg_loss: 0.3605 - val_op_main_accuracy: 0.8366 - val_op_conv_accuracy: 0.8376 - val_avg_accuracy: 0.8385\n",
      "Epoch 23/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.4712 - op_main_loss: 0.3709 - op_conv_loss: 0.3396 - avg_loss: 0.3464 - op_main_accuracy: 0.8447 - op_conv_accuracy: 0.8546 - avg_accuracy: 0.8575\n",
      "Epoch 00023: val_avg_accuracy improved from 0.83853 to 0.84042, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4710 - op_main_loss: 0.3712 - op_conv_loss: 0.3393 - avg_loss: 0.3463 - op_main_accuracy: 0.8455 - op_conv_accuracy: 0.8552 - avg_accuracy: 0.8582 - val_loss: 1.4858 - val_op_main_loss: 0.3686 - val_op_conv_loss: 0.3537 - val_avg_loss: 0.3524 - val_op_main_accuracy: 0.8404 - val_op_conv_accuracy: 0.8480 - val_avg_accuracy: 0.8404\n",
      "Epoch 24/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.4325 - op_main_loss: 0.3603 - op_conv_loss: 0.3269 - avg_loss: 0.3352 - op_main_accuracy: 0.8532 - op_conv_accuracy: 0.8665 - avg_accuracy: 0.8698\n",
      "Epoch 00024: val_avg_accuracy improved from 0.84042 to 0.84891, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4325 - op_main_loss: 0.3605 - op_conv_loss: 0.3267 - avg_loss: 0.3352 - op_main_accuracy: 0.8533 - op_conv_accuracy: 0.8665 - avg_accuracy: 0.8698 - val_loss: 1.4554 - val_op_main_loss: 0.3656 - val_op_conv_loss: 0.3401 - val_avg_loss: 0.3443 - val_op_main_accuracy: 0.8404 - val_op_conv_accuracy: 0.8451 - val_avg_accuracy: 0.8489\n",
      "Epoch 25/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.4124 - op_main_loss: 0.3616 - op_conv_loss: 0.3153 - avg_loss: 0.3307 - op_main_accuracy: 0.8523 - op_conv_accuracy: 0.8677 - avg_accuracy: 0.8731\n",
      "Epoch 00025: val_avg_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.4123 - op_main_loss: 0.3616 - op_conv_loss: 0.3152 - avg_loss: 0.3307 - op_main_accuracy: 0.8523 - op_conv_accuracy: 0.8679 - avg_accuracy: 0.8733 - val_loss: 1.4427 - val_op_main_loss: 0.3636 - val_op_conv_loss: 0.3354 - val_avg_loss: 0.3406 - val_op_main_accuracy: 0.8414 - val_op_conv_accuracy: 0.8489 - val_avg_accuracy: 0.8480\n",
      "Epoch 26/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.3998 - op_main_loss: 0.3580 - op_conv_loss: 0.3134 - avg_loss: 0.3266 - op_main_accuracy: 0.8565 - op_conv_accuracy: 0.8673 - avg_accuracy: 0.8673\n",
      "Epoch 00026: val_avg_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3965 - op_main_loss: 0.3570 - op_conv_loss: 0.3121 - avg_loss: 0.3256 - op_main_accuracy: 0.8573 - op_conv_accuracy: 0.8684 - avg_accuracy: 0.8681 - val_loss: 1.4208 - val_op_main_loss: 0.3524 - val_op_conv_loss: 0.3334 - val_avg_loss: 0.3338 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8461 - val_avg_accuracy: 0.8480\n",
      "Epoch 27/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.3647 - op_main_loss: 0.3501 - op_conv_loss: 0.2991 - avg_loss: 0.3158 - op_main_accuracy: 0.8632 - op_conv_accuracy: 0.8755 - avg_accuracy: 0.8736\n",
      "Epoch 00027: val_avg_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3644 - op_main_loss: 0.3500 - op_conv_loss: 0.2990 - avg_loss: 0.3157 - op_main_accuracy: 0.8632 - op_conv_accuracy: 0.8755 - avg_accuracy: 0.8736 - val_loss: 1.4609 - val_op_main_loss: 0.3637 - val_op_conv_loss: 0.3536 - val_avg_loss: 0.3446 - val_op_main_accuracy: 0.8347 - val_op_conv_accuracy: 0.8432 - val_avg_accuracy: 0.8461\n",
      "Epoch 28/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.3414 - op_main_loss: 0.3432 - op_conv_loss: 0.2919 - avg_loss: 0.3085 - op_main_accuracy: 0.8656 - op_conv_accuracy: 0.8777 - avg_accuracy: 0.8801\n",
      "Epoch 00028: val_avg_accuracy improved from 0.84891 to 0.85458, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3400 - op_main_loss: 0.3430 - op_conv_loss: 0.2912 - avg_loss: 0.3082 - op_main_accuracy: 0.8651 - op_conv_accuracy: 0.8776 - avg_accuracy: 0.8795 - val_loss: 1.3958 - val_op_main_loss: 0.3463 - val_op_conv_loss: 0.3278 - val_avg_loss: 0.3262 - val_op_main_accuracy: 0.8432 - val_op_conv_accuracy: 0.8565 - val_avg_accuracy: 0.8546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.3467 - op_main_loss: 0.3488 - op_conv_loss: 0.2916 - avg_loss: 0.3118 - op_main_accuracy: 0.8635 - op_conv_accuracy: 0.8776 - avg_accuracy: 0.8776\n",
      "Epoch 00029: val_avg_accuracy did not improve from 0.85458\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3505 - op_main_loss: 0.3497 - op_conv_loss: 0.2934 - avg_loss: 0.3129 - op_main_accuracy: 0.8632 - op_conv_accuracy: 0.8774 - avg_accuracy: 0.8776 - val_loss: 1.4999 - val_op_main_loss: 0.3678 - val_op_conv_loss: 0.3805 - val_avg_loss: 0.3586 - val_op_main_accuracy: 0.8404 - val_op_conv_accuracy: 0.8319 - val_avg_accuracy: 0.8366\n",
      "Epoch 30/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.3311 - op_main_loss: 0.3441 - op_conv_loss: 0.2882 - avg_loss: 0.3074 - op_main_accuracy: 0.8665 - op_conv_accuracy: 0.8830 - avg_accuracy: 0.8840\n",
      "Epoch 00030: val_avg_accuracy did not improve from 0.85458\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3315 - op_main_loss: 0.3444 - op_conv_loss: 0.2882 - avg_loss: 0.3076 - op_main_accuracy: 0.8663 - op_conv_accuracy: 0.8828 - avg_accuracy: 0.8840 - val_loss: 1.3971 - val_op_main_loss: 0.3417 - val_op_conv_loss: 0.3374 - val_avg_loss: 0.3263 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8508 - val_avg_accuracy: 0.8489\n",
      "Epoch 31/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.2952 - op_main_loss: 0.3329 - op_conv_loss: 0.2758 - avg_loss: 0.2954 - op_main_accuracy: 0.8721 - op_conv_accuracy: 0.8876 - avg_accuracy: 0.8869\n",
      "Epoch 00031: val_avg_accuracy improved from 0.85458 to 0.86025, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2952 - op_main_loss: 0.3329 - op_conv_loss: 0.2758 - avg_loss: 0.2954 - op_main_accuracy: 0.8705 - op_conv_accuracy: 0.8871 - avg_accuracy: 0.8861 - val_loss: 1.3556 - val_op_main_loss: 0.3366 - val_op_conv_loss: 0.3139 - val_avg_loss: 0.3152 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8602 - val_avg_accuracy: 0.8602\n",
      "Epoch 32/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.2762 - op_main_loss: 0.3282 - op_conv_loss: 0.2688 - avg_loss: 0.2895 - op_main_accuracy: 0.8703 - op_conv_accuracy: 0.8894 - avg_accuracy: 0.8920\n",
      "Epoch 00032: val_avg_accuracy did not improve from 0.86025\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2783 - op_main_loss: 0.3289 - op_conv_loss: 0.2695 - avg_loss: 0.2902 - op_main_accuracy: 0.8698 - op_conv_accuracy: 0.8889 - avg_accuracy: 0.8915 - val_loss: 1.3452 - val_op_main_loss: 0.3351 - val_op_conv_loss: 0.3100 - val_avg_loss: 0.3118 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8555 - val_avg_accuracy: 0.8565\n",
      "Epoch 33/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.2647 - op_main_loss: 0.3261 - op_conv_loss: 0.2641 - avg_loss: 0.2862 - op_main_accuracy: 0.8688 - op_conv_accuracy: 0.8878 - avg_accuracy: 0.8890\n",
      "Epoch 00033: val_avg_accuracy improved from 0.86025 to 0.86119, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2640 - op_main_loss: 0.3259 - op_conv_loss: 0.2638 - avg_loss: 0.2860 - op_main_accuracy: 0.8691 - op_conv_accuracy: 0.8880 - avg_accuracy: 0.8892 - val_loss: 1.3368 - val_op_main_loss: 0.3326 - val_op_conv_loss: 0.3079 - val_avg_loss: 0.3094 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8612\n",
      "Epoch 34/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.2554 - op_main_loss: 0.3261 - op_conv_loss: 0.2593 - avg_loss: 0.2832 - op_main_accuracy: 0.8692 - op_conv_accuracy: 0.9007 - avg_accuracy: 0.8912\n",
      "Epoch 00034: val_avg_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2578 - op_main_loss: 0.3261 - op_conv_loss: 0.2610 - avg_loss: 0.2839 - op_main_accuracy: 0.8700 - op_conv_accuracy: 0.9003 - avg_accuracy: 0.8911 - val_loss: 1.3330 - val_op_main_loss: 0.3302 - val_op_conv_loss: 0.3086 - val_avg_loss: 0.3079 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8602\n",
      "Epoch 35/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2445 - op_main_loss: 0.3221 - op_conv_loss: 0.2573 - avg_loss: 0.2798 - op_main_accuracy: 0.8709 - op_conv_accuracy: 0.8950 - avg_accuracy: 0.8918\n",
      "Epoch 00035: val_avg_accuracy did not improve from 0.86119\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2472 - op_main_loss: 0.3232 - op_conv_loss: 0.2581 - avg_loss: 0.2807 - op_main_accuracy: 0.8710 - op_conv_accuracy: 0.8944 - avg_accuracy: 0.8913 - val_loss: 1.3408 - val_op_main_loss: 0.3333 - val_op_conv_loss: 0.3140 - val_avg_loss: 0.3106 - val_op_main_accuracy: 0.8536 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8593\n",
      "Epoch 36/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.2257 - op_main_loss: 0.3176 - op_conv_loss: 0.2499 - avg_loss: 0.2749 - op_main_accuracy: 0.8762 - op_conv_accuracy: 0.9009 - avg_accuracy: 0.8978\n",
      "Epoch 00036: val_avg_accuracy improved from 0.86119 to 0.86686, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2292 - op_main_loss: 0.3188 - op_conv_loss: 0.2510 - avg_loss: 0.2761 - op_main_accuracy: 0.8755 - op_conv_accuracy: 0.9003 - avg_accuracy: 0.8967 - val_loss: 1.3383 - val_op_main_loss: 0.3306 - val_op_conv_loss: 0.3153 - val_avg_loss: 0.3097 - val_op_main_accuracy: 0.8593 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8669\n",
      "Epoch 37/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.2334 - op_main_loss: 0.3235 - op_conv_loss: 0.2505 - avg_loss: 0.2775 - op_main_accuracy: 0.8638 - op_conv_accuracy: 0.8991 - avg_accuracy: 0.8898\n",
      "Epoch 00037: val_avg_accuracy improved from 0.86686 to 0.87252, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2316 - op_main_loss: 0.3228 - op_conv_loss: 0.2500 - avg_loss: 0.2769 - op_main_accuracy: 0.8639 - op_conv_accuracy: 0.8993 - avg_accuracy: 0.8901 - val_loss: 1.3086 - val_op_main_loss: 0.3243 - val_op_conv_loss: 0.3015 - val_avg_loss: 0.3010 - val_op_main_accuracy: 0.8593 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8725\n",
      "Epoch 38/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.1951 - op_main_loss: 0.3096 - op_conv_loss: 0.2387 - avg_loss: 0.2651 - op_main_accuracy: 0.8840 - op_conv_accuracy: 0.9034 - avg_accuracy: 0.9022\n",
      "Epoch 00038: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1943 - op_main_loss: 0.3093 - op_conv_loss: 0.2384 - avg_loss: 0.2648 - op_main_accuracy: 0.8840 - op_conv_accuracy: 0.9034 - avg_accuracy: 0.9022 - val_loss: 1.3033 - val_op_main_loss: 0.3209 - val_op_conv_loss: 0.3024 - val_avg_loss: 0.2993 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8650\n",
      "Epoch 39/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.1893 - op_main_loss: 0.3102 - op_conv_loss: 0.2352 - avg_loss: 0.2633 - op_main_accuracy: 0.8798 - op_conv_accuracy: 0.9079 - avg_accuracy: 0.9036\n",
      "Epoch 00039: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1871 - op_main_loss: 0.3094 - op_conv_loss: 0.2346 - avg_loss: 0.2626 - op_main_accuracy: 0.8800 - op_conv_accuracy: 0.9081 - avg_accuracy: 0.9038 - val_loss: 1.3321 - val_op_main_loss: 0.3295 - val_op_conv_loss: 0.3147 - val_avg_loss: 0.3080 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8631\n",
      "Epoch 40/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.1781 - op_main_loss: 0.3063 - op_conv_loss: 0.2331 - avg_loss: 0.2594 - op_main_accuracy: 0.8804 - op_conv_accuracy: 0.9070 - avg_accuracy: 0.9048\n",
      "Epoch 00040: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1775 - op_main_loss: 0.3061 - op_conv_loss: 0.2329 - avg_loss: 0.2592 - op_main_accuracy: 0.8804 - op_conv_accuracy: 0.9069 - avg_accuracy: 0.9048 - val_loss: 1.3121 - val_op_main_loss: 0.3217 - val_op_conv_loss: 0.3111 - val_avg_loss: 0.3007 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.1728 - op_main_loss: 0.3053 - op_conv_loss: 0.2316 - avg_loss: 0.2582 - op_main_accuracy: 0.8814 - op_conv_accuracy: 0.9117 - avg_accuracy: 0.9086\n",
      "Epoch 00041: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1736 - op_main_loss: 0.3058 - op_conv_loss: 0.2317 - avg_loss: 0.2585 - op_main_accuracy: 0.8809 - op_conv_accuracy: 0.9116 - avg_accuracy: 0.9083 - val_loss: 1.3828 - val_op_main_loss: 0.3463 - val_op_conv_loss: 0.3353 - val_avg_loss: 0.3244 - val_op_main_accuracy: 0.8480 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8574\n",
      "Epoch 42/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.1735 - op_main_loss: 0.3063 - op_conv_loss: 0.2321 - avg_loss: 0.2591 - op_main_accuracy: 0.8776 - op_conv_accuracy: 0.9070 - avg_accuracy: 0.9046\n",
      "Epoch 00042: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1730 - op_main_loss: 0.3061 - op_conv_loss: 0.2319 - avg_loss: 0.2590 - op_main_accuracy: 0.8778 - op_conv_accuracy: 0.9067 - avg_accuracy: 0.9045 - val_loss: 1.2944 - val_op_main_loss: 0.3169 - val_op_conv_loss: 0.3056 - val_avg_loss: 0.2967 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8650\n",
      "Epoch 43/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1768 - op_main_loss: 0.3033 - op_conv_loss: 0.2381 - avg_loss: 0.2598 - op_main_accuracy: 0.8828 - op_conv_accuracy: 0.9038 - avg_accuracy: 0.9062\n",
      "Epoch 00043: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1776 - op_main_loss: 0.3041 - op_conv_loss: 0.2378 - avg_loss: 0.2601 - op_main_accuracy: 0.8819 - op_conv_accuracy: 0.9036 - avg_accuracy: 0.9057 - val_loss: 1.2791 - val_op_main_loss: 0.3131 - val_op_conv_loss: 0.2994 - val_avg_loss: 0.2916 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8687\n",
      "Epoch 44/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1438 - op_main_loss: 0.2982 - op_conv_loss: 0.2202 - avg_loss: 0.2503 - op_main_accuracy: 0.8893 - op_conv_accuracy: 0.9103 - avg_accuracy: 0.9039\n",
      "Epoch 00044: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1435 - op_main_loss: 0.2978 - op_conv_loss: 0.2204 - avg_loss: 0.2502 - op_main_accuracy: 0.8889 - op_conv_accuracy: 0.9095 - avg_accuracy: 0.9034 - val_loss: 1.2697 - val_op_main_loss: 0.3071 - val_op_conv_loss: 0.2996 - val_avg_loss: 0.2882 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8669\n",
      "Epoch 45/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1402 - op_main_loss: 0.2939 - op_conv_loss: 0.2232 - avg_loss: 0.2490 - op_main_accuracy: 0.8898 - op_conv_accuracy: 0.9094 - avg_accuracy: 0.9101\n",
      "Epoch 00045: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1388 - op_main_loss: 0.2936 - op_conv_loss: 0.2225 - avg_loss: 0.2486 - op_main_accuracy: 0.8897 - op_conv_accuracy: 0.9100 - avg_accuracy: 0.9104 - val_loss: 1.4861 - val_op_main_loss: 0.3647 - val_op_conv_loss: 0.3944 - val_avg_loss: 0.3526 - val_op_main_accuracy: 0.8395 - val_op_conv_accuracy: 0.8461 - val_avg_accuracy: 0.8442\n",
      "Epoch 46/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1406 - op_main_loss: 0.2978 - op_conv_loss: 0.2205 - avg_loss: 0.2488 - op_main_accuracy: 0.8837 - op_conv_accuracy: 0.9123 - avg_accuracy: 0.9084\n",
      "Epoch 00046: val_avg_accuracy improved from 0.87252 to 0.87535, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1377 - op_main_loss: 0.2970 - op_conv_loss: 0.2193 - avg_loss: 0.2478 - op_main_accuracy: 0.8842 - op_conv_accuracy: 0.9128 - avg_accuracy: 0.9090 - val_loss: 1.2673 - val_op_main_loss: 0.3115 - val_op_conv_loss: 0.2954 - val_avg_loss: 0.2877 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8754\n",
      "Epoch 47/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1117 - op_main_loss: 0.2883 - op_conv_loss: 0.2103 - avg_loss: 0.2397 - op_main_accuracy: 0.8891 - op_conv_accuracy: 0.9179 - avg_accuracy: 0.9113\n",
      "Epoch 00047: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1099 - op_main_loss: 0.2881 - op_conv_loss: 0.2093 - avg_loss: 0.2390 - op_main_accuracy: 0.8894 - op_conv_accuracy: 0.9187 - avg_accuracy: 0.9121 - val_loss: 1.2635 - val_op_main_loss: 0.3077 - val_op_conv_loss: 0.2970 - val_avg_loss: 0.2866 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8697\n",
      "Epoch 48/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0993 - op_main_loss: 0.2847 - op_conv_loss: 0.2060 - avg_loss: 0.2367 - op_main_accuracy: 0.8925 - op_conv_accuracy: 0.9156 - avg_accuracy: 0.9123\n",
      "Epoch 00048: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0996 - op_main_loss: 0.2844 - op_conv_loss: 0.2065 - avg_loss: 0.2369 - op_main_accuracy: 0.8925 - op_conv_accuracy: 0.9154 - avg_accuracy: 0.9123 - val_loss: 1.2529 - val_op_main_loss: 0.3058 - val_op_conv_loss: 0.2922 - val_avg_loss: 0.2833 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8744\n",
      "Epoch 49/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1120 - op_main_loss: 0.2896 - op_conv_loss: 0.2110 - avg_loss: 0.2403 - op_main_accuracy: 0.8871 - op_conv_accuracy: 0.9203 - avg_accuracy: 0.9157\n",
      "Epoch 00049: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1065 - op_main_loss: 0.2879 - op_conv_loss: 0.2089 - avg_loss: 0.2385 - op_main_accuracy: 0.8885 - op_conv_accuracy: 0.9213 - avg_accuracy: 0.9168 - val_loss: 1.2581 - val_op_main_loss: 0.3057 - val_op_conv_loss: 0.2962 - val_avg_loss: 0.2863 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8697\n",
      "Epoch 50/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0979 - op_main_loss: 0.2853 - op_conv_loss: 0.2066 - avg_loss: 0.2353 - op_main_accuracy: 0.8885 - op_conv_accuracy: 0.9185 - avg_accuracy: 0.9159\n",
      "Epoch 00050: val_avg_accuracy improved from 0.87535 to 0.88102, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0953 - op_main_loss: 0.2847 - op_conv_loss: 0.2054 - avg_loss: 0.2345 - op_main_accuracy: 0.8894 - op_conv_accuracy: 0.9194 - avg_accuracy: 0.9171 - val_loss: 1.2449 - val_op_main_loss: 0.3040 - val_op_conv_loss: 0.2900 - val_avg_loss: 0.2811 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8810\n",
      "Epoch 51/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0841 - op_main_loss: 0.2815 - op_conv_loss: 0.2008 - avg_loss: 0.2316 - op_main_accuracy: 0.8927 - op_conv_accuracy: 0.9184 - avg_accuracy: 0.9152\n",
      "Epoch 00051: val_avg_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0838 - op_main_loss: 0.2810 - op_conv_loss: 0.2009 - avg_loss: 0.2315 - op_main_accuracy: 0.8922 - op_conv_accuracy: 0.9178 - avg_accuracy: 0.9147 - val_loss: 1.2265 - val_op_main_loss: 0.2987 - val_op_conv_loss: 0.2831 - val_avg_loss: 0.2752 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8725\n",
      "Epoch 52/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1102 - op_main_loss: 0.2863 - op_conv_loss: 0.2155 - avg_loss: 0.2411 - op_main_accuracy: 0.8900 - op_conv_accuracy: 0.9138 - avg_accuracy: 0.9118\n",
      "Epoch 00052: val_avg_accuracy did not improve from 0.88102\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1070 - op_main_loss: 0.2852 - op_conv_loss: 0.2144 - avg_loss: 0.2400 - op_main_accuracy: 0.8908 - op_conv_accuracy: 0.9140 - avg_accuracy: 0.9123 - val_loss: 1.2286 - val_op_main_loss: 0.3002 - val_op_conv_loss: 0.2840 - val_avg_loss: 0.2776 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0726 - op_main_loss: 0.2799 - op_conv_loss: 0.1973 - avg_loss: 0.2284 - op_main_accuracy: 0.8923 - op_conv_accuracy: 0.9269 - avg_accuracy: 0.9185\n",
      "Epoch 00053: val_avg_accuracy improved from 0.88102 to 0.88385, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0763 - op_main_loss: 0.2811 - op_conv_loss: 0.1986 - avg_loss: 0.2296 - op_main_accuracy: 0.8918 - op_conv_accuracy: 0.9258 - avg_accuracy: 0.9178 - val_loss: 1.2630 - val_op_main_loss: 0.3033 - val_op_conv_loss: 0.3070 - val_avg_loss: 0.2865 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8839\n",
      "Epoch 54/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0756 - op_main_loss: 0.2785 - op_conv_loss: 0.2012 - avg_loss: 0.2295 - op_main_accuracy: 0.8886 - op_conv_accuracy: 0.9193 - avg_accuracy: 0.9167\n",
      "Epoch 00054: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0755 - op_main_loss: 0.2780 - op_conv_loss: 0.2016 - avg_loss: 0.2295 - op_main_accuracy: 0.8880 - op_conv_accuracy: 0.9190 - avg_accuracy: 0.9161 - val_loss: 1.2475 - val_op_main_loss: 0.3006 - val_op_conv_loss: 0.2996 - val_avg_loss: 0.2816 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8820\n",
      "Epoch 55/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0672 - op_main_loss: 0.2810 - op_conv_loss: 0.1937 - avg_loss: 0.2275 - op_main_accuracy: 0.8927 - op_conv_accuracy: 0.9241 - avg_accuracy: 0.9156\n",
      "Epoch 00055: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0658 - op_main_loss: 0.2802 - op_conv_loss: 0.1935 - avg_loss: 0.2271 - op_main_accuracy: 0.8932 - op_conv_accuracy: 0.9239 - avg_accuracy: 0.9156 - val_loss: 1.2293 - val_op_main_loss: 0.3000 - val_op_conv_loss: 0.2874 - val_avg_loss: 0.2769 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8829\n",
      "Epoch 56/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0627 - op_main_loss: 0.2768 - op_conv_loss: 0.1956 - avg_loss: 0.2262 - op_main_accuracy: 0.8920 - op_conv_accuracy: 0.9218 - avg_accuracy: 0.9145\n",
      "Epoch 00056: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0627 - op_main_loss: 0.2768 - op_conv_loss: 0.1956 - avg_loss: 0.2262 - op_main_accuracy: 0.8920 - op_conv_accuracy: 0.9218 - avg_accuracy: 0.9145 - val_loss: 1.2115 - val_op_main_loss: 0.2948 - val_op_conv_loss: 0.2812 - val_avg_loss: 0.2727 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8735\n",
      "Epoch 57/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0628 - op_main_loss: 0.2753 - op_conv_loss: 0.1980 - avg_loss: 0.2269 - op_main_accuracy: 0.8995 - op_conv_accuracy: 0.9226 - avg_accuracy: 0.9221\n",
      "Epoch 00057: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0635 - op_main_loss: 0.2751 - op_conv_loss: 0.1987 - avg_loss: 0.2271 - op_main_accuracy: 0.9003 - op_conv_accuracy: 0.9225 - avg_accuracy: 0.9223 - val_loss: 1.2058 - val_op_main_loss: 0.2964 - val_op_conv_loss: 0.2755 - val_avg_loss: 0.2712 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8735\n",
      "Epoch 58/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0346 - op_main_loss: 0.2690 - op_conv_loss: 0.1845 - avg_loss: 0.2183 - op_main_accuracy: 0.8943 - op_conv_accuracy: 0.9287 - avg_accuracy: 0.9239\n",
      "Epoch 00058: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0361 - op_main_loss: 0.2696 - op_conv_loss: 0.1849 - avg_loss: 0.2188 - op_main_accuracy: 0.8937 - op_conv_accuracy: 0.9284 - avg_accuracy: 0.9237 - val_loss: 1.2092 - val_op_main_loss: 0.2941 - val_op_conv_loss: 0.2823 - val_avg_loss: 0.2707 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8772\n",
      "Epoch 59/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0404 - op_main_loss: 0.2698 - op_conv_loss: 0.1892 - avg_loss: 0.2192 - op_main_accuracy: 0.8986 - op_conv_accuracy: 0.9245 - avg_accuracy: 0.9200\n",
      "Epoch 00059: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0416 - op_main_loss: 0.2703 - op_conv_loss: 0.1895 - avg_loss: 0.2196 - op_main_accuracy: 0.8982 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9197 - val_loss: 1.4643 - val_op_main_loss: 0.3468 - val_op_conv_loss: 0.4109 - val_avg_loss: 0.3456 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8508 - val_avg_accuracy: 0.8489\n",
      "Epoch 60/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0426 - op_main_loss: 0.2715 - op_conv_loss: 0.1893 - avg_loss: 0.2209 - op_main_accuracy: 0.8993 - op_conv_accuracy: 0.9289 - avg_accuracy: 0.9253\n",
      "Epoch 00060: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0426 - op_main_loss: 0.2715 - op_conv_loss: 0.1893 - avg_loss: 0.2209 - op_main_accuracy: 0.8993 - op_conv_accuracy: 0.9289 - avg_accuracy: 0.9253 - val_loss: 1.2230 - val_op_main_loss: 0.2920 - val_op_conv_loss: 0.2946 - val_avg_loss: 0.2749 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8791\n",
      "Epoch 61/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0150 - op_main_loss: 0.2622 - op_conv_loss: 0.1808 - avg_loss: 0.2109 - op_main_accuracy: 0.9031 - op_conv_accuracy: 0.9293 - avg_accuracy: 0.9296\n",
      "Epoch 00061: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0150 - op_main_loss: 0.2622 - op_conv_loss: 0.1808 - avg_loss: 0.2109 - op_main_accuracy: 0.9031 - op_conv_accuracy: 0.9293 - avg_accuracy: 0.9296 - val_loss: 1.2086 - val_op_main_loss: 0.2881 - val_op_conv_loss: 0.2901 - val_avg_loss: 0.2701 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8791\n",
      "Epoch 62/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0203 - op_main_loss: 0.2643 - op_conv_loss: 0.1822 - avg_loss: 0.2140 - op_main_accuracy: 0.9010 - op_conv_accuracy: 0.9250 - avg_accuracy: 0.9219\n",
      "Epoch 00062: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0187 - op_main_loss: 0.2639 - op_conv_loss: 0.1815 - avg_loss: 0.2135 - op_main_accuracy: 0.9015 - op_conv_accuracy: 0.9253 - avg_accuracy: 0.9227 - val_loss: 1.3350 - val_op_main_loss: 0.3198 - val_op_conv_loss: 0.3493 - val_avg_loss: 0.3066 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8725\n",
      "Epoch 63/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0169 - op_main_loss: 0.2630 - op_conv_loss: 0.1821 - avg_loss: 0.2129 - op_main_accuracy: 0.9007 - op_conv_accuracy: 0.9264 - avg_accuracy: 0.9256\n",
      "Epoch 00063: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0138 - op_main_loss: 0.2621 - op_conv_loss: 0.1809 - avg_loss: 0.2120 - op_main_accuracy: 0.9012 - op_conv_accuracy: 0.9270 - avg_accuracy: 0.9263 - val_loss: 1.1751 - val_op_main_loss: 0.2836 - val_op_conv_loss: 0.2716 - val_avg_loss: 0.2612 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8791\n",
      "Epoch 64/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0301 - op_main_loss: 0.2668 - op_conv_loss: 0.1873 - avg_loss: 0.2170 - op_main_accuracy: 0.8993 - op_conv_accuracy: 0.9260 - avg_accuracy: 0.9260\n",
      "Epoch 00064: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0301 - op_main_loss: 0.2668 - op_conv_loss: 0.1873 - avg_loss: 0.2170 - op_main_accuracy: 0.8993 - op_conv_accuracy: 0.9260 - avg_accuracy: 0.9260 - val_loss: 1.1908 - val_op_main_loss: 0.2856 - val_op_conv_loss: 0.2818 - val_avg_loss: 0.2651 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8829\n",
      "Epoch 65/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 1.0010 - op_main_loss: 0.2584 - op_conv_loss: 0.1763 - avg_loss: 0.2077 - op_main_accuracy: 0.9053 - op_conv_accuracy: 0.9295 - avg_accuracy: 0.9288\n",
      "Epoch 00065: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0042 - op_main_loss: 0.2591 - op_conv_loss: 0.1776 - avg_loss: 0.2088 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9289 - avg_accuracy: 0.9279 - val_loss: 1.1745 - val_op_main_loss: 0.2829 - val_op_conv_loss: 0.2724 - val_avg_loss: 0.2614 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8791\n",
      "Epoch 66/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0135 - op_main_loss: 0.2632 - op_conv_loss: 0.1804 - avg_loss: 0.2122 - op_main_accuracy: 0.9019 - op_conv_accuracy: 0.9291 - avg_accuracy: 0.9284\n",
      "Epoch 00066: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0150 - op_main_loss: 0.2633 - op_conv_loss: 0.1814 - avg_loss: 0.2127 - op_main_accuracy: 0.9022 - op_conv_accuracy: 0.9284 - avg_accuracy: 0.9284 - val_loss: 1.1848 - val_op_main_loss: 0.2870 - val_op_conv_loss: 0.2781 - val_avg_loss: 0.2639 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8801\n",
      "Epoch 67/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0027 - op_main_loss: 0.2578 - op_conv_loss: 0.1806 - avg_loss: 0.2082 - op_main_accuracy: 0.9058 - op_conv_accuracy: 0.9304 - avg_accuracy: 0.9278\n",
      "Epoch 00067: val_avg_accuracy did not improve from 0.88385\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0026 - op_main_loss: 0.2579 - op_conv_loss: 0.1805 - avg_loss: 0.2081 - op_main_accuracy: 0.9057 - op_conv_accuracy: 0.9305 - avg_accuracy: 0.9279 - val_loss: 1.3868 - val_op_main_loss: 0.3325 - val_op_conv_loss: 0.3747 - val_avg_loss: 0.3242 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8602\n",
      "Epoch 68/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9899 - op_main_loss: 0.2561 - op_conv_loss: 0.1733 - avg_loss: 0.2051 - op_main_accuracy: 0.9015 - op_conv_accuracy: 0.9302 - avg_accuracy: 0.9254\n",
      "Epoch 00068: val_avg_accuracy improved from 0.88385 to 0.89046, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9906 - op_main_loss: 0.2565 - op_conv_loss: 0.1734 - avg_loss: 0.2054 - op_main_accuracy: 0.9012 - op_conv_accuracy: 0.9301 - avg_accuracy: 0.9251 - val_loss: 1.2008 - val_op_main_loss: 0.2829 - val_op_conv_loss: 0.2951 - val_avg_loss: 0.2677 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8905\n",
      "Epoch 69/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9879 - op_main_loss: 0.2558 - op_conv_loss: 0.1723 - avg_loss: 0.2043 - op_main_accuracy: 0.9076 - op_conv_accuracy: 0.9338 - avg_accuracy: 0.9341\n",
      "Epoch 00069: val_avg_accuracy did not improve from 0.89046\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9879 - op_main_loss: 0.2558 - op_conv_loss: 0.1723 - avg_loss: 0.2043 - op_main_accuracy: 0.9076 - op_conv_accuracy: 0.9338 - avg_accuracy: 0.9341 - val_loss: 1.1617 - val_op_main_loss: 0.2792 - val_op_conv_loss: 0.2697 - val_avg_loss: 0.2573 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8905\n",
      "Epoch 70/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9846 - op_main_loss: 0.2527 - op_conv_loss: 0.1732 - avg_loss: 0.2034 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9329 - avg_accuracy: 0.9327\n",
      "Epoch 00070: val_avg_accuracy improved from 0.89046 to 0.89141, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9846 - op_main_loss: 0.2527 - op_conv_loss: 0.1732 - avg_loss: 0.2034 - op_main_accuracy: 0.9043 - op_conv_accuracy: 0.9329 - avg_accuracy: 0.9327 - val_loss: 1.1996 - val_op_main_loss: 0.2908 - val_op_conv_loss: 0.2858 - val_avg_loss: 0.2688 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8914\n",
      "Epoch 71/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9941 - op_main_loss: 0.2580 - op_conv_loss: 0.1756 - avg_loss: 0.2072 - op_main_accuracy: 0.9026 - op_conv_accuracy: 0.9298 - avg_accuracy: 0.9263\n",
      "Epoch 00071: val_avg_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9941 - op_main_loss: 0.2580 - op_conv_loss: 0.1756 - avg_loss: 0.2072 - op_main_accuracy: 0.9026 - op_conv_accuracy: 0.9298 - avg_accuracy: 0.9263 - val_loss: 1.2967 - val_op_main_loss: 0.3086 - val_op_conv_loss: 0.3378 - val_avg_loss: 0.2979 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8763\n",
      "Epoch 72/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9717 - op_main_loss: 0.2513 - op_conv_loss: 0.1679 - avg_loss: 0.1998 - op_main_accuracy: 0.9094 - op_conv_accuracy: 0.9329 - avg_accuracy: 0.9303\n",
      "Epoch 00072: val_avg_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9733 - op_main_loss: 0.2521 - op_conv_loss: 0.1681 - avg_loss: 0.2003 - op_main_accuracy: 0.9086 - op_conv_accuracy: 0.9329 - avg_accuracy: 0.9298 - val_loss: 1.1801 - val_op_main_loss: 0.2830 - val_op_conv_loss: 0.2816 - val_avg_loss: 0.2624 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8895\n",
      "Epoch 73/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9792 - op_main_loss: 0.2492 - op_conv_loss: 0.1746 - avg_loss: 0.2025 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9279 - avg_accuracy: 0.9301\n",
      "Epoch 00073: val_avg_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9792 - op_main_loss: 0.2492 - op_conv_loss: 0.1746 - avg_loss: 0.2025 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9279 - avg_accuracy: 0.9301 - val_loss: 1.3447 - val_op_main_loss: 0.3083 - val_op_conv_loss: 0.3754 - val_avg_loss: 0.3082 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8631 - val_avg_accuracy: 0.8669\n",
      "Epoch 74/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9742 - op_main_loss: 0.2510 - op_conv_loss: 0.1699 - avg_loss: 0.2012 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9322 - avg_accuracy: 0.9298\n",
      "Epoch 00074: val_avg_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9742 - op_main_loss: 0.2510 - op_conv_loss: 0.1699 - avg_loss: 0.2012 - op_main_accuracy: 0.9074 - op_conv_accuracy: 0.9322 - avg_accuracy: 0.9298 - val_loss: 1.4150 - val_op_main_loss: 0.3228 - val_op_conv_loss: 0.4124 - val_avg_loss: 0.3286 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8527 - val_avg_accuracy: 0.8555\n",
      "Epoch 75/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9828 - op_main_loss: 0.2557 - op_conv_loss: 0.1723 - avg_loss: 0.2036 - op_main_accuracy: 0.9029 - op_conv_accuracy: 0.9323 - avg_accuracy: 0.9309\n",
      "Epoch 00075: val_avg_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9831 - op_main_loss: 0.2558 - op_conv_loss: 0.1723 - avg_loss: 0.2037 - op_main_accuracy: 0.9029 - op_conv_accuracy: 0.9322 - avg_accuracy: 0.9308 - val_loss: 1.1501 - val_op_main_loss: 0.2759 - val_op_conv_loss: 0.2681 - val_avg_loss: 0.2548 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8829\n",
      "Epoch 76/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9646 - op_main_loss: 0.2492 - op_conv_loss: 0.1657 - avg_loss: 0.1983 - op_main_accuracy: 0.9127 - op_conv_accuracy: 0.9332 - avg_accuracy: 0.9317\n",
      "Epoch 00076: val_avg_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9608 - op_main_loss: 0.2478 - op_conv_loss: 0.1645 - avg_loss: 0.1971 - op_main_accuracy: 0.9130 - op_conv_accuracy: 0.9338 - avg_accuracy: 0.9322 - val_loss: 1.1837 - val_op_main_loss: 0.2768 - val_op_conv_loss: 0.2919 - val_avg_loss: 0.2644 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9572 - op_main_loss: 0.2450 - op_conv_loss: 0.1652 - avg_loss: 0.1961 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9345 - avg_accuracy: 0.9343\n",
      "Epoch 00077: val_avg_accuracy did not improve from 0.89141\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9572 - op_main_loss: 0.2450 - op_conv_loss: 0.1652 - avg_loss: 0.1961 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9345 - avg_accuracy: 0.9343 - val_loss: 1.1529 - val_op_main_loss: 0.2781 - val_op_conv_loss: 0.2686 - val_avg_loss: 0.2559 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8867\n",
      "Epoch 78/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9496 - op_main_loss: 0.2431 - op_conv_loss: 0.1629 - avg_loss: 0.1935 - op_main_accuracy: 0.9090 - op_conv_accuracy: 0.9364 - avg_accuracy: 0.9362\n",
      "Epoch 00078: val_avg_accuracy improved from 0.89141 to 0.89330, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9496 - op_main_loss: 0.2431 - op_conv_loss: 0.1629 - avg_loss: 0.1935 - op_main_accuracy: 0.9090 - op_conv_accuracy: 0.9364 - avg_accuracy: 0.9362 - val_loss: 1.1447 - val_op_main_loss: 0.2732 - val_op_conv_loss: 0.2700 - val_avg_loss: 0.2522 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8933\n",
      "Epoch 79/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9576 - op_main_loss: 0.2454 - op_conv_loss: 0.1669 - avg_loss: 0.1958 - op_main_accuracy: 0.9075 - op_conv_accuracy: 0.9346 - avg_accuracy: 0.9341\n",
      "Epoch 00079: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9604 - op_main_loss: 0.2464 - op_conv_loss: 0.1679 - avg_loss: 0.1966 - op_main_accuracy: 0.9064 - op_conv_accuracy: 0.9341 - avg_accuracy: 0.9336 - val_loss: 1.2083 - val_op_main_loss: 0.2884 - val_op_conv_loss: 0.2985 - val_avg_loss: 0.2734 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8810\n",
      "Epoch 80/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9806 - op_main_loss: 0.2531 - op_conv_loss: 0.1753 - avg_loss: 0.2034 - op_main_accuracy: 0.9015 - op_conv_accuracy: 0.9291 - avg_accuracy: 0.9267\n",
      "Epoch 00080: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9806 - op_main_loss: 0.2531 - op_conv_loss: 0.1753 - avg_loss: 0.2034 - op_main_accuracy: 0.9015 - op_conv_accuracy: 0.9291 - avg_accuracy: 0.9267 - val_loss: 1.2038 - val_op_main_loss: 0.2888 - val_op_conv_loss: 0.2964 - val_avg_loss: 0.2706 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8810\n",
      "Epoch 81/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9313 - op_main_loss: 0.2406 - op_conv_loss: 0.1541 - avg_loss: 0.1886 - op_main_accuracy: 0.9155 - op_conv_accuracy: 0.9382 - avg_accuracy: 0.9360\n",
      "Epoch 00081: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9325 - op_main_loss: 0.2410 - op_conv_loss: 0.1545 - avg_loss: 0.1890 - op_main_accuracy: 0.9142 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9360 - val_loss: 1.2028 - val_op_main_loss: 0.2822 - val_op_conv_loss: 0.3050 - val_avg_loss: 0.2670 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8924\n",
      "Epoch 82/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9369 - op_main_loss: 0.2420 - op_conv_loss: 0.1573 - avg_loss: 0.1895 - op_main_accuracy: 0.9141 - op_conv_accuracy: 0.9382 - avg_accuracy: 0.9349\n",
      "Epoch 00082: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9392 - op_main_loss: 0.2431 - op_conv_loss: 0.1579 - avg_loss: 0.1902 - op_main_accuracy: 0.9128 - op_conv_accuracy: 0.9379 - avg_accuracy: 0.9343 - val_loss: 1.1751 - val_op_main_loss: 0.2808 - val_op_conv_loss: 0.2858 - val_avg_loss: 0.2614 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8867\n",
      "Epoch 83/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9354 - op_main_loss: 0.2394 - op_conv_loss: 0.1589 - avg_loss: 0.1898 - op_main_accuracy: 0.9121 - op_conv_accuracy: 0.9360 - avg_accuracy: 0.9357\n",
      "Epoch 00083: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9354 - op_main_loss: 0.2394 - op_conv_loss: 0.1589 - avg_loss: 0.1898 - op_main_accuracy: 0.9121 - op_conv_accuracy: 0.9360 - avg_accuracy: 0.9357 - val_loss: 1.1503 - val_op_main_loss: 0.2729 - val_op_conv_loss: 0.2772 - val_avg_loss: 0.2531 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8895\n",
      "Epoch 84/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9357 - op_main_loss: 0.2392 - op_conv_loss: 0.1597 - avg_loss: 0.1896 - op_main_accuracy: 0.9128 - op_conv_accuracy: 0.9376 - avg_accuracy: 0.9353\n",
      "Epoch 00084: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9357 - op_main_loss: 0.2392 - op_conv_loss: 0.1597 - avg_loss: 0.1896 - op_main_accuracy: 0.9128 - op_conv_accuracy: 0.9376 - avg_accuracy: 0.9353 - val_loss: 1.4464 - val_op_main_loss: 0.3331 - val_op_conv_loss: 0.4297 - val_avg_loss: 0.3368 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8612\n",
      "Epoch 85/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9443 - op_main_loss: 0.2423 - op_conv_loss: 0.1638 - avg_loss: 0.1921 - op_main_accuracy: 0.9139 - op_conv_accuracy: 0.9358 - avg_accuracy: 0.9306\n",
      "Epoch 00085: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9468 - op_main_loss: 0.2424 - op_conv_loss: 0.1656 - avg_loss: 0.1926 - op_main_accuracy: 0.9133 - op_conv_accuracy: 0.9353 - avg_accuracy: 0.9301 - val_loss: 1.1368 - val_op_main_loss: 0.2688 - val_op_conv_loss: 0.2727 - val_avg_loss: 0.2496 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8848\n",
      "Epoch 86/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9230 - op_main_loss: 0.2374 - op_conv_loss: 0.1536 - avg_loss: 0.1864 - op_main_accuracy: 0.9150 - op_conv_accuracy: 0.9387 - avg_accuracy: 0.9361\n",
      "Epoch 00086: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9225 - op_main_loss: 0.2373 - op_conv_loss: 0.1534 - avg_loss: 0.1863 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9388 - avg_accuracy: 0.9362 - val_loss: 1.1494 - val_op_main_loss: 0.2704 - val_op_conv_loss: 0.2796 - val_avg_loss: 0.2537 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8857\n",
      "Epoch 87/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9030 - op_main_loss: 0.2313 - op_conv_loss: 0.1465 - avg_loss: 0.1799 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9416\n",
      "Epoch 00087: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9048 - op_main_loss: 0.2318 - op_conv_loss: 0.1472 - avg_loss: 0.1804 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9419 - val_loss: 1.1515 - val_op_main_loss: 0.2697 - val_op_conv_loss: 0.2813 - val_avg_loss: 0.2549 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8905\n",
      "Epoch 88/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9270 - op_main_loss: 0.2399 - op_conv_loss: 0.1545 - avg_loss: 0.1871 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9377 - avg_accuracy: 0.9375\n",
      "Epoch 00088: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9285 - op_main_loss: 0.2400 - op_conv_loss: 0.1555 - avg_loss: 0.1876 - op_main_accuracy: 0.9116 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9371 - val_loss: 1.1535 - val_op_main_loss: 0.2716 - val_op_conv_loss: 0.2822 - val_avg_loss: 0.2543 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8933\n",
      "Epoch 89/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.9226 - op_main_loss: 0.2353 - op_conv_loss: 0.1563 - avg_loss: 0.1861 - op_main_accuracy: 0.9112 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9345\n",
      "Epoch 00089: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9226 - op_main_loss: 0.2353 - op_conv_loss: 0.1563 - avg_loss: 0.1861 - op_main_accuracy: 0.9112 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9345 - val_loss: 1.1314 - val_op_main_loss: 0.2677 - val_op_conv_loss: 0.2697 - val_avg_loss: 0.2493 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8876\n",
      "Epoch 90/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9112 - op_main_loss: 0.2368 - op_conv_loss: 0.1474 - avg_loss: 0.1823 - op_main_accuracy: 0.9099 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9392\n",
      "Epoch 00090: val_avg_accuracy did not improve from 0.89330\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9133 - op_main_loss: 0.2374 - op_conv_loss: 0.1481 - avg_loss: 0.1830 - op_main_accuracy: 0.9095 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9383 - val_loss: 1.2288 - val_op_main_loss: 0.2885 - val_op_conv_loss: 0.3200 - val_avg_loss: 0.2753 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8924\n",
      "Epoch 91/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9102 - op_main_loss: 0.2356 - op_conv_loss: 0.1479 - avg_loss: 0.1818 - op_main_accuracy: 0.9167 - op_conv_accuracy: 0.9436 - avg_accuracy: 0.9419\n",
      "Epoch 00091: val_avg_accuracy improved from 0.89330 to 0.89424, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9100 - op_main_loss: 0.2356 - op_conv_loss: 0.1477 - avg_loss: 0.1818 - op_main_accuracy: 0.9168 - op_conv_accuracy: 0.9438 - avg_accuracy: 0.9416 - val_loss: 1.1090 - val_op_main_loss: 0.2639 - val_op_conv_loss: 0.2587 - val_avg_loss: 0.2415 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8942\n",
      "Epoch 92/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8924 - op_main_loss: 0.2248 - op_conv_loss: 0.1462 - avg_loss: 0.1765 - op_main_accuracy: 0.9204 - op_conv_accuracy: 0.9404 - avg_accuracy: 0.9392\n",
      "Epoch 00092: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8913 - op_main_loss: 0.2244 - op_conv_loss: 0.1458 - avg_loss: 0.1761 - op_main_accuracy: 0.9206 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9393 - val_loss: 1.1468 - val_op_main_loss: 0.2686 - val_op_conv_loss: 0.2810 - val_avg_loss: 0.2525 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8924\n",
      "Epoch 93/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9166 - op_main_loss: 0.2323 - op_conv_loss: 0.1566 - avg_loss: 0.1840 - op_main_accuracy: 0.9173 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9357\n",
      "Epoch 00093: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9166 - op_main_loss: 0.2323 - op_conv_loss: 0.1566 - avg_loss: 0.1840 - op_main_accuracy: 0.9173 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9357 - val_loss: 1.1353 - val_op_main_loss: 0.2679 - val_op_conv_loss: 0.2754 - val_avg_loss: 0.2493 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8895\n",
      "Epoch 94/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9175 - op_main_loss: 0.2341 - op_conv_loss: 0.1555 - avg_loss: 0.1855 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9395\n",
      "Epoch 00094: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9175 - op_main_loss: 0.2341 - op_conv_loss: 0.1555 - avg_loss: 0.1855 - op_main_accuracy: 0.9152 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9395 - val_loss: 1.1334 - val_op_main_loss: 0.2663 - val_op_conv_loss: 0.2748 - val_avg_loss: 0.2498 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8924\n",
      "Epoch 95/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8854 - op_main_loss: 0.2243 - op_conv_loss: 0.1434 - avg_loss: 0.1748 - op_main_accuracy: 0.9244 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9443\n",
      "Epoch 00095: val_avg_accuracy improved from 0.89424 to 0.89707, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8870 - op_main_loss: 0.2245 - op_conv_loss: 0.1442 - avg_loss: 0.1753 - op_main_accuracy: 0.9239 - op_conv_accuracy: 0.9416 - avg_accuracy: 0.9438 - val_loss: 1.1384 - val_op_main_loss: 0.2660 - val_op_conv_loss: 0.2793 - val_avg_loss: 0.2503 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8971\n",
      "Epoch 96/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8905 - op_main_loss: 0.2283 - op_conv_loss: 0.1433 - avg_loss: 0.1765 - op_main_accuracy: 0.9177 - op_conv_accuracy: 0.9413 - avg_accuracy: 0.9387\n",
      "Epoch 00096: val_avg_accuracy did not improve from 0.89707\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8911 - op_main_loss: 0.2284 - op_conv_loss: 0.1437 - avg_loss: 0.1766 - op_main_accuracy: 0.9178 - op_conv_accuracy: 0.9414 - avg_accuracy: 0.9388 - val_loss: 1.1983 - val_op_main_loss: 0.2774 - val_op_conv_loss: 0.3101 - val_avg_loss: 0.2687 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8867\n",
      "Epoch 97/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8809 - op_main_loss: 0.2235 - op_conv_loss: 0.1427 - avg_loss: 0.1732 - op_main_accuracy: 0.9219 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9425\n",
      "Epoch 00097: val_avg_accuracy did not improve from 0.89707\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8847 - op_main_loss: 0.2245 - op_conv_loss: 0.1445 - avg_loss: 0.1743 - op_main_accuracy: 0.9213 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9423 - val_loss: 1.1739 - val_op_main_loss: 0.2717 - val_op_conv_loss: 0.3001 - val_avg_loss: 0.2602 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8876\n",
      "Epoch 98/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8701 - op_main_loss: 0.2192 - op_conv_loss: 0.1393 - avg_loss: 0.1707 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9440\n",
      "Epoch 00098: val_avg_accuracy did not improve from 0.89707\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8701 - op_main_loss: 0.2192 - op_conv_loss: 0.1393 - avg_loss: 0.1707 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9440 - val_loss: 1.1805 - val_op_main_loss: 0.2693 - val_op_conv_loss: 0.3081 - val_avg_loss: 0.2621 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8857\n",
      "Epoch 99/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8793 - op_main_loss: 0.2258 - op_conv_loss: 0.1397 - avg_loss: 0.1728 - op_main_accuracy: 0.9189 - op_conv_accuracy: 0.9463 - avg_accuracy: 0.9437\n",
      "Epoch 00099: val_avg_accuracy did not improve from 0.89707\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8809 - op_main_loss: 0.2262 - op_conv_loss: 0.1404 - avg_loss: 0.1733 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9428 - val_loss: 1.1693 - val_op_main_loss: 0.2724 - val_op_conv_loss: 0.2977 - val_avg_loss: 0.2583 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8952\n",
      "Epoch 100/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8688 - op_main_loss: 0.2200 - op_conv_loss: 0.1381 - avg_loss: 0.1698 - op_main_accuracy: 0.9193 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9426\n",
      "Epoch 00100: val_avg_accuracy did not improve from 0.89707\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8708 - op_main_loss: 0.2207 - op_conv_loss: 0.1387 - avg_loss: 0.1705 - op_main_accuracy: 0.9185 - op_conv_accuracy: 0.9442 - avg_accuracy: 0.9421 - val_loss: 1.1511 - val_op_main_loss: 0.2699 - val_op_conv_loss: 0.2870 - val_avg_loss: 0.2542 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8723 - op_main_loss: 0.2183 - op_conv_loss: 0.1432 - avg_loss: 0.1706 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9449\n",
      "Epoch 00101: val_avg_accuracy did not improve from 0.89707\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8733 - op_main_loss: 0.2187 - op_conv_loss: 0.1434 - avg_loss: 0.1709 - op_main_accuracy: 0.9244 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9449 - val_loss: 1.1536 - val_op_main_loss: 0.2741 - val_op_conv_loss: 0.2847 - val_avg_loss: 0.2552 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8961\n",
      "Epoch 102/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8847 - op_main_loss: 0.2272 - op_conv_loss: 0.1430 - avg_loss: 0.1756 - op_main_accuracy: 0.9200 - op_conv_accuracy: 0.9446 - avg_accuracy: 0.9411\n",
      "Epoch 00102: val_avg_accuracy improved from 0.89707 to 0.89896, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8839 - op_main_loss: 0.2269 - op_conv_loss: 0.1428 - avg_loss: 0.1753 - op_main_accuracy: 0.9201 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9412 - val_loss: 1.1274 - val_op_main_loss: 0.2676 - val_op_conv_loss: 0.2724 - val_avg_loss: 0.2492 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8990\n",
      "Epoch 103/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8754 - op_main_loss: 0.2263 - op_conv_loss: 0.1378 - avg_loss: 0.1732 - op_main_accuracy: 0.9206 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9449\n",
      "Epoch 00103: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8755 - op_main_loss: 0.2259 - op_conv_loss: 0.1382 - avg_loss: 0.1732 - op_main_accuracy: 0.9206 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9447 - val_loss: 1.2517 - val_op_main_loss: 0.2871 - val_op_conv_loss: 0.3445 - val_avg_loss: 0.2817 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8933\n",
      "Epoch 104/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8635 - op_main_loss: 0.2179 - op_conv_loss: 0.1388 - avg_loss: 0.1685 - op_main_accuracy: 0.9219 - op_conv_accuracy: 0.9451 - avg_accuracy: 0.9396\n",
      "Epoch 00104: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8628 - op_main_loss: 0.2176 - op_conv_loss: 0.1386 - avg_loss: 0.1683 - op_main_accuracy: 0.9220 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9397 - val_loss: 1.2170 - val_op_main_loss: 0.2855 - val_op_conv_loss: 0.3220 - val_avg_loss: 0.2715 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8924\n",
      "Epoch 105/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8594 - op_main_loss: 0.2179 - op_conv_loss: 0.1356 - avg_loss: 0.1679 - op_main_accuracy: 0.9197 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9454\n",
      "Epoch 00105: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8594 - op_main_loss: 0.2179 - op_conv_loss: 0.1356 - avg_loss: 0.1679 - op_main_accuracy: 0.9197 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9454 - val_loss: 1.1690 - val_op_main_loss: 0.2745 - val_op_conv_loss: 0.2981 - val_avg_loss: 0.2585 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8924\n",
      "Epoch 106/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8591 - op_main_loss: 0.2184 - op_conv_loss: 0.1357 - avg_loss: 0.1675 - op_main_accuracy: 0.9257 - op_conv_accuracy: 0.9477 - avg_accuracy: 0.9422\n",
      "Epoch 00106: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8593 - op_main_loss: 0.2185 - op_conv_loss: 0.1357 - avg_loss: 0.1675 - op_main_accuracy: 0.9256 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9421 - val_loss: 1.1487 - val_op_main_loss: 0.2634 - val_op_conv_loss: 0.2954 - val_avg_loss: 0.2529 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8924\n",
      "Epoch 107/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8343 - op_main_loss: 0.2090 - op_conv_loss: 0.1278 - avg_loss: 0.1600 - op_main_accuracy: 0.9290 - op_conv_accuracy: 0.9508 - avg_accuracy: 0.9491\n",
      "Epoch 00107: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8349 - op_main_loss: 0.2088 - op_conv_loss: 0.1285 - avg_loss: 0.1601 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9504 - avg_accuracy: 0.9492 - val_loss: 1.1944 - val_op_main_loss: 0.2763 - val_op_conv_loss: 0.3157 - val_avg_loss: 0.2652 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8952\n",
      "Epoch 108/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8608 - op_main_loss: 0.2176 - op_conv_loss: 0.1379 - avg_loss: 0.1682 - op_main_accuracy: 0.9195 - op_conv_accuracy: 0.9470 - avg_accuracy: 0.9437\n",
      "Epoch 00108: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8615 - op_main_loss: 0.2178 - op_conv_loss: 0.1381 - avg_loss: 0.1685 - op_main_accuracy: 0.9194 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9435 - val_loss: 1.1716 - val_op_main_loss: 0.2733 - val_op_conv_loss: 0.3035 - val_avg_loss: 0.2576 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8961\n",
      "Epoch 109/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8623 - op_main_loss: 0.2220 - op_conv_loss: 0.1349 - avg_loss: 0.1691 - op_main_accuracy: 0.9167 - op_conv_accuracy: 0.9448 - avg_accuracy: 0.9425\n",
      "Epoch 00109: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8641 - op_main_loss: 0.2225 - op_conv_loss: 0.1358 - avg_loss: 0.1695 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9421 - val_loss: 1.1210 - val_op_main_loss: 0.2662 - val_op_conv_loss: 0.2736 - val_avg_loss: 0.2452 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8971\n",
      "Epoch 110/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8727 - op_main_loss: 0.2247 - op_conv_loss: 0.1388 - avg_loss: 0.1724 - op_main_accuracy: 0.9196 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9387\n",
      "Epoch 00110: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8750 - op_main_loss: 0.2250 - op_conv_loss: 0.1404 - avg_loss: 0.1729 - op_main_accuracy: 0.9199 - op_conv_accuracy: 0.9445 - avg_accuracy: 0.9388 - val_loss: 1.1352 - val_op_main_loss: 0.2680 - val_op_conv_loss: 0.2807 - val_avg_loss: 0.2501 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8924\n",
      "Epoch 111/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8380 - op_main_loss: 0.2087 - op_conv_loss: 0.1318 - avg_loss: 0.1609 - op_main_accuracy: 0.9264 - op_conv_accuracy: 0.9493 - avg_accuracy: 0.9481\n",
      "Epoch 00111: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8389 - op_main_loss: 0.2089 - op_conv_loss: 0.1321 - avg_loss: 0.1612 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9492 - avg_accuracy: 0.9480 - val_loss: 1.2805 - val_op_main_loss: 0.2789 - val_op_conv_loss: 0.3743 - val_avg_loss: 0.2905 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8602 - val_avg_accuracy: 0.8744\n",
      "Epoch 112/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8509 - op_main_loss: 0.2151 - op_conv_loss: 0.1343 - avg_loss: 0.1652 - op_main_accuracy: 0.9237 - op_conv_accuracy: 0.9479 - avg_accuracy: 0.9457\n",
      "Epoch 00112: val_avg_accuracy improved from 0.89896 to 0.90085, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8463 - op_main_loss: 0.2136 - op_conv_loss: 0.1326 - avg_loss: 0.1638 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9464 - val_loss: 1.1588 - val_op_main_loss: 0.2734 - val_op_conv_loss: 0.2949 - val_avg_loss: 0.2542 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8447 - op_main_loss: 0.2155 - op_conv_loss: 0.1297 - avg_loss: 0.1635 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9481 - avg_accuracy: 0.9447\n",
      "Epoch 00113: val_avg_accuracy improved from 0.90085 to 0.90274, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8432 - op_main_loss: 0.2147 - op_conv_loss: 0.1295 - avg_loss: 0.1630 - op_main_accuracy: 0.9159 - op_conv_accuracy: 0.9483 - avg_accuracy: 0.9449 - val_loss: 1.1442 - val_op_main_loss: 0.2695 - val_op_conv_loss: 0.2864 - val_avg_loss: 0.2525 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9027\n",
      "Epoch 114/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8577 - op_main_loss: 0.2157 - op_conv_loss: 0.1399 - avg_loss: 0.1673 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9464\n",
      "Epoch 00114: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8577 - op_main_loss: 0.2157 - op_conv_loss: 0.1399 - avg_loss: 0.1673 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9464 - val_loss: 1.2869 - val_op_main_loss: 0.2878 - val_op_conv_loss: 0.3774 - val_avg_loss: 0.2874 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8820\n",
      "Epoch 115/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8456 - op_main_loss: 0.2162 - op_conv_loss: 0.1315 - avg_loss: 0.1636 - op_main_accuracy: 0.9185 - op_conv_accuracy: 0.9524 - avg_accuracy: 0.9450\n",
      "Epoch 00115: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8445 - op_main_loss: 0.2161 - op_conv_loss: 0.1308 - avg_loss: 0.1633 - op_main_accuracy: 0.9182 - op_conv_accuracy: 0.9527 - avg_accuracy: 0.9457 - val_loss: 1.1842 - val_op_main_loss: 0.2729 - val_op_conv_loss: 0.3135 - val_avg_loss: 0.2631 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8914\n",
      "Epoch 116/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8286 - op_main_loss: 0.2077 - op_conv_loss: 0.1281 - avg_loss: 0.1586 - op_main_accuracy: 0.9238 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9471\n",
      "Epoch 00116: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8289 - op_main_loss: 0.2074 - op_conv_loss: 0.1287 - avg_loss: 0.1586 - op_main_accuracy: 0.9241 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9475 - val_loss: 1.1189 - val_op_main_loss: 0.2594 - val_op_conv_loss: 0.2803 - val_avg_loss: 0.2452 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8961\n",
      "Epoch 117/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8167 - op_main_loss: 0.2031 - op_conv_loss: 0.1251 - avg_loss: 0.1551 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9534 - avg_accuracy: 0.9513\n",
      "Epoch 00117: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8167 - op_main_loss: 0.2031 - op_conv_loss: 0.1251 - avg_loss: 0.1551 - op_main_accuracy: 0.9291 - op_conv_accuracy: 0.9534 - avg_accuracy: 0.9513 - val_loss: 1.1619 - val_op_main_loss: 0.2620 - val_op_conv_loss: 0.3118 - val_avg_loss: 0.2549 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8961\n",
      "Epoch 118/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8315 - op_main_loss: 0.2086 - op_conv_loss: 0.1304 - avg_loss: 0.1597 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9487 - avg_accuracy: 0.9466\n",
      "Epoch 00118: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8315 - op_main_loss: 0.2086 - op_conv_loss: 0.1304 - avg_loss: 0.1597 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9487 - avg_accuracy: 0.9466 - val_loss: 1.1929 - val_op_main_loss: 0.2658 - val_op_conv_loss: 0.3328 - val_avg_loss: 0.2617 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8905\n",
      "Epoch 119/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8280 - op_main_loss: 0.2076 - op_conv_loss: 0.1289 - avg_loss: 0.1594 - op_main_accuracy: 0.9248 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9459\n",
      "Epoch 00119: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8291 - op_main_loss: 0.2079 - op_conv_loss: 0.1294 - avg_loss: 0.1597 - op_main_accuracy: 0.9249 - op_conv_accuracy: 0.9471 - avg_accuracy: 0.9459 - val_loss: 1.1817 - val_op_main_loss: 0.2792 - val_op_conv_loss: 0.3082 - val_avg_loss: 0.2623 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8961\n",
      "Epoch 120/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8375 - op_main_loss: 0.2151 - op_conv_loss: 0.1291 - avg_loss: 0.1613 - op_main_accuracy: 0.9202 - op_conv_accuracy: 0.9531 - avg_accuracy: 0.9466\n",
      "Epoch 00120: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8392 - op_main_loss: 0.2157 - op_conv_loss: 0.1297 - avg_loss: 0.1619 - op_main_accuracy: 0.9194 - op_conv_accuracy: 0.9525 - avg_accuracy: 0.9461 - val_loss: 1.2747 - val_op_main_loss: 0.2796 - val_op_conv_loss: 0.3800 - val_avg_loss: 0.2834 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8782\n",
      "Epoch 121/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8430 - op_main_loss: 0.2134 - op_conv_loss: 0.1339 - avg_loss: 0.1640 - op_main_accuracy: 0.9254 - op_conv_accuracy: 0.9477 - avg_accuracy: 0.9474\n",
      "Epoch 00121: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8420 - op_main_loss: 0.2133 - op_conv_loss: 0.1333 - avg_loss: 0.1638 - op_main_accuracy: 0.9258 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9475 - val_loss: 1.1126 - val_op_main_loss: 0.2585 - val_op_conv_loss: 0.2787 - val_avg_loss: 0.2441 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8933\n",
      "Epoch 122/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8166 - op_main_loss: 0.2043 - op_conv_loss: 0.1249 - avg_loss: 0.1561 - op_main_accuracy: 0.9327 - op_conv_accuracy: 0.9501 - avg_accuracy: 0.9511\n",
      "Epoch 00122: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8182 - op_main_loss: 0.2047 - op_conv_loss: 0.1256 - avg_loss: 0.1566 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9504 - avg_accuracy: 0.9511 - val_loss: 1.1644 - val_op_main_loss: 0.2673 - val_op_conv_loss: 0.3100 - val_avg_loss: 0.2556 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8971\n",
      "Epoch 123/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8288 - op_main_loss: 0.2074 - op_conv_loss: 0.1305 - avg_loss: 0.1598 - op_main_accuracy: 0.9255 - op_conv_accuracy: 0.9483 - avg_accuracy: 0.9476\n",
      "Epoch 00123: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8307 - op_main_loss: 0.2084 - op_conv_loss: 0.1307 - avg_loss: 0.1604 - op_main_accuracy: 0.9244 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9473 - val_loss: 1.0974 - val_op_main_loss: 0.2561 - val_op_conv_loss: 0.2693 - val_avg_loss: 0.2407 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9027\n",
      "Epoch 124/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8459 - op_main_loss: 0.2171 - op_conv_loss: 0.1333 - avg_loss: 0.1649 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9473\n",
      "Epoch 00124: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8459 - op_main_loss: 0.2171 - op_conv_loss: 0.1333 - avg_loss: 0.1649 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9473 - val_loss: 1.2138 - val_op_main_loss: 0.2683 - val_op_conv_loss: 0.3433 - val_avg_loss: 0.2713 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8640 - val_avg_accuracy: 0.8782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8182 - op_main_loss: 0.2053 - op_conv_loss: 0.1254 - avg_loss: 0.1566 - op_main_accuracy: 0.9278 - op_conv_accuracy: 0.9501 - avg_accuracy: 0.9450\n",
      "Epoch 00125: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8185 - op_main_loss: 0.2055 - op_conv_loss: 0.1254 - avg_loss: 0.1567 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9501 - avg_accuracy: 0.9452 - val_loss: 1.1961 - val_op_main_loss: 0.2721 - val_op_conv_loss: 0.3259 - val_avg_loss: 0.2671 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 126/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8138 - op_main_loss: 0.2057 - op_conv_loss: 0.1222 - avg_loss: 0.1553 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9530 - avg_accuracy: 0.9516\n",
      "Epoch 00126: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8138 - op_main_loss: 0.2057 - op_conv_loss: 0.1222 - avg_loss: 0.1553 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9530 - avg_accuracy: 0.9516 - val_loss: 1.2454 - val_op_main_loss: 0.2856 - val_op_conv_loss: 0.3490 - val_avg_loss: 0.2803 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8848\n",
      "Epoch 127/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8070 - op_main_loss: 0.2016 - op_conv_loss: 0.1225 - avg_loss: 0.1526 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9490\n",
      "Epoch 00127: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8070 - op_main_loss: 0.2016 - op_conv_loss: 0.1225 - avg_loss: 0.1526 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9490 - val_loss: 1.1880 - val_op_main_loss: 0.2733 - val_op_conv_loss: 0.3201 - val_avg_loss: 0.2648 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8867\n",
      "Epoch 128/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8068 - op_main_loss: 0.2023 - op_conv_loss: 0.1221 - avg_loss: 0.1530 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9516\n",
      "Epoch 00128: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8068 - op_main_loss: 0.2023 - op_conv_loss: 0.1221 - avg_loss: 0.1530 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9516 - val_loss: 1.1743 - val_op_main_loss: 0.2735 - val_op_conv_loss: 0.3089 - val_avg_loss: 0.2617 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8876\n",
      "Epoch 129/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8143 - op_main_loss: 0.2047 - op_conv_loss: 0.1247 - avg_loss: 0.1551 - op_main_accuracy: 0.9287 - op_conv_accuracy: 0.9531 - avg_accuracy: 0.9498\n",
      "Epoch 00129: val_avg_accuracy did not improve from 0.90274\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8139 - op_main_loss: 0.2046 - op_conv_loss: 0.1246 - avg_loss: 0.1550 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9532 - avg_accuracy: 0.9499 - val_loss: 1.1469 - val_op_main_loss: 0.2609 - val_op_conv_loss: 0.3058 - val_avg_loss: 0.2517 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8952\n",
      "Epoch 130/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8000 - op_main_loss: 0.2020 - op_conv_loss: 0.1184 - avg_loss: 0.1511 - op_main_accuracy: 0.9268 - op_conv_accuracy: 0.9535 - avg_accuracy: 0.9489\n",
      "Epoch 00130: val_avg_accuracy improved from 0.90274 to 0.90840, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7989 - op_main_loss: 0.2017 - op_conv_loss: 0.1179 - avg_loss: 0.1508 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9532 - avg_accuracy: 0.9490 - val_loss: 1.0592 - val_op_main_loss: 0.2450 - val_op_conv_loss: 0.2574 - val_avg_loss: 0.2289 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9084\n",
      "Epoch 131/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7927 - op_main_loss: 0.1978 - op_conv_loss: 0.1181 - avg_loss: 0.1490 - op_main_accuracy: 0.9299 - op_conv_accuracy: 0.9545 - avg_accuracy: 0.9536\n",
      "Epoch 00131: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7922 - op_main_loss: 0.1976 - op_conv_loss: 0.1179 - avg_loss: 0.1488 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9537 - val_loss: 1.1554 - val_op_main_loss: 0.2650 - val_op_conv_loss: 0.3084 - val_avg_loss: 0.2542 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8924\n",
      "Epoch 132/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8415 - op_main_loss: 0.2145 - op_conv_loss: 0.1355 - avg_loss: 0.1640 - op_main_accuracy: 0.9220 - op_conv_accuracy: 0.9474 - avg_accuracy: 0.9448\n",
      "Epoch 00132: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8388 - op_main_loss: 0.2136 - op_conv_loss: 0.1345 - avg_loss: 0.1632 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9480 - avg_accuracy: 0.9452 - val_loss: 1.0735 - val_op_main_loss: 0.2480 - val_op_conv_loss: 0.2667 - val_avg_loss: 0.2321 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9075\n",
      "Epoch 133/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8243 - op_main_loss: 0.2078 - op_conv_loss: 0.1296 - avg_loss: 0.1594 - op_main_accuracy: 0.9252 - op_conv_accuracy: 0.9489 - avg_accuracy: 0.9460\n",
      "Epoch 00133: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8252 - op_main_loss: 0.2081 - op_conv_loss: 0.1299 - avg_loss: 0.1597 - op_main_accuracy: 0.9249 - op_conv_accuracy: 0.9487 - avg_accuracy: 0.9459 - val_loss: 1.1621 - val_op_main_loss: 0.2586 - val_op_conv_loss: 0.3209 - val_avg_loss: 0.2558 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8942\n",
      "Epoch 134/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7854 - op_main_loss: 0.1960 - op_conv_loss: 0.1156 - avg_loss: 0.1471 - op_main_accuracy: 0.9335 - op_conv_accuracy: 0.9579 - avg_accuracy: 0.9548\n",
      "Epoch 00134: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7849 - op_main_loss: 0.1958 - op_conv_loss: 0.1154 - avg_loss: 0.1469 - op_main_accuracy: 0.9336 - op_conv_accuracy: 0.9579 - avg_accuracy: 0.9549 - val_loss: 1.0865 - val_op_main_loss: 0.2564 - val_op_conv_loss: 0.2667 - val_avg_loss: 0.2369 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.8990\n",
      "Epoch 135/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8025 - op_main_loss: 0.2002 - op_conv_loss: 0.1233 - avg_loss: 0.1524 - op_main_accuracy: 0.9320 - op_conv_accuracy: 0.9481 - avg_accuracy: 0.9481\n",
      "Epoch 00135: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8015 - op_main_loss: 0.2000 - op_conv_loss: 0.1229 - avg_loss: 0.1520 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9485 - avg_accuracy: 0.9483 - val_loss: 1.1752 - val_op_main_loss: 0.2640 - val_op_conv_loss: 0.3271 - val_avg_loss: 0.2577 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 136/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7949 - op_main_loss: 0.1974 - op_conv_loss: 0.1207 - avg_loss: 0.1502 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9568\n",
      "Epoch 00136: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7949 - op_main_loss: 0.1974 - op_conv_loss: 0.1207 - avg_loss: 0.1502 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9568 - val_loss: 1.1116 - val_op_main_loss: 0.2535 - val_op_conv_loss: 0.2905 - val_avg_loss: 0.2417 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7851 - op_main_loss: 0.1966 - op_conv_loss: 0.1159 - avg_loss: 0.1469 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9556\n",
      "Epoch 00137: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7851 - op_main_loss: 0.1966 - op_conv_loss: 0.1159 - avg_loss: 0.1469 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9556 - val_loss: 1.0743 - val_op_main_loss: 0.2500 - val_op_conv_loss: 0.2651 - val_avg_loss: 0.2345 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9037\n",
      "Epoch 138/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8013 - op_main_loss: 0.2015 - op_conv_loss: 0.1222 - avg_loss: 0.1521 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9527\n",
      "Epoch 00138: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8013 - op_main_loss: 0.2015 - op_conv_loss: 0.1222 - avg_loss: 0.1521 - op_main_accuracy: 0.9284 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9527 - val_loss: 1.0997 - val_op_main_loss: 0.2525 - val_op_conv_loss: 0.2817 - val_avg_loss: 0.2401 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9075\n",
      "Epoch 139/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8045 - op_main_loss: 0.2013 - op_conv_loss: 0.1239 - avg_loss: 0.1538 - op_main_accuracy: 0.9302 - op_conv_accuracy: 0.9519 - avg_accuracy: 0.9503\n",
      "Epoch 00139: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8045 - op_main_loss: 0.2013 - op_conv_loss: 0.1238 - avg_loss: 0.1539 - op_main_accuracy: 0.9301 - op_conv_accuracy: 0.9520 - avg_accuracy: 0.9501 - val_loss: 1.1192 - val_op_main_loss: 0.2560 - val_op_conv_loss: 0.2930 - val_avg_loss: 0.2443 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.8990\n",
      "Epoch 140/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7955 - op_main_loss: 0.1983 - op_conv_loss: 0.1216 - avg_loss: 0.1506 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9504 - avg_accuracy: 0.9485\n",
      "Epoch 00140: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7987 - op_main_loss: 0.1991 - op_conv_loss: 0.1230 - avg_loss: 0.1517 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9478 - val_loss: 1.0919 - val_op_main_loss: 0.2551 - val_op_conv_loss: 0.2730 - val_avg_loss: 0.2408 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8933\n",
      "Epoch 141/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8073 - op_main_loss: 0.2039 - op_conv_loss: 0.1243 - avg_loss: 0.1557 - op_main_accuracy: 0.9271 - op_conv_accuracy: 0.9508 - avg_accuracy: 0.9491\n",
      "Epoch 00141: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8053 - op_main_loss: 0.2026 - op_conv_loss: 0.1245 - avg_loss: 0.1548 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9497 - val_loss: 1.1118 - val_op_main_loss: 0.2534 - val_op_conv_loss: 0.2915 - val_avg_loss: 0.2432 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8952\n",
      "Epoch 142/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7832 - op_main_loss: 0.1979 - op_conv_loss: 0.1140 - avg_loss: 0.1475 - op_main_accuracy: 0.9315 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9525\n",
      "Epoch 00142: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7832 - op_main_loss: 0.1979 - op_conv_loss: 0.1140 - avg_loss: 0.1475 - op_main_accuracy: 0.9315 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9525 - val_loss: 1.4648 - val_op_main_loss: 0.3168 - val_op_conv_loss: 0.4883 - val_avg_loss: 0.3358 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8669\n",
      "Epoch 143/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7942 - op_main_loss: 0.1960 - op_conv_loss: 0.1233 - avg_loss: 0.1507 - op_main_accuracy: 0.9285 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9506\n",
      "Epoch 00143: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7954 - op_main_loss: 0.1972 - op_conv_loss: 0.1230 - avg_loss: 0.1511 - op_main_accuracy: 0.9279 - op_conv_accuracy: 0.9518 - avg_accuracy: 0.9501 - val_loss: 1.1162 - val_op_main_loss: 0.2556 - val_op_conv_loss: 0.2911 - val_avg_loss: 0.2456 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8971\n",
      "Epoch 144/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7702 - op_main_loss: 0.1904 - op_conv_loss: 0.1127 - avg_loss: 0.1430 - op_main_accuracy: 0.9346 - op_conv_accuracy: 0.9583 - avg_accuracy: 0.9561\n",
      "Epoch 00144: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7682 - op_main_loss: 0.1896 - op_conv_loss: 0.1122 - avg_loss: 0.1424 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9584 - avg_accuracy: 0.9563 - val_loss: 1.1125 - val_op_main_loss: 0.2497 - val_op_conv_loss: 0.2958 - val_avg_loss: 0.2426 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.8942\n",
      "Epoch 145/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7630 - op_main_loss: 0.1869 - op_conv_loss: 0.1103 - avg_loss: 0.1412 - op_main_accuracy: 0.9360 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9549\n",
      "Epoch 00145: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7630 - op_main_loss: 0.1869 - op_conv_loss: 0.1103 - avg_loss: 0.1412 - op_main_accuracy: 0.9360 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9549 - val_loss: 1.1211 - val_op_main_loss: 0.2537 - val_op_conv_loss: 0.2990 - val_avg_loss: 0.2443 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 146/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7869 - op_main_loss: 0.1981 - op_conv_loss: 0.1178 - avg_loss: 0.1482 - op_main_accuracy: 0.9282 - op_conv_accuracy: 0.9563 - avg_accuracy: 0.9532\n",
      "Epoch 00146: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7855 - op_main_loss: 0.1975 - op_conv_loss: 0.1174 - avg_loss: 0.1477 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9565 - avg_accuracy: 0.9534 - val_loss: 1.2453 - val_op_main_loss: 0.2943 - val_op_conv_loss: 0.3455 - val_avg_loss: 0.2834 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8839\n",
      "Epoch 147/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7761 - op_main_loss: 0.1935 - op_conv_loss: 0.1149 - avg_loss: 0.1453 - op_main_accuracy: 0.9335 - op_conv_accuracy: 0.9562 - avg_accuracy: 0.9519\n",
      "Epoch 00147: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7756 - op_main_loss: 0.1934 - op_conv_loss: 0.1147 - avg_loss: 0.1451 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9563 - avg_accuracy: 0.9520 - val_loss: 1.0938 - val_op_main_loss: 0.2508 - val_op_conv_loss: 0.2818 - val_avg_loss: 0.2395 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9008\n",
      "Epoch 148/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7896 - op_main_loss: 0.1980 - op_conv_loss: 0.1193 - avg_loss: 0.1506 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9486\n",
      "Epoch 00148: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7869 - op_main_loss: 0.1971 - op_conv_loss: 0.1182 - avg_loss: 0.1497 - op_main_accuracy: 0.9298 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9492 - val_loss: 1.2676 - val_op_main_loss: 0.2920 - val_op_conv_loss: 0.3660 - val_avg_loss: 0.2877 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8810\n",
      "Epoch 149/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.7528 - op_main_loss: 0.1861 - op_conv_loss: 0.1057 - avg_loss: 0.1385 - op_main_accuracy: 0.9397 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9598\n",
      "Epoch 00149: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7552 - op_main_loss: 0.1870 - op_conv_loss: 0.1063 - avg_loss: 0.1393 - op_main_accuracy: 0.9393 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9594 - val_loss: 1.3451 - val_op_main_loss: 0.2931 - val_op_conv_loss: 0.4231 - val_avg_loss: 0.3059 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8754\n",
      "Epoch 150/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7781 - op_main_loss: 0.1910 - op_conv_loss: 0.1183 - avg_loss: 0.1462 - op_main_accuracy: 0.9346 - op_conv_accuracy: 0.9535 - avg_accuracy: 0.9523\n",
      "Epoch 00150: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7790 - op_main_loss: 0.1913 - op_conv_loss: 0.1186 - avg_loss: 0.1466 - op_main_accuracy: 0.9345 - op_conv_accuracy: 0.9530 - avg_accuracy: 0.9518 - val_loss: 1.0862 - val_op_main_loss: 0.2482 - val_op_conv_loss: 0.2789 - val_avg_loss: 0.2369 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9027\n",
      "Epoch 151/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7785 - op_main_loss: 0.1955 - op_conv_loss: 0.1152 - avg_loss: 0.1458 - op_main_accuracy: 0.9322 - op_conv_accuracy: 0.9566 - avg_accuracy: 0.9564\n",
      "Epoch 00151: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7751 - op_main_loss: 0.1947 - op_conv_loss: 0.1137 - avg_loss: 0.1447 - op_main_accuracy: 0.9324 - op_conv_accuracy: 0.9572 - avg_accuracy: 0.9572 - val_loss: 1.1417 - val_op_main_loss: 0.2623 - val_op_conv_loss: 0.3034 - val_avg_loss: 0.2539 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8952\n",
      "Epoch 152/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7659 - op_main_loss: 0.1925 - op_conv_loss: 0.1093 - avg_loss: 0.1422 - op_main_accuracy: 0.9308 - op_conv_accuracy: 0.9563 - avg_accuracy: 0.9553\n",
      "Epoch 00152: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7659 - op_main_loss: 0.1925 - op_conv_loss: 0.1093 - avg_loss: 0.1422 - op_main_accuracy: 0.9308 - op_conv_accuracy: 0.9563 - avg_accuracy: 0.9553 - val_loss: 1.1017 - val_op_main_loss: 0.2485 - val_op_conv_loss: 0.2915 - val_avg_loss: 0.2397 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9018\n",
      "Epoch 153/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7829 - op_main_loss: 0.1937 - op_conv_loss: 0.1202 - avg_loss: 0.1475 - op_main_accuracy: 0.9287 - op_conv_accuracy: 0.9544 - avg_accuracy: 0.9523\n",
      "Epoch 00153: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7812 - op_main_loss: 0.1931 - op_conv_loss: 0.1197 - avg_loss: 0.1469 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9525 - val_loss: 1.2102 - val_op_main_loss: 0.2722 - val_op_conv_loss: 0.3469 - val_avg_loss: 0.2694 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8839\n",
      "Epoch 154/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7637 - op_main_loss: 0.1885 - op_conv_loss: 0.1116 - avg_loss: 0.1419 - op_main_accuracy: 0.9353 - op_conv_accuracy: 0.9540 - avg_accuracy: 0.9566\n",
      "Epoch 00154: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7619 - op_main_loss: 0.1874 - op_conv_loss: 0.1115 - avg_loss: 0.1413 - op_main_accuracy: 0.9360 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9570 - val_loss: 1.0777 - val_op_main_loss: 0.2450 - val_op_conv_loss: 0.2760 - val_avg_loss: 0.2351 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9056\n",
      "Epoch 155/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7929 - op_main_loss: 0.1994 - op_conv_loss: 0.1229 - avg_loss: 0.1501 - op_main_accuracy: 0.9295 - op_conv_accuracy: 0.9554 - avg_accuracy: 0.9520\n",
      "Epoch 00155: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7906 - op_main_loss: 0.1990 - op_conv_loss: 0.1217 - avg_loss: 0.1494 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9560 - avg_accuracy: 0.9523 - val_loss: 1.2838 - val_op_main_loss: 0.2799 - val_op_conv_loss: 0.3927 - val_avg_loss: 0.2906 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8763\n",
      "Epoch 156/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7396 - op_main_loss: 0.1812 - op_conv_loss: 0.1032 - avg_loss: 0.1341 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9591\n",
      "Epoch 00156: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7396 - op_main_loss: 0.1812 - op_conv_loss: 0.1032 - avg_loss: 0.1341 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9591 - val_loss: 1.0942 - val_op_main_loss: 0.2493 - val_op_conv_loss: 0.2847 - val_avg_loss: 0.2393 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9056\n",
      "Epoch 157/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7361 - op_main_loss: 0.1806 - op_conv_loss: 0.1010 - avg_loss: 0.1334 - op_main_accuracy: 0.9377 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9599\n",
      "Epoch 00157: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7356 - op_main_loss: 0.1803 - op_conv_loss: 0.1009 - avg_loss: 0.1332 - op_main_accuracy: 0.9379 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9598 - val_loss: 1.2684 - val_op_main_loss: 0.2819 - val_op_conv_loss: 0.3779 - val_avg_loss: 0.2875 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8876\n",
      "Epoch 158/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7519 - op_main_loss: 0.1837 - op_conv_loss: 0.1092 - avg_loss: 0.1381 - op_main_accuracy: 0.9383 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9563\n",
      "Epoch 00158: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7519 - op_main_loss: 0.1837 - op_conv_loss: 0.1092 - avg_loss: 0.1381 - op_main_accuracy: 0.9383 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9563 - val_loss: 1.0929 - val_op_main_loss: 0.2495 - val_op_conv_loss: 0.2845 - val_avg_loss: 0.2384 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9093 - val_avg_accuracy: 0.9046\n",
      "Epoch 159/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7597 - op_main_loss: 0.1835 - op_conv_loss: 0.1154 - avg_loss: 0.1407 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9552\n",
      "Epoch 00159: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7584 - op_main_loss: 0.1834 - op_conv_loss: 0.1146 - avg_loss: 0.1403 - op_main_accuracy: 0.9407 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9544 - val_loss: 1.1676 - val_op_main_loss: 0.2757 - val_op_conv_loss: 0.3072 - val_avg_loss: 0.2652 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8971\n",
      "Epoch 160/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7529 - op_main_loss: 0.1846 - op_conv_loss: 0.1101 - avg_loss: 0.1388 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9575\n",
      "Epoch 00160: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7529 - op_main_loss: 0.1846 - op_conv_loss: 0.1101 - avg_loss: 0.1388 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9575 - val_loss: 1.1082 - val_op_main_loss: 0.2574 - val_op_conv_loss: 0.2879 - val_avg_loss: 0.2439 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9112 - val_avg_accuracy: 0.9075\n",
      "Epoch 161/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.7414 - op_main_loss: 0.1825 - op_conv_loss: 0.1048 - avg_loss: 0.1352 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9613 - avg_accuracy: 0.9563\n",
      "Epoch 00161: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7415 - op_main_loss: 0.1830 - op_conv_loss: 0.1044 - avg_loss: 0.1353 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9563 - val_loss: 1.0949 - val_op_main_loss: 0.2535 - val_op_conv_loss: 0.2812 - val_avg_loss: 0.2422 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9065\n",
      "Epoch 162/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7647 - op_main_loss: 0.1900 - op_conv_loss: 0.1141 - avg_loss: 0.1429 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9553 - avg_accuracy: 0.9565\n",
      "Epoch 00162: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7647 - op_main_loss: 0.1900 - op_conv_loss: 0.1141 - avg_loss: 0.1429 - op_main_accuracy: 0.9317 - op_conv_accuracy: 0.9553 - avg_accuracy: 0.9565 - val_loss: 1.5392 - val_op_main_loss: 0.3559 - val_op_conv_loss: 0.4978 - val_avg_loss: 0.3676 - val_op_main_accuracy: 0.8565 - val_op_conv_accuracy: 0.8574 - val_avg_accuracy: 0.8631\n",
      "Epoch 163/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7701 - op_main_loss: 0.1927 - op_conv_loss: 0.1157 - avg_loss: 0.1442 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9570 - avg_accuracy: 0.9548\n",
      "Epoch 00163: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7691 - op_main_loss: 0.1922 - op_conv_loss: 0.1154 - avg_loss: 0.1439 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9572 - avg_accuracy: 0.9549 - val_loss: 1.1085 - val_op_main_loss: 0.2525 - val_op_conv_loss: 0.2942 - val_avg_loss: 0.2446 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8980\n",
      "Epoch 164/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7524 - op_main_loss: 0.1815 - op_conv_loss: 0.1136 - avg_loss: 0.1398 - op_main_accuracy: 0.9387 - op_conv_accuracy: 0.9543 - avg_accuracy: 0.9543\n",
      "Epoch 00164: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7513 - op_main_loss: 0.1812 - op_conv_loss: 0.1132 - avg_loss: 0.1394 - op_main_accuracy: 0.9383 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9544 - val_loss: 1.3131 - val_op_main_loss: 0.3113 - val_op_conv_loss: 0.3782 - val_avg_loss: 0.3058 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8829\n",
      "Epoch 165/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7377 - op_main_loss: 0.1813 - op_conv_loss: 0.1043 - avg_loss: 0.1349 - op_main_accuracy: 0.9413 - op_conv_accuracy: 0.9618 - avg_accuracy: 0.9618\n",
      "Epoch 00165: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7379 - op_main_loss: 0.1812 - op_conv_loss: 0.1045 - avg_loss: 0.1350 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9617 - val_loss: 1.0692 - val_op_main_loss: 0.2413 - val_op_conv_loss: 0.2783 - val_avg_loss: 0.2325 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9056\n",
      "Epoch 166/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7399 - op_main_loss: 0.1870 - op_conv_loss: 0.1005 - avg_loss: 0.1352 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9622 - avg_accuracy: 0.9596\n",
      "Epoch 00166: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7399 - op_main_loss: 0.1870 - op_conv_loss: 0.1005 - avg_loss: 0.1352 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9622 - avg_accuracy: 0.9596 - val_loss: 1.2005 - val_op_main_loss: 0.2629 - val_op_conv_loss: 0.3539 - val_avg_loss: 0.2668 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8876\n",
      "Epoch 167/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7335 - op_main_loss: 0.1815 - op_conv_loss: 0.1019 - avg_loss: 0.1334 - op_main_accuracy: 0.9385 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9588\n",
      "Epoch 00167: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7360 - op_main_loss: 0.1829 - op_conv_loss: 0.1024 - avg_loss: 0.1341 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9591 - val_loss: 1.1096 - val_op_main_loss: 0.2501 - val_op_conv_loss: 0.2998 - val_avg_loss: 0.2435 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8999\n",
      "Epoch 168/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7495 - op_main_loss: 0.1831 - op_conv_loss: 0.1112 - avg_loss: 0.1388 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9561 - avg_accuracy: 0.9561\n",
      "Epoch 00168: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7480 - op_main_loss: 0.1825 - op_conv_loss: 0.1107 - avg_loss: 0.1383 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9565 - avg_accuracy: 0.9565 - val_loss: 1.1235 - val_op_main_loss: 0.2451 - val_op_conv_loss: 0.3177 - val_avg_loss: 0.2439 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9008\n",
      "Epoch 169/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7370 - op_main_loss: 0.1800 - op_conv_loss: 0.1059 - avg_loss: 0.1344 - op_main_accuracy: 0.9385 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9611\n",
      "Epoch 00169: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7408 - op_main_loss: 0.1811 - op_conv_loss: 0.1073 - avg_loss: 0.1357 - op_main_accuracy: 0.9376 - op_conv_accuracy: 0.9584 - avg_accuracy: 0.9603 - val_loss: 1.1315 - val_op_main_loss: 0.2582 - val_op_conv_loss: 0.3108 - val_avg_loss: 0.2456 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9103 - val_avg_accuracy: 0.9037\n",
      "Epoch 170/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7339 - op_main_loss: 0.1800 - op_conv_loss: 0.1037 - avg_loss: 0.1336 - op_main_accuracy: 0.9363 - op_conv_accuracy: 0.9591 - avg_accuracy: 0.9572\n",
      "Epoch 00170: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7349 - op_main_loss: 0.1809 - op_conv_loss: 0.1035 - avg_loss: 0.1339 - op_main_accuracy: 0.9362 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9577 - val_loss: 1.1295 - val_op_main_loss: 0.2655 - val_op_conv_loss: 0.2948 - val_avg_loss: 0.2532 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8990\n",
      "Epoch 171/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7882 - op_main_loss: 0.2027 - op_conv_loss: 0.1191 - avg_loss: 0.1508 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9537 - avg_accuracy: 0.9483\n",
      "Epoch 00171: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7882 - op_main_loss: 0.2027 - op_conv_loss: 0.1191 - avg_loss: 0.1508 - op_main_accuracy: 0.9234 - op_conv_accuracy: 0.9537 - avg_accuracy: 0.9483 - val_loss: 1.0754 - val_op_main_loss: 0.2463 - val_op_conv_loss: 0.2778 - val_avg_loss: 0.2367 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9018\n",
      "Epoch 172/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7351 - op_main_loss: 0.1824 - op_conv_loss: 0.1025 - avg_loss: 0.1348 - op_main_accuracy: 0.9373 - op_conv_accuracy: 0.9602 - avg_accuracy: 0.9576\n",
      "Epoch 00172: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7353 - op_main_loss: 0.1825 - op_conv_loss: 0.1025 - avg_loss: 0.1348 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9577 - val_loss: 1.0760 - val_op_main_loss: 0.2448 - val_op_conv_loss: 0.2802 - val_avg_loss: 0.2349 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9075\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.7223 - op_main_loss: 0.1749 - op_conv_loss: 0.1008 - avg_loss: 0.1305 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9599 - avg_accuracy: 0.9596\n",
      "Epoch 00173: val_avg_accuracy did not improve from 0.90840\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7211 - op_main_loss: 0.1745 - op_conv_loss: 0.1004 - avg_loss: 0.1301 - op_main_accuracy: 0.9445 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9598 - val_loss: 1.0923 - val_op_main_loss: 0.2454 - val_op_conv_loss: 0.2922 - val_avg_loss: 0.2387 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9037\n",
      "Epoch 174/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7384 - op_main_loss: 0.1818 - op_conv_loss: 0.1056 - avg_loss: 0.1358 - op_main_accuracy: 0.9389 - op_conv_accuracy: 0.9568 - avg_accuracy: 0.9573\n",
      "Epoch 00174: val_avg_accuracy improved from 0.90840 to 0.91029, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7396 - op_main_loss: 0.1820 - op_conv_loss: 0.1063 - avg_loss: 0.1361 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9570 - avg_accuracy: 0.9575 - val_loss: 1.0685 - val_op_main_loss: 0.2435 - val_op_conv_loss: 0.2765 - val_avg_loss: 0.2340 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9103\n",
      "Epoch 175/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7286 - op_main_loss: 0.1773 - op_conv_loss: 0.1056 - avg_loss: 0.1312 - op_main_accuracy: 0.9370 - op_conv_accuracy: 0.9613 - avg_accuracy: 0.9572\n",
      "Epoch 00175: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7287 - op_main_loss: 0.1771 - op_conv_loss: 0.1058 - avg_loss: 0.1312 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9572 - val_loss: 1.1098 - val_op_main_loss: 0.2450 - val_op_conv_loss: 0.3097 - val_avg_loss: 0.2411 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9065\n",
      "Epoch 176/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7239 - op_main_loss: 0.1759 - op_conv_loss: 0.1030 - avg_loss: 0.1310 - op_main_accuracy: 0.9414 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9610\n",
      "Epoch 00176: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7239 - op_main_loss: 0.1759 - op_conv_loss: 0.1030 - avg_loss: 0.1310 - op_main_accuracy: 0.9414 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9610 - val_loss: 1.0853 - val_op_main_loss: 0.2455 - val_op_conv_loss: 0.2891 - val_avg_loss: 0.2373 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9093\n",
      "Epoch 177/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7229 - op_main_loss: 0.1752 - op_conv_loss: 0.1031 - avg_loss: 0.1307 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9617\n",
      "Epoch 00177: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7213 - op_main_loss: 0.1744 - op_conv_loss: 0.1028 - avg_loss: 0.1303 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9617 - val_loss: 1.0805 - val_op_main_loss: 0.2459 - val_op_conv_loss: 0.2834 - val_avg_loss: 0.2370 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9046\n",
      "Epoch 178/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7232 - op_main_loss: 0.1773 - op_conv_loss: 0.1011 - avg_loss: 0.1313 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9608 - avg_accuracy: 0.9586\n",
      "Epoch 00178: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7228 - op_main_loss: 0.1773 - op_conv_loss: 0.1009 - avg_loss: 0.1311 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9608 - avg_accuracy: 0.9589 - val_loss: 1.1286 - val_op_main_loss: 0.2651 - val_op_conv_loss: 0.3023 - val_avg_loss: 0.2484 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9046\n",
      "Epoch 179/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7368 - op_main_loss: 0.1791 - op_conv_loss: 0.1095 - avg_loss: 0.1358 - op_main_accuracy: 0.9392 - op_conv_accuracy: 0.9523 - avg_accuracy: 0.9545\n",
      "Epoch 00179: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7359 - op_main_loss: 0.1786 - op_conv_loss: 0.1093 - avg_loss: 0.1355 - op_main_accuracy: 0.9393 - op_conv_accuracy: 0.9527 - avg_accuracy: 0.9546 - val_loss: 1.1263 - val_op_main_loss: 0.2544 - val_op_conv_loss: 0.3101 - val_avg_loss: 0.2490 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8961\n",
      "Epoch 180/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7443 - op_main_loss: 0.1815 - op_conv_loss: 0.1119 - avg_loss: 0.1384 - op_main_accuracy: 0.9354 - op_conv_accuracy: 0.9540 - avg_accuracy: 0.9544\n",
      "Epoch 00180: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7441 - op_main_loss: 0.1814 - op_conv_loss: 0.1117 - avg_loss: 0.1383 - op_main_accuracy: 0.9355 - op_conv_accuracy: 0.9542 - avg_accuracy: 0.9544 - val_loss: 1.1304 - val_op_main_loss: 0.2577 - val_op_conv_loss: 0.3108 - val_avg_loss: 0.2494 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9018\n",
      "Epoch 181/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7168 - op_main_loss: 0.1759 - op_conv_loss: 0.0994 - avg_loss: 0.1297 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9598\n",
      "Epoch 00181: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7168 - op_main_loss: 0.1759 - op_conv_loss: 0.0994 - avg_loss: 0.1297 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9598 - val_loss: 1.2319 - val_op_main_loss: 0.2670 - val_op_conv_loss: 0.3763 - val_avg_loss: 0.2772 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8782\n",
      "Epoch 182/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7210 - op_main_loss: 0.1721 - op_conv_loss: 0.1068 - avg_loss: 0.1307 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9605\n",
      "Epoch 00182: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7210 - op_main_loss: 0.1721 - op_conv_loss: 0.1068 - avg_loss: 0.1307 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9605 - val_loss: 1.1029 - val_op_main_loss: 0.2470 - val_op_conv_loss: 0.3003 - val_avg_loss: 0.2444 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9027\n",
      "Epoch 183/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7418 - op_main_loss: 0.1851 - op_conv_loss: 0.1083 - avg_loss: 0.1378 - op_main_accuracy: 0.9377 - op_conv_accuracy: 0.9599 - avg_accuracy: 0.9596\n",
      "Epoch 00183: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7432 - op_main_loss: 0.1853 - op_conv_loss: 0.1091 - avg_loss: 0.1383 - op_main_accuracy: 0.9379 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9589 - val_loss: 1.1305 - val_op_main_loss: 0.2452 - val_op_conv_loss: 0.3273 - val_avg_loss: 0.2476 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8924\n",
      "Epoch 184/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7198 - op_main_loss: 0.1725 - op_conv_loss: 0.1058 - avg_loss: 0.1306 - op_main_accuracy: 0.9437 - op_conv_accuracy: 0.9611 - avg_accuracy: 0.9602\n",
      "Epoch 00184: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7192 - op_main_loss: 0.1727 - op_conv_loss: 0.1053 - avg_loss: 0.1304 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9605 - val_loss: 1.1093 - val_op_main_loss: 0.2543 - val_op_conv_loss: 0.2999 - val_avg_loss: 0.2439 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9093\n",
      "Epoch 185/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.7219 - op_main_loss: 0.1753 - op_conv_loss: 0.1038 - avg_loss: 0.1314 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9588\n",
      "Epoch 00185: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7215 - op_main_loss: 0.1758 - op_conv_loss: 0.1030 - avg_loss: 0.1313 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9594 - val_loss: 1.0883 - val_op_main_loss: 0.2454 - val_op_conv_loss: 0.2933 - val_avg_loss: 0.2389 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9103 - val_avg_accuracy: 0.9046\n",
      "Epoch 186/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7175 - op_main_loss: 0.1768 - op_conv_loss: 0.1004 - avg_loss: 0.1302 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9632 - avg_accuracy: 0.9599\n",
      "Epoch 00186: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7190 - op_main_loss: 0.1773 - op_conv_loss: 0.1009 - avg_loss: 0.1307 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9596 - val_loss: 1.0821 - val_op_main_loss: 0.2407 - val_op_conv_loss: 0.2950 - val_avg_loss: 0.2364 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8961\n",
      "Epoch 187/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7192 - op_main_loss: 0.1768 - op_conv_loss: 0.1012 - avg_loss: 0.1309 - op_main_accuracy: 0.9432 - op_conv_accuracy: 0.9626 - avg_accuracy: 0.9581\n",
      "Epoch 00187: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7186 - op_main_loss: 0.1766 - op_conv_loss: 0.1011 - avg_loss: 0.1307 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9582 - val_loss: 1.1271 - val_op_main_loss: 0.2594 - val_op_conv_loss: 0.3068 - val_avg_loss: 0.2512 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8990\n",
      "Epoch 188/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7252 - op_main_loss: 0.1814 - op_conv_loss: 0.1014 - avg_loss: 0.1326 - op_main_accuracy: 0.9364 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9589\n",
      "Epoch 00188: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7252 - op_main_loss: 0.1814 - op_conv_loss: 0.1014 - avg_loss: 0.1326 - op_main_accuracy: 0.9364 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9589 - val_loss: 1.1145 - val_op_main_loss: 0.2462 - val_op_conv_loss: 0.3134 - val_avg_loss: 0.2455 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 189/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6990 - op_main_loss: 0.1689 - op_conv_loss: 0.0963 - avg_loss: 0.1242 - op_main_accuracy: 0.9451 - op_conv_accuracy: 0.9628 - avg_accuracy: 0.9595\n",
      "Epoch 00189: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6987 - op_main_loss: 0.1689 - op_conv_loss: 0.0962 - avg_loss: 0.1241 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9596 - val_loss: 1.1459 - val_op_main_loss: 0.2553 - val_op_conv_loss: 0.3271 - val_avg_loss: 0.2543 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8942\n",
      "Epoch 190/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7004 - op_main_loss: 0.1702 - op_conv_loss: 0.0966 - avg_loss: 0.1240 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9651 - avg_accuracy: 0.9610\n",
      "Epoch 00190: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6998 - op_main_loss: 0.1697 - op_conv_loss: 0.0968 - avg_loss: 0.1237 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9608 - val_loss: 1.0776 - val_op_main_loss: 0.2424 - val_op_conv_loss: 0.2899 - val_avg_loss: 0.2355 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9103\n",
      "Epoch 191/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7321 - op_main_loss: 0.1859 - op_conv_loss: 0.1020 - avg_loss: 0.1341 - op_main_accuracy: 0.9286 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9543\n",
      "Epoch 00191: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7297 - op_main_loss: 0.1850 - op_conv_loss: 0.1013 - avg_loss: 0.1334 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9549 - val_loss: 1.2109 - val_op_main_loss: 0.2760 - val_op_conv_loss: 0.3503 - val_avg_loss: 0.2751 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8857\n",
      "Epoch 192/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6958 - op_main_loss: 0.1695 - op_conv_loss: 0.0931 - avg_loss: 0.1234 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9634\n",
      "Epoch 00192: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6958 - op_main_loss: 0.1695 - op_conv_loss: 0.0931 - avg_loss: 0.1234 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9634 - val_loss: 1.0782 - val_op_main_loss: 0.2457 - val_op_conv_loss: 0.2858 - val_avg_loss: 0.2372 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9027\n",
      "Epoch 193/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7398 - op_main_loss: 0.1812 - op_conv_loss: 0.1124 - avg_loss: 0.1369 - op_main_accuracy: 0.9354 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9583\n",
      "Epoch 00193: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7418 - op_main_loss: 0.1815 - op_conv_loss: 0.1134 - avg_loss: 0.1376 - op_main_accuracy: 0.9348 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9575 - val_loss: 1.1130 - val_op_main_loss: 0.2438 - val_op_conv_loss: 0.3194 - val_avg_loss: 0.2414 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9018\n",
      "Epoch 194/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7209 - op_main_loss: 0.1757 - op_conv_loss: 0.1049 - avg_loss: 0.1319 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9599 - avg_accuracy: 0.9606\n",
      "Epoch 00194: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7240 - op_main_loss: 0.1765 - op_conv_loss: 0.1063 - avg_loss: 0.1328 - op_main_accuracy: 0.9414 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9598 - val_loss: 1.0718 - val_op_main_loss: 0.2409 - val_op_conv_loss: 0.2870 - val_avg_loss: 0.2352 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9046\n",
      "Epoch 195/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7130 - op_main_loss: 0.1725 - op_conv_loss: 0.1026 - avg_loss: 0.1284 - op_main_accuracy: 0.9379 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9584\n",
      "Epoch 00195: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7130 - op_main_loss: 0.1725 - op_conv_loss: 0.1026 - avg_loss: 0.1284 - op_main_accuracy: 0.9379 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9584 - val_loss: 1.1756 - val_op_main_loss: 0.2589 - val_op_conv_loss: 0.3474 - val_avg_loss: 0.2603 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8942\n",
      "Epoch 196/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7020 - op_main_loss: 0.1679 - op_conv_loss: 0.0997 - avg_loss: 0.1258 - op_main_accuracy: 0.9472 - op_conv_accuracy: 0.9588 - avg_accuracy: 0.9583\n",
      "Epoch 00196: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7020 - op_main_loss: 0.1681 - op_conv_loss: 0.0994 - avg_loss: 0.1258 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9584 - avg_accuracy: 0.9577 - val_loss: 1.0730 - val_op_main_loss: 0.2440 - val_op_conv_loss: 0.2848 - val_avg_loss: 0.2358 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9027\n",
      "Epoch 197/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.7012 - op_main_loss: 0.1691 - op_conv_loss: 0.0981 - avg_loss: 0.1257 - op_main_accuracy: 0.9456 - op_conv_accuracy: 0.9642 - avg_accuracy: 0.9625\n",
      "Epoch 00197: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7037 - op_main_loss: 0.1699 - op_conv_loss: 0.0991 - avg_loss: 0.1264 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9620 - val_loss: 1.0684 - val_op_main_loss: 0.2388 - val_op_conv_loss: 0.2877 - val_avg_loss: 0.2334 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9027\n",
      "Epoch 198/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6932 - op_main_loss: 0.1669 - op_conv_loss: 0.0949 - avg_loss: 0.1234 - op_main_accuracy: 0.9482 - op_conv_accuracy: 0.9644 - avg_accuracy: 0.9627\n",
      "Epoch 00198: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6925 - op_main_loss: 0.1667 - op_conv_loss: 0.0946 - avg_loss: 0.1232 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9629 - val_loss: 1.1319 - val_op_main_loss: 0.2536 - val_op_conv_loss: 0.3203 - val_avg_loss: 0.2501 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8980\n",
      "Epoch 199/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7052 - op_main_loss: 0.1738 - op_conv_loss: 0.0965 - avg_loss: 0.1270 - op_main_accuracy: 0.9382 - op_conv_accuracy: 0.9637 - avg_accuracy: 0.9615\n",
      "Epoch 00199: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7078 - op_main_loss: 0.1744 - op_conv_loss: 0.0975 - avg_loss: 0.1279 - op_main_accuracy: 0.9374 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9610 - val_loss: 1.1144 - val_op_main_loss: 0.2486 - val_op_conv_loss: 0.3097 - val_avg_loss: 0.2486 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9056\n",
      "Epoch 200/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6900 - op_main_loss: 0.1651 - op_conv_loss: 0.0951 - avg_loss: 0.1224 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9633 - avg_accuracy: 0.9640\n",
      "Epoch 00200: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6902 - op_main_loss: 0.1652 - op_conv_loss: 0.0951 - avg_loss: 0.1225 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9641 - val_loss: 1.0751 - val_op_main_loss: 0.2446 - val_op_conv_loss: 0.2857 - val_avg_loss: 0.2377 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9018\n",
      "Epoch 201/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7019 - op_main_loss: 0.1721 - op_conv_loss: 0.0961 - avg_loss: 0.1265 - op_main_accuracy: 0.9415 - op_conv_accuracy: 0.9633 - avg_accuracy: 0.9607\n",
      "Epoch 00201: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7032 - op_main_loss: 0.1725 - op_conv_loss: 0.0966 - avg_loss: 0.1269 - op_main_accuracy: 0.9414 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9605 - val_loss: 1.1229 - val_op_main_loss: 0.2475 - val_op_conv_loss: 0.3222 - val_avg_loss: 0.2470 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8971\n",
      "Epoch 202/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7071 - op_main_loss: 0.1707 - op_conv_loss: 0.1029 - avg_loss: 0.1276 - op_main_accuracy: 0.9440 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9601\n",
      "Epoch 00202: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7071 - op_main_loss: 0.1707 - op_conv_loss: 0.1029 - avg_loss: 0.1276 - op_main_accuracy: 0.9440 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9601 - val_loss: 1.1910 - val_op_main_loss: 0.2607 - val_op_conv_loss: 0.3600 - val_avg_loss: 0.2635 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8895\n",
      "Epoch 203/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6762 - op_main_loss: 0.1616 - op_conv_loss: 0.0892 - avg_loss: 0.1187 - op_main_accuracy: 0.9479 - op_conv_accuracy: 0.9652 - avg_accuracy: 0.9645\n",
      "Epoch 00203: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6758 - op_main_loss: 0.1615 - op_conv_loss: 0.0891 - avg_loss: 0.1186 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9646 - val_loss: 1.1126 - val_op_main_loss: 0.2473 - val_op_conv_loss: 0.3162 - val_avg_loss: 0.2427 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 204/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6983 - op_main_loss: 0.1708 - op_conv_loss: 0.0964 - avg_loss: 0.1250 - op_main_accuracy: 0.9430 - op_conv_accuracy: 0.9651 - avg_accuracy: 0.9627\n",
      "Epoch 00204: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6963 - op_main_loss: 0.1704 - op_conv_loss: 0.0954 - avg_loss: 0.1244 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9629 - val_loss: 1.1127 - val_op_main_loss: 0.2390 - val_op_conv_loss: 0.3271 - val_avg_loss: 0.2407 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9008\n",
      "Epoch 205/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6898 - op_main_loss: 0.1654 - op_conv_loss: 0.0958 - avg_loss: 0.1229 - op_main_accuracy: 0.9444 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9633\n",
      "Epoch 00205: val_avg_accuracy did not improve from 0.91029\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6895 - op_main_loss: 0.1653 - op_conv_loss: 0.0957 - avg_loss: 0.1228 - op_main_accuracy: 0.9445 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9634 - val_loss: 1.0818 - val_op_main_loss: 0.2451 - val_op_conv_loss: 0.2945 - val_avg_loss: 0.2371 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9056\n",
      "Epoch 206/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6866 - op_main_loss: 0.1657 - op_conv_loss: 0.0936 - avg_loss: 0.1221 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9652 - avg_accuracy: 0.9623\n",
      "Epoch 00206: val_avg_accuracy improved from 0.91029 to 0.91313, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6848 - op_main_loss: 0.1651 - op_conv_loss: 0.0930 - avg_loss: 0.1215 - op_main_accuracy: 0.9440 - op_conv_accuracy: 0.9655 - avg_accuracy: 0.9627 - val_loss: 1.0942 - val_op_main_loss: 0.2479 - val_op_conv_loss: 0.3004 - val_avg_loss: 0.2411 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9131\n",
      "Epoch 207/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6819 - op_main_loss: 0.1645 - op_conv_loss: 0.0921 - avg_loss: 0.1205 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9645\n",
      "Epoch 00207: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6821 - op_main_loss: 0.1648 - op_conv_loss: 0.0921 - avg_loss: 0.1205 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9643 - val_loss: 1.3154 - val_op_main_loss: 0.2844 - val_op_conv_loss: 0.4255 - val_avg_loss: 0.3016 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8687\n",
      "Epoch 208/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6975 - op_main_loss: 0.1708 - op_conv_loss: 0.0974 - avg_loss: 0.1254 - op_main_accuracy: 0.9439 - op_conv_accuracy: 0.9625 - avg_accuracy: 0.9637\n",
      "Epoch 00208: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6984 - op_main_loss: 0.1706 - op_conv_loss: 0.0984 - avg_loss: 0.1255 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9638 - val_loss: 1.1749 - val_op_main_loss: 0.2473 - val_op_conv_loss: 0.3646 - val_avg_loss: 0.2589 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8876\n",
      "Epoch 209/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.7025 - op_main_loss: 0.1762 - op_conv_loss: 0.0952 - avg_loss: 0.1268 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9615\n",
      "Epoch 00209: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7025 - op_main_loss: 0.1762 - op_conv_loss: 0.0952 - avg_loss: 0.1268 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9615 - val_loss: 1.1086 - val_op_main_loss: 0.2473 - val_op_conv_loss: 0.3116 - val_avg_loss: 0.2458 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9046\n",
      "Epoch 210/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7046 - op_main_loss: 0.1687 - op_conv_loss: 0.1036 - avg_loss: 0.1278 - op_main_accuracy: 0.9458 - op_conv_accuracy: 0.9600 - avg_accuracy: 0.9595\n",
      "Epoch 00210: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7041 - op_main_loss: 0.1686 - op_conv_loss: 0.1035 - avg_loss: 0.1277 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9596 - val_loss: 1.1542 - val_op_main_loss: 0.2558 - val_op_conv_loss: 0.3373 - val_avg_loss: 0.2571 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8999\n",
      "Epoch 211/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6769 - op_main_loss: 0.1647 - op_conv_loss: 0.0890 - avg_loss: 0.1194 - op_main_accuracy: 0.9450 - op_conv_accuracy: 0.9663 - avg_accuracy: 0.9641\n",
      "Epoch 00211: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6784 - op_main_loss: 0.1651 - op_conv_loss: 0.0894 - avg_loss: 0.1199 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9641 - val_loss: 1.1981 - val_op_main_loss: 0.2644 - val_op_conv_loss: 0.3652 - val_avg_loss: 0.2658 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8933\n",
      "Epoch 212/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6886 - op_main_loss: 0.1661 - op_conv_loss: 0.0962 - avg_loss: 0.1227 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9630 - avg_accuracy: 0.9623\n",
      "Epoch 00212: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6901 - op_main_loss: 0.1663 - op_conv_loss: 0.0970 - avg_loss: 0.1232 - op_main_accuracy: 0.9468 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9620 - val_loss: 1.1111 - val_op_main_loss: 0.2517 - val_op_conv_loss: 0.3106 - val_avg_loss: 0.2456 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 213/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6823 - op_main_loss: 0.1646 - op_conv_loss: 0.0927 - avg_loss: 0.1213 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9620\n",
      "Epoch 00213: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6823 - op_main_loss: 0.1646 - op_conv_loss: 0.0927 - avg_loss: 0.1213 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9620 - val_loss: 1.0997 - val_op_main_loss: 0.2459 - val_op_conv_loss: 0.3072 - val_avg_loss: 0.2431 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9037\n",
      "Epoch 214/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7008 - op_main_loss: 0.1691 - op_conv_loss: 0.1029 - avg_loss: 0.1254 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9624 - avg_accuracy: 0.9620\n",
      "Epoch 00214: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7008 - op_main_loss: 0.1691 - op_conv_loss: 0.1029 - avg_loss: 0.1254 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9624 - avg_accuracy: 0.9620 - val_loss: 1.0872 - val_op_main_loss: 0.2409 - val_op_conv_loss: 0.3041 - val_avg_loss: 0.2381 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 215/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6945 - op_main_loss: 0.1671 - op_conv_loss: 0.0987 - avg_loss: 0.1247 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9599 - avg_accuracy: 0.9596\n",
      "Epoch 00215: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6943 - op_main_loss: 0.1668 - op_conv_loss: 0.0989 - avg_loss: 0.1247 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9596 - val_loss: 1.2259 - val_op_main_loss: 0.2901 - val_op_conv_loss: 0.3491 - val_avg_loss: 0.2841 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8914\n",
      "Epoch 216/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6955 - op_main_loss: 0.1672 - op_conv_loss: 0.1004 - avg_loss: 0.1252 - op_main_accuracy: 0.9448 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9617\n",
      "Epoch 00216: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6949 - op_main_loss: 0.1670 - op_conv_loss: 0.1002 - avg_loss: 0.1250 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9622 - val_loss: 1.1078 - val_op_main_loss: 0.2544 - val_op_conv_loss: 0.3044 - val_avg_loss: 0.2457 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9056\n",
      "Epoch 217/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6693 - op_main_loss: 0.1594 - op_conv_loss: 0.0895 - avg_loss: 0.1171 - op_main_accuracy: 0.9455 - op_conv_accuracy: 0.9654 - avg_accuracy: 0.9620\n",
      "Epoch 00217: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6722 - op_main_loss: 0.1599 - op_conv_loss: 0.0910 - avg_loss: 0.1182 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9610 - val_loss: 1.2603 - val_op_main_loss: 0.2876 - val_op_conv_loss: 0.3798 - val_avg_loss: 0.2903 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8829\n",
      "Epoch 218/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6896 - op_main_loss: 0.1684 - op_conv_loss: 0.0953 - avg_loss: 0.1234 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9603\n",
      "Epoch 00218: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6894 - op_main_loss: 0.1682 - op_conv_loss: 0.0954 - avg_loss: 0.1234 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9603 - val_loss: 1.0960 - val_op_main_loss: 0.2408 - val_op_conv_loss: 0.3131 - val_avg_loss: 0.2395 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9018\n",
      "Epoch 219/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6850 - op_main_loss: 0.1635 - op_conv_loss: 0.0965 - avg_loss: 0.1217 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9634\n",
      "Epoch 00219: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6840 - op_main_loss: 0.1632 - op_conv_loss: 0.0962 - avg_loss: 0.1214 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9636 - val_loss: 1.2697 - val_op_main_loss: 0.2843 - val_op_conv_loss: 0.3951 - val_avg_loss: 0.2874 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8895\n",
      "Epoch 220/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6901 - op_main_loss: 0.1678 - op_conv_loss: 0.0958 - avg_loss: 0.1238 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9656 - avg_accuracy: 0.9593\n",
      "Epoch 00220: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6880 - op_main_loss: 0.1671 - op_conv_loss: 0.0952 - avg_loss: 0.1231 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9596 - val_loss: 1.1270 - val_op_main_loss: 0.2510 - val_op_conv_loss: 0.3245 - val_avg_loss: 0.2491 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9008\n",
      "Epoch 221/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.6723 - op_main_loss: 0.1622 - op_conv_loss: 0.0899 - avg_loss: 0.1181 - op_main_accuracy: 0.9474 - op_conv_accuracy: 0.9663 - avg_accuracy: 0.9635\n",
      "Epoch 00221: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6754 - op_main_loss: 0.1632 - op_conv_loss: 0.0911 - avg_loss: 0.1191 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9629 - val_loss: 1.3315 - val_op_main_loss: 0.2845 - val_op_conv_loss: 0.4430 - val_avg_loss: 0.3022 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8839\n",
      "Epoch 222/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6756 - op_main_loss: 0.1625 - op_conv_loss: 0.0913 - avg_loss: 0.1195 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9639 - avg_accuracy: 0.9630\n",
      "Epoch 00222: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6767 - op_main_loss: 0.1625 - op_conv_loss: 0.0921 - avg_loss: 0.1199 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9631 - val_loss: 1.0741 - val_op_main_loss: 0.2408 - val_op_conv_loss: 0.2949 - val_avg_loss: 0.2365 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9103\n",
      "Epoch 223/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6654 - op_main_loss: 0.1583 - op_conv_loss: 0.0884 - avg_loss: 0.1167 - op_main_accuracy: 0.9508 - op_conv_accuracy: 0.9640 - avg_accuracy: 0.9650\n",
      "Epoch 00223: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6653 - op_main_loss: 0.1584 - op_conv_loss: 0.0884 - avg_loss: 0.1167 - op_main_accuracy: 0.9509 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9650 - val_loss: 1.0461 - val_op_main_loss: 0.2329 - val_op_conv_loss: 0.2853 - val_avg_loss: 0.2268 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9112\n",
      "Epoch 224/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6675 - op_main_loss: 0.1613 - op_conv_loss: 0.0874 - avg_loss: 0.1171 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9662\n",
      "Epoch 00224: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6675 - op_main_loss: 0.1613 - op_conv_loss: 0.0874 - avg_loss: 0.1171 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9662 - val_loss: 1.1452 - val_op_main_loss: 0.2556 - val_op_conv_loss: 0.3328 - val_avg_loss: 0.2551 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8980\n",
      "Epoch 225/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6746 - op_main_loss: 0.1610 - op_conv_loss: 0.0933 - avg_loss: 0.1190 - op_main_accuracy: 0.9482 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9663\n",
      "Epoch 00225: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6742 - op_main_loss: 0.1609 - op_conv_loss: 0.0931 - avg_loss: 0.1189 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9667 - val_loss: 1.1369 - val_op_main_loss: 0.2620 - val_op_conv_loss: 0.3196 - val_avg_loss: 0.2535 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9056\n",
      "Epoch 226/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6644 - op_main_loss: 0.1588 - op_conv_loss: 0.0879 - avg_loss: 0.1164 - op_main_accuracy: 0.9458 - op_conv_accuracy: 0.9678 - avg_accuracy: 0.9654\n",
      "Epoch 00226: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6640 - op_main_loss: 0.1586 - op_conv_loss: 0.0877 - avg_loss: 0.1163 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9655 - val_loss: 1.1015 - val_op_main_loss: 0.2380 - val_op_conv_loss: 0.3200 - val_avg_loss: 0.2424 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8933\n",
      "Epoch 227/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6742 - op_main_loss: 0.1606 - op_conv_loss: 0.0935 - avg_loss: 0.1192 - op_main_accuracy: 0.9478 - op_conv_accuracy: 0.9632 - avg_accuracy: 0.9623\n",
      "Epoch 00227: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6724 - op_main_loss: 0.1596 - op_conv_loss: 0.0932 - avg_loss: 0.1186 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9627 - val_loss: 1.0719 - val_op_main_loss: 0.2384 - val_op_conv_loss: 0.2969 - val_avg_loss: 0.2356 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9065\n",
      "Epoch 228/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6765 - op_main_loss: 0.1600 - op_conv_loss: 0.0961 - avg_loss: 0.1198 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9647\n",
      "Epoch 00228: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6771 - op_main_loss: 0.1600 - op_conv_loss: 0.0966 - avg_loss: 0.1200 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9648 - val_loss: 1.2857 - val_op_main_loss: 0.2851 - val_op_conv_loss: 0.4088 - val_avg_loss: 0.2917 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8820\n",
      "Epoch 229/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6862 - op_main_loss: 0.1684 - op_conv_loss: 0.0951 - avg_loss: 0.1227 - op_main_accuracy: 0.9445 - op_conv_accuracy: 0.9635 - avg_accuracy: 0.9611\n",
      "Epoch 00229: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6892 - op_main_loss: 0.1694 - op_conv_loss: 0.0962 - avg_loss: 0.1237 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9605 - val_loss: 1.1439 - val_op_main_loss: 0.2503 - val_op_conv_loss: 0.3388 - val_avg_loss: 0.2544 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8980\n",
      "Epoch 230/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6743 - op_main_loss: 0.1640 - op_conv_loss: 0.0915 - avg_loss: 0.1189 - op_main_accuracy: 0.9437 - op_conv_accuracy: 0.9666 - avg_accuracy: 0.9642\n",
      "Epoch 00230: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6722 - op_main_loss: 0.1633 - op_conv_loss: 0.0908 - avg_loss: 0.1183 - op_main_accuracy: 0.9440 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9646 - val_loss: 1.0481 - val_op_main_loss: 0.2316 - val_op_conv_loss: 0.2895 - val_avg_loss: 0.2274 - val_op_main_accuracy: 0.9112 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9075\n",
      "Epoch 231/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6508 - op_main_loss: 0.1554 - op_conv_loss: 0.0832 - avg_loss: 0.1123 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9695\n",
      "Epoch 00231: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6498 - op_main_loss: 0.1546 - op_conv_loss: 0.0834 - avg_loss: 0.1120 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9698 - val_loss: 1.0687 - val_op_main_loss: 0.2360 - val_op_conv_loss: 0.3009 - val_avg_loss: 0.2319 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9075\n",
      "Epoch 232/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6630 - op_main_loss: 0.1578 - op_conv_loss: 0.0893 - avg_loss: 0.1158 - op_main_accuracy: 0.9489 - op_conv_accuracy: 0.9654 - avg_accuracy: 0.9625\n",
      "Epoch 00232: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6625 - op_main_loss: 0.1581 - op_conv_loss: 0.0887 - avg_loss: 0.1157 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9627 - val_loss: 1.0827 - val_op_main_loss: 0.2360 - val_op_conv_loss: 0.3114 - val_avg_loss: 0.2354 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9065\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.6875 - op_main_loss: 0.1667 - op_conv_loss: 0.0979 - avg_loss: 0.1231 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9639 - avg_accuracy: 0.9618\n",
      "Epoch 00233: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6908 - op_main_loss: 0.1681 - op_conv_loss: 0.0989 - avg_loss: 0.1240 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9612 - val_loss: 1.4244 - val_op_main_loss: 0.3275 - val_op_conv_loss: 0.4622 - val_avg_loss: 0.3344 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8763\n",
      "Epoch 234/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6747 - op_main_loss: 0.1604 - op_conv_loss: 0.0954 - avg_loss: 0.1185 - op_main_accuracy: 0.9436 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9625\n",
      "Epoch 00234: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6756 - op_main_loss: 0.1607 - op_conv_loss: 0.0956 - avg_loss: 0.1189 - op_main_accuracy: 0.9435 - op_conv_accuracy: 0.9615 - avg_accuracy: 0.9624 - val_loss: 1.2354 - val_op_main_loss: 0.2702 - val_op_conv_loss: 0.3860 - val_avg_loss: 0.2791 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8924\n",
      "Epoch 235/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6789 - op_main_loss: 0.1633 - op_conv_loss: 0.0947 - avg_loss: 0.1209 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9656 - avg_accuracy: 0.9625\n",
      "Epoch 00235: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6802 - op_main_loss: 0.1637 - op_conv_loss: 0.0951 - avg_loss: 0.1213 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9622 - val_loss: 1.1577 - val_op_main_loss: 0.2680 - val_op_conv_loss: 0.3309 - val_avg_loss: 0.2589 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.9027\n",
      "Epoch 236/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6582 - op_main_loss: 0.1587 - op_conv_loss: 0.0853 - avg_loss: 0.1145 - op_main_accuracy: 0.9524 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9712\n",
      "Epoch 00236: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6586 - op_main_loss: 0.1583 - op_conv_loss: 0.0860 - avg_loss: 0.1145 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9709 - val_loss: 1.2247 - val_op_main_loss: 0.2737 - val_op_conv_loss: 0.3741 - val_avg_loss: 0.2769 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8905\n",
      "Epoch 237/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6694 - op_main_loss: 0.1589 - op_conv_loss: 0.0935 - avg_loss: 0.1176 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9646\n",
      "Epoch 00237: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6694 - op_main_loss: 0.1589 - op_conv_loss: 0.0935 - avg_loss: 0.1176 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9646 - val_loss: 1.1380 - val_op_main_loss: 0.2616 - val_op_conv_loss: 0.3220 - val_avg_loss: 0.2556 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9018\n",
      "Epoch 238/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6543 - op_main_loss: 0.1570 - op_conv_loss: 0.0844 - avg_loss: 0.1138 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9671\n",
      "Epoch 00238: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6548 - op_main_loss: 0.1570 - op_conv_loss: 0.0847 - avg_loss: 0.1139 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9672 - val_loss: 1.1694 - val_op_main_loss: 0.2614 - val_op_conv_loss: 0.3469 - val_avg_loss: 0.2625 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8924\n",
      "Epoch 239/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6493 - op_main_loss: 0.1517 - op_conv_loss: 0.0870 - avg_loss: 0.1116 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9676\n",
      "Epoch 00239: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6493 - op_main_loss: 0.1517 - op_conv_loss: 0.0870 - avg_loss: 0.1116 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9676 - val_loss: 1.0798 - val_op_main_loss: 0.2390 - val_op_conv_loss: 0.3073 - val_avg_loss: 0.2346 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9141 - val_avg_accuracy: 0.9122\n",
      "Epoch 240/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6303 - op_main_loss: 0.1495 - op_conv_loss: 0.0767 - avg_loss: 0.1059 - op_main_accuracy: 0.9578 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9735\n",
      "Epoch 00240: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6299 - op_main_loss: 0.1495 - op_conv_loss: 0.0764 - avg_loss: 0.1058 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9738 - val_loss: 1.0989 - val_op_main_loss: 0.2419 - val_op_conv_loss: 0.3162 - val_avg_loss: 0.2426 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8971\n",
      "Epoch 241/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6327 - op_main_loss: 0.1493 - op_conv_loss: 0.0783 - avg_loss: 0.1066 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9695\n",
      "Epoch 00241: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6327 - op_main_loss: 0.1493 - op_conv_loss: 0.0783 - avg_loss: 0.1066 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9695 - val_loss: 1.1027 - val_op_main_loss: 0.2444 - val_op_conv_loss: 0.3158 - val_avg_loss: 0.2444 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9027\n",
      "Epoch 242/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6747 - op_main_loss: 0.1656 - op_conv_loss: 0.0916 - avg_loss: 0.1198 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9617\n",
      "Epoch 00242: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6747 - op_main_loss: 0.1656 - op_conv_loss: 0.0916 - avg_loss: 0.1198 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9617 - val_loss: 1.2110 - val_op_main_loss: 0.2732 - val_op_conv_loss: 0.3674 - val_avg_loss: 0.2731 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8942\n",
      "Epoch 243/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6525 - op_main_loss: 0.1543 - op_conv_loss: 0.0879 - avg_loss: 0.1130 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9667\n",
      "Epoch 00243: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6525 - op_main_loss: 0.1543 - op_conv_loss: 0.0879 - avg_loss: 0.1130 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9667 - val_loss: 1.1494 - val_op_main_loss: 0.2583 - val_op_conv_loss: 0.3387 - val_avg_loss: 0.2555 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9008\n",
      "Epoch 244/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6793 - op_main_loss: 0.1600 - op_conv_loss: 0.1015 - avg_loss: 0.1212 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9635 - avg_accuracy: 0.9627\n",
      "Epoch 00244: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6775 - op_main_loss: 0.1595 - op_conv_loss: 0.1007 - avg_loss: 0.1206 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9624 - val_loss: 1.0657 - val_op_main_loss: 0.2392 - val_op_conv_loss: 0.2952 - val_avg_loss: 0.2353 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8999\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.6340 - op_main_loss: 0.1501 - op_conv_loss: 0.0792 - avg_loss: 0.1082 - op_main_accuracy: 0.9524 - op_conv_accuracy: 0.9699 - avg_accuracy: 0.9678\n",
      "Epoch 00245: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6346 - op_main_loss: 0.1505 - op_conv_loss: 0.0793 - avg_loss: 0.1084 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9676 - val_loss: 1.0698 - val_op_main_loss: 0.2349 - val_op_conv_loss: 0.3061 - val_avg_loss: 0.2319 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9056\n",
      "Epoch 246/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6566 - op_main_loss: 0.1542 - op_conv_loss: 0.0907 - avg_loss: 0.1148 - op_main_accuracy: 0.9531 - op_conv_accuracy: 0.9637 - avg_accuracy: 0.9632\n",
      "Epoch 00246: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6548 - op_main_loss: 0.1540 - op_conv_loss: 0.0897 - avg_loss: 0.1142 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9636 - val_loss: 1.0878 - val_op_main_loss: 0.2455 - val_op_conv_loss: 0.3019 - val_avg_loss: 0.2435 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9008\n",
      "Epoch 247/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6657 - op_main_loss: 0.1584 - op_conv_loss: 0.0924 - avg_loss: 0.1181 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9629\n",
      "Epoch 00247: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6657 - op_main_loss: 0.1584 - op_conv_loss: 0.0924 - avg_loss: 0.1181 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9629 - val_loss: 1.0678 - val_op_main_loss: 0.2371 - val_op_conv_loss: 0.2993 - val_avg_loss: 0.2344 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9103\n",
      "Epoch 248/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6715 - op_main_loss: 0.1575 - op_conv_loss: 0.0982 - avg_loss: 0.1194 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9610\n",
      "Epoch 00248: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6715 - op_main_loss: 0.1575 - op_conv_loss: 0.0982 - avg_loss: 0.1194 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9610 - val_loss: 1.0857 - val_op_main_loss: 0.2359 - val_op_conv_loss: 0.3162 - val_avg_loss: 0.2375 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 249/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6556 - op_main_loss: 0.1565 - op_conv_loss: 0.0884 - avg_loss: 0.1148 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9660\n",
      "Epoch 00249: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6556 - op_main_loss: 0.1565 - op_conv_loss: 0.0884 - avg_loss: 0.1148 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9660 - val_loss: 1.1537 - val_op_main_loss: 0.2444 - val_op_conv_loss: 0.3604 - val_avg_loss: 0.2532 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8980\n",
      "Epoch 250/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6642 - op_main_loss: 0.1553 - op_conv_loss: 0.0954 - avg_loss: 0.1175 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9622\n",
      "Epoch 00250: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6610 - op_main_loss: 0.1544 - op_conv_loss: 0.0941 - avg_loss: 0.1165 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9624 - val_loss: 1.0972 - val_op_main_loss: 0.2442 - val_op_conv_loss: 0.3166 - val_avg_loss: 0.2414 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9075\n",
      "Epoch 251/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6523 - op_main_loss: 0.1541 - op_conv_loss: 0.0893 - avg_loss: 0.1136 - op_main_accuracy: 0.9508 - op_conv_accuracy: 0.9659 - avg_accuracy: 0.9666\n",
      "Epoch 00251: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6519 - op_main_loss: 0.1540 - op_conv_loss: 0.0892 - avg_loss: 0.1134 - op_main_accuracy: 0.9509 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9667 - val_loss: 1.1281 - val_op_main_loss: 0.2458 - val_op_conv_loss: 0.3380 - val_avg_loss: 0.2494 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9008\n",
      "Epoch 252/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6417 - op_main_loss: 0.1506 - op_conv_loss: 0.0847 - avg_loss: 0.1113 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9688\n",
      "Epoch 00252: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6417 - op_main_loss: 0.1506 - op_conv_loss: 0.0847 - avg_loss: 0.1113 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9688 - val_loss: 1.1696 - val_op_main_loss: 0.2609 - val_op_conv_loss: 0.3510 - val_avg_loss: 0.2626 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8971\n",
      "Epoch 253/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6352 - op_main_loss: 0.1493 - op_conv_loss: 0.0824 - avg_loss: 0.1089 - op_main_accuracy: 0.9535 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9683\n",
      "Epoch 00253: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6342 - op_main_loss: 0.1492 - op_conv_loss: 0.0817 - avg_loss: 0.1086 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9683 - val_loss: 1.1595 - val_op_main_loss: 0.2450 - val_op_conv_loss: 0.3599 - val_avg_loss: 0.2602 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8914\n",
      "Epoch 254/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6643 - op_main_loss: 0.1576 - op_conv_loss: 0.0940 - avg_loss: 0.1181 - op_main_accuracy: 0.9473 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9648\n",
      "Epoch 00254: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6643 - op_main_loss: 0.1576 - op_conv_loss: 0.0940 - avg_loss: 0.1181 - op_main_accuracy: 0.9473 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9648 - val_loss: 1.1333 - val_op_main_loss: 0.2507 - val_op_conv_loss: 0.3372 - val_avg_loss: 0.2511 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9018\n",
      "Epoch 255/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6706 - op_main_loss: 0.1639 - op_conv_loss: 0.0920 - avg_loss: 0.1199 - op_main_accuracy: 0.9389 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9599\n",
      "Epoch 00255: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6683 - op_main_loss: 0.1634 - op_conv_loss: 0.0910 - avg_loss: 0.1192 - op_main_accuracy: 0.9393 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9603 - val_loss: 1.0615 - val_op_main_loss: 0.2345 - val_op_conv_loss: 0.3003 - val_avg_loss: 0.2317 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9084\n",
      "Epoch 256/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6507 - op_main_loss: 0.1555 - op_conv_loss: 0.0872 - avg_loss: 0.1134 - op_main_accuracy: 0.9503 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9671\n",
      "Epoch 00256: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6516 - op_main_loss: 0.1556 - op_conv_loss: 0.0878 - avg_loss: 0.1137 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9672 - val_loss: 1.0579 - val_op_main_loss: 0.2324 - val_op_conv_loss: 0.2980 - val_avg_loss: 0.2331 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9037\n",
      "Epoch 257/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6433 - op_main_loss: 0.1501 - op_conv_loss: 0.0871 - avg_loss: 0.1120 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9688\n",
      "Epoch 00257: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6433 - op_main_loss: 0.1501 - op_conv_loss: 0.0871 - avg_loss: 0.1120 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9688 - val_loss: 1.4774 - val_op_main_loss: 0.3297 - val_op_conv_loss: 0.5065 - val_avg_loss: 0.3476 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8763\n",
      "Epoch 258/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6394 - op_main_loss: 0.1535 - op_conv_loss: 0.0819 - avg_loss: 0.1105 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9664\n",
      "Epoch 00258: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6394 - op_main_loss: 0.1535 - op_conv_loss: 0.0819 - avg_loss: 0.1105 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9664 - val_loss: 1.1906 - val_op_main_loss: 0.2614 - val_op_conv_loss: 0.3686 - val_avg_loss: 0.2663 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8933\n",
      "Epoch 259/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6464 - op_main_loss: 0.1521 - op_conv_loss: 0.0876 - avg_loss: 0.1123 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9664\n",
      "Epoch 00259: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6464 - op_main_loss: 0.1522 - op_conv_loss: 0.0875 - avg_loss: 0.1123 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9664 - val_loss: 1.0899 - val_op_main_loss: 0.2425 - val_op_conv_loss: 0.3112 - val_avg_loss: 0.2420 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9046\n",
      "Epoch 260/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6528 - op_main_loss: 0.1567 - op_conv_loss: 0.0876 - avg_loss: 0.1140 - op_main_accuracy: 0.9482 - op_conv_accuracy: 0.9652 - avg_accuracy: 0.9652\n",
      "Epoch 00260: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6525 - op_main_loss: 0.1566 - op_conv_loss: 0.0875 - avg_loss: 0.1139 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9653 - val_loss: 1.0847 - val_op_main_loss: 0.2386 - val_op_conv_loss: 0.3138 - val_avg_loss: 0.2377 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9093 - val_avg_accuracy: 0.9084\n",
      "Epoch 261/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6464 - op_main_loss: 0.1506 - op_conv_loss: 0.0890 - avg_loss: 0.1120 - op_main_accuracy: 0.9524 - op_conv_accuracy: 0.9678 - avg_accuracy: 0.9659\n",
      "Epoch 00261: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6452 - op_main_loss: 0.1502 - op_conv_loss: 0.0886 - avg_loss: 0.1116 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9662 - val_loss: 1.2232 - val_op_main_loss: 0.2786 - val_op_conv_loss: 0.3738 - val_avg_loss: 0.2767 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8971\n",
      "Epoch 262/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6353 - op_main_loss: 0.1492 - op_conv_loss: 0.0830 - avg_loss: 0.1092 - op_main_accuracy: 0.9491 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9657\n",
      "Epoch 00262: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6356 - op_main_loss: 0.1492 - op_conv_loss: 0.0831 - avg_loss: 0.1093 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9657 - val_loss: 1.0979 - val_op_main_loss: 0.2426 - val_op_conv_loss: 0.3181 - val_avg_loss: 0.2431 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 263/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6383 - op_main_loss: 0.1490 - op_conv_loss: 0.0850 - avg_loss: 0.1106 - op_main_accuracy: 0.9510 - op_conv_accuracy: 0.9649 - avg_accuracy: 0.9659\n",
      "Epoch 00263: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6384 - op_main_loss: 0.1491 - op_conv_loss: 0.0849 - avg_loss: 0.1106 - op_main_accuracy: 0.9509 - op_conv_accuracy: 0.9648 - avg_accuracy: 0.9657 - val_loss: 1.1519 - val_op_main_loss: 0.2606 - val_op_conv_loss: 0.3367 - val_avg_loss: 0.2604 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8942\n",
      "Epoch 264/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6304 - op_main_loss: 0.1501 - op_conv_loss: 0.0783 - avg_loss: 0.1080 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9699 - avg_accuracy: 0.9706\n",
      "Epoch 00264: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6302 - op_main_loss: 0.1500 - op_conv_loss: 0.0782 - avg_loss: 0.1079 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9707 - val_loss: 1.0989 - val_op_main_loss: 0.2381 - val_op_conv_loss: 0.3257 - val_avg_loss: 0.2412 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9037\n",
      "Epoch 265/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6289 - op_main_loss: 0.1486 - op_conv_loss: 0.0799 - avg_loss: 0.1069 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9700\n",
      "Epoch 00265: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6265 - op_main_loss: 0.1476 - op_conv_loss: 0.0793 - avg_loss: 0.1062 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9702 - val_loss: 1.2621 - val_op_main_loss: 0.2526 - val_op_conv_loss: 0.4396 - val_avg_loss: 0.2772 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8744\n",
      "Epoch 266/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6469 - op_main_loss: 0.1516 - op_conv_loss: 0.0899 - avg_loss: 0.1126 - op_main_accuracy: 0.9524 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9683\n",
      "Epoch 00266: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6467 - op_main_loss: 0.1516 - op_conv_loss: 0.0898 - avg_loss: 0.1125 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9683 - val_loss: 1.1557 - val_op_main_loss: 0.2522 - val_op_conv_loss: 0.3546 - val_avg_loss: 0.2559 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.8990\n",
      "Epoch 267/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6228 - op_main_loss: 0.1459 - op_conv_loss: 0.0791 - avg_loss: 0.1048 - op_main_accuracy: 0.9562 - op_conv_accuracy: 0.9732 - avg_accuracy: 0.9721\n",
      "Epoch 00267: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6223 - op_main_loss: 0.1457 - op_conv_loss: 0.0790 - avg_loss: 0.1046 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9721 - val_loss: 1.1346 - val_op_main_loss: 0.2511 - val_op_conv_loss: 0.3380 - val_avg_loss: 0.2532 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8990\n",
      "Epoch 268/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6433 - op_main_loss: 0.1539 - op_conv_loss: 0.0856 - avg_loss: 0.1113 - op_main_accuracy: 0.9491 - op_conv_accuracy: 0.9671 - avg_accuracy: 0.9658\n",
      "Epoch 00268: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6437 - op_main_loss: 0.1541 - op_conv_loss: 0.0855 - avg_loss: 0.1114 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9655 - val_loss: 1.2228 - val_op_main_loss: 0.2596 - val_op_conv_loss: 0.3996 - val_avg_loss: 0.2706 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8942\n",
      "Epoch 269/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6401 - op_main_loss: 0.1503 - op_conv_loss: 0.0867 - avg_loss: 0.1106 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9676\n",
      "Epoch 00269: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6401 - op_main_loss: 0.1503 - op_conv_loss: 0.0867 - avg_loss: 0.1106 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9676 - val_loss: 1.1138 - val_op_main_loss: 0.2480 - val_op_conv_loss: 0.3246 - val_avg_loss: 0.2491 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9065\n",
      "Epoch 270/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6371 - op_main_loss: 0.1477 - op_conv_loss: 0.0874 - avg_loss: 0.1102 - op_main_accuracy: 0.9545 - op_conv_accuracy: 0.9651 - avg_accuracy: 0.9661\n",
      "Epoch 00270: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6395 - op_main_loss: 0.1490 - op_conv_loss: 0.0878 - avg_loss: 0.1109 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9648 - avg_accuracy: 0.9653 - val_loss: 1.1932 - val_op_main_loss: 0.2508 - val_op_conv_loss: 0.3864 - val_avg_loss: 0.2645 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 271/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6261 - op_main_loss: 0.1499 - op_conv_loss: 0.0778 - avg_loss: 0.1068 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9688\n",
      "Epoch 00271: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6261 - op_main_loss: 0.1499 - op_conv_loss: 0.0778 - avg_loss: 0.1068 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9688 - val_loss: 1.1316 - val_op_main_loss: 0.2401 - val_op_conv_loss: 0.3524 - val_avg_loss: 0.2472 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9056\n",
      "Epoch 272/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6209 - op_main_loss: 0.1431 - op_conv_loss: 0.0804 - avg_loss: 0.1058 - op_main_accuracy: 0.9571 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9671\n",
      "Epoch 00272: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6214 - op_main_loss: 0.1434 - op_conv_loss: 0.0804 - avg_loss: 0.1059 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9672 - val_loss: 1.2782 - val_op_main_loss: 0.2787 - val_op_conv_loss: 0.4194 - val_avg_loss: 0.2887 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8857\n",
      "Epoch 273/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6593 - op_main_loss: 0.1536 - op_conv_loss: 0.0991 - avg_loss: 0.1153 - op_main_accuracy: 0.9505 - op_conv_accuracy: 0.9659 - avg_accuracy: 0.9666\n",
      "Epoch 00273: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6571 - op_main_loss: 0.1529 - op_conv_loss: 0.0982 - avg_loss: 0.1147 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9669 - val_loss: 1.1294 - val_op_main_loss: 0.2480 - val_op_conv_loss: 0.3403 - val_avg_loss: 0.2495 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8980\n",
      "Epoch 274/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6303 - op_main_loss: 0.1484 - op_conv_loss: 0.0816 - avg_loss: 0.1086 - op_main_accuracy: 0.9524 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9654\n",
      "Epoch 00274: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6299 - op_main_loss: 0.1489 - op_conv_loss: 0.0809 - avg_loss: 0.1084 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9657 - val_loss: 1.0718 - val_op_main_loss: 0.2325 - val_op_conv_loss: 0.3137 - val_avg_loss: 0.2341 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9093\n",
      "Epoch 275/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6457 - op_main_loss: 0.1537 - op_conv_loss: 0.0881 - avg_loss: 0.1130 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9657\n",
      "Epoch 00275: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6457 - op_main_loss: 0.1537 - op_conv_loss: 0.0881 - avg_loss: 0.1130 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9657 - val_loss: 1.0585 - val_op_main_loss: 0.2348 - val_op_conv_loss: 0.3008 - val_avg_loss: 0.2323 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9103 - val_avg_accuracy: 0.9122\n",
      "Epoch 276/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6269 - op_main_loss: 0.1462 - op_conv_loss: 0.0819 - avg_loss: 0.1076 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9654\n",
      "Epoch 00276: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6267 - op_main_loss: 0.1461 - op_conv_loss: 0.0818 - avg_loss: 0.1076 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9655 - val_loss: 1.2290 - val_op_main_loss: 0.2719 - val_op_conv_loss: 0.3900 - val_avg_loss: 0.2761 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8914\n",
      "Epoch 277/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6219 - op_main_loss: 0.1441 - op_conv_loss: 0.0817 - avg_loss: 0.1056 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9707\n",
      "Epoch 00277: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6261 - op_main_loss: 0.1457 - op_conv_loss: 0.0830 - avg_loss: 0.1070 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9695 - val_loss: 1.1438 - val_op_main_loss: 0.2462 - val_op_conv_loss: 0.3553 - val_avg_loss: 0.2524 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 278/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6331 - op_main_loss: 0.1492 - op_conv_loss: 0.0844 - avg_loss: 0.1087 - op_main_accuracy: 0.9535 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9695\n",
      "Epoch 00278: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6307 - op_main_loss: 0.1484 - op_conv_loss: 0.0834 - avg_loss: 0.1080 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9698 - val_loss: 1.2122 - val_op_main_loss: 0.2572 - val_op_conv_loss: 0.3916 - val_avg_loss: 0.2722 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8961\n",
      "Epoch 279/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6431 - op_main_loss: 0.1504 - op_conv_loss: 0.0900 - avg_loss: 0.1120 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9660\n",
      "Epoch 00279: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6431 - op_main_loss: 0.1504 - op_conv_loss: 0.0900 - avg_loss: 0.1120 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9660 - val_loss: 1.0929 - val_op_main_loss: 0.2442 - val_op_conv_loss: 0.3119 - val_avg_loss: 0.2458 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9046\n",
      "Epoch 280/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6467 - op_main_loss: 0.1505 - op_conv_loss: 0.0915 - avg_loss: 0.1136 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9652\n",
      "Epoch 00280: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6466 - op_main_loss: 0.1503 - op_conv_loss: 0.0915 - avg_loss: 0.1136 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9650 - val_loss: 1.3210 - val_op_main_loss: 0.2984 - val_op_conv_loss: 0.4260 - val_avg_loss: 0.3057 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8810\n",
      "Epoch 281/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6506 - op_main_loss: 0.1519 - op_conv_loss: 0.0939 - avg_loss: 0.1147 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9662\n",
      "Epoch 00281: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6506 - op_main_loss: 0.1519 - op_conv_loss: 0.0939 - avg_loss: 0.1147 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9662 - val_loss: 1.1732 - val_op_main_loss: 0.2558 - val_op_conv_loss: 0.3632 - val_avg_loss: 0.2641 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 282/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6138 - op_main_loss: 0.1430 - op_conv_loss: 0.0773 - avg_loss: 0.1035 - op_main_accuracy: 0.9578 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9721\n",
      "Epoch 00282: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6185 - op_main_loss: 0.1450 - op_conv_loss: 0.0786 - avg_loss: 0.1050 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9716 - val_loss: 1.0390 - val_op_main_loss: 0.2310 - val_op_conv_loss: 0.2902 - val_avg_loss: 0.2285 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9103 - val_avg_accuracy: 0.9112\n",
      "Epoch 283/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6182 - op_main_loss: 0.1470 - op_conv_loss: 0.0766 - avg_loss: 0.1053 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9698\n",
      "Epoch 00283: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6182 - op_main_loss: 0.1470 - op_conv_loss: 0.0766 - avg_loss: 0.1053 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9698 - val_loss: 1.1259 - val_op_main_loss: 0.2432 - val_op_conv_loss: 0.3458 - val_avg_loss: 0.2471 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8990\n",
      "Epoch 284/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6191 - op_main_loss: 0.1429 - op_conv_loss: 0.0816 - avg_loss: 0.1050 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9680\n",
      "Epoch 00284: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6198 - op_main_loss: 0.1434 - op_conv_loss: 0.0816 - avg_loss: 0.1052 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9679 - val_loss: 1.0376 - val_op_main_loss: 0.2237 - val_op_conv_loss: 0.2988 - val_avg_loss: 0.2261 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9056\n",
      "Epoch 285/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5939 - op_main_loss: 0.1362 - op_conv_loss: 0.0711 - avg_loss: 0.0972 - op_main_accuracy: 0.9639 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9695\n",
      "Epoch 00285: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5926 - op_main_loss: 0.1359 - op_conv_loss: 0.0706 - avg_loss: 0.0968 - op_main_accuracy: 0.9638 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9698 - val_loss: 1.1204 - val_op_main_loss: 0.2412 - val_op_conv_loss: 0.3419 - val_avg_loss: 0.2480 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9018\n",
      "Epoch 286/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6318 - op_main_loss: 0.1469 - op_conv_loss: 0.0864 - avg_loss: 0.1090 - op_main_accuracy: 0.9529 - op_conv_accuracy: 0.9692 - avg_accuracy: 0.9680\n",
      "Epoch 00286: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6347 - op_main_loss: 0.1473 - op_conv_loss: 0.0881 - avg_loss: 0.1098 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9674 - val_loss: 1.0637 - val_op_main_loss: 0.2304 - val_op_conv_loss: 0.3097 - val_avg_loss: 0.2337 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9046\n",
      "Epoch 287/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6223 - op_main_loss: 0.1424 - op_conv_loss: 0.0844 - avg_loss: 0.1057 - op_main_accuracy: 0.9562 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9678\n",
      "Epoch 00287: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6221 - op_main_loss: 0.1421 - op_conv_loss: 0.0844 - avg_loss: 0.1057 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9679 - val_loss: 1.1155 - val_op_main_loss: 0.2516 - val_op_conv_loss: 0.3222 - val_avg_loss: 0.2520 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9065\n",
      "Epoch 288/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6187 - op_main_loss: 0.1416 - op_conv_loss: 0.0823 - avg_loss: 0.1053 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9668 - avg_accuracy: 0.9680\n",
      "Epoch 00288: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6206 - op_main_loss: 0.1422 - op_conv_loss: 0.0831 - avg_loss: 0.1059 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9676 - val_loss: 1.5470 - val_op_main_loss: 0.3381 - val_op_conv_loss: 0.5589 - val_avg_loss: 0.3608 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8772\n",
      "Epoch 289/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6186 - op_main_loss: 0.1442 - op_conv_loss: 0.0807 - avg_loss: 0.1048 - op_main_accuracy: 0.9517 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9685\n",
      "Epoch 00289: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6186 - op_main_loss: 0.1442 - op_conv_loss: 0.0807 - avg_loss: 0.1048 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9683 - val_loss: 1.0570 - val_op_main_loss: 0.2300 - val_op_conv_loss: 0.3055 - val_avg_loss: 0.2325 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9065\n",
      "Epoch 290/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6306 - op_main_loss: 0.1452 - op_conv_loss: 0.0883 - avg_loss: 0.1084 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9654 - avg_accuracy: 0.9646\n",
      "Epoch 00290: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6294 - op_main_loss: 0.1448 - op_conv_loss: 0.0878 - avg_loss: 0.1081 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9655 - avg_accuracy: 0.9648 - val_loss: 1.0961 - val_op_main_loss: 0.2397 - val_op_conv_loss: 0.3225 - val_avg_loss: 0.2457 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8980\n",
      "Epoch 291/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6155 - op_main_loss: 0.1445 - op_conv_loss: 0.0786 - avg_loss: 0.1042 - op_main_accuracy: 0.9535 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9680\n",
      "Epoch 00291: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6149 - op_main_loss: 0.1445 - op_conv_loss: 0.0783 - avg_loss: 0.1040 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9681 - val_loss: 1.1317 - val_op_main_loss: 0.2582 - val_op_conv_loss: 0.3285 - val_avg_loss: 0.2575 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9037\n",
      "Epoch 292/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6342 - op_main_loss: 0.1495 - op_conv_loss: 0.0862 - avg_loss: 0.1108 - op_main_accuracy: 0.9497 - op_conv_accuracy: 0.9656 - avg_accuracy: 0.9647\n",
      "Epoch 00292: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6343 - op_main_loss: 0.1498 - op_conv_loss: 0.0860 - avg_loss: 0.1108 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9646 - val_loss: 1.0992 - val_op_main_loss: 0.2483 - val_op_conv_loss: 0.3165 - val_avg_loss: 0.2470 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9093 - val_avg_accuracy: 0.9075\n",
      "Epoch 293/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6420 - op_main_loss: 0.1506 - op_conv_loss: 0.0915 - avg_loss: 0.1122 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9666 - avg_accuracy: 0.9683\n",
      "Epoch 00293: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6416 - op_main_loss: 0.1505 - op_conv_loss: 0.0913 - avg_loss: 0.1121 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9683 - val_loss: 1.1988 - val_op_main_loss: 0.2740 - val_op_conv_loss: 0.3574 - val_avg_loss: 0.2795 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8905\n",
      "Epoch 294/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6073 - op_main_loss: 0.1388 - op_conv_loss: 0.0784 - avg_loss: 0.1021 - op_main_accuracy: 0.9548 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9700\n",
      "Epoch 00294: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6097 - op_main_loss: 0.1396 - op_conv_loss: 0.0792 - avg_loss: 0.1028 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9695 - val_loss: 1.1345 - val_op_main_loss: 0.2468 - val_op_conv_loss: 0.3426 - val_avg_loss: 0.2571 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 295/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6416 - op_main_loss: 0.1545 - op_conv_loss: 0.0862 - avg_loss: 0.1127 - op_main_accuracy: 0.9485 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9660\n",
      "Epoch 00295: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6416 - op_main_loss: 0.1545 - op_conv_loss: 0.0862 - avg_loss: 0.1127 - op_main_accuracy: 0.9485 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9660 - val_loss: 1.0843 - val_op_main_loss: 0.2333 - val_op_conv_loss: 0.3257 - val_avg_loss: 0.2372 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9056\n",
      "Epoch 296/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6119 - op_main_loss: 0.1410 - op_conv_loss: 0.0793 - avg_loss: 0.1035 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9692 - avg_accuracy: 0.9711\n",
      "Epoch 00296: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6166 - op_main_loss: 0.1422 - op_conv_loss: 0.0813 - avg_loss: 0.1049 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9705 - val_loss: 1.0863 - val_op_main_loss: 0.2335 - val_op_conv_loss: 0.3260 - val_avg_loss: 0.2386 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9084\n",
      "Epoch 297/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6058 - op_main_loss: 0.1378 - op_conv_loss: 0.0786 - avg_loss: 0.1016 - op_main_accuracy: 0.9606 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9702\n",
      "Epoch 00297: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6047 - op_main_loss: 0.1375 - op_conv_loss: 0.0782 - avg_loss: 0.1013 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9702 - val_loss: 1.0925 - val_op_main_loss: 0.2398 - val_op_conv_loss: 0.3228 - val_avg_loss: 0.2424 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9027\n",
      "Epoch 298/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6203 - op_main_loss: 0.1456 - op_conv_loss: 0.0807 - avg_loss: 0.1065 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9683\n",
      "Epoch 00298: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6205 - op_main_loss: 0.1454 - op_conv_loss: 0.0810 - avg_loss: 0.1066 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9679 - val_loss: 1.0943 - val_op_main_loss: 0.2381 - val_op_conv_loss: 0.3279 - val_avg_loss: 0.2409 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 299/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6085 - op_main_loss: 0.1408 - op_conv_loss: 0.0781 - avg_loss: 0.1020 - op_main_accuracy: 0.9554 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9700\n",
      "Epoch 00299: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6080 - op_main_loss: 0.1405 - op_conv_loss: 0.0779 - avg_loss: 0.1019 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9698 - val_loss: 1.1448 - val_op_main_loss: 0.2600 - val_op_conv_loss: 0.3395 - val_avg_loss: 0.2576 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9027\n",
      "Epoch 300/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6179 - op_main_loss: 0.1451 - op_conv_loss: 0.0799 - avg_loss: 0.1055 - op_main_accuracy: 0.9557 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9721\n",
      "Epoch 00300: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6199 - op_main_loss: 0.1451 - op_conv_loss: 0.0813 - avg_loss: 0.1061 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9719 - val_loss: 1.1742 - val_op_main_loss: 0.2556 - val_op_conv_loss: 0.3683 - val_avg_loss: 0.2630 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8971\n",
      "Epoch 301/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6090 - op_main_loss: 0.1398 - op_conv_loss: 0.0788 - avg_loss: 0.1030 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9685\n",
      "Epoch 00301: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6103 - op_main_loss: 0.1401 - op_conv_loss: 0.0795 - avg_loss: 0.1033 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9683 - val_loss: 1.0948 - val_op_main_loss: 0.2344 - val_op_conv_loss: 0.3308 - val_avg_loss: 0.2428 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8952\n",
      "Epoch 302/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6208 - op_main_loss: 0.1433 - op_conv_loss: 0.0844 - avg_loss: 0.1063 - op_main_accuracy: 0.9545 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9678\n",
      "Epoch 00302: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6204 - op_main_loss: 0.1431 - op_conv_loss: 0.0843 - avg_loss: 0.1062 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9679 - val_loss: 1.3079 - val_op_main_loss: 0.2837 - val_op_conv_loss: 0.4400 - val_avg_loss: 0.2973 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8905\n",
      "Epoch 303/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6276 - op_main_loss: 0.1513 - op_conv_loss: 0.0806 - avg_loss: 0.1081 - op_main_accuracy: 0.9460 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9656\n",
      "Epoch 00303: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6266 - op_main_loss: 0.1509 - op_conv_loss: 0.0804 - avg_loss: 0.1078 - op_main_accuracy: 0.9464 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9657 - val_loss: 1.1670 - val_op_main_loss: 0.2596 - val_op_conv_loss: 0.3535 - val_avg_loss: 0.2663 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9027\n",
      "Epoch 304/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6097 - op_main_loss: 0.1401 - op_conv_loss: 0.0792 - avg_loss: 0.1029 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9704\n",
      "Epoch 00304: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6119 - op_main_loss: 0.1411 - op_conv_loss: 0.0797 - avg_loss: 0.1036 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9698 - val_loss: 1.1999 - val_op_main_loss: 0.2645 - val_op_conv_loss: 0.3774 - val_avg_loss: 0.2704 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8999\n",
      "Epoch 305/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.6197 - op_main_loss: 0.1462 - op_conv_loss: 0.0807 - avg_loss: 0.1057 - op_main_accuracy: 0.9493 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9695\n",
      "Epoch 00305: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6201 - op_main_loss: 0.1462 - op_conv_loss: 0.0809 - avg_loss: 0.1058 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9695 - val_loss: 1.1450 - val_op_main_loss: 0.2504 - val_op_conv_loss: 0.3519 - val_avg_loss: 0.2559 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9018\n",
      "Epoch 306/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5981 - op_main_loss: 0.1366 - op_conv_loss: 0.0751 - avg_loss: 0.0997 - op_main_accuracy: 0.9588 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9700\n",
      "Epoch 00306: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6055 - op_main_loss: 0.1385 - op_conv_loss: 0.0783 - avg_loss: 0.1020 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9698 - val_loss: 1.1230 - val_op_main_loss: 0.2433 - val_op_conv_loss: 0.3432 - val_avg_loss: 0.2493 - val_op_main_accuracy: 0.9122 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9084\n",
      "Epoch 307/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6155 - op_main_loss: 0.1441 - op_conv_loss: 0.0799 - avg_loss: 0.1041 - op_main_accuracy: 0.9529 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9738\n",
      "Epoch 00307: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6136 - op_main_loss: 0.1435 - op_conv_loss: 0.0791 - avg_loss: 0.1035 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9740 - val_loss: 1.3295 - val_op_main_loss: 0.3113 - val_op_conv_loss: 0.4202 - val_avg_loss: 0.3104 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8857\n",
      "Epoch 308/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6145 - op_main_loss: 0.1422 - op_conv_loss: 0.0810 - avg_loss: 0.1039 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9681\n",
      "Epoch 00308: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6145 - op_main_loss: 0.1422 - op_conv_loss: 0.0810 - avg_loss: 0.1039 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9681 - val_loss: 1.1191 - val_op_main_loss: 0.2493 - val_op_conv_loss: 0.3344 - val_avg_loss: 0.2485 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9065\n",
      "Epoch 309/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5989 - op_main_loss: 0.1352 - op_conv_loss: 0.0770 - avg_loss: 0.0999 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9736 - avg_accuracy: 0.9716\n",
      "Epoch 00309: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5984 - op_main_loss: 0.1351 - op_conv_loss: 0.0768 - avg_loss: 0.0997 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9716 - val_loss: 1.0645 - val_op_main_loss: 0.2351 - val_op_conv_loss: 0.3088 - val_avg_loss: 0.2341 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9084\n",
      "Epoch 310/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6135 - op_main_loss: 0.1435 - op_conv_loss: 0.0795 - avg_loss: 0.1041 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9688\n",
      "Epoch 00310: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6114 - op_main_loss: 0.1427 - op_conv_loss: 0.0789 - avg_loss: 0.1034 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9693 - val_loss: 1.1633 - val_op_main_loss: 0.2622 - val_op_conv_loss: 0.3537 - val_avg_loss: 0.2606 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9018\n",
      "Epoch 311/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6219 - op_main_loss: 0.1469 - op_conv_loss: 0.0820 - avg_loss: 0.1064 - op_main_accuracy: 0.9500 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9690\n",
      "Epoch 00311: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6226 - op_main_loss: 0.1471 - op_conv_loss: 0.0822 - avg_loss: 0.1067 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9690 - val_loss: 1.2866 - val_op_main_loss: 0.2503 - val_op_conv_loss: 0.4707 - val_avg_loss: 0.2799 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8810\n",
      "Epoch 312/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6187 - op_main_loss: 0.1434 - op_conv_loss: 0.0836 - avg_loss: 0.1057 - op_main_accuracy: 0.9519 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9675\n",
      "Epoch 00312: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6190 - op_main_loss: 0.1436 - op_conv_loss: 0.0836 - avg_loss: 0.1059 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9676 - val_loss: 1.0535 - val_op_main_loss: 0.2295 - val_op_conv_loss: 0.3067 - val_avg_loss: 0.2315 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9084\n",
      "Epoch 313/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5982 - op_main_loss: 0.1372 - op_conv_loss: 0.0759 - avg_loss: 0.0994 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9733\n",
      "Epoch 00313: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5982 - op_main_loss: 0.1372 - op_conv_loss: 0.0759 - avg_loss: 0.0994 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9733 - val_loss: 1.0778 - val_op_main_loss: 0.2363 - val_op_conv_loss: 0.3165 - val_avg_loss: 0.2391 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9103\n",
      "Epoch 314/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6021 - op_main_loss: 0.1391 - op_conv_loss: 0.0769 - avg_loss: 0.1003 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9717 - avg_accuracy: 0.9700\n",
      "Epoch 00314: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6023 - op_main_loss: 0.1383 - op_conv_loss: 0.0779 - avg_loss: 0.1003 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9700 - val_loss: 1.4756 - val_op_main_loss: 0.3612 - val_op_conv_loss: 0.4735 - val_avg_loss: 0.3554 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8829\n",
      "Epoch 315/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6291 - op_main_loss: 0.1485 - op_conv_loss: 0.0861 - avg_loss: 0.1092 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9652 - avg_accuracy: 0.9633\n",
      "Epoch 00315: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6285 - op_main_loss: 0.1480 - op_conv_loss: 0.0862 - avg_loss: 0.1090 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9631 - val_loss: 1.1274 - val_op_main_loss: 0.2494 - val_op_conv_loss: 0.3421 - val_avg_loss: 0.2510 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9093\n",
      "Epoch 316/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6109 - op_main_loss: 0.1404 - op_conv_loss: 0.0818 - avg_loss: 0.1032 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9676\n",
      "Epoch 00316: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6107 - op_main_loss: 0.1403 - op_conv_loss: 0.0817 - avg_loss: 0.1032 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9676 - val_loss: 1.3324 - val_op_main_loss: 0.2628 - val_op_conv_loss: 0.4879 - val_avg_loss: 0.2962 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8754\n",
      "Epoch 317/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5885 - op_main_loss: 0.1328 - op_conv_loss: 0.0732 - avg_loss: 0.0970 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9709\n",
      "Epoch 00317: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5885 - op_main_loss: 0.1328 - op_conv_loss: 0.0732 - avg_loss: 0.0970 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9709 - val_loss: 1.0859 - val_op_main_loss: 0.2339 - val_op_conv_loss: 0.3261 - val_avg_loss: 0.2402 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9084\n",
      "Epoch 318/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6093 - op_main_loss: 0.1431 - op_conv_loss: 0.0781 - avg_loss: 0.1028 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9702\n",
      "Epoch 00318: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6093 - op_main_loss: 0.1431 - op_conv_loss: 0.0781 - avg_loss: 0.1028 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9702 - val_loss: 1.2791 - val_op_main_loss: 0.2903 - val_op_conv_loss: 0.4074 - val_avg_loss: 0.2963 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8924\n",
      "Epoch 319/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6144 - op_main_loss: 0.1468 - op_conv_loss: 0.0782 - avg_loss: 0.1041 - op_main_accuracy: 0.9516 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9688\n",
      "Epoch 00319: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6124 - op_main_loss: 0.1461 - op_conv_loss: 0.0776 - avg_loss: 0.1035 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9688 - val_loss: 1.0907 - val_op_main_loss: 0.2403 - val_op_conv_loss: 0.3204 - val_avg_loss: 0.2448 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9065\n",
      "Epoch 320/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5776 - op_main_loss: 0.1294 - op_conv_loss: 0.0694 - avg_loss: 0.0938 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9736 - avg_accuracy: 0.9734\n",
      "Epoch 00320: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5757 - op_main_loss: 0.1289 - op_conv_loss: 0.0687 - avg_loss: 0.0932 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9738 - val_loss: 1.5854 - val_op_main_loss: 0.3346 - val_op_conv_loss: 0.5950 - val_avg_loss: 0.3711 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8527 - val_avg_accuracy: 0.8612\n",
      "Epoch 321/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6175 - op_main_loss: 0.1430 - op_conv_loss: 0.0831 - avg_loss: 0.1062 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9678\n",
      "Epoch 00321: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6161 - op_main_loss: 0.1423 - op_conv_loss: 0.0829 - avg_loss: 0.1057 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9681 - val_loss: 1.0716 - val_op_main_loss: 0.2276 - val_op_conv_loss: 0.3251 - val_avg_loss: 0.2339 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9037\n",
      "Epoch 322/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6083 - op_main_loss: 0.1406 - op_conv_loss: 0.0801 - avg_loss: 0.1029 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9718 - avg_accuracy: 0.9695\n",
      "Epoch 00322: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6081 - op_main_loss: 0.1405 - op_conv_loss: 0.0800 - avg_loss: 0.1028 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9695 - val_loss: 1.1058 - val_op_main_loss: 0.2439 - val_op_conv_loss: 0.3288 - val_avg_loss: 0.2480 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 323/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5920 - op_main_loss: 0.1357 - op_conv_loss: 0.0727 - avg_loss: 0.0985 - op_main_accuracy: 0.9559 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9719\n",
      "Epoch 00323: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5961 - op_main_loss: 0.1371 - op_conv_loss: 0.0741 - avg_loss: 0.0998 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9714 - val_loss: 1.1200 - val_op_main_loss: 0.2448 - val_op_conv_loss: 0.3401 - val_avg_loss: 0.2497 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9065\n",
      "Epoch 324/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5999 - op_main_loss: 0.1387 - op_conv_loss: 0.0755 - avg_loss: 0.1005 - op_main_accuracy: 0.9547 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9724\n",
      "Epoch 00324: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5991 - op_main_loss: 0.1384 - op_conv_loss: 0.0752 - avg_loss: 0.1003 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9724 - val_loss: 1.1024 - val_op_main_loss: 0.2403 - val_op_conv_loss: 0.3321 - val_avg_loss: 0.2450 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9056\n",
      "Epoch 325/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5918 - op_main_loss: 0.1356 - op_conv_loss: 0.0737 - avg_loss: 0.0977 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9702\n",
      "Epoch 00325: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5918 - op_main_loss: 0.1356 - op_conv_loss: 0.0737 - avg_loss: 0.0977 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9702 - val_loss: 1.0996 - val_op_main_loss: 0.2406 - val_op_conv_loss: 0.3323 - val_avg_loss: 0.2426 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9075\n",
      "Epoch 326/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5899 - op_main_loss: 0.1340 - op_conv_loss: 0.0746 - avg_loss: 0.0976 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9700\n",
      "Epoch 00326: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5899 - op_main_loss: 0.1340 - op_conv_loss: 0.0746 - avg_loss: 0.0976 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9700 - val_loss: 1.0546 - val_op_main_loss: 0.2264 - val_op_conv_loss: 0.3147 - val_avg_loss: 0.2295 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9093\n",
      "Epoch 327/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6013 - op_main_loss: 0.1389 - op_conv_loss: 0.0780 - avg_loss: 0.1007 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9712\n",
      "Epoch 00327: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6005 - op_main_loss: 0.1387 - op_conv_loss: 0.0776 - avg_loss: 0.1005 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9709 - val_loss: 1.0603 - val_op_main_loss: 0.2330 - val_op_conv_loss: 0.3093 - val_avg_loss: 0.2346 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9093 - val_avg_accuracy: 0.9065\n",
      "Epoch 328/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6063 - op_main_loss: 0.1407 - op_conv_loss: 0.0795 - avg_loss: 0.1027 - op_main_accuracy: 0.9550 - op_conv_accuracy: 0.9699 - avg_accuracy: 0.9683\n",
      "Epoch 00328: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6058 - op_main_loss: 0.1405 - op_conv_loss: 0.0793 - avg_loss: 0.1026 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9683 - val_loss: 1.1359 - val_op_main_loss: 0.2441 - val_op_conv_loss: 0.3554 - val_avg_loss: 0.2529 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.8999\n",
      "Epoch 329/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.6024 - op_main_loss: 0.1391 - op_conv_loss: 0.0781 - avg_loss: 0.1017 - op_main_accuracy: 0.9531 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9697\n",
      "Epoch 00329: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6021 - op_main_loss: 0.1390 - op_conv_loss: 0.0780 - avg_loss: 0.1016 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9698 - val_loss: 1.1399 - val_op_main_loss: 0.2433 - val_op_conv_loss: 0.3620 - val_avg_loss: 0.2509 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9056\n",
      "Epoch 330/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6016 - op_main_loss: 0.1386 - op_conv_loss: 0.0777 - avg_loss: 0.1015 - op_main_accuracy: 0.9559 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9702\n",
      "Epoch 00330: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6017 - op_main_loss: 0.1385 - op_conv_loss: 0.0779 - avg_loss: 0.1016 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9702 - val_loss: 1.2574 - val_op_main_loss: 0.2570 - val_op_conv_loss: 0.4406 - val_avg_loss: 0.2761 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8999\n",
      "Epoch 331/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5948 - op_main_loss: 0.1355 - op_conv_loss: 0.0772 - avg_loss: 0.0985 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9714\n",
      "Epoch 00331: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5937 - op_main_loss: 0.1352 - op_conv_loss: 0.0767 - avg_loss: 0.0982 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9714 - val_loss: 1.0813 - val_op_main_loss: 0.2370 - val_op_conv_loss: 0.3206 - val_avg_loss: 0.2402 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9150 - val_avg_accuracy: 0.9131\n",
      "Epoch 332/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6185 - op_main_loss: 0.1437 - op_conv_loss: 0.0845 - avg_loss: 0.1066 - op_main_accuracy: 0.9491 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9664\n",
      "Epoch 00332: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6182 - op_main_loss: 0.1436 - op_conv_loss: 0.0844 - avg_loss: 0.1065 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9664 - val_loss: 1.0692 - val_op_main_loss: 0.2317 - val_op_conv_loss: 0.3169 - val_avg_loss: 0.2368 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9018\n",
      "Epoch 333/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5814 - op_main_loss: 0.1343 - op_conv_loss: 0.0687 - avg_loss: 0.0949 - op_main_accuracy: 0.9599 - op_conv_accuracy: 0.9762 - avg_accuracy: 0.9728\n",
      "Epoch 00333: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5825 - op_main_loss: 0.1344 - op_conv_loss: 0.0693 - avg_loss: 0.0953 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9724 - val_loss: 1.0984 - val_op_main_loss: 0.2383 - val_op_conv_loss: 0.3330 - val_avg_loss: 0.2441 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9056\n",
      "Epoch 334/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6087 - op_main_loss: 0.1426 - op_conv_loss: 0.0793 - avg_loss: 0.1038 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9693\n",
      "Epoch 00334: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6087 - op_main_loss: 0.1426 - op_conv_loss: 0.0793 - avg_loss: 0.1038 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9693 - val_loss: 1.1933 - val_op_main_loss: 0.2399 - val_op_conv_loss: 0.4106 - val_avg_loss: 0.2599 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8999\n",
      "Epoch 335/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5667 - op_main_loss: 0.1272 - op_conv_loss: 0.0660 - avg_loss: 0.0905 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9738\n",
      "Epoch 00335: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5666 - op_main_loss: 0.1273 - op_conv_loss: 0.0658 - avg_loss: 0.0905 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9740 - val_loss: 1.2217 - val_op_main_loss: 0.2489 - val_op_conv_loss: 0.4187 - val_avg_loss: 0.2712 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8933\n",
      "Epoch 336/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5768 - op_main_loss: 0.1317 - op_conv_loss: 0.0690 - avg_loss: 0.0932 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9754\n",
      "Epoch 00336: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5768 - op_main_loss: 0.1317 - op_conv_loss: 0.0690 - avg_loss: 0.0932 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9754 - val_loss: 1.0896 - val_op_main_loss: 0.2367 - val_op_conv_loss: 0.3285 - val_avg_loss: 0.2417 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9093\n",
      "Epoch 337/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6064 - op_main_loss: 0.1421 - op_conv_loss: 0.0796 - avg_loss: 0.1026 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9690\n",
      "Epoch 00337: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6064 - op_main_loss: 0.1421 - op_conv_loss: 0.0796 - avg_loss: 0.1026 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9690 - val_loss: 1.1542 - val_op_main_loss: 0.2484 - val_op_conv_loss: 0.3681 - val_avg_loss: 0.2550 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9046\n",
      "Epoch 338/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5952 - op_main_loss: 0.1360 - op_conv_loss: 0.0774 - avg_loss: 0.0998 - op_main_accuracy: 0.9566 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9707\n",
      "Epoch 00338: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5939 - op_main_loss: 0.1362 - op_conv_loss: 0.0765 - avg_loss: 0.0994 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9709 - val_loss: 1.1506 - val_op_main_loss: 0.2466 - val_op_conv_loss: 0.3664 - val_avg_loss: 0.2559 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8990\n",
      "Epoch 339/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5816 - op_main_loss: 0.1318 - op_conv_loss: 0.0725 - avg_loss: 0.0955 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9736 - avg_accuracy: 0.9726\n",
      "Epoch 00339: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5827 - op_main_loss: 0.1319 - op_conv_loss: 0.0731 - avg_loss: 0.0957 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9728 - val_loss: 1.1223 - val_op_main_loss: 0.2392 - val_op_conv_loss: 0.3525 - val_avg_loss: 0.2487 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8971\n",
      "Epoch 340/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6028 - op_main_loss: 0.1382 - op_conv_loss: 0.0811 - avg_loss: 0.1017 - op_main_accuracy: 0.9543 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9683\n",
      "Epoch 00340: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6025 - op_main_loss: 0.1380 - op_conv_loss: 0.0810 - avg_loss: 0.1016 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9683 - val_loss: 1.0328 - val_op_main_loss: 0.2262 - val_op_conv_loss: 0.2964 - val_avg_loss: 0.2290 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 341/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6010 - op_main_loss: 0.1395 - op_conv_loss: 0.0786 - avg_loss: 0.1019 - op_main_accuracy: 0.9581 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9721\n",
      "Epoch 00341: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6001 - op_main_loss: 0.1394 - op_conv_loss: 0.0781 - avg_loss: 0.1017 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9724 - val_loss: 1.1069 - val_op_main_loss: 0.2480 - val_op_conv_loss: 0.3274 - val_avg_loss: 0.2513 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 342/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6002 - op_main_loss: 0.1355 - op_conv_loss: 0.0822 - avg_loss: 0.1021 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9678 - avg_accuracy: 0.9683\n",
      "Epoch 00342: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6000 - op_main_loss: 0.1354 - op_conv_loss: 0.0821 - avg_loss: 0.1021 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9683 - val_loss: 1.1079 - val_op_main_loss: 0.2544 - val_op_conv_loss: 0.3184 - val_avg_loss: 0.2543 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9046\n",
      "Epoch 343/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5880 - op_main_loss: 0.1358 - op_conv_loss: 0.0731 - avg_loss: 0.0982 - op_main_accuracy: 0.9583 - op_conv_accuracy: 0.9741 - avg_accuracy: 0.9734\n",
      "Epoch 00343: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5846 - op_main_loss: 0.1344 - op_conv_loss: 0.0722 - avg_loss: 0.0971 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9738 - val_loss: 1.0695 - val_op_main_loss: 0.2313 - val_op_conv_loss: 0.3211 - val_avg_loss: 0.2369 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9046\n",
      "Epoch 344/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5807 - op_main_loss: 0.1321 - op_conv_loss: 0.0725 - avg_loss: 0.0959 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9704\n",
      "Epoch 00344: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5847 - op_main_loss: 0.1332 - op_conv_loss: 0.0740 - avg_loss: 0.0972 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9698 - val_loss: 1.0463 - val_op_main_loss: 0.2263 - val_op_conv_loss: 0.3091 - val_avg_loss: 0.2307 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9065\n",
      "Epoch 345/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5796 - op_main_loss: 0.1286 - op_conv_loss: 0.0754 - avg_loss: 0.0950 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9748 - avg_accuracy: 0.9738\n",
      "Epoch 00345: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5811 - op_main_loss: 0.1293 - op_conv_loss: 0.0757 - avg_loss: 0.0955 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9738 - val_loss: 1.1346 - val_op_main_loss: 0.2539 - val_op_conv_loss: 0.3410 - val_avg_loss: 0.2594 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8980\n",
      "Epoch 346/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6016 - op_main_loss: 0.1383 - op_conv_loss: 0.0803 - avg_loss: 0.1025 - op_main_accuracy: 0.9545 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9668\n",
      "Epoch 00346: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6037 - op_main_loss: 0.1382 - op_conv_loss: 0.0819 - avg_loss: 0.1030 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9664 - val_loss: 1.3230 - val_op_main_loss: 0.3117 - val_op_conv_loss: 0.4207 - val_avg_loss: 0.3097 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8933\n",
      "Epoch 347/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5776 - op_main_loss: 0.1322 - op_conv_loss: 0.0703 - avg_loss: 0.0950 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9733\n",
      "Epoch 00347: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5766 - op_main_loss: 0.1319 - op_conv_loss: 0.0700 - avg_loss: 0.0947 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9733 - val_loss: 1.1054 - val_op_main_loss: 0.2328 - val_op_conv_loss: 0.3501 - val_avg_loss: 0.2431 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9065\n",
      "Epoch 348/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5779 - op_main_loss: 0.1311 - op_conv_loss: 0.0725 - avg_loss: 0.0949 - op_main_accuracy: 0.9621 - op_conv_accuracy: 0.9751 - avg_accuracy: 0.9742\n",
      "Epoch 00348: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5775 - op_main_loss: 0.1309 - op_conv_loss: 0.0724 - avg_loss: 0.0948 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9742 - val_loss: 1.1774 - val_op_main_loss: 0.2491 - val_op_conv_loss: 0.3857 - val_avg_loss: 0.2633 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8924\n",
      "Epoch 349/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5913 - op_main_loss: 0.1342 - op_conv_loss: 0.0778 - avg_loss: 0.0997 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9688\n",
      "Epoch 00349: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5912 - op_main_loss: 0.1339 - op_conv_loss: 0.0780 - avg_loss: 0.0997 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9686 - val_loss: 1.3358 - val_op_main_loss: 0.2864 - val_op_conv_loss: 0.4638 - val_avg_loss: 0.3063 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8857\n",
      "Epoch 350/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6042 - op_main_loss: 0.1440 - op_conv_loss: 0.0789 - avg_loss: 0.1023 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9697\n",
      "Epoch 00350: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6054 - op_main_loss: 0.1445 - op_conv_loss: 0.0793 - avg_loss: 0.1027 - op_main_accuracy: 0.9485 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9693 - val_loss: 1.1013 - val_op_main_loss: 0.2357 - val_op_conv_loss: 0.3406 - val_avg_loss: 0.2463 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8971\n",
      "Epoch 351/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5840 - op_main_loss: 0.1351 - op_conv_loss: 0.0731 - avg_loss: 0.0971 - op_main_accuracy: 0.9559 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9709\n",
      "Epoch 00351: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5842 - op_main_loss: 0.1353 - op_conv_loss: 0.0730 - avg_loss: 0.0971 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9707 - val_loss: 1.1251 - val_op_main_loss: 0.2428 - val_op_conv_loss: 0.3519 - val_avg_loss: 0.2516 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9027\n",
      "Epoch 352/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5634 - op_main_loss: 0.1240 - op_conv_loss: 0.0696 - avg_loss: 0.0909 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9714\n",
      "Epoch 00352: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5634 - op_main_loss: 0.1240 - op_conv_loss: 0.0696 - avg_loss: 0.0909 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9714 - val_loss: 1.0589 - val_op_main_loss: 0.2319 - val_op_conv_loss: 0.3119 - val_avg_loss: 0.2363 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9037\n",
      "Epoch 353/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.5924 - op_main_loss: 0.1368 - op_conv_loss: 0.0768 - avg_loss: 0.0999 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9704\n",
      "Epoch 00353: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5919 - op_main_loss: 0.1364 - op_conv_loss: 0.0768 - avg_loss: 0.0997 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9707 - val_loss: 1.0465 - val_op_main_loss: 0.2287 - val_op_conv_loss: 0.3082 - val_avg_loss: 0.2306 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9103\n",
      "Epoch 354/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5930 - op_main_loss: 0.1350 - op_conv_loss: 0.0802 - avg_loss: 0.0992 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9716\n",
      "Epoch 00354: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5930 - op_main_loss: 0.1348 - op_conv_loss: 0.0804 - avg_loss: 0.0992 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9716 - val_loss: 1.1528 - val_op_main_loss: 0.2627 - val_op_conv_loss: 0.3493 - val_avg_loss: 0.2629 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.8990\n",
      "Epoch 355/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5895 - op_main_loss: 0.1353 - op_conv_loss: 0.0766 - avg_loss: 0.0994 - op_main_accuracy: 0.9593 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9688\n",
      "Epoch 00355: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5892 - op_main_loss: 0.1352 - op_conv_loss: 0.0765 - avg_loss: 0.0993 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9688 - val_loss: 1.1236 - val_op_main_loss: 0.2457 - val_op_conv_loss: 0.3459 - val_avg_loss: 0.2538 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8952\n",
      "Epoch 356/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5892 - op_main_loss: 0.1337 - op_conv_loss: 0.0789 - avg_loss: 0.0990 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9716\n",
      "Epoch 00356: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5892 - op_main_loss: 0.1337 - op_conv_loss: 0.0789 - avg_loss: 0.0990 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9716 - val_loss: 1.1183 - val_op_main_loss: 0.2496 - val_op_conv_loss: 0.3371 - val_avg_loss: 0.2543 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8999\n",
      "Epoch 357/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5753 - op_main_loss: 0.1322 - op_conv_loss: 0.0710 - avg_loss: 0.0948 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9705\n",
      "Epoch 00357: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5753 - op_main_loss: 0.1322 - op_conv_loss: 0.0710 - avg_loss: 0.0948 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9705 - val_loss: 1.0420 - val_op_main_loss: 0.2296 - val_op_conv_loss: 0.3011 - val_avg_loss: 0.2343 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9065\n",
      "Epoch 358/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5845 - op_main_loss: 0.1334 - op_conv_loss: 0.0761 - avg_loss: 0.0973 - op_main_accuracy: 0.9571 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9688\n",
      "Epoch 00358: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5844 - op_main_loss: 0.1333 - op_conv_loss: 0.0761 - avg_loss: 0.0973 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9688 - val_loss: 1.0271 - val_op_main_loss: 0.2228 - val_op_conv_loss: 0.3014 - val_avg_loss: 0.2253 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9093 - val_avg_accuracy: 0.9112\n",
      "Epoch 359/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5758 - op_main_loss: 0.1311 - op_conv_loss: 0.0722 - avg_loss: 0.0952 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9731\n",
      "Epoch 00359: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5758 - op_main_loss: 0.1311 - op_conv_loss: 0.0722 - avg_loss: 0.0952 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9731 - val_loss: 1.2117 - val_op_main_loss: 0.2789 - val_op_conv_loss: 0.3775 - val_avg_loss: 0.2779 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9027\n",
      "Epoch 360/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5551 - op_main_loss: 0.1254 - op_conv_loss: 0.0635 - avg_loss: 0.0887 - op_main_accuracy: 0.9630 - op_conv_accuracy: 0.9786 - avg_accuracy: 0.9760\n",
      "Epoch 00360: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5557 - op_main_loss: 0.1252 - op_conv_loss: 0.0641 - avg_loss: 0.0889 - op_main_accuracy: 0.9629 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9757 - val_loss: 1.0415 - val_op_main_loss: 0.2281 - val_op_conv_loss: 0.3067 - val_avg_loss: 0.2296 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9112\n",
      "Epoch 361/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5791 - op_main_loss: 0.1302 - op_conv_loss: 0.0754 - avg_loss: 0.0963 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9693\n",
      "Epoch 00361: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5791 - op_main_loss: 0.1302 - op_conv_loss: 0.0754 - avg_loss: 0.0963 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9693 - val_loss: 1.0525 - val_op_main_loss: 0.2250 - val_op_conv_loss: 0.3188 - val_avg_loss: 0.2313 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9046\n",
      "Epoch 362/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5601 - op_main_loss: 0.1267 - op_conv_loss: 0.0657 - avg_loss: 0.0907 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9763 - avg_accuracy: 0.9744\n",
      "Epoch 00362: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5598 - op_main_loss: 0.1266 - op_conv_loss: 0.0656 - avg_loss: 0.0906 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9745 - val_loss: 1.1491 - val_op_main_loss: 0.2459 - val_op_conv_loss: 0.3681 - val_avg_loss: 0.2585 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 363/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5862 - op_main_loss: 0.1338 - op_conv_loss: 0.0777 - avg_loss: 0.0980 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9712\n",
      "Epoch 00363: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5864 - op_main_loss: 0.1336 - op_conv_loss: 0.0781 - avg_loss: 0.0980 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9714 - val_loss: 1.0494 - val_op_main_loss: 0.2274 - val_op_conv_loss: 0.3135 - val_avg_loss: 0.2317 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9037\n",
      "Epoch 364/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5711 - op_main_loss: 0.1265 - op_conv_loss: 0.0744 - avg_loss: 0.0936 - op_main_accuracy: 0.9630 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9716\n",
      "Epoch 00364: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5737 - op_main_loss: 0.1277 - op_conv_loss: 0.0750 - avg_loss: 0.0944 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9714 - val_loss: 1.3489 - val_op_main_loss: 0.2771 - val_op_conv_loss: 0.4881 - val_avg_loss: 0.3072 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8801\n",
      "Epoch 365/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5777 - op_main_loss: 0.1271 - op_conv_loss: 0.0790 - avg_loss: 0.0953 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9745\n",
      "Epoch 00365: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5777 - op_main_loss: 0.1271 - op_conv_loss: 0.0790 - avg_loss: 0.0953 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9745 - val_loss: 1.0692 - val_op_main_loss: 0.2320 - val_op_conv_loss: 0.3231 - val_avg_loss: 0.2376 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 366/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5634 - op_main_loss: 0.1268 - op_conv_loss: 0.0683 - avg_loss: 0.0920 - op_main_accuracy: 0.9623 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9733\n",
      "Epoch 00366: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5704 - op_main_loss: 0.1287 - op_conv_loss: 0.0715 - avg_loss: 0.0940 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9724 - val_loss: 1.1163 - val_op_main_loss: 0.2512 - val_op_conv_loss: 0.3333 - val_avg_loss: 0.2557 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8990\n",
      "Epoch 367/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5595 - op_main_loss: 0.1258 - op_conv_loss: 0.0668 - avg_loss: 0.0908 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9768 - avg_accuracy: 0.9747\n",
      "Epoch 00367: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5595 - op_main_loss: 0.1257 - op_conv_loss: 0.0669 - avg_loss: 0.0908 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9747 - val_loss: 1.0729 - val_op_main_loss: 0.2417 - val_op_conv_loss: 0.3160 - val_avg_loss: 0.2394 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9065\n",
      "Epoch 368/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5942 - op_main_loss: 0.1373 - op_conv_loss: 0.0797 - avg_loss: 0.1010 - op_main_accuracy: 0.9538 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9702\n",
      "Epoch 00368: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5932 - op_main_loss: 0.1368 - op_conv_loss: 0.0795 - avg_loss: 0.1007 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9702 - val_loss: 1.0691 - val_op_main_loss: 0.2317 - val_op_conv_loss: 0.3220 - val_avg_loss: 0.2390 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.9027\n",
      "Epoch 369/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5918 - op_main_loss: 0.1375 - op_conv_loss: 0.0770 - avg_loss: 0.1007 - op_main_accuracy: 0.9557 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9714\n",
      "Epoch 00369: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5900 - op_main_loss: 0.1370 - op_conv_loss: 0.0763 - avg_loss: 0.1001 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9716 - val_loss: 1.3513 - val_op_main_loss: 0.2958 - val_op_conv_loss: 0.4625 - val_avg_loss: 0.3168 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8801\n",
      "Epoch 370/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5583 - op_main_loss: 0.1236 - op_conv_loss: 0.0683 - avg_loss: 0.0906 - op_main_accuracy: 0.9664 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9745\n",
      "Epoch 00370: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5583 - op_main_loss: 0.1236 - op_conv_loss: 0.0683 - avg_loss: 0.0906 - op_main_accuracy: 0.9664 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9745 - val_loss: 1.2569 - val_op_main_loss: 0.2596 - val_op_conv_loss: 0.4385 - val_avg_loss: 0.2828 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8971\n",
      "Epoch 371/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5716 - op_main_loss: 0.1279 - op_conv_loss: 0.0736 - avg_loss: 0.0942 - op_main_accuracy: 0.9587 - op_conv_accuracy: 0.9723 - avg_accuracy: 0.9723\n",
      "Epoch 00371: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5718 - op_main_loss: 0.1283 - op_conv_loss: 0.0733 - avg_loss: 0.0942 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9724 - val_loss: 1.8374 - val_op_main_loss: 0.3799 - val_op_conv_loss: 0.7443 - val_avg_loss: 0.4371 - val_op_main_accuracy: 0.8442 - val_op_conv_accuracy: 0.8395 - val_avg_accuracy: 0.8395\n",
      "Epoch 372/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5850 - op_main_loss: 0.1325 - op_conv_loss: 0.0783 - avg_loss: 0.0984 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9726\n",
      "Epoch 00372: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5850 - op_main_loss: 0.1325 - op_conv_loss: 0.0783 - avg_loss: 0.0984 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9726 - val_loss: 1.1665 - val_op_main_loss: 0.2623 - val_op_conv_loss: 0.3570 - val_avg_loss: 0.2720 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8895\n",
      "Epoch 373/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5735 - op_main_loss: 0.1294 - op_conv_loss: 0.0738 - avg_loss: 0.0952 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9692\n",
      "Epoch 00373: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5739 - op_main_loss: 0.1295 - op_conv_loss: 0.0740 - avg_loss: 0.0954 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9695 - val_loss: 1.0625 - val_op_main_loss: 0.2275 - val_op_conv_loss: 0.3241 - val_avg_loss: 0.2357 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9027\n",
      "Epoch 374/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5582 - op_main_loss: 0.1251 - op_conv_loss: 0.0678 - avg_loss: 0.0902 - op_main_accuracy: 0.9614 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9733\n",
      "Epoch 00374: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5599 - op_main_loss: 0.1258 - op_conv_loss: 0.0683 - avg_loss: 0.0907 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9728 - val_loss: 1.0727 - val_op_main_loss: 0.2364 - val_op_conv_loss: 0.3201 - val_avg_loss: 0.2413 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9093\n",
      "Epoch 375/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5844 - op_main_loss: 0.1303 - op_conv_loss: 0.0803 - avg_loss: 0.0985 - op_main_accuracy: 0.9599 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9690\n",
      "Epoch 00375: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5839 - op_main_loss: 0.1303 - op_conv_loss: 0.0799 - avg_loss: 0.0984 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9693 - val_loss: 1.1084 - val_op_main_loss: 0.2343 - val_op_conv_loss: 0.3531 - val_avg_loss: 0.2464 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8990\n",
      "Epoch 376/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5917 - op_main_loss: 0.1349 - op_conv_loss: 0.0813 - avg_loss: 0.1001 - op_main_accuracy: 0.9578 - op_conv_accuracy: 0.9717 - avg_accuracy: 0.9702\n",
      "Epoch 00376: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5926 - op_main_loss: 0.1357 - op_conv_loss: 0.0810 - avg_loss: 0.1005 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9702 - val_loss: 1.1461 - val_op_main_loss: 0.2488 - val_op_conv_loss: 0.3633 - val_avg_loss: 0.2589 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9018\n",
      "Epoch 377/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5843 - op_main_loss: 0.1343 - op_conv_loss: 0.0766 - avg_loss: 0.0982 - op_main_accuracy: 0.9583 - op_conv_accuracy: 0.9736 - avg_accuracy: 0.9709\n",
      "Epoch 00377: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5848 - op_main_loss: 0.1343 - op_conv_loss: 0.0769 - avg_loss: 0.0984 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9705 - val_loss: 1.1948 - val_op_main_loss: 0.2514 - val_op_conv_loss: 0.4001 - val_avg_loss: 0.2683 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8961\n",
      "Epoch 378/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5737 - op_main_loss: 0.1274 - op_conv_loss: 0.0760 - avg_loss: 0.0952 - op_main_accuracy: 0.9632 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9728\n",
      "Epoch 00378: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5732 - op_main_loss: 0.1272 - op_conv_loss: 0.0759 - avg_loss: 0.0951 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9731 - val_loss: 1.3187 - val_op_main_loss: 0.2867 - val_op_conv_loss: 0.4497 - val_avg_loss: 0.3075 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8839\n",
      "Epoch 379/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5653 - op_main_loss: 0.1278 - op_conv_loss: 0.0698 - avg_loss: 0.0929 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9721\n",
      "Epoch 00379: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5653 - op_main_loss: 0.1278 - op_conv_loss: 0.0698 - avg_loss: 0.0929 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9721 - val_loss: 1.1713 - val_op_main_loss: 0.2511 - val_op_conv_loss: 0.3818 - val_avg_loss: 0.2637 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8980\n",
      "Epoch 380/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5655 - op_main_loss: 0.1297 - op_conv_loss: 0.0682 - avg_loss: 0.0930 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9767 - avg_accuracy: 0.9726\n",
      "Epoch 00380: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5640 - op_main_loss: 0.1290 - op_conv_loss: 0.0677 - avg_loss: 0.0925 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9731 - val_loss: 1.1302 - val_op_main_loss: 0.2546 - val_op_conv_loss: 0.3423 - val_avg_loss: 0.2590 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9056\n",
      "Epoch 381/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5744 - op_main_loss: 0.1300 - op_conv_loss: 0.0744 - avg_loss: 0.0955 - op_main_accuracy: 0.9543 - op_conv_accuracy: 0.9725 - avg_accuracy: 0.9702\n",
      "Epoch 00381: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5747 - op_main_loss: 0.1302 - op_conv_loss: 0.0743 - avg_loss: 0.0956 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9702 - val_loss: 1.1594 - val_op_main_loss: 0.2496 - val_op_conv_loss: 0.3717 - val_avg_loss: 0.2635 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8942\n",
      "Epoch 382/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5741 - op_main_loss: 0.1310 - op_conv_loss: 0.0728 - avg_loss: 0.0955 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9709\n",
      "Epoch 00382: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5742 - op_main_loss: 0.1312 - op_conv_loss: 0.0727 - avg_loss: 0.0956 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9709 - val_loss: 1.2120 - val_op_main_loss: 0.2815 - val_op_conv_loss: 0.3706 - val_avg_loss: 0.2854 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 383/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5794 - op_main_loss: 0.1312 - op_conv_loss: 0.0764 - avg_loss: 0.0971 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9706\n",
      "Epoch 00383: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5795 - op_main_loss: 0.1312 - op_conv_loss: 0.0764 - avg_loss: 0.0972 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9707 - val_loss: 1.2104 - val_op_main_loss: 0.2575 - val_op_conv_loss: 0.4052 - val_avg_loss: 0.2733 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 384/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5633 - op_main_loss: 0.1295 - op_conv_loss: 0.0675 - avg_loss: 0.0919 - op_main_accuracy: 0.9580 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9728\n",
      "Epoch 00384: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5651 - op_main_loss: 0.1299 - op_conv_loss: 0.0683 - avg_loss: 0.0925 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9726 - val_loss: 1.1135 - val_op_main_loss: 0.2340 - val_op_conv_loss: 0.3581 - val_avg_loss: 0.2471 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9027\n",
      "Epoch 385/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5684 - op_main_loss: 0.1309 - op_conv_loss: 0.0699 - avg_loss: 0.0933 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9743 - avg_accuracy: 0.9734\n",
      "Epoch 00385: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5676 - op_main_loss: 0.1308 - op_conv_loss: 0.0694 - avg_loss: 0.0931 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9733 - val_loss: 1.1450 - val_op_main_loss: 0.2514 - val_op_conv_loss: 0.3602 - val_avg_loss: 0.2590 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9046\n",
      "Epoch 386/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5605 - op_main_loss: 0.1254 - op_conv_loss: 0.0700 - avg_loss: 0.0912 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9743 - avg_accuracy: 0.9721\n",
      "Epoch 00386: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5593 - op_main_loss: 0.1248 - op_conv_loss: 0.0697 - avg_loss: 0.0909 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9724 - val_loss: 1.0832 - val_op_main_loss: 0.2355 - val_op_conv_loss: 0.3313 - val_avg_loss: 0.2428 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9046\n",
      "Epoch 387/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5548 - op_main_loss: 0.1251 - op_conv_loss: 0.0668 - avg_loss: 0.0892 - op_main_accuracy: 0.9614 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9732\n",
      "Epoch 00387: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5545 - op_main_loss: 0.1250 - op_conv_loss: 0.0667 - avg_loss: 0.0891 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9733 - val_loss: 1.6159 - val_op_main_loss: 0.3206 - val_op_conv_loss: 0.6527 - val_avg_loss: 0.3689 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8517 - val_avg_accuracy: 0.8612\n",
      "Epoch 388/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5633 - op_main_loss: 0.1261 - op_conv_loss: 0.0716 - avg_loss: 0.0924 - op_main_accuracy: 0.9618 - op_conv_accuracy: 0.9743 - avg_accuracy: 0.9738\n",
      "Epoch 00388: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5612 - op_main_loss: 0.1251 - op_conv_loss: 0.0711 - avg_loss: 0.0917 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9742 - val_loss: 1.1965 - val_op_main_loss: 0.2639 - val_op_conv_loss: 0.3875 - val_avg_loss: 0.2718 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8999\n",
      "Epoch 389/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.5710 - op_main_loss: 0.1267 - op_conv_loss: 0.0758 - avg_loss: 0.0951 - op_main_accuracy: 0.9607 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9704\n",
      "Epoch 00389: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5708 - op_main_loss: 0.1268 - op_conv_loss: 0.0757 - avg_loss: 0.0950 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9705 - val_loss: 1.1574 - val_op_main_loss: 0.2571 - val_op_conv_loss: 0.3618 - val_avg_loss: 0.2652 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9008\n",
      "Epoch 390/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5724 - op_main_loss: 0.1280 - op_conv_loss: 0.0761 - avg_loss: 0.0950 - op_main_accuracy: 0.9599 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9702\n",
      "Epoch 00390: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5738 - op_main_loss: 0.1285 - op_conv_loss: 0.0765 - avg_loss: 0.0955 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9700 - val_loss: 1.1711 - val_op_main_loss: 0.2557 - val_op_conv_loss: 0.3747 - val_avg_loss: 0.2678 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8999\n",
      "Epoch 391/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5655 - op_main_loss: 0.1282 - op_conv_loss: 0.0712 - avg_loss: 0.0932 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9736 - avg_accuracy: 0.9734\n",
      "Epoch 00391: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5643 - op_main_loss: 0.1274 - op_conv_loss: 0.0711 - avg_loss: 0.0929 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9735 - val_loss: 1.1437 - val_op_main_loss: 0.2564 - val_op_conv_loss: 0.3531 - val_avg_loss: 0.2617 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9112\n",
      "Epoch 392/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5568 - op_main_loss: 0.1239 - op_conv_loss: 0.0691 - avg_loss: 0.0911 - op_main_accuracy: 0.9654 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9754\n",
      "Epoch 00392: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5562 - op_main_loss: 0.1237 - op_conv_loss: 0.0689 - avg_loss: 0.0909 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9754 - val_loss: 1.0930 - val_op_main_loss: 0.2334 - val_op_conv_loss: 0.3428 - val_avg_loss: 0.2440 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 393/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5339 - op_main_loss: 0.1176 - op_conv_loss: 0.0603 - avg_loss: 0.0832 - op_main_accuracy: 0.9647 - op_conv_accuracy: 0.9763 - avg_accuracy: 0.9768\n",
      "Epoch 00393: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5352 - op_main_loss: 0.1179 - op_conv_loss: 0.0609 - avg_loss: 0.0836 - op_main_accuracy: 0.9646 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9764 - val_loss: 1.0690 - val_op_main_loss: 0.2269 - val_op_conv_loss: 0.3339 - val_avg_loss: 0.2357 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8990\n",
      "Epoch 394/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5505 - op_main_loss: 0.1259 - op_conv_loss: 0.0630 - avg_loss: 0.0893 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9750\n",
      "Epoch 00394: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5505 - op_main_loss: 0.1259 - op_conv_loss: 0.0630 - avg_loss: 0.0893 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9750 - val_loss: 1.0873 - val_op_main_loss: 0.2409 - val_op_conv_loss: 0.3267 - val_avg_loss: 0.2473 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9056\n",
      "Epoch 395/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5980 - op_main_loss: 0.1389 - op_conv_loss: 0.0835 - avg_loss: 0.1026 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9692 - avg_accuracy: 0.9668\n",
      "Epoch 00395: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5966 - op_main_loss: 0.1383 - op_conv_loss: 0.0830 - avg_loss: 0.1023 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9667 - val_loss: 1.0817 - val_op_main_loss: 0.2319 - val_op_conv_loss: 0.3352 - val_avg_loss: 0.2417 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9018\n",
      "Epoch 396/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5655 - op_main_loss: 0.1275 - op_conv_loss: 0.0715 - avg_loss: 0.0929 - op_main_accuracy: 0.9618 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9714\n",
      "Epoch 00396: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5636 - op_main_loss: 0.1268 - op_conv_loss: 0.0709 - avg_loss: 0.0923 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9719 - val_loss: 1.0827 - val_op_main_loss: 0.2364 - val_op_conv_loss: 0.3310 - val_avg_loss: 0.2416 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9027\n",
      "Epoch 397/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5561 - op_main_loss: 0.1212 - op_conv_loss: 0.0709 - avg_loss: 0.0901 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9695\n",
      "Epoch 00397: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5561 - op_main_loss: 0.1212 - op_conv_loss: 0.0709 - avg_loss: 0.0901 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9695 - val_loss: 1.0615 - val_op_main_loss: 0.2309 - val_op_conv_loss: 0.3207 - val_avg_loss: 0.2359 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9122\n",
      "Epoch 398/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5475 - op_main_loss: 0.1202 - op_conv_loss: 0.0662 - avg_loss: 0.0875 - op_main_accuracy: 0.9656 - op_conv_accuracy: 0.9746 - avg_accuracy: 0.9741\n",
      "Epoch 00398: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5455 - op_main_loss: 0.1197 - op_conv_loss: 0.0653 - avg_loss: 0.0869 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9745 - val_loss: 1.0387 - val_op_main_loss: 0.2238 - val_op_conv_loss: 0.3124 - val_avg_loss: 0.2292 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9131\n",
      "Epoch 399/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5700 - op_main_loss: 0.1286 - op_conv_loss: 0.0742 - avg_loss: 0.0943 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9733\n",
      "Epoch 00399: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5693 - op_main_loss: 0.1281 - op_conv_loss: 0.0743 - avg_loss: 0.0940 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9733 - val_loss: 1.1126 - val_op_main_loss: 0.2483 - val_op_conv_loss: 0.3385 - val_avg_loss: 0.2528 - val_op_main_accuracy: 0.9008 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9037\n",
      "Epoch 400/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6039 - op_main_loss: 0.1362 - op_conv_loss: 0.0902 - avg_loss: 0.1048 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9636\n",
      "Epoch 00400: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6039 - op_main_loss: 0.1362 - op_conv_loss: 0.0902 - avg_loss: 0.1048 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9636 - val_loss: 1.0988 - val_op_main_loss: 0.2401 - val_op_conv_loss: 0.3365 - val_avg_loss: 0.2499 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8999\n",
      "Epoch 401/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.5446 - op_main_loss: 0.1188 - op_conv_loss: 0.0663 - avg_loss: 0.0869 - op_main_accuracy: 0.9623 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9743\n",
      "Epoch 00401: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5458 - op_main_loss: 0.1190 - op_conv_loss: 0.0670 - avg_loss: 0.0873 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9745 - val_loss: 1.0723 - val_op_main_loss: 0.2322 - val_op_conv_loss: 0.3285 - val_avg_loss: 0.2400 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9131 - val_avg_accuracy: 0.9131\n",
      "Epoch 402/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5617 - op_main_loss: 0.1230 - op_conv_loss: 0.0741 - avg_loss: 0.0928 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9731\n",
      "Epoch 00402: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5620 - op_main_loss: 0.1234 - op_conv_loss: 0.0739 - avg_loss: 0.0929 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9731 - val_loss: 1.0546 - val_op_main_loss: 0.2352 - val_op_conv_loss: 0.3087 - val_avg_loss: 0.2391 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9075\n",
      "Epoch 403/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5688 - op_main_loss: 0.1290 - op_conv_loss: 0.0743 - avg_loss: 0.0944 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9721\n",
      "Epoch 00403: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5676 - op_main_loss: 0.1288 - op_conv_loss: 0.0736 - avg_loss: 0.0941 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9724 - val_loss: 1.2372 - val_op_main_loss: 0.2618 - val_op_conv_loss: 0.4265 - val_avg_loss: 0.2776 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9037\n",
      "Epoch 404/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5672 - op_main_loss: 0.1291 - op_conv_loss: 0.0725 - avg_loss: 0.0940 - op_main_accuracy: 0.9583 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9730\n",
      "Epoch 00404: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5654 - op_main_loss: 0.1284 - op_conv_loss: 0.0720 - avg_loss: 0.0934 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9733 - val_loss: 1.0972 - val_op_main_loss: 0.2337 - val_op_conv_loss: 0.3477 - val_avg_loss: 0.2442 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9018\n",
      "Epoch 405/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5461 - op_main_loss: 0.1223 - op_conv_loss: 0.0638 - avg_loss: 0.0880 - op_main_accuracy: 0.9583 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9740\n",
      "Epoch 00405: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5473 - op_main_loss: 0.1227 - op_conv_loss: 0.0642 - avg_loss: 0.0884 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9735 - val_loss: 1.0628 - val_op_main_loss: 0.2319 - val_op_conv_loss: 0.3201 - val_avg_loss: 0.2385 - val_op_main_accuracy: 0.9112 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9122\n",
      "Epoch 406/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5533 - op_main_loss: 0.1220 - op_conv_loss: 0.0697 - avg_loss: 0.0897 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9746 - avg_accuracy: 0.9731\n",
      "Epoch 00406: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5539 - op_main_loss: 0.1222 - op_conv_loss: 0.0700 - avg_loss: 0.0898 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9731 - val_loss: 1.0908 - val_op_main_loss: 0.2408 - val_op_conv_loss: 0.3298 - val_avg_loss: 0.2484 - val_op_main_accuracy: 0.9160 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9112\n",
      "Epoch 407/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5551 - op_main_loss: 0.1241 - op_conv_loss: 0.0694 - avg_loss: 0.0901 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9729\n",
      "Epoch 00407: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5573 - op_main_loss: 0.1250 - op_conv_loss: 0.0699 - avg_loss: 0.0908 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9731 - val_loss: 1.2119 - val_op_main_loss: 0.2657 - val_op_conv_loss: 0.3941 - val_avg_loss: 0.2807 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8999\n",
      "Epoch 408/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5850 - op_main_loss: 0.1350 - op_conv_loss: 0.0788 - avg_loss: 0.0995 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9702\n",
      "Epoch 00408: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5847 - op_main_loss: 0.1351 - op_conv_loss: 0.0785 - avg_loss: 0.0994 - op_main_accuracy: 0.9506 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9702 - val_loss: 1.0548 - val_op_main_loss: 0.2306 - val_op_conv_loss: 0.3175 - val_avg_loss: 0.2356 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9084\n",
      "Epoch 409/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5546 - op_main_loss: 0.1250 - op_conv_loss: 0.0681 - avg_loss: 0.0905 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9741 - avg_accuracy: 0.9738\n",
      "Epoch 00409: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5549 - op_main_loss: 0.1251 - op_conv_loss: 0.0681 - avg_loss: 0.0907 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9740 - val_loss: 1.0709 - val_op_main_loss: 0.2335 - val_op_conv_loss: 0.3254 - val_avg_loss: 0.2408 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.9056\n",
      "Epoch 410/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5789 - op_main_loss: 0.1337 - op_conv_loss: 0.0758 - avg_loss: 0.0980 - op_main_accuracy: 0.9541 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9668\n",
      "Epoch 00410: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5811 - op_main_loss: 0.1340 - op_conv_loss: 0.0770 - avg_loss: 0.0986 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9667 - val_loss: 1.1169 - val_op_main_loss: 0.2383 - val_op_conv_loss: 0.3565 - val_avg_loss: 0.2502 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 411/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5953 - op_main_loss: 0.1393 - op_conv_loss: 0.0820 - avg_loss: 0.1015 - op_main_accuracy: 0.9477 - op_conv_accuracy: 0.9692 - avg_accuracy: 0.9685\n",
      "Epoch 00411: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5964 - op_main_loss: 0.1400 - op_conv_loss: 0.0820 - avg_loss: 0.1019 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9681 - val_loss: 1.0979 - val_op_main_loss: 0.2366 - val_op_conv_loss: 0.3408 - val_avg_loss: 0.2477 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9056\n",
      "Epoch 412/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5345 - op_main_loss: 0.1147 - op_conv_loss: 0.0636 - avg_loss: 0.0834 - op_main_accuracy: 0.9675 - op_conv_accuracy: 0.9765 - avg_accuracy: 0.9743\n",
      "Epoch 00412: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5337 - op_main_loss: 0.1140 - op_conv_loss: 0.0638 - avg_loss: 0.0831 - op_main_accuracy: 0.9679 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9742 - val_loss: 1.1341 - val_op_main_loss: 0.2453 - val_op_conv_loss: 0.3624 - val_avg_loss: 0.2539 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9056\n",
      "Epoch 413/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5492 - op_main_loss: 0.1198 - op_conv_loss: 0.0685 - avg_loss: 0.0890 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9712\n",
      "Epoch 00413: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5479 - op_main_loss: 0.1196 - op_conv_loss: 0.0679 - avg_loss: 0.0886 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9714 - val_loss: 1.3245 - val_op_main_loss: 0.2771 - val_op_conv_loss: 0.4705 - val_avg_loss: 0.3058 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8829\n",
      "Epoch 414/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5387 - op_main_loss: 0.1165 - op_conv_loss: 0.0658 - avg_loss: 0.0848 - op_main_accuracy: 0.9674 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9752\n",
      "Epoch 00414: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5387 - op_main_loss: 0.1165 - op_conv_loss: 0.0658 - avg_loss: 0.0848 - op_main_accuracy: 0.9674 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9752 - val_loss: 1.4873 - val_op_main_loss: 0.3243 - val_op_conv_loss: 0.5411 - val_avg_loss: 0.3503 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8791\n",
      "Epoch 415/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5691 - op_main_loss: 0.1349 - op_conv_loss: 0.0688 - avg_loss: 0.0944 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9756 - avg_accuracy: 0.9730\n",
      "Epoch 00415: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5690 - op_main_loss: 0.1349 - op_conv_loss: 0.0688 - avg_loss: 0.0944 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9731 - val_loss: 1.1630 - val_op_main_loss: 0.2293 - val_op_conv_loss: 0.4110 - val_avg_loss: 0.2514 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8971\n",
      "Epoch 416/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5311 - op_main_loss: 0.1160 - op_conv_loss: 0.0607 - avg_loss: 0.0829 - op_main_accuracy: 0.9680 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9764\n",
      "Epoch 00416: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5310 - op_main_loss: 0.1162 - op_conv_loss: 0.0604 - avg_loss: 0.0829 - op_main_accuracy: 0.9676 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9766 - val_loss: 1.1166 - val_op_main_loss: 0.2408 - val_op_conv_loss: 0.3526 - val_avg_loss: 0.2518 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8999\n",
      "Epoch 417/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5372 - op_main_loss: 0.1183 - op_conv_loss: 0.0632 - avg_loss: 0.0848 - op_main_accuracy: 0.9602 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9766\n",
      "Epoch 00417: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5366 - op_main_loss: 0.1182 - op_conv_loss: 0.0629 - avg_loss: 0.0846 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9768 - val_loss: 1.1335 - val_op_main_loss: 0.2441 - val_op_conv_loss: 0.3642 - val_avg_loss: 0.2544 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9065\n",
      "Epoch 418/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5321 - op_main_loss: 0.1144 - op_conv_loss: 0.0634 - avg_loss: 0.0833 - op_main_accuracy: 0.9651 - op_conv_accuracy: 0.9772 - avg_accuracy: 0.9753\n",
      "Epoch 00418: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5331 - op_main_loss: 0.1151 - op_conv_loss: 0.0634 - avg_loss: 0.0837 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9747 - val_loss: 1.1458 - val_op_main_loss: 0.2541 - val_op_conv_loss: 0.3605 - val_avg_loss: 0.2611 - val_op_main_accuracy: 0.9131 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9084\n",
      "Epoch 419/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5377 - op_main_loss: 0.1195 - op_conv_loss: 0.0631 - avg_loss: 0.0854 - op_main_accuracy: 0.9638 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9744\n",
      "Epoch 00419: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5373 - op_main_loss: 0.1194 - op_conv_loss: 0.0630 - avg_loss: 0.0853 - op_main_accuracy: 0.9638 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9745 - val_loss: 1.0702 - val_op_main_loss: 0.2381 - val_op_conv_loss: 0.3227 - val_avg_loss: 0.2401 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9046\n",
      "Epoch 420/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5530 - op_main_loss: 0.1253 - op_conv_loss: 0.0679 - avg_loss: 0.0904 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9753 - avg_accuracy: 0.9750\n",
      "Epoch 00420: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5507 - op_main_loss: 0.1245 - op_conv_loss: 0.0671 - avg_loss: 0.0896 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9754 - val_loss: 1.0607 - val_op_main_loss: 0.2271 - val_op_conv_loss: 0.3277 - val_avg_loss: 0.2365 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8999\n",
      "Epoch 421/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5444 - op_main_loss: 0.1199 - op_conv_loss: 0.0682 - avg_loss: 0.0871 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9758 - avg_accuracy: 0.9741\n",
      "Epoch 00421: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5478 - op_main_loss: 0.1215 - op_conv_loss: 0.0690 - avg_loss: 0.0882 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9733 - val_loss: 1.0605 - val_op_main_loss: 0.2290 - val_op_conv_loss: 0.3260 - val_avg_loss: 0.2364 - val_op_main_accuracy: 0.9122 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9103\n",
      "Epoch 422/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5528 - op_main_loss: 0.1225 - op_conv_loss: 0.0705 - avg_loss: 0.0903 - op_main_accuracy: 0.9595 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9699\n",
      "Epoch 00422: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5525 - op_main_loss: 0.1224 - op_conv_loss: 0.0704 - avg_loss: 0.0902 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9700 - val_loss: 1.1144 - val_op_main_loss: 0.2427 - val_op_conv_loss: 0.3510 - val_avg_loss: 0.2516 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9065\n",
      "Epoch 423/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5397 - op_main_loss: 0.1174 - op_conv_loss: 0.0672 - avg_loss: 0.0862 - op_main_accuracy: 0.9659 - op_conv_accuracy: 0.9763 - avg_accuracy: 0.9766\n",
      "Epoch 00423: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5411 - op_main_loss: 0.1178 - op_conv_loss: 0.0679 - avg_loss: 0.0867 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9764 - val_loss: 1.0635 - val_op_main_loss: 0.2280 - val_op_conv_loss: 0.3315 - val_avg_loss: 0.2358 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9037\n",
      "Epoch 424/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5693 - op_main_loss: 0.1296 - op_conv_loss: 0.0755 - avg_loss: 0.0956 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9704\n",
      "Epoch 00424: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5687 - op_main_loss: 0.1294 - op_conv_loss: 0.0753 - avg_loss: 0.0955 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9707 - val_loss: 1.1466 - val_op_main_loss: 0.2436 - val_op_conv_loss: 0.3770 - val_avg_loss: 0.2572 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9018\n",
      "Epoch 425/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5457 - op_main_loss: 0.1189 - op_conv_loss: 0.0699 - avg_loss: 0.0881 - op_main_accuracy: 0.9653 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9759\n",
      "Epoch 00425: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5457 - op_main_loss: 0.1189 - op_conv_loss: 0.0699 - avg_loss: 0.0881 - op_main_accuracy: 0.9653 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9759 - val_loss: 1.0742 - val_op_main_loss: 0.2350 - val_op_conv_loss: 0.3290 - val_avg_loss: 0.2414 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9093\n",
      "Epoch 426/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5446 - op_main_loss: 0.1210 - op_conv_loss: 0.0676 - avg_loss: 0.0877 - op_main_accuracy: 0.9616 - op_conv_accuracy: 0.9723 - avg_accuracy: 0.9721\n",
      "Epoch 00426: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5446 - op_main_loss: 0.1209 - op_conv_loss: 0.0676 - avg_loss: 0.0877 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9721 - val_loss: 1.0656 - val_op_main_loss: 0.2347 - val_op_conv_loss: 0.3216 - val_avg_loss: 0.2410 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9103\n",
      "Epoch 427/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5453 - op_main_loss: 0.1219 - op_conv_loss: 0.0669 - avg_loss: 0.0881 - op_main_accuracy: 0.9618 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9754\n",
      "Epoch 00427: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5477 - op_main_loss: 0.1227 - op_conv_loss: 0.0677 - avg_loss: 0.0888 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9752 - val_loss: 1.7074 - val_op_main_loss: 0.3683 - val_op_conv_loss: 0.6629 - val_avg_loss: 0.4075 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8574 - val_avg_accuracy: 0.8631\n",
      "Epoch 428/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5969 - op_main_loss: 0.1388 - op_conv_loss: 0.0859 - avg_loss: 0.1037 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9655\n",
      "Epoch 00428: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5969 - op_main_loss: 0.1388 - op_conv_loss: 0.0859 - avg_loss: 0.1037 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9655 - val_loss: 1.1107 - val_op_main_loss: 0.2504 - val_op_conv_loss: 0.3404 - val_avg_loss: 0.2513 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9112 - val_avg_accuracy: 0.9084\n",
      "Epoch 429/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5404 - op_main_loss: 0.1187 - op_conv_loss: 0.0665 - avg_loss: 0.0869 - op_main_accuracy: 0.9645 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9790\n",
      "Epoch 00429: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5411 - op_main_loss: 0.1191 - op_conv_loss: 0.0666 - avg_loss: 0.0871 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9787 - val_loss: 1.1122 - val_op_main_loss: 0.2372 - val_op_conv_loss: 0.3585 - val_avg_loss: 0.2480 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9037\n",
      "Epoch 430/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5552 - op_main_loss: 0.1246 - op_conv_loss: 0.0706 - avg_loss: 0.0912 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9726\n",
      "Epoch 00430: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5552 - op_main_loss: 0.1246 - op_conv_loss: 0.0706 - avg_loss: 0.0912 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9726 - val_loss: 1.0410 - val_op_main_loss: 0.2271 - val_op_conv_loss: 0.3109 - val_avg_loss: 0.2347 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9056\n",
      "Epoch 431/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5349 - op_main_loss: 0.1176 - op_conv_loss: 0.0636 - avg_loss: 0.0856 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9742\n",
      "Epoch 00431: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5349 - op_main_loss: 0.1177 - op_conv_loss: 0.0635 - avg_loss: 0.0856 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9742 - val_loss: 1.0723 - val_op_main_loss: 0.2300 - val_op_conv_loss: 0.3345 - val_avg_loss: 0.2399 - val_op_main_accuracy: 0.9131 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9065\n",
      "Epoch 432/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5493 - op_main_loss: 0.1232 - op_conv_loss: 0.0688 - avg_loss: 0.0892 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9726\n",
      "Epoch 00432: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5493 - op_main_loss: 0.1232 - op_conv_loss: 0.0688 - avg_loss: 0.0892 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9726 - val_loss: 1.0552 - val_op_main_loss: 0.2339 - val_op_conv_loss: 0.3151 - val_avg_loss: 0.2381 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9065\n",
      "Epoch 433/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5610 - op_main_loss: 0.1246 - op_conv_loss: 0.0751 - avg_loss: 0.0933 - op_main_accuracy: 0.9614 - op_conv_accuracy: 0.9730 - avg_accuracy: 0.9742\n",
      "Epoch 00433: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5616 - op_main_loss: 0.1246 - op_conv_loss: 0.0755 - avg_loss: 0.0935 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9740 - val_loss: 1.1810 - val_op_main_loss: 0.2667 - val_op_conv_loss: 0.3702 - val_avg_loss: 0.2758 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8933\n",
      "Epoch 434/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5617 - op_main_loss: 0.1256 - op_conv_loss: 0.0740 - avg_loss: 0.0933 - op_main_accuracy: 0.9593 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9736\n",
      "Epoch 00434: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5620 - op_main_loss: 0.1257 - op_conv_loss: 0.0741 - avg_loss: 0.0934 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9733 - val_loss: 1.1061 - val_op_main_loss: 0.2448 - val_op_conv_loss: 0.3419 - val_avg_loss: 0.2508 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.9008\n",
      "Epoch 435/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5443 - op_main_loss: 0.1213 - op_conv_loss: 0.0667 - avg_loss: 0.0882 - op_main_accuracy: 0.9607 - op_conv_accuracy: 0.9744 - avg_accuracy: 0.9744\n",
      "Epoch 00435: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5442 - op_main_loss: 0.1213 - op_conv_loss: 0.0666 - avg_loss: 0.0882 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9745 - val_loss: 1.0731 - val_op_main_loss: 0.2388 - val_op_conv_loss: 0.3248 - val_avg_loss: 0.2415 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9093\n",
      "Epoch 436/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5524 - op_main_loss: 0.1229 - op_conv_loss: 0.0710 - avg_loss: 0.0905 - op_main_accuracy: 0.9659 - op_conv_accuracy: 0.9730 - avg_accuracy: 0.9750\n",
      "Epoch 00436: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5511 - op_main_loss: 0.1224 - op_conv_loss: 0.0706 - avg_loss: 0.0901 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9752 - val_loss: 1.1200 - val_op_main_loss: 0.2471 - val_op_conv_loss: 0.3482 - val_avg_loss: 0.2563 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9093\n",
      "Epoch 437/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.5295 - op_main_loss: 0.1173 - op_conv_loss: 0.0605 - avg_loss: 0.0835 - op_main_accuracy: 0.9621 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9775\n",
      "Epoch 00437: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5294 - op_main_loss: 0.1172 - op_conv_loss: 0.0605 - avg_loss: 0.0834 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9776 - val_loss: 1.1171 - val_op_main_loss: 0.2459 - val_op_conv_loss: 0.3473 - val_avg_loss: 0.2564 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.9008\n",
      "Epoch 438/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5484 - op_main_loss: 0.1215 - op_conv_loss: 0.0696 - avg_loss: 0.0896 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9731\n",
      "Epoch 00438: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5484 - op_main_loss: 0.1215 - op_conv_loss: 0.0696 - avg_loss: 0.0896 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9731 - val_loss: 1.5572 - val_op_main_loss: 0.3436 - val_op_conv_loss: 0.5747 - val_avg_loss: 0.3705 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8820\n",
      "Epoch 439/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5740 - op_main_loss: 0.1285 - op_conv_loss: 0.0799 - avg_loss: 0.0976 - op_main_accuracy: 0.9566 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9688\n",
      "Epoch 00439: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5714 - op_main_loss: 0.1279 - op_conv_loss: 0.0787 - avg_loss: 0.0968 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9695 - val_loss: 1.4422 - val_op_main_loss: 0.3036 - val_op_conv_loss: 0.5375 - val_avg_loss: 0.3330 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8886\n",
      "Epoch 440/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5252 - op_main_loss: 0.1177 - op_conv_loss: 0.0578 - avg_loss: 0.0818 - op_main_accuracy: 0.9646 - op_conv_accuracy: 0.9787 - avg_accuracy: 0.9794\n",
      "Epoch 00440: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5252 - op_main_loss: 0.1177 - op_conv_loss: 0.0578 - avg_loss: 0.0818 - op_main_accuracy: 0.9646 - op_conv_accuracy: 0.9787 - avg_accuracy: 0.9794 - val_loss: 1.1534 - val_op_main_loss: 0.2460 - val_op_conv_loss: 0.3793 - val_avg_loss: 0.2609 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 441/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5371 - op_main_loss: 0.1194 - op_conv_loss: 0.0648 - avg_loss: 0.0858 - op_main_accuracy: 0.9649 - op_conv_accuracy: 0.9758 - avg_accuracy: 0.9782\n",
      "Epoch 00441: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5359 - op_main_loss: 0.1193 - op_conv_loss: 0.0640 - avg_loss: 0.0854 - op_main_accuracy: 0.9650 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9783 - val_loss: 1.4305 - val_op_main_loss: 0.3203 - val_op_conv_loss: 0.5028 - val_avg_loss: 0.3399 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8810\n",
      "Epoch 442/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5342 - op_main_loss: 0.1190 - op_conv_loss: 0.0628 - avg_loss: 0.0853 - op_main_accuracy: 0.9644 - op_conv_accuracy: 0.9772 - avg_accuracy: 0.9755\n",
      "Epoch 00442: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5355 - op_main_loss: 0.1192 - op_conv_loss: 0.0634 - avg_loss: 0.0858 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9768 - avg_accuracy: 0.9752 - val_loss: 1.2337 - val_op_main_loss: 0.2730 - val_op_conv_loss: 0.4087 - val_avg_loss: 0.2850 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8971\n",
      "Epoch 443/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5333 - op_main_loss: 0.1176 - op_conv_loss: 0.0633 - avg_loss: 0.0854 - op_main_accuracy: 0.9668 - op_conv_accuracy: 0.9777 - avg_accuracy: 0.9770\n",
      "Epoch 00443: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5326 - op_main_loss: 0.1172 - op_conv_loss: 0.0631 - avg_loss: 0.0852 - op_main_accuracy: 0.9669 - op_conv_accuracy: 0.9776 - avg_accuracy: 0.9768 - val_loss: 1.1032 - val_op_main_loss: 0.2451 - val_op_conv_loss: 0.3388 - val_avg_loss: 0.2526 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9046\n",
      "Epoch 444/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5484 - op_main_loss: 0.1196 - op_conv_loss: 0.0727 - avg_loss: 0.0891 - op_main_accuracy: 0.9638 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9759\n",
      "Epoch 00444: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5484 - op_main_loss: 0.1196 - op_conv_loss: 0.0727 - avg_loss: 0.0891 - op_main_accuracy: 0.9638 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9759 - val_loss: 1.3087 - val_op_main_loss: 0.2906 - val_op_conv_loss: 0.4440 - val_avg_loss: 0.3068 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8886\n",
      "Epoch 445/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5173 - op_main_loss: 0.1112 - op_conv_loss: 0.0596 - avg_loss: 0.0800 - op_main_accuracy: 0.9702 - op_conv_accuracy: 0.9784 - avg_accuracy: 0.9808\n",
      "Epoch 00445: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5186 - op_main_loss: 0.1119 - op_conv_loss: 0.0597 - avg_loss: 0.0804 - op_main_accuracy: 0.9698 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9804 - val_loss: 1.0874 - val_op_main_loss: 0.2336 - val_op_conv_loss: 0.3456 - val_avg_loss: 0.2423 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9084 - val_avg_accuracy: 0.9103\n",
      "Epoch 446/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5287 - op_main_loss: 0.1157 - op_conv_loss: 0.0634 - avg_loss: 0.0840 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9757\n",
      "Epoch 00446: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5287 - op_main_loss: 0.1157 - op_conv_loss: 0.0634 - avg_loss: 0.0840 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9757 - val_loss: 1.0674 - val_op_main_loss: 0.2327 - val_op_conv_loss: 0.3305 - val_avg_loss: 0.2387 - val_op_main_accuracy: 0.9141 - val_op_conv_accuracy: 0.9122 - val_avg_accuracy: 0.9093\n",
      "Epoch 447/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5347 - op_main_loss: 0.1198 - op_conv_loss: 0.0635 - avg_loss: 0.0857 - op_main_accuracy: 0.9621 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9728\n",
      "Epoch 00447: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5339 - op_main_loss: 0.1195 - op_conv_loss: 0.0632 - avg_loss: 0.0855 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9731 - val_loss: 1.3549 - val_op_main_loss: 0.3068 - val_op_conv_loss: 0.4617 - val_avg_loss: 0.3203 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8905\n",
      "Epoch 448/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5195 - op_main_loss: 0.1134 - op_conv_loss: 0.0593 - avg_loss: 0.0809 - op_main_accuracy: 0.9651 - op_conv_accuracy: 0.9781 - avg_accuracy: 0.9762\n",
      "Epoch 00448: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5184 - op_main_loss: 0.1131 - op_conv_loss: 0.0589 - avg_loss: 0.0806 - op_main_accuracy: 0.9653 - op_conv_accuracy: 0.9783 - avg_accuracy: 0.9764 - val_loss: 1.1503 - val_op_main_loss: 0.2569 - val_op_conv_loss: 0.3653 - val_avg_loss: 0.2626 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9056\n",
      "Epoch 449/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.5419 - op_main_loss: 0.1225 - op_conv_loss: 0.0661 - avg_loss: 0.0876 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9774\n",
      "Epoch 00449: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5425 - op_main_loss: 0.1228 - op_conv_loss: 0.0662 - avg_loss: 0.0879 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9773 - val_loss: 1.1106 - val_op_main_loss: 0.2460 - val_op_conv_loss: 0.3451 - val_avg_loss: 0.2532 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9075\n",
      "Epoch 450/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5566 - op_main_loss: 0.1230 - op_conv_loss: 0.0745 - avg_loss: 0.0922 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9714\n",
      "Epoch 00450: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5589 - op_main_loss: 0.1240 - op_conv_loss: 0.0750 - avg_loss: 0.0930 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9712 - val_loss: 1.1300 - val_op_main_loss: 0.2442 - val_op_conv_loss: 0.3664 - val_avg_loss: 0.2534 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.9046 - val_avg_accuracy: 0.9056\n",
      "Epoch 451/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5274 - op_main_loss: 0.1165 - op_conv_loss: 0.0620 - avg_loss: 0.0830 - op_main_accuracy: 0.9678 - op_conv_accuracy: 0.9775 - avg_accuracy: 0.9775\n",
      "Epoch 00451: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5283 - op_main_loss: 0.1166 - op_conv_loss: 0.0625 - avg_loss: 0.0834 - op_main_accuracy: 0.9674 - op_conv_accuracy: 0.9768 - avg_accuracy: 0.9766 - val_loss: 1.0808 - val_op_main_loss: 0.2352 - val_op_conv_loss: 0.3366 - val_avg_loss: 0.2429 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9037\n",
      "Epoch 452/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5324 - op_main_loss: 0.1157 - op_conv_loss: 0.0662 - avg_loss: 0.0846 - op_main_accuracy: 0.9661 - op_conv_accuracy: 0.9784 - avg_accuracy: 0.9776\n",
      "Epoch 00452: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5314 - op_main_loss: 0.1152 - op_conv_loss: 0.0661 - avg_loss: 0.0843 - op_main_accuracy: 0.9660 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9778 - val_loss: 1.2308 - val_op_main_loss: 0.2735 - val_op_conv_loss: 0.4072 - val_avg_loss: 0.2846 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9008\n",
      "Epoch 453/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5308 - op_main_loss: 0.1150 - op_conv_loss: 0.0660 - avg_loss: 0.0844 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9750\n",
      "Epoch 00453: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5308 - op_main_loss: 0.1150 - op_conv_loss: 0.0660 - avg_loss: 0.0844 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9750 - val_loss: 1.0432 - val_op_main_loss: 0.2276 - val_op_conv_loss: 0.3143 - val_avg_loss: 0.2356 - val_op_main_accuracy: 0.9122 - val_op_conv_accuracy: 0.9075 - val_avg_accuracy: 0.9084\n",
      "Epoch 454/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5439 - op_main_loss: 0.1196 - op_conv_loss: 0.0703 - avg_loss: 0.0887 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9740\n",
      "Epoch 00454: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5436 - op_main_loss: 0.1195 - op_conv_loss: 0.0702 - avg_loss: 0.0886 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9740 - val_loss: 1.0708 - val_op_main_loss: 0.2356 - val_op_conv_loss: 0.3295 - val_avg_loss: 0.2409 - val_op_main_accuracy: 0.9103 - val_op_conv_accuracy: 0.9093 - val_avg_accuracy: 0.9122\n",
      "Epoch 455/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5435 - op_main_loss: 0.1203 - op_conv_loss: 0.0687 - avg_loss: 0.0891 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9729 - avg_accuracy: 0.9714\n",
      "Epoch 00455: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5396 - op_main_loss: 0.1189 - op_conv_loss: 0.0674 - avg_loss: 0.0879 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9721 - val_loss: 1.1753 - val_op_main_loss: 0.2497 - val_op_conv_loss: 0.3940 - val_avg_loss: 0.2662 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8971\n",
      "Epoch 456/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5367 - op_main_loss: 0.1189 - op_conv_loss: 0.0664 - avg_loss: 0.0862 - op_main_accuracy: 0.9646 - op_conv_accuracy: 0.9767 - avg_accuracy: 0.9782\n",
      "Epoch 00456: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5358 - op_main_loss: 0.1190 - op_conv_loss: 0.0656 - avg_loss: 0.0859 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9773 - avg_accuracy: 0.9785 - val_loss: 1.1799 - val_op_main_loss: 0.2501 - val_op_conv_loss: 0.3974 - val_avg_loss: 0.2671 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8999\n",
      "Epoch 457/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5518 - op_main_loss: 0.1231 - op_conv_loss: 0.0730 - avg_loss: 0.0905 - op_main_accuracy: 0.9587 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9707\n",
      "Epoch 00457: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5517 - op_main_loss: 0.1230 - op_conv_loss: 0.0731 - avg_loss: 0.0905 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9707 - val_loss: 1.0885 - val_op_main_loss: 0.2461 - val_op_conv_loss: 0.3290 - val_avg_loss: 0.2482 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9084\n",
      "Epoch 458/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5407 - op_main_loss: 0.1204 - op_conv_loss: 0.0678 - avg_loss: 0.0874 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9784 - avg_accuracy: 0.9750\n",
      "Epoch 00458: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5402 - op_main_loss: 0.1205 - op_conv_loss: 0.0673 - avg_loss: 0.0872 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9752 - val_loss: 1.2475 - val_op_main_loss: 0.2698 - val_op_conv_loss: 0.4262 - val_avg_loss: 0.2861 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8971\n",
      "Epoch 459/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5499 - op_main_loss: 0.1205 - op_conv_loss: 0.0743 - avg_loss: 0.0894 - op_main_accuracy: 0.9623 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9707\n",
      "Epoch 00459: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5489 - op_main_loss: 0.1200 - op_conv_loss: 0.0741 - avg_loss: 0.0891 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9709 - val_loss: 1.4890 - val_op_main_loss: 0.3078 - val_op_conv_loss: 0.5667 - val_avg_loss: 0.3482 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8631 - val_avg_accuracy: 0.8659\n",
      "Epoch 460/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5425 - op_main_loss: 0.1210 - op_conv_loss: 0.0672 - avg_loss: 0.0877 - op_main_accuracy: 0.9626 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9747\n",
      "Epoch 00460: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5424 - op_main_loss: 0.1210 - op_conv_loss: 0.0671 - avg_loss: 0.0877 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9747 - val_loss: 1.0903 - val_op_main_loss: 0.2314 - val_op_conv_loss: 0.3492 - val_avg_loss: 0.2432 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9027\n",
      "Epoch 461/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.5480 - op_main_loss: 0.1206 - op_conv_loss: 0.0713 - avg_loss: 0.0902 - op_main_accuracy: 0.9647 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9712\n",
      "Epoch 00461: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5483 - op_main_loss: 0.1206 - op_conv_loss: 0.0715 - avg_loss: 0.0903 - op_main_accuracy: 0.9648 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9712 - val_loss: 1.1344 - val_op_main_loss: 0.2342 - val_op_conv_loss: 0.3819 - val_avg_loss: 0.2526 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8971\n",
      "Epoch 462/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5243 - op_main_loss: 0.1120 - op_conv_loss: 0.0645 - avg_loss: 0.0822 - op_main_accuracy: 0.9674 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9764\n",
      "Epoch 00462: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5243 - op_main_loss: 0.1120 - op_conv_loss: 0.0645 - avg_loss: 0.0822 - op_main_accuracy: 0.9674 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9764 - val_loss: 1.0849 - val_op_main_loss: 0.2380 - val_op_conv_loss: 0.3346 - val_avg_loss: 0.2471 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9037 - val_avg_accuracy: 0.9046\n",
      "Epoch 463/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5410 - op_main_loss: 0.1160 - op_conv_loss: 0.0724 - avg_loss: 0.0875 - op_main_accuracy: 0.9659 - op_conv_accuracy: 0.9723 - avg_accuracy: 0.9707\n",
      "Epoch 00463: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5402 - op_main_loss: 0.1156 - op_conv_loss: 0.0722 - avg_loss: 0.0872 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9709 - val_loss: 1.0915 - val_op_main_loss: 0.2437 - val_op_conv_loss: 0.3340 - val_avg_loss: 0.2489 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9131 - val_avg_accuracy: 0.9112\n",
      "Epoch 464/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5227 - op_main_loss: 0.1141 - op_conv_loss: 0.0616 - avg_loss: 0.0821 - op_main_accuracy: 0.9646 - op_conv_accuracy: 0.9776 - avg_accuracy: 0.9773\n",
      "Epoch 00464: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5227 - op_main_loss: 0.1141 - op_conv_loss: 0.0616 - avg_loss: 0.0821 - op_main_accuracy: 0.9646 - op_conv_accuracy: 0.9776 - avg_accuracy: 0.9773 - val_loss: 1.2902 - val_op_main_loss: 0.2765 - val_op_conv_loss: 0.4518 - val_avg_loss: 0.2968 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8914\n",
      "Epoch 465/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5283 - op_main_loss: 0.1136 - op_conv_loss: 0.0655 - avg_loss: 0.0840 - op_main_accuracy: 0.9661 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9754\n",
      "Epoch 00465: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5277 - op_main_loss: 0.1134 - op_conv_loss: 0.0653 - avg_loss: 0.0838 - op_main_accuracy: 0.9660 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9754 - val_loss: 1.0771 - val_op_main_loss: 0.2341 - val_op_conv_loss: 0.3349 - val_avg_loss: 0.2429 - val_op_main_accuracy: 0.9112 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9103\n",
      "Epoch 466/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5278 - op_main_loss: 0.1172 - op_conv_loss: 0.0611 - avg_loss: 0.0844 - op_main_accuracy: 0.9611 - op_conv_accuracy: 0.9767 - avg_accuracy: 0.9757\n",
      "Epoch 00466: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5323 - op_main_loss: 0.1180 - op_conv_loss: 0.0636 - avg_loss: 0.0856 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9766 - avg_accuracy: 0.9757 - val_loss: 1.2319 - val_op_main_loss: 0.2527 - val_op_conv_loss: 0.4383 - val_avg_loss: 0.2756 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8990\n",
      "Epoch 467/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5341 - op_main_loss: 0.1211 - op_conv_loss: 0.0623 - avg_loss: 0.0855 - op_main_accuracy: 0.9607 - op_conv_accuracy: 0.9737 - avg_accuracy: 0.9730\n",
      "Epoch 00467: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5338 - op_main_loss: 0.1210 - op_conv_loss: 0.0622 - avg_loss: 0.0855 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9731 - val_loss: 1.1762 - val_op_main_loss: 0.2755 - val_op_conv_loss: 0.3591 - val_avg_loss: 0.2764 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8980 - val_avg_accuracy: 0.8971\n",
      "Epoch 468/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5321 - op_main_loss: 0.1153 - op_conv_loss: 0.0673 - avg_loss: 0.0843 - op_main_accuracy: 0.9644 - op_conv_accuracy: 0.9797 - avg_accuracy: 0.9777\n",
      "Epoch 00468: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5306 - op_main_loss: 0.1146 - op_conv_loss: 0.0669 - avg_loss: 0.0839 - op_main_accuracy: 0.9650 - op_conv_accuracy: 0.9797 - avg_accuracy: 0.9778 - val_loss: 1.2054 - val_op_main_loss: 0.2554 - val_op_conv_loss: 0.4093 - val_avg_loss: 0.2759 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8990\n",
      "Epoch 469/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5514 - op_main_loss: 0.1256 - op_conv_loss: 0.0701 - avg_loss: 0.0912 - op_main_accuracy: 0.9583 - op_conv_accuracy: 0.9732 - avg_accuracy: 0.9730\n",
      "Epoch 00469: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5513 - op_main_loss: 0.1256 - op_conv_loss: 0.0700 - avg_loss: 0.0912 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9731 - val_loss: 1.0898 - val_op_main_loss: 0.2358 - val_op_conv_loss: 0.3431 - val_avg_loss: 0.2463 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.9027\n",
      "Epoch 470/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5290 - op_main_loss: 0.1142 - op_conv_loss: 0.0654 - avg_loss: 0.0845 - op_main_accuracy: 0.9654 - op_conv_accuracy: 0.9748 - avg_accuracy: 0.9763\n",
      "Epoch 00470: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5302 - op_main_loss: 0.1144 - op_conv_loss: 0.0660 - avg_loss: 0.0849 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9764 - val_loss: 1.2322 - val_op_main_loss: 0.2455 - val_op_conv_loss: 0.4462 - val_avg_loss: 0.2756 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8914\n",
      "Epoch 471/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5162 - op_main_loss: 0.1107 - op_conv_loss: 0.0601 - avg_loss: 0.0807 - op_main_accuracy: 0.9685 - op_conv_accuracy: 0.9767 - avg_accuracy: 0.9765\n",
      "Epoch 00471: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5193 - op_main_loss: 0.1117 - op_conv_loss: 0.0612 - avg_loss: 0.0816 - op_main_accuracy: 0.9681 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9761 - val_loss: 1.1043 - val_op_main_loss: 0.2414 - val_op_conv_loss: 0.3479 - val_avg_loss: 0.2511 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 472/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5616 - op_main_loss: 0.1279 - op_conv_loss: 0.0741 - avg_loss: 0.0947 - op_main_accuracy: 0.9573 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9716\n",
      "Epoch 00472: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5608 - op_main_loss: 0.1277 - op_conv_loss: 0.0738 - avg_loss: 0.0945 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9719 - val_loss: 1.1277 - val_op_main_loss: 0.2400 - val_op_conv_loss: 0.3666 - val_avg_loss: 0.2556 - val_op_main_accuracy: 0.9027 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8942\n",
      "Epoch 473/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5269 - op_main_loss: 0.1128 - op_conv_loss: 0.0653 - avg_loss: 0.0833 - op_main_accuracy: 0.9683 - op_conv_accuracy: 0.9746 - avg_accuracy: 0.9787\n",
      "Epoch 00473: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5290 - op_main_loss: 0.1135 - op_conv_loss: 0.0660 - avg_loss: 0.0839 - op_main_accuracy: 0.9676 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9780 - val_loss: 1.0533 - val_op_main_loss: 0.2299 - val_op_conv_loss: 0.3225 - val_avg_loss: 0.2356 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9075\n",
      "Epoch 474/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5235 - op_main_loss: 0.1120 - op_conv_loss: 0.0646 - avg_loss: 0.0820 - op_main_accuracy: 0.9671 - op_conv_accuracy: 0.9743 - avg_accuracy: 0.9760\n",
      "Epoch 00474: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5287 - op_main_loss: 0.1141 - op_conv_loss: 0.0659 - avg_loss: 0.0837 - op_main_accuracy: 0.9660 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9754 - val_loss: 1.1871 - val_op_main_loss: 0.2713 - val_op_conv_loss: 0.3713 - val_avg_loss: 0.2801 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8942\n",
      "Epoch 475/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5162 - op_main_loss: 0.1123 - op_conv_loss: 0.0590 - avg_loss: 0.0802 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9750\n",
      "Epoch 00475: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5162 - op_main_loss: 0.1123 - op_conv_loss: 0.0590 - avg_loss: 0.0802 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9750 - val_loss: 1.1486 - val_op_main_loss: 0.2559 - val_op_conv_loss: 0.3645 - val_avg_loss: 0.2633 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9065\n",
      "Epoch 476/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5375 - op_main_loss: 0.1169 - op_conv_loss: 0.0692 - avg_loss: 0.0868 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9742\n",
      "Epoch 00476: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5373 - op_main_loss: 0.1169 - op_conv_loss: 0.0691 - avg_loss: 0.0868 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9742 - val_loss: 1.2237 - val_op_main_loss: 0.2677 - val_op_conv_loss: 0.4102 - val_avg_loss: 0.2818 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8942\n",
      "Epoch 477/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5322 - op_main_loss: 0.1171 - op_conv_loss: 0.0653 - avg_loss: 0.0853 - op_main_accuracy: 0.9654 - op_conv_accuracy: 0.9734 - avg_accuracy: 0.9753\n",
      "Epoch 00477: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5300 - op_main_loss: 0.1167 - op_conv_loss: 0.0642 - avg_loss: 0.0846 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9759 - val_loss: 1.5204 - val_op_main_loss: 0.3059 - val_op_conv_loss: 0.5984 - val_avg_loss: 0.3516 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8584 - val_avg_accuracy: 0.8650\n",
      "Epoch 478/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5381 - op_main_loss: 0.1168 - op_conv_loss: 0.0698 - avg_loss: 0.0875 - op_main_accuracy: 0.9623 - op_conv_accuracy: 0.9730 - avg_accuracy: 0.9740\n",
      "Epoch 00478: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5374 - op_main_loss: 0.1167 - op_conv_loss: 0.0694 - avg_loss: 0.0873 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9742 - val_loss: 1.1189 - val_op_main_loss: 0.2443 - val_op_conv_loss: 0.3568 - val_avg_loss: 0.2544 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.9027 - val_avg_accuracy: 0.9065\n",
      "Epoch 479/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5363 - op_main_loss: 0.1213 - op_conv_loss: 0.0643 - avg_loss: 0.0869 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9761\n",
      "Epoch 00479: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5363 - op_main_loss: 0.1213 - op_conv_loss: 0.0643 - avg_loss: 0.0869 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9761 - val_loss: 1.0950 - val_op_main_loss: 0.2330 - val_op_conv_loss: 0.3541 - val_avg_loss: 0.2439 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9027\n",
      "Epoch 480/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5336 - op_main_loss: 0.1156 - op_conv_loss: 0.0684 - avg_loss: 0.0859 - op_main_accuracy: 0.9642 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9742\n",
      "Epoch 00480: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5326 - op_main_loss: 0.1153 - op_conv_loss: 0.0680 - avg_loss: 0.0856 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9745 - val_loss: 1.1877 - val_op_main_loss: 0.2559 - val_op_conv_loss: 0.4000 - val_avg_loss: 0.2686 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9027\n",
      "Epoch 481/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5403 - op_main_loss: 0.1193 - op_conv_loss: 0.0700 - avg_loss: 0.0878 - op_main_accuracy: 0.9625 - op_conv_accuracy: 0.9736 - avg_accuracy: 0.9755\n",
      "Epoch 00481: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5381 - op_main_loss: 0.1185 - op_conv_loss: 0.0693 - avg_loss: 0.0871 - op_main_accuracy: 0.9629 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9757 - val_loss: 1.0982 - val_op_main_loss: 0.2376 - val_op_conv_loss: 0.3488 - val_avg_loss: 0.2480 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9037\n",
      "Epoch 482/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5109 - op_main_loss: 0.1106 - op_conv_loss: 0.0572 - avg_loss: 0.0792 - op_main_accuracy: 0.9668 - op_conv_accuracy: 0.9788 - avg_accuracy: 0.9769\n",
      "Epoch 00482: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5112 - op_main_loss: 0.1107 - op_conv_loss: 0.0573 - avg_loss: 0.0793 - op_main_accuracy: 0.9667 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9766 - val_loss: 1.0985 - val_op_main_loss: 0.2394 - val_op_conv_loss: 0.3489 - val_avg_loss: 0.2463 - val_op_main_accuracy: 0.9056 - val_op_conv_accuracy: 0.9065 - val_avg_accuracy: 0.9065\n",
      "Epoch 483/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5365 - op_main_loss: 0.1168 - op_conv_loss: 0.0698 - avg_loss: 0.0859 - op_main_accuracy: 0.9642 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9743\n",
      "Epoch 00483: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5345 - op_main_loss: 0.1164 - op_conv_loss: 0.0689 - avg_loss: 0.0852 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9745 - val_loss: 1.1543 - val_op_main_loss: 0.2609 - val_op_conv_loss: 0.3658 - val_avg_loss: 0.2639 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.9018 - val_avg_accuracy: 0.9046\n",
      "Epoch 484/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5396 - op_main_loss: 0.1196 - op_conv_loss: 0.0695 - avg_loss: 0.0871 - op_main_accuracy: 0.9637 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9743\n",
      "Epoch 00484: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5378 - op_main_loss: 0.1190 - op_conv_loss: 0.0689 - avg_loss: 0.0866 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9745 - val_loss: 1.5580 - val_op_main_loss: 0.3203 - val_op_conv_loss: 0.6173 - val_avg_loss: 0.3565 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8791\n",
      "Epoch 485/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5351 - op_main_loss: 0.1138 - op_conv_loss: 0.0719 - avg_loss: 0.0855 - op_main_accuracy: 0.9653 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9764\n",
      "Epoch 00485: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5351 - op_main_loss: 0.1138 - op_conv_loss: 0.0719 - avg_loss: 0.0855 - op_main_accuracy: 0.9653 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9764 - val_loss: 1.1374 - val_op_main_loss: 0.2388 - val_op_conv_loss: 0.3804 - val_avg_loss: 0.2545 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9037\n",
      "Epoch 486/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5411 - op_main_loss: 0.1162 - op_conv_loss: 0.0735 - avg_loss: 0.0882 - op_main_accuracy: 0.9639 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9728\n",
      "Epoch 00486: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5437 - op_main_loss: 0.1175 - op_conv_loss: 0.0740 - avg_loss: 0.0890 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9728 - val_loss: 1.2624 - val_op_main_loss: 0.2754 - val_op_conv_loss: 0.4302 - val_avg_loss: 0.2939 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8886\n",
      "Epoch 487/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5170 - op_main_loss: 0.1106 - op_conv_loss: 0.0624 - avg_loss: 0.0810 - op_main_accuracy: 0.9697 - op_conv_accuracy: 0.9777 - avg_accuracy: 0.9755\n",
      "Epoch 00487: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5178 - op_main_loss: 0.1116 - op_conv_loss: 0.0620 - avg_loss: 0.0812 - op_main_accuracy: 0.9690 - op_conv_accuracy: 0.9780 - avg_accuracy: 0.9752 - val_loss: 1.0921 - val_op_main_loss: 0.2332 - val_op_conv_loss: 0.3521 - val_avg_loss: 0.2440 - val_op_main_accuracy: 0.9065 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9046\n",
      "Epoch 488/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5526 - op_main_loss: 0.1271 - op_conv_loss: 0.0712 - avg_loss: 0.0915 - op_main_accuracy: 0.9576 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9737\n",
      "Epoch 00488: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5528 - op_main_loss: 0.1273 - op_conv_loss: 0.0712 - avg_loss: 0.0915 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9738 - val_loss: 1.1549 - val_op_main_loss: 0.2476 - val_op_conv_loss: 0.3787 - val_avg_loss: 0.2660 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8876\n",
      "Epoch 489/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5306 - op_main_loss: 0.1164 - op_conv_loss: 0.0663 - avg_loss: 0.0846 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9747\n",
      "Epoch 00489: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5306 - op_main_loss: 0.1164 - op_conv_loss: 0.0663 - avg_loss: 0.0846 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9747 - val_loss: 1.1587 - val_op_main_loss: 0.2550 - val_op_conv_loss: 0.3742 - val_avg_loss: 0.2664 - val_op_main_accuracy: 0.9018 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9027\n",
      "Epoch 490/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5386 - op_main_loss: 0.1173 - op_conv_loss: 0.0714 - avg_loss: 0.0871 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9759\n",
      "Epoch 00490: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5386 - op_main_loss: 0.1173 - op_conv_loss: 0.0714 - avg_loss: 0.0871 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9759 - val_loss: 1.1896 - val_op_main_loss: 0.2596 - val_op_conv_loss: 0.3921 - val_avg_loss: 0.2752 - val_op_main_accuracy: 0.8999 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.8980\n",
      "Epoch 491/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5113 - op_main_loss: 0.1093 - op_conv_loss: 0.0600 - avg_loss: 0.0794 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9780\n",
      "Epoch 00491: val_avg_accuracy did not improve from 0.91313\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5113 - op_main_loss: 0.1093 - op_conv_loss: 0.0600 - avg_loss: 0.0794 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9778 - avg_accuracy: 0.9780 - val_loss: 1.0847 - val_op_main_loss: 0.2396 - val_op_conv_loss: 0.3361 - val_avg_loss: 0.2465 - val_op_main_accuracy: 0.9075 - val_op_conv_accuracy: 0.9056 - val_avg_accuracy: 0.9075\n",
      "Epoch 492/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5232 - op_main_loss: 0.1160 - op_conv_loss: 0.0613 - avg_loss: 0.0832 - op_main_accuracy: 0.9651 - op_conv_accuracy: 0.9797 - avg_accuracy: 0.9775\n",
      "Epoch 00492: val_avg_accuracy improved from 0.91313 to 0.91501, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5265 - op_main_loss: 0.1168 - op_conv_loss: 0.0627 - avg_loss: 0.0842 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9787 - avg_accuracy: 0.9764 - val_loss: 1.0863 - val_op_main_loss: 0.2330 - val_op_conv_loss: 0.3476 - val_avg_loss: 0.2430 - val_op_main_accuracy: 0.9093 - val_op_conv_accuracy: 0.9141 - val_avg_accuracy: 0.9150\n",
      "Epoch 493/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5278 - op_main_loss: 0.1169 - op_conv_loss: 0.0641 - avg_loss: 0.0841 - op_main_accuracy: 0.9654 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9761\n",
      "Epoch 00493: val_avg_accuracy did not improve from 0.91501\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5270 - op_main_loss: 0.1167 - op_conv_loss: 0.0638 - avg_loss: 0.0839 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9771 - avg_accuracy: 0.9761 - val_loss: 1.0481 - val_op_main_loss: 0.2300 - val_op_conv_loss: 0.3191 - val_avg_loss: 0.2363 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.8971 - val_avg_accuracy: 0.9093\n",
      "Epoch 494/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5340 - op_main_loss: 0.1180 - op_conv_loss: 0.0670 - avg_loss: 0.0864 - op_main_accuracy: 0.9647 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9745\n",
      "Epoch 00494: val_avg_accuracy did not improve from 0.91501\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5330 - op_main_loss: 0.1179 - op_conv_loss: 0.0664 - avg_loss: 0.0861 - op_main_accuracy: 0.9648 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9747 - val_loss: 1.1435 - val_op_main_loss: 0.2512 - val_op_conv_loss: 0.3699 - val_avg_loss: 0.2602 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.9056\n",
      "Epoch 495/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5238 - op_main_loss: 0.1115 - op_conv_loss: 0.0666 - avg_loss: 0.0833 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9759\n",
      "Epoch 00495: val_avg_accuracy did not improve from 0.91501\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5238 - op_main_loss: 0.1115 - op_conv_loss: 0.0666 - avg_loss: 0.0833 - op_main_accuracy: 0.9657 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9759 - val_loss: 1.1103 - val_op_main_loss: 0.2421 - val_op_conv_loss: 0.3531 - val_avg_loss: 0.2522 - val_op_main_accuracy: 0.9150 - val_op_conv_accuracy: 0.9103 - val_avg_accuracy: 0.9103\n",
      "Epoch 496/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5234 - op_main_loss: 0.1145 - op_conv_loss: 0.0625 - avg_loss: 0.0835 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9741 - avg_accuracy: 0.9743\n",
      "Epoch 00496: val_avg_accuracy did not improve from 0.91501\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5231 - op_main_loss: 0.1144 - op_conv_loss: 0.0624 - avg_loss: 0.0835 - op_main_accuracy: 0.9638 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9740 - val_loss: 1.1246 - val_op_main_loss: 0.2469 - val_op_conv_loss: 0.3561 - val_avg_loss: 0.2598 - val_op_main_accuracy: 0.9046 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.9018\n",
      "Epoch 497/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5377 - op_main_loss: 0.1181 - op_conv_loss: 0.0703 - avg_loss: 0.0871 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9736 - avg_accuracy: 0.9746\n",
      "Epoch 00497: val_avg_accuracy did not improve from 0.91501\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5427 - op_main_loss: 0.1190 - op_conv_loss: 0.0729 - avg_loss: 0.0885 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9740 - val_loss: 1.2688 - val_op_main_loss: 0.2505 - val_op_conv_loss: 0.4759 - val_avg_loss: 0.2801 - val_op_main_accuracy: 0.9112 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8933\n",
      "Epoch 498/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5257 - op_main_loss: 0.1128 - op_conv_loss: 0.0669 - avg_loss: 0.0839 - op_main_accuracy: 0.9635 - op_conv_accuracy: 0.9744 - avg_accuracy: 0.9751\n",
      "Epoch 00498: val_avg_accuracy did not improve from 0.91501\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5254 - op_main_loss: 0.1127 - op_conv_loss: 0.0668 - avg_loss: 0.0838 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9752 - val_loss: 1.1187 - val_op_main_loss: 0.2466 - val_op_conv_loss: 0.3556 - val_avg_loss: 0.2545 - val_op_main_accuracy: 0.9037 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9037\n",
      "Epoch 499/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5069 - op_main_loss: 0.1108 - op_conv_loss: 0.0556 - avg_loss: 0.0787 - op_main_accuracy: 0.9639 - op_conv_accuracy: 0.9784 - avg_accuracy: 0.9784\n",
      "Epoch 00499: val_avg_accuracy did not improve from 0.91501\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5107 - op_main_loss: 0.1113 - op_conv_loss: 0.0581 - avg_loss: 0.0796 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9785 - avg_accuracy: 0.9785 - val_loss: 1.0629 - val_op_main_loss: 0.2317 - val_op_conv_loss: 0.3305 - val_avg_loss: 0.2391 - val_op_main_accuracy: 0.9131 - val_op_conv_accuracy: 0.9093 - val_avg_accuracy: 0.9112\n",
      "Epoch 500/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4998 - op_main_loss: 0.1076 - op_conv_loss: 0.0543 - avg_loss: 0.0763 - op_main_accuracy: 0.9714 - op_conv_accuracy: 0.9809 - avg_accuracy: 0.9804\n",
      "Epoch 00500: val_avg_accuracy did not improve from 0.91501\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4998 - op_main_loss: 0.1076 - op_conv_loss: 0.0543 - avg_loss: 0.0763 - op_main_accuracy: 0.9714 - op_conv_accuracy: 0.9809 - avg_accuracy: 0.9804 - val_loss: 1.1541 - val_op_main_loss: 0.2568 - val_op_conv_loss: 0.3716 - val_avg_loss: 0.2642 - val_op_main_accuracy: 0.9084 - val_op_conv_accuracy: 0.9008 - val_avg_accuracy: 0.9027\n"
     ]
    }
   ],
   "source": [
    "def nlp_lstm2(w2v):\n",
    "    inputs = Input(shape=(X_train[0].shape[-1],))\n",
    "\n",
    "    embedding_layer = gensim_to_keras_embedding(w2v)\n",
    "    \n",
    "    embedding = embedding_layer(inputs)\n",
    "\n",
    "    lstm1 = LSTM(lstm_units,return_sequences=True, return_state=True, kernel_regularizer=l2(w_decay),recurrent_regularizer=l2(w_decay), dropout=dropout_rate)(embedding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='op_main')(lstm1[1])\n",
    "    \n",
    "\n",
    "    output_td_gap = GlobalAveragePooling1D(data_format='channels_first')(lstm1[0])\n",
    "    \n",
    "    output_td = TimeDistributed(Dense(units=1, activation='sigmoid'))(lstm1[0])\n",
    "    output_td = Flatten()(output_td)\n",
    "    \n",
    "    output_td = Multiply()([output_td_gap, output_td])\n",
    "    \n",
    "    output_td = Activation('relu', name='before_split')(output_td)\n",
    "    \n",
    "    output_td_splits = tf.split(output_td, 10, axis=-1)\n",
    "    \n",
    "    features = concatenate([output_td_splits[0], output_td_splits[1], output_td_splits[-2], output_td_splits[-1]])\n",
    "    \n",
    "    print(features.shape)\n",
    "    \n",
    "    output_td = Reshape((8, 10, 1))(features)\n",
    "    \n",
    "    output_td = Conv2D(2, 8, padding='same', strides=1, activation='relu', kernel_regularizer=l2(w_decay))(output_td)\n",
    "    output_td = BatchNormalization()(output_td)\n",
    "    output_td = Flatten()(output_td)\n",
    "   \n",
    "\n",
    "    output_td = Dense(units=1, activation='sigmoid', name='op_conv')(output_td)\n",
    "    \n",
    "    \n",
    "    \n",
    "    avg = tf.keras.layers.Average(name='avg')([output, output_td])\n",
    "    \n",
    "\n",
    "    model = Model(inputs, [output, output_td, avg])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = nlp_lstm2(w2v_model)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./weight_cp/weight_lstm2.hdf5', save_freq=\"epoch\",  verbose=1, monitor='val_avg_accuracy', save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(0.0001)\n",
    "model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=epochs_to_run, validation_data=(X_val, y_val), callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7013a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['avg_accuracy'])\n",
    "plt.plot(history.history['val_avg_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11c0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./weight_cp/weight_lstm2.hdf5')\n",
    "predictionss = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2963b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF Multi-Supervised LSTM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.89       661\n",
      "           1       0.92      0.86      0.89       662\n",
      "\n",
      "    accuracy                           0.89      1323\n",
      "   macro avg       0.89      0.89      0.89      1323\n",
      "weighted avg       0.89      0.89      0.89      1323\n",
      "\n",
      "0.891156462585034\n"
     ]
    }
   ],
   "source": [
    "predictions = np.where(predictionss[-1] > 0.5, 1, 0)\n",
    "y_pred = []\n",
    "for p in predictions:\n",
    "    y_pred.append(p[0])\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"CLASSIFICATION REPORT OF Multi-Supervised LSTM\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8777138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2 0\n",
      "embedding_1 1\n",
      "lstm_1 2\n",
      "time_distributed 3\n",
      "global_average_pooling1d 4\n",
      "flatten 5\n",
      "multiply 6\n",
      "before_split 7\n",
      "tf_op_layer_split 8\n",
      "concatenate 9\n",
      "reshape 10\n",
      "conv2d 11\n",
      "batch_normalization 12\n",
      "flatten_1 13\n",
      "op_main 14\n",
      "op_conv 15\n",
      "avg 16\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(layer.name, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c7d38a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prr = 71\n",
    "f = Model(model.input, model.layers[7].output)\n",
    "predictions = f.predict(X_test)\n",
    "plt.imshow(predictions[prr].reshape((20, 10)), cmap='viridis', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecf8872e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = Model(model.input, model.layers[10].output)\n",
    "predictions = f.predict(X_test)\n",
    "plt.imshow(predictions[prr], cmap='viridis', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b2d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
